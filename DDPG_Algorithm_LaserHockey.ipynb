{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "308d6939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import laserhockey.laser_hockey_env as lh\n",
    "import gymnasium as gym\n",
    "from importlib import reload\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from copy import deepcopy\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acbaa513",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f73d6669",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Version 1\n",
    "class FCQV(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dims=(256, 256), activation_fc=F.relu):\n",
    "        super(FCQV, self).__init__()\n",
    "\n",
    "        self.activation_fc = activation_fc\n",
    "        self.input_layer = nn.Linear(state_dim + action_dim, hidden_dims[0])\n",
    "        self.bn_input = nn.BatchNorm1d(hidden_dims[0])\n",
    "        \n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.bn_hidden = nn.ModuleList()\n",
    "        \n",
    "        # Add two hidden layers with 128 neurons each\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
    "            bn_layer = nn.BatchNorm1d(hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "            self.bn_hidden.append(bn_layer)\n",
    "        \n",
    "        # Output layer with a single neuron for the Q-value estimate\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        # Combining state and action right at the beginning\n",
    "        x = torch.cat((state, action), dim=1)\n",
    "        x = self.activation_fc(self.bn_input(self.input_layer(x)))\n",
    "        \n",
    "        for hidden_layer, bn_layer in zip(self.hidden_layers, self.bn_hidden):\n",
    "            x = self.activation_fc(bn_layer(hidden_layer(x)))\n",
    "            \n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a139cac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Version 2 \n",
    "class FCQV(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 output_dim, \n",
    "                 hidden_dims=(128,128), \n",
    "                 activation_fc=F.relu):\n",
    "        super(FCQV, self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            in_dim = hidden_dims[i]\n",
    "            if i == 0: \n",
    "                in_dim += output_dim\n",
    "            hidden_layer = nn.Linear(in_dim, hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], 1)\n",
    "\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "        self.device = torch.device(device)\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def _format(self, state, action):\n",
    "        x, u = state, action\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, \n",
    "                             device=self.device, \n",
    "                             dtype=torch.float32)\n",
    "            x = x.unsqueeze(0)\n",
    "        if not isinstance(u, torch.Tensor):\n",
    "            u = torch.tensor(u, \n",
    "                             device=self.device, \n",
    "                             dtype=torch.float32)\n",
    "            u = u.unsqueeze(0)\n",
    "        return x, u\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x, u = self._format(state, action)\n",
    "        x = self.activation_fc(self.input_layer(x))\n",
    "        for i, hidden_layer in enumerate(self.hidden_layers):\n",
    "            if i == 0:\n",
    "                x = torch.cat((x, u), dim=1)\n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        return self.output_layer(x)\n",
    "    \n",
    "    def load(self, experiences):\n",
    "        states, actions, new_states, rewards, is_terminals = experiences\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        actions = torch.from_numpy(actions).float().to(self.device)\n",
    "        new_states = torch.from_numpy(new_states).float().to(self.device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(self.device)\n",
    "        is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n",
    "        return states, actions, new_states, rewards, is_terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "73fea739",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Version 1 Policy\n",
    "class FCDP(nn.Module):\n",
    "    def __init__(self, input_dim, action_bounds, hidden_dims=(256, 256), activation_fc=F.relu, out_activation_fc=F.tanh):\n",
    "        super(FCDP, self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "        self.out_activation_fc = out_activation_fc\n",
    "        self.env_min, self.env_max = action_bounds\n",
    "        \n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.bn_input = nn.BatchNorm1d(hidden_dims[0])\n",
    "        \n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.bn_hidden = nn.ModuleList()\n",
    "        \n",
    "        # Add two hidden layers with 128 neurons each\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
    "            bn_layer = nn.BatchNorm1d(hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "            self.bn_hidden.append(bn_layer)\n",
    "            \n",
    "        # Output layer with the same number of neurons as the action bounds\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], len(self.env_max))\n",
    "        \n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.env_min_tensor = torch.tensor(self.env_min, dtype=torch.float32, device=device)\n",
    "        self.env_max_tensor = torch.tensor(self.env_max, dtype=torch.float32, device=device)\n",
    "        self.rescale_fn = lambda x: (x + 1) / 2 * (self.env_max_tensor - self.env_min_tensor) + self.env_min_tensor\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state  # Assuming state is already formatted\n",
    "        x = self.activation_fc(self.bn_input(self.input_layer(x)))\n",
    "        \n",
    "        for hidden_layer, bn_layer in zip(self.hidden_layers, self.bn_hidden):\n",
    "            x = self.activation_fc(bn_layer(hidden_layer(x)))\n",
    "            \n",
    "        x = self.output_layer(x)\n",
    "        x = self.out_activation_fc(x)\n",
    "        return self.rescale_fn(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a38dc7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Version 2 Policy\n",
    "\n",
    "class FCDP(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim,\n",
    "                 action_bounds,\n",
    "                 hidden_dims=(128,128), \n",
    "                 activation_fc=F.relu,\n",
    "                 out_activation_fc=F.tanh):\n",
    "        super(FCDP, self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "        self.out_activation_fc = out_activation_fc\n",
    "        self.env_min, self.env_max = action_bounds\n",
    "\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], len(self.env_max))\n",
    "\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "        self.device = torch.device(device)\n",
    "        self.to(self.device)\n",
    "        \n",
    "        self.env_min = torch.tensor(self.env_min,\n",
    "                                    device=self.device, \n",
    "                                    dtype=torch.float32)\n",
    "\n",
    "        self.env_max = torch.tensor(self.env_max,\n",
    "                                    device=self.device, \n",
    "                                    dtype=torch.float32)\n",
    "        \n",
    "        self.nn_min = self.out_activation_fc(\n",
    "            torch.Tensor([float('-inf')])).to(self.device)\n",
    "        self.nn_max = self.out_activation_fc(\n",
    "            torch.Tensor([float('inf')])).to(self.device)\n",
    "        self.rescale_fn = lambda x: (x - self.nn_min) * (self.env_max - self.env_min) / \\\n",
    "                                    (self.nn_max - self.nn_min) + self.env_min\n",
    "\n",
    "    def _format(self, state):\n",
    "        x = state\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, \n",
    "                             device=self.device, \n",
    "                             dtype=torch.float32)\n",
    "            x = x.unsqueeze(0)\n",
    "        return x\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self._format(state)\n",
    "        x = self.activation_fc(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        x = self.out_activation_fc(x)\n",
    "        return self.rescale_fn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "42fc7c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent:\n",
    "    def __init__(self, state_dim, action_dim, action_bounds, gamma=0.95, lr_value=0.0002, lr_policy=0.0002, value_max_grad_norm=1.0, policy_max_grad_norm=1.0):\n",
    "        self.gamma = gamma\n",
    "        self.tau = 0.005\n",
    "        self.value_max_grad_norm = value_max_grad_norm\n",
    "        self.policy_max_grad_norm = policy_max_grad_norm\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "       \n",
    "        \n",
    "        # Action bounds\n",
    "        self.env_min, self.env_max = action_bounds\n",
    "        \n",
    "        # Action dimension\n",
    "        self.action_dim = action_dim  \n",
    "        \n",
    "        # Network models\n",
    "        self.online_value_model = FCQV(state_dim, action_dim)\n",
    "        self.target_value_model = FCQV(state_dim, action_dim)\n",
    "        self.online_policy_model = FCDP(state_dim, action_bounds)\n",
    "        self.target_policy_model = FCDP(state_dim, action_bounds)\n",
    "        self.target_value_model.load_state_dict(self.online_value_model.state_dict())\n",
    "        self.target_policy_model.load_state_dict(self.online_policy_model.state_dict())\n",
    "        self.value_optimizer = optim.Adam(self.online_value_model.parameters(), lr=lr_value)\n",
    "        self.policy_optimizer = optim.Adam(self.online_policy_model.parameters(), lr=lr_policy)\n",
    "\n",
    "    #def soft_update(self, online_model, target_model):\n",
    "    #    for target_param, online_param in zip(target_model.parameters(), online_model.parameters()):\n",
    "    #        target_param.data.copy_(self.tau * online_param.data + (1.0 - self.tau) * target_param.data)\n",
    "\n",
    "    def soft_update(self, online_model, target_model):\n",
    "        tau = 0.0001\n",
    "        for target, online in zip(self.target_value_model.parameters(), \n",
    "                                  self.online_value_model.parameters()):\n",
    "            target_ratio = (1.0 - tau) * target.data\n",
    "            online_ratio = tau * online.data\n",
    "            mixed_weights = target_ratio + online_ratio\n",
    "            target.data.copy_(mixed_weights)\n",
    "\n",
    "        for target, online in zip(self.target_policy_model.parameters(), \n",
    "                                  self.online_policy_model.parameters()):\n",
    "            target_ratio = (1.0 - tau) * target.data\n",
    "            online_ratio = tau * online.data\n",
    "            mixed_weights = target_ratio + online_ratio\n",
    "            target.data.copy_(mixed_weights)\n",
    "\n",
    "    def optimize_model(self, experiences):\n",
    "        states, actions, rewards, next_states, is_terminals = experiences\n",
    "        batch_size = len(is_terminals)\n",
    "\n",
    "        \n",
    "\n",
    "        argmax_a_q_sp = self.target_policy_model(next_states)\n",
    "        max_a_q_sp = self.target_value_model(next_states, argmax_a_q_sp)\n",
    "\n",
    "        target_q_sa = rewards + self.gamma * max_a_q_sp * (1 - is_terminals)\n",
    "\n",
    "        #L2 Loss\n",
    "        q_sa = self.online_value_model(states, actions)\n",
    "        td_error = q_sa - target_q_sa.detach()\n",
    "        value_loss = td_error.pow(2).mul(0.5).mean()\n",
    "\n",
    "        #Huber Loss\n",
    "        #loss_function = torch.nn.SmoothL1Loss(reduction='mean')\n",
    "        #value_loss = loss_function(q_sa, target_q_sa.detach())\n",
    "\n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.online_value_model.parameters(), self.value_max_grad_norm)\n",
    "        self.value_optimizer.step()\n",
    "        \n",
    "        argmax_a_q_s = self.online_policy_model(states)\n",
    "        max_a_q_s = self.online_value_model(states, argmax_a_q_s)\n",
    "        policy_loss = -max_a_q_s.mean()\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.online_policy_model.parameters(), self.policy_max_grad_norm)\n",
    "        self.policy_optimizer.step()\n",
    "        # Soft Update der Ziel-Netzwerke\n",
    "        self.soft_update(self.online_value_model, self.target_value_model)\n",
    "        self.soft_update(self.online_policy_model, self.target_policy_model)\n",
    "\n",
    "        return value_loss.item(), policy_loss.item()\n",
    "    \n",
    "\n",
    "    \n",
    "    def he_initialization(self):\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            \n",
    "        # Apply He initialization to the network models\n",
    "        self.online_value_model.apply(init_weights)\n",
    "        self.target_value_model.apply(init_weights)\n",
    "        self.online_policy_model.apply(init_weights)\n",
    "        self.target_policy_model.apply(init_weights)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e54043c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalNoiseStrategy():\n",
    "    def __init__(self, low, high, exploration_noise_ratio=0.1):\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "        self.exploration_noise_ratio = exploration_noise_ratio\n",
    "        self.ratio_noise_injected = 0\n",
    "\n",
    "    def _noise_ratio_update(self):\n",
    "        self.exploration_noise_ratio *= 0.9999\n",
    "        return self.exploration_noise_ratio\n",
    "\n",
    "    def select_action(self, model, state, max_exploration=False):\n",
    "        if max_exploration:\n",
    "            noise_scale = self.high\n",
    "        else:\n",
    "            noise_scale = self.exploration_noise_ratio * self.high\n",
    "\n",
    "        with torch.no_grad():\n",
    "            greedy_action = model(state).cpu().detach().data.numpy().squeeze()\n",
    "\n",
    "        noise = np.random.normal(loc=0, scale=noise_scale, size=len(self.high))\n",
    "        noisy_action = greedy_action + noise\n",
    "\n",
    "        # Keep Player 2 static at all times\n",
    "        noisy_action[3] = 0  # Movement in x-direction for Player 2\n",
    "        noisy_action[4] = 0  # Movement in y-direction for Player 2\n",
    "        noisy_action[5] = 0  # Rotation for Player 2\n",
    "\n",
    "        action = np.clip(noisy_action, self.low, self.high)\n",
    "        \n",
    "        self.ratio_noise_injected = np.mean(abs((greedy_action - action) / (self.high - self.low)))\n",
    "        self._noise_ratio_update()\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61d47e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon Greedy Strategy\n",
    "class EpsilonGreedyStrategy:\n",
    "    def __init__(self, start_epsilon, end_epsilon, decay, action_space):\n",
    "        self.epsilon = start_epsilon\n",
    "        self.end_epsilon = end_epsilon\n",
    "        self.decay = decay\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def select_action(self, model, state_tensor, episode):\n",
    "        if episode < train_start:\n",
    "            # Only allow movements up or down for Player 1\n",
    "            random_action = self.action_space.sample()\n",
    "            random_action[0] = 0  # Zufällige Bewegung in x-Richtung für Spieler 1\n",
    "            random_action[1] = np.random.uniform(-1, 1)  # Zufällige Bewegung in y-Richtung für Spieler 1\n",
    "            random_action[2] = np.random.uniform(-1, 1)  # Zufällige Rotation für Spieler 1\n",
    "            random_action[3] = 0  # Zufällige Bewegung in x-Richtung für Spieler 2\n",
    "            random_action[4] = 0 # Zufällige Bewegung in y-Richtung für Spieler 2\n",
    "            random_action[5] = 0  # Zufällige Rotation für Spieler 2\n",
    "            self.epsilon = 1.0\n",
    "            return random_action\n",
    "        elif np.random.rand() > self.epsilon:  # Exploitation: Mit Wahrscheinlichkeit 1-epsilon\n",
    "            with torch.no_grad():\n",
    "                return model(state_tensor).cpu().data.numpy().squeeze()\n",
    "        else:  # Exploration: Mit Wahrscheinlichkeit epsilon\n",
    "            random_action = self.action_space.sample()\n",
    "            random_action[3] = 0  # Zufällige Bewegung in x-Richtung für Spieler 2\n",
    "            random_action[4] = 0 # Zufällige Bewegung in y-Richtung für Spieler 2\n",
    "            random_action[5] = 0  # Zufällige Rotation für Spieler 2\n",
    "            return random_action\n",
    "\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        if self.epsilon > self.end_epsilon:\n",
    "            self.epsilon *= self.decay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replay Buffer for DDPG\n",
    "import random\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def store(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = args\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return zip(*random.sample(self.memory, batch_size))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cddeca53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexej\\anaconda3\\lib\\site-packages\\gymnasium\\envs\\registration.py:693: UserWarning: \u001b[33mWARN: Overriding environment LaserHockey-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "c:\\Users\\Alexej\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Draw\n",
      "Episode 1, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 1, Reward: -0.5538771603699899, Moving Avg Reward: -0.5538771603699899, Replay Buffer Size: 80\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 1 episodes: 0.00\n",
      "Episode 1, Reward: -0.5538771603699899, Moving Avg Reward: -0.5538771603699899, Replay Buffer Size: 80\n",
      "Current Epsilon: 0.995\n",
      "Episode 2: Draw\n",
      "Episode 2, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 2, Reward: -0.5325112672323703, Moving Avg Reward: -0.54319421380118, Replay Buffer Size: 160\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 2 episodes: 0.00\n",
      "Episode 2, Reward: -0.5325112672323703, Moving Avg Reward: -0.54319421380118, Replay Buffer Size: 160\n",
      "Current Epsilon: 0.995\n",
      "Episode 3: Draw\n",
      "Episode 3, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 3, Reward: -0.8060921754830603, Moving Avg Reward: -0.6308268676951402, Replay Buffer Size: 240\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 3 episodes: 0.00\n",
      "Episode 3, Reward: -0.8060921754830603, Moving Avg Reward: -0.6308268676951402, Replay Buffer Size: 240\n",
      "Current Epsilon: 0.995\n",
      "Episode 4: Draw\n",
      "Episode 4, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 4, Reward: -0.7609692998909589, Moving Avg Reward: -0.6633624757440948, Replay Buffer Size: 320\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 4 episodes: 0.00\n",
      "Episode 4, Reward: -0.7609692998909589, Moving Avg Reward: -0.6633624757440948, Replay Buffer Size: 320\n",
      "Current Epsilon: 0.995\n",
      "Episode 5: Draw\n",
      "Episode 5, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 5, Reward: -0.48403052911799294, Moving Avg Reward: -0.6274960864188743, Replay Buffer Size: 400\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 5 episodes: 0.00\n",
      "Episode 5, Reward: -0.48403052911799294, Moving Avg Reward: -0.6274960864188743, Replay Buffer Size: 400\n",
      "Current Epsilon: 0.995\n",
      "Episode 6: Draw\n",
      "Episode 6, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 6, Reward: -0.6440346087763976, Moving Avg Reward: -0.6302525068117949, Replay Buffer Size: 480\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 6 episodes: 0.00\n",
      "Episode 6, Reward: -0.6440346087763976, Moving Avg Reward: -0.6302525068117949, Replay Buffer Size: 480\n",
      "Current Epsilon: 0.995\n",
      "Episode 7: Draw\n",
      "Episode 7, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 7, Reward: -0.5605978175881207, Moving Avg Reward: -0.6203018369226986, Replay Buffer Size: 560\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 7 episodes: 0.00\n",
      "Episode 7, Reward: -0.5605978175881207, Moving Avg Reward: -0.6203018369226986, Replay Buffer Size: 560\n",
      "Current Epsilon: 0.995\n",
      "Episode 8: Draw\n",
      "Episode 8, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 8, Reward: -0.5649574157683286, Moving Avg Reward: -0.6133837842784023, Replay Buffer Size: 640\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 8 episodes: 0.00\n",
      "Episode 8, Reward: -0.5649574157683286, Moving Avg Reward: -0.6133837842784023, Replay Buffer Size: 640\n",
      "Current Epsilon: 0.995\n",
      "Episode 9: Draw\n",
      "Episode 9, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 9, Reward: -0.6225422535027183, Moving Avg Reward: -0.614401391969993, Replay Buffer Size: 720\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 9 episodes: 0.00\n",
      "Episode 9, Reward: -0.6225422535027183, Moving Avg Reward: -0.614401391969993, Replay Buffer Size: 720\n",
      "Current Epsilon: 0.995\n",
      "Episode 10: Draw\n",
      "Episode 10, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 10, Reward: -0.7892016520987812, Moving Avg Reward: -0.6318814179828719, Replay Buffer Size: 800\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 10 episodes: 0.00\n",
      "Episode 10, Reward: -0.7892016520987812, Moving Avg Reward: -0.6318814179828719, Replay Buffer Size: 800\n",
      "Current Epsilon: 0.995\n",
      "Episode 11: Draw\n",
      "Episode 11, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 11, Reward: -0.4395052646734062, Moving Avg Reward: -0.6143926767729204, Replay Buffer Size: 880\n",
      "Current Epsilon: 0.990025\n",
      "Loss rate over the last 11 episodes: 0.00\n",
      "Episode 11, Reward: -0.4395052646734062, Moving Avg Reward: -0.6143926767729204, Replay Buffer Size: 880\n",
      "Current Epsilon: 0.990025\n",
      "Episode 12: Draw\n",
      "Episode 12, Avg Value Loss: 0.5956476865336299, Avg Policy Loss: -0.3340449085459113\n",
      "Episode 12, Reward: -0.5595668567721869, Moving Avg Reward: -0.609823858439526, Replay Buffer Size: 960\n",
      "Current Epsilon: 0.985074875\n",
      "Loss rate over the last 12 episodes: 0.00\n",
      "Episode 12, Reward: -0.5595668567721869, Moving Avg Reward: -0.609823858439526, Replay Buffer Size: 960\n",
      "Current Epsilon: 0.985074875\n",
      "Episode 13: Draw\n",
      "Episode 13, Avg Value Loss: 0.008241864328738302, Avg Policy Loss: -0.31397578711621466\n",
      "Episode 13, Reward: -0.41885220678799295, Moving Avg Reward: -0.5951337313894081, Replay Buffer Size: 1040\n",
      "Current Epsilon: 0.9801495006250001\n",
      "Loss rate over the last 13 episodes: 0.00\n",
      "Episode 13, Reward: -0.41885220678799295, Moving Avg Reward: -0.5951337313894081, Replay Buffer Size: 1040\n",
      "Current Epsilon: 0.9801495006250001\n",
      "Episode 14: Draw\n",
      "Episode 14, Avg Value Loss: 0.004121088641113602, Avg Policy Loss: -0.3636083029210567\n",
      "Episode 14, Reward: -0.5543276543291294, Moving Avg Reward: -0.5922190115993882, Replay Buffer Size: 1120\n",
      "Current Epsilon: 0.9752487531218751\n",
      "Loss rate over the last 14 episodes: 0.00\n",
      "Episode 14, Reward: -0.5543276543291294, Moving Avg Reward: -0.5922190115993882, Replay Buffer Size: 1120\n",
      "Current Epsilon: 0.9752487531218751\n",
      "Episode 15: Draw\n",
      "Episode 15, Avg Value Loss: 0.004264589017839171, Avg Policy Loss: -0.2624956561718136\n",
      "Episode 15, Reward: -0.47254721463783195, Moving Avg Reward: -0.584240891801951, Replay Buffer Size: 1200\n",
      "Current Epsilon: 0.9703725093562657\n",
      "Loss rate over the last 15 episodes: 0.00\n",
      "Episode 15, Reward: -0.47254721463783195, Moving Avg Reward: -0.584240891801951, Replay Buffer Size: 1200\n",
      "Current Epsilon: 0.9703725093562657\n",
      "Episode 16: Draw\n",
      "Episode 16, Avg Value Loss: 0.003241741275996901, Avg Policy Loss: -0.16094003203324975\n",
      "Episode 16, Reward: -0.5032112122674831, Moving Avg Reward: -0.5791765368310469, Replay Buffer Size: 1280\n",
      "Current Epsilon: 0.9655206468094844\n",
      "Loss rate over the last 16 episodes: 0.00\n",
      "Episode 16, Reward: -0.5032112122674831, Moving Avg Reward: -0.5791765368310469, Replay Buffer Size: 1280\n",
      "Current Epsilon: 0.9655206468094844\n",
      "Episode 17: Draw\n",
      "Episode 17, Avg Value Loss: 0.002156331673904788, Avg Policy Loss: -0.31029449333436787\n",
      "Episode 17, Reward: -0.5955986128718074, Moving Avg Reward: -0.5801425413040328, Replay Buffer Size: 1360\n",
      "Current Epsilon: 0.960693043575437\n",
      "Loss rate over the last 17 episodes: 0.00\n",
      "Episode 17, Reward: -0.5955986128718074, Moving Avg Reward: -0.5801425413040328, Replay Buffer Size: 1360\n",
      "Current Epsilon: 0.960693043575437\n",
      "Episode 18: Draw\n",
      "Episode 18, Avg Value Loss: 0.00223906018800335, Avg Policy Loss: -0.30542886108160017\n",
      "Episode 18, Reward: -0.654101898289367, Moving Avg Reward: -0.5842513944698847, Replay Buffer Size: 1440\n",
      "Current Epsilon: 0.9558895783575597\n",
      "Loss rate over the last 18 episodes: 0.00\n",
      "Episode 18, Reward: -0.654101898289367, Moving Avg Reward: -0.5842513944698847, Replay Buffer Size: 1440\n",
      "Current Epsilon: 0.9558895783575597\n",
      "Episode 19: Draw\n",
      "Episode 19, Avg Value Loss: 0.0027914577032788655, Avg Policy Loss: -0.27620011046528814\n",
      "Episode 19, Reward: -0.8289539929754072, Moving Avg Reward: -0.5971304786017543, Replay Buffer Size: 1520\n",
      "Current Epsilon: 0.9511101304657719\n",
      "Loss rate over the last 19 episodes: 0.00\n",
      "Episode 19, Reward: -0.8289539929754072, Moving Avg Reward: -0.5971304786017543, Replay Buffer Size: 1520\n",
      "Current Epsilon: 0.9511101304657719\n",
      "Episode 20: Draw\n",
      "Episode 20, Avg Value Loss: 0.0021126123669091613, Avg Policy Loss: -0.2865350440144539\n",
      "Episode 20, Reward: -0.5017402267263242, Moving Avg Reward: -0.5923609660079828, Replay Buffer Size: 1600\n",
      "Current Epsilon: 0.946354579813443\n",
      "Loss rate over the last 20 episodes: 0.00\n",
      "Episode 20, Reward: -0.5017402267263242, Moving Avg Reward: -0.5923609660079828, Replay Buffer Size: 1600\n",
      "Current Epsilon: 0.946354579813443\n",
      "Episode 21: Draw\n",
      "Episode 21, Avg Value Loss: 0.002754624003136996, Avg Policy Loss: -0.30780527740716934\n",
      "Episode 21, Reward: -0.43193610130359417, Moving Avg Reward: -0.5847216867363452, Replay Buffer Size: 1680\n",
      "Current Epsilon: 0.9416228069143757\n",
      "Loss rate over the last 21 episodes: 0.00\n",
      "Episode 21, Reward: -0.43193610130359417, Moving Avg Reward: -0.5847216867363452, Replay Buffer Size: 1680\n",
      "Current Epsilon: 0.9416228069143757\n",
      "Episode 22: Draw\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-679f6e369d15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_terminals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_np\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx_np\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstates_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_terminals_np\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m             \u001b[0mvalue_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_terminals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m             \u001b[0mepisode_value_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[0mepisode_policy_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-43-1f30fa3e3787>\u001b[0m in \u001b[0;36moptimize_model\u001b[1;34m(self, experiences)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;31m# Soft Update der Ziel-Netzwerke\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoft_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monline_value_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_value_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoft_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monline_policy_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_policy_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-43-1f30fa3e3787>\u001b[0m in \u001b[0;36msoft_update\u001b[1;34m(self, online_model, target_model)\u001b[0m\n\u001b[0;32m     33\u001b[0m         for target, online in zip(self.target_value_model.parameters(), \n\u001b[0;32m     34\u001b[0m                                   self.online_value_model.parameters()):\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0mtarget_ratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtau\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m             \u001b[0monline_ratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtau\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0monline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mmixed_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_ratio\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0monline_ratio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize DDPG Agent and Environment\n",
    "np.set_printoptions(suppress=True)\n",
    "reload(lh)\n",
    "\n",
    "# Set the environment mode to TRAIN DEFENSE\n",
    "env = lh.LaserHockeyEnv()\n",
    "env.reset(mode=lh.LaserHockeyEnv.TRAIN_SHOOTING)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_bounds = (env.action_space.low, env.action_space.high)\n",
    "#action_strategy = NormalNoiseDecayStrategy(action_bounds[0], action_bounds[1], initial_noise_ratio=1.2)\n",
    "epsilon_strategy = EpsilonGreedyStrategy(1.0, 0.01, 0.995, env.action_space)\n",
    "\n",
    "# Initialize the Replay Buffer and the DDPG Agent\n",
    "replay_buffer = ReplayBuffer(100000)\n",
    "agent = DDPGAgent(state_dim=state_dim, action_dim=action_dim, action_bounds=action_bounds)\n",
    "\n",
    "# Use the method to apply He initialization\n",
    "agent.he_initialization()\n",
    "\n",
    "num_episodes = 10000\n",
    "batch_size = 128\n",
    "train_start = 10  \n",
    "update_frequency = 1  # Update the agent every 1 steps\n",
    "\n",
    "losses = []\n",
    "all_rewards = []  # Store all episode rewards for moving average calculation\n",
    "\n",
    "wins =[]\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs = env.reset()\n",
    "    obs_agent1 = env._get_obs()\n",
    "    episode_reward = 0\n",
    "    episode_value_losses = []\n",
    "    episode_policy_losses = []\n",
    "\n",
    "    \n",
    "\n",
    "    # Determine the result for the current episode\n",
    "    last_80_results = wins[-80:]  # Collect results of the last 80 steps\n",
    "    if 1 in last_80_results:\n",
    "        episode_result = \"Won\"\n",
    "    elif -1 in last_80_results:\n",
    "        episode_result = \"Lost\"\n",
    "    else:\n",
    "        episode_result = \"Draw\"\n",
    "\n",
    "    print(f\"Episode {episode + 1}: {episode_result}\")\n",
    "\n",
    "    for step in range(80):\n",
    "        env.render()\n",
    "        state_tensor = torch.FloatTensor(obs_agent1).unsqueeze(0)\n",
    "\n",
    "        # Set the policy model to evaluation mode\n",
    "        agent.online_policy_model.eval()\n",
    "        action_player1 = epsilon_strategy.select_action(agent.online_policy_model, state_tensor, episode)\n",
    "\n",
    "        \n",
    "\n",
    "        agent.online_policy_model.train()\n",
    "        \n",
    "        next_obs, reward, done, _, info = env.step(action_player1)\n",
    "        reward += info['winner']\n",
    "        reward += info['reward_closeness_to_puck']\n",
    "        reward += info['reward_touch_puck']\n",
    "        reward += info['reward_puck_direction']\n",
    "\n",
    "        episode_reward += reward\n",
    "        #Count wins (successes in defense)\n",
    "        wins.append(info['winner'])\n",
    "\n",
    "        obs_agent1 = env._get_obs()\n",
    "        experience = (obs_agent1, action_player1, reward, next_obs, done)\n",
    "        replay_buffer.store(*experience)\n",
    "\n",
    "        # DDPG Training\n",
    "        if episode > train_start and len(replay_buffer) > batch_size and step % update_frequency == 0:\n",
    "            experiences = replay_buffer.sample(batch_size)\n",
    "\n",
    "            \n",
    "            states_np, actions_np, rewards_np, next_states_np, is_terminals_np = [np.array(x) for x in experiences]\n",
    "            states, actions, rewards, next_states, is_terminals = [torch.FloatTensor(x_np).to(device) for x_np in [states_np, actions_np, rewards_np, next_states_np, is_terminals_np]]\n",
    "\n",
    "            value_loss, policy_loss = agent.optimize_model((states, actions, rewards, next_states, is_terminals))\n",
    "            episode_value_losses.append(value_loss)\n",
    "            episode_policy_losses.append(policy_loss)\n",
    "\n",
    "        if step == 79:  \n",
    "            done = True\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "\n",
    "    epsilon_strategy.decay_epsilon()\n",
    "    all_rewards.append(episode_reward)\n",
    "    moving_avg_reward = np.mean(all_rewards[-100:])  # Calculate moving average of the last 100 episode rewards\n",
    "    avg_value_loss = sum(episode_value_losses) / len(episode_value_losses) if episode_value_losses else 0\n",
    "    avg_policy_loss = sum(episode_policy_losses) / len(episode_policy_losses) if episode_policy_losses else 0\n",
    "\n",
    "    print(f\"Episode {episode + 1}, Avg Value Loss: {avg_value_loss}, Avg Policy Loss: {avg_policy_loss}\")\n",
    "    \n",
    "\n",
    "    print(f\"Episode {episode + 1}, Reward: {episode_reward}, Moving Avg Reward: {moving_avg_reward}, Replay Buffer Size: {len(replay_buffer)}\")\n",
    "    print(f\"Current Epsilon: {epsilon_strategy.epsilon}\")\n",
    "\n",
    "    # Calculate loss rate for the last 150 episodes (or less if episode number is < 150)\n",
    "    recent_games = wins[-150 * 80:] if episode >= 150 else wins  # Look back at results of the last 150 episodes (each episode has 80 steps)\n",
    "    recent_losses = [1 for i in range(0, len(recent_games), 80) if -1 in recent_games[i:i+80]].count(1)  # Count how many episodes in the recent games have a loss\n",
    "    loss_rate = recent_losses / (len(recent_games) / 80)  # Calculate loss rate\n",
    "    print(f\"Loss rate over the last {int(len(recent_games)/80)} episodes: {loss_rate:.2f}\")\n",
    "\n",
    "    print(f\"Episode {episode + 1}, Reward: {episode_reward}, Moving Avg Reward: {moving_avg_reward}, Replay Buffer Size: {len(replay_buffer)}\")\n",
    "    print(f\"Current Epsilon: {epsilon_strategy.epsilon}\")\n",
    "\n",
    "# Save the agent's model after training\n",
    "torch.save(agent.online_policy_model.state_dict(), 'online_policy_model_checkpoint.pth')\n",
    "torch.save(agent.online_value_model.state_dict(), 'online_value_model_checkpoint.pth')\n",
    "torch.save(agent.target_policy_model.state_dict(), 'target_policy_model_checkpoint.pth')\n",
    "torch.save(agent.target_value_model.state_dict(), 'target_value_model_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8faa5788",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hindsight Experience Replay Buffer class\n",
    "class HERBuffer:\n",
    "    def __init__(self, buffer_size, goal_selection_strategy, reward_function):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.goal_selection_strategy = goal_selection_strategy\n",
    "        self.reward_function = reward_function\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def store(self, *args):\n",
    "        if len(self.memory) < self.buffer_size:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = args\n",
    "        self.position = (self.position + 1) % self.buffer_size\n",
    "\n",
    "    def store_episode(self, episode):\n",
    "        goal = self.goal_selection_strategy(episode)\n",
    "        \n",
    "        for state, action, _, next_state, done in episode:\n",
    "            reward = self.reward_function(state, action, goal)\n",
    "            self.store(state, action, reward, next_state, goal)\n",
    "            \n",
    "            # HER storage\n",
    "            additional_goals = self.goal_selection_strategy(episode)\n",
    "            for g in additional_goals:\n",
    "                her_reward = self.reward_function(state, action, g)\n",
    "                self.store(state, action, her_reward, next_state, g)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return zip(*random.sample(self.memory, batch_size))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3af3fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 1, Reward: -0.15751856180049464, Moving Avg Reward: -0.15751856180049464, HER Buffer Size: 160\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 2, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 2, Reward: -11.39032215801075, Moving Avg Reward: -5.773920359905623, HER Buffer Size: 242\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 3, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 3, Reward: -11.356689256428453, Moving Avg Reward: -7.634843325413233, HER Buffer Size: 324\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 4, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 4, Reward: -0.39075220754125634, Moving Avg Reward: -5.82382054594524, HER Buffer Size: 484\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 5, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 5, Reward: -11.285190693622859, Moving Avg Reward: -6.916094575480765, HER Buffer Size: 570\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 6, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 6, Reward: -10.292953799428698, Moving Avg Reward: -7.4789044461387535, HER Buffer Size: 672\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 7, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 7, Reward: 0.6912611946703459, Moving Avg Reward: -6.311737926023168, HER Buffer Size: 832\n",
      "Current Epsilon: 0.995\n",
      "Episode 8, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 8, Reward: 0.46120874453906757, Moving Avg Reward: -5.465119592202887, HER Buffer Size: 992\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 9, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 9, Reward: 0.6644054377837054, Moving Avg Reward: -4.7840612555377104, HER Buffer Size: 1152\n",
      "Current Epsilon: 0.995\n",
      "Episode 10, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 10, Reward: 0.3852564273000347, Moving Avg Reward: -4.267129487253936, HER Buffer Size: 1312\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 11, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 11, Reward: -11.252624118521526, Moving Avg Reward: -4.902174453732807, HER Buffer Size: 1400\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 12, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 12, Reward: -11.237862088601334, Moving Avg Reward: -5.430148423305185, HER Buffer Size: 1490\n",
      "Current Epsilon: 0.995\n",
      "Episode 13, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 13, Reward: 0.5283456575003251, Moving Avg Reward: -4.971802724781684, HER Buffer Size: 1650\n",
      "Current Epsilon: 0.995\n",
      "Episode 14, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 14, Reward: 0.4207277925917294, Moving Avg Reward: -4.586621973540725, HER Buffer Size: 1810\n",
      "Current Epsilon: 0.995\n",
      "Episode 15, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 15, Reward: 1.5894108305517145, Moving Avg Reward: -4.174886453267897, HER Buffer Size: 1970\n",
      "Current Epsilon: 0.995\n",
      "Episode 16, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 16, Reward: 0.6894404187394345, Moving Avg Reward: -3.870866023767438, HER Buffer Size: 2130\n",
      "Current Epsilon: 0.995\n",
      "Episode 17, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 17, Reward: 0.7099218725614009, Moving Avg Reward: -3.601407912218683, HER Buffer Size: 2290\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 18, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 18, Reward: 11.034519161224365, Moving Avg Reward: -2.788300852582958, HER Buffer Size: 2360\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 19, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 19, Reward: 0.7342169728924495, Moving Avg Reward: -2.6029051775579366, HER Buffer Size: 2520\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 20, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 20, Reward: 0.6600582010077252, Moving Avg Reward: -2.4397570086296536, HER Buffer Size: 2680\n",
      "Current Epsilon: 0.995\n",
      "Episode 21, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 21, Reward: 0.6408463347596872, Moving Avg Reward: -2.2930616113253994, HER Buffer Size: 2840\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 22, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 22, Reward: -11.33456808287813, Moving Avg Reward: -2.7040391782141597, HER Buffer Size: 2922\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 23, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 23, Reward: 0.6060463547694803, Moving Avg Reward: -2.560122415910523, HER Buffer Size: 3082\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 24, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 24, Reward: 11.01233970489502, Moving Avg Reward: -1.9946031608769592, HER Buffer Size: 3150\n",
      "Current Epsilon: 0.995\n",
      "Episode 25, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 25, Reward: 1.593571504599881, Moving Avg Reward: -1.8510761742578856, HER Buffer Size: 3310\n",
      "Current Epsilon: 0.995\n",
      "Episode 26, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 26, Reward: -0.7066940035587411, Moving Avg Reward: -1.8070614753848415, HER Buffer Size: 3470\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 27, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 27, Reward: 0.6449565036275393, Moving Avg Reward: -1.7162459946806792, HER Buffer Size: 3630\n",
      "Current Epsilon: 0.995\n",
      "Episode 28, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 28, Reward: 0.7521855107179937, Moving Avg Reward: -1.6280877266307265, HER Buffer Size: 3790\n",
      "Current Epsilon: 0.995\n",
      "Episode 29, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 29, Reward: 0.6870512928441631, Moving Avg Reward: -1.548255346648834, HER Buffer Size: 3950\n",
      "Current Epsilon: 0.995\n",
      "Episode 30, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 30, Reward: -0.5557715344132576, Moving Avg Reward: -1.515172552907648, HER Buffer Size: 4110\n",
      "Current Epsilon: 0.995\n",
      "Episode 31, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 31, Reward: 0.5658418517913121, Moving Avg Reward: -1.4480430559818753, HER Buffer Size: 4270\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 32, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 32, Reward: -11.299011394151176, Moving Avg Reward: -1.7558858165496656, HER Buffer Size: 4352\n",
      "Current Epsilon: 0.995\n",
      "Episode 33, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 33, Reward: -1.495861298478984, Moving Avg Reward: -1.7480062856990388, HER Buffer Size: 4512\n",
      "Current Epsilon: 0.995\n",
      "Episode 34, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 34, Reward: 0.6570531361363625, Moving Avg Reward: -1.6772692438803507, HER Buffer Size: 4672\n",
      "Current Epsilon: 0.995\n",
      "Episode 35, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 35, Reward: -1.7311131553928245, Moving Avg Reward: -1.6788076413521356, HER Buffer Size: 4832\n",
      "Current Epsilon: 0.995\n",
      "Episode 36, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 36, Reward: 0.6417840196470367, Moving Avg Reward: -1.6143467618799365, HER Buffer Size: 4992\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 37, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 37, Reward: -10.232039813828763, Moving Avg Reward: -1.8472573849055807, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 38, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 38, Reward: -11.238006432861809, Moving Avg Reward: -2.094382359851797, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 39, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 39, Reward: -11.244700193664551, Moving Avg Reward: -2.3290058940521243, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 40, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 40, Reward: 0.6638729433757677, Moving Avg Reward: -2.2541839231164267, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 41, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 41, Reward: 0.7053696471509412, Moving Avg Reward: -2.1819996896952714, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 42, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 42, Reward: -11.277381881557519, Moving Avg Reward: -2.398556408549134, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 43, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 43, Reward: 0.7089264912754347, Moving Avg Reward: -2.3262893643671676, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 44, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 44, Reward: 0.44478126428003556, Moving Avg Reward: -2.2633104864433675, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 45, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 45, Reward: -11.297601283889724, Moving Avg Reward: -2.4640725041643976, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 46, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 46, Reward: 0.6842381276649663, Moving Avg Reward: -2.3956309686898463, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 47, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 47, Reward: 0.5234312123427662, Moving Avg Reward: -2.3335232627104285, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 48, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 48, Reward: -11.24931408343971, Moving Avg Reward: -2.5192689048089556, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 49, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 49, Reward: -11.273728390727882, Moving Avg Reward: -2.6979313432970966, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 50, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 50, Reward: -0.6523159893532945, Moving Avg Reward: -2.6570190362182204, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 51, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 51, Reward: 0.6621405438657882, Moving Avg Reward: -2.591937475824417, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 52, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 52, Reward: 11.00698235731125, Moving Avg Reward: -2.330419786725654, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 53, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 53, Reward: -11.27514037159286, Moving Avg Reward: -2.499188099647677, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 54, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 54, Reward: -11.303580673278903, Moving Avg Reward: -2.6622324065667735, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 55, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 55, Reward: -0.6044440835411136, Moving Avg Reward: -2.624818073420853, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 56, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 56, Reward: -11.265949017009675, Moving Avg Reward: -2.7791239831277954, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 57, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 57, Reward: -11.274087769439767, Moving Avg Reward: -2.9281584355192334, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 58, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 58, Reward: -11.225328158166548, Moving Avg Reward: -3.0712130859097044, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 59, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 59, Reward: -11.251302529268669, Moving Avg Reward: -3.2098586696954494, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 60, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 60, Reward: -11.24499027942847, Moving Avg Reward: -3.3437775298576664, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 61, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 61, Reward: -11.351135231730186, Moving Avg Reward: -3.4750456889047565, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 62, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 62, Reward: 0.6774217454921178, Moving Avg Reward: -3.408070407704807, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 63, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 63, Reward: -0.015303411290553372, Moving Avg Reward: -3.354216963317279, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 64, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 64, Reward: -9.462750164368174, Moving Avg Reward: -3.4496627945836997, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 65, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 65, Reward: -11.261599979148828, Moving Avg Reward: -3.5698464435770094, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 66, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 66, Reward: 0.5677984093216634, Moving Avg Reward: -3.5071548548967266, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 67, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 67, Reward: 11.00775327091217, Moving Avg Reward: -3.290514435108534, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 68, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 68, Reward: 0.39851499176146893, Moving Avg Reward: -3.2362640023604463, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 69, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 69, Reward: -11.237394560657838, Moving Avg Reward: -3.352222416248814, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 70, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 70, Reward: -0.5620282302011789, Moving Avg Reward: -3.3123624993052765, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 71, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 71, Reward: -11.31762198854059, Moving Avg Reward: -3.425112632956478, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 72, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 72, Reward: -11.298229421509875, Moving Avg Reward: -3.5344614772419423, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 73, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 73, Reward: 11.016641049575806, Moving Avg Reward: -3.3351313056416987, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 74, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 74, Reward: -1.6993898824606282, Moving Avg Reward: -3.3130266918149274, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 75, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 75, Reward: -11.313705238755354, Moving Avg Reward: -3.4197024057741334, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 76, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 76, Reward: -11.30662962368291, Moving Avg Reward: -3.5234777639045123, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 77, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 77, Reward: 0.653151214855576, Moving Avg Reward: -3.4692358291154197, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 78, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 78, Reward: -0.5000821218273637, Moving Avg Reward: -3.43116975594506, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 79, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 79, Reward: -11.25568244345033, Moving Avg Reward: -3.530214220343861, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 80, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 80, Reward: -11.270146962874803, Moving Avg Reward: -3.626963379625498, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 81, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 81, Reward: -11.23817404283404, Moving Avg Reward: -3.720928943368813, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 82, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 82, Reward: -11.277872508722526, Moving Avg Reward: -3.813086791726785, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 83, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 83, Reward: -11.281859251145159, Moving Avg Reward: -3.903072002081223, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 84, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 84, Reward: 2.527988981311859, Moving Avg Reward: -3.8265117522789245, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 85, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 85, Reward: -11.26170075717138, Moving Avg Reward: -3.913984564101188, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 86, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 86, Reward: 0.49227358312387104, Moving Avg Reward: -3.8627490042497343, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 87, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 87, Reward: 0.5663305709090573, Moving Avg Reward: -3.8118400436157254, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 88, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 88, Reward: -10.365419731119731, Moving Avg Reward: -3.886312540064635, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 89, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 89, Reward: -11.220751067473085, Moving Avg Reward: -3.9687219617209095, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 90, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 90, Reward: 0.5651653155347965, Moving Avg Reward: -3.9183454364180688, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 91, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 91, Reward: 0.3006390737533463, Moving Avg Reward: -3.8719829692733274, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 92, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 92, Reward: -1.8195893773884264, Moving Avg Reward: -3.8496743432745784, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 93, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 93, Reward: -11.280205164129507, Moving Avg Reward: -3.9295725241439863, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 94, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 94, Reward: -11.249963459218776, Moving Avg Reward: -4.007449023453292, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 95, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 95, Reward: -11.287932718017373, Moving Avg Reward: -4.084085693922388, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 96, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 96, Reward: 0.4732955055222372, Moving Avg Reward: -4.03661297309484, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 97, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 97, Reward: 0.5190747433492936, Moving Avg Reward: -3.9896471203479935, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 98, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 98, Reward: -11.248004714947278, Moving Avg Reward: -4.063711993762272, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 99, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 99, Reward: 0.7418490037414203, Moving Avg Reward: -4.015170973585467, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 100, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 100, Reward: -11.280869582193874, Moving Avg Reward: -4.087827959671551, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 101, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 101, Reward: -11.387285341877194, Moving Avg Reward: -4.2001256274723175, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 102, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 102, Reward: 0.5585512468822921, Moving Avg Reward: -4.080636893423387, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 103, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 103, Reward: -0.004766636533290146, Moving Avg Reward: -3.967117667224436, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 104, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 104, Reward: 0.5679634699675703, Moving Avg Reward: -3.9575305104493474, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 105, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 105, Reward: -10.269796887599666, Moving Avg Reward: -3.9473765723891154, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 106, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 106, Reward: 0.5637327186239348, Moving Avg Reward: -3.838809707208589, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 107, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 107, Reward: -11.268545216852573, Moving Avg Reward: -3.9584077713238184, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 108, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 108, Reward: -11.284645643854487, Moving Avg Reward: -4.0758663152077546, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 109, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 109, Reward: 0.6734986472493917, Moving Avg Reward: -4.075775383113098, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 110, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 110, Reward: 0.730039975660339, Moving Avg Reward: -4.072327547629493, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 111, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 111, Reward: 0.013936970365047454, Moving Avg Reward: -3.9596619367406287, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 112, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 112, Reward: 0.6364252121729643, Moving Avg Reward: -3.840919063732885, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 113, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 113, Reward: 0.6705236293688972, Moving Avg Reward: -3.8394972840142, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 114, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 114, Reward: -10.241896302221573, Moving Avg Reward: -3.9461235249623328, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 115, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 115, Reward: 0.5952495215062852, Moving Avg Reward: -3.9560651380527867, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 116, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 116, Reward: 0.36262795695277156, Moving Avg Reward: -3.9593332626706537, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 117, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 117, Reward: 0.4365936726275559, Moving Avg Reward: -3.9620665446699923, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 118, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 118, Reward: -0.42665188481692684, Moving Avg Reward: -4.076678255130405, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 119, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 119, Reward: -0.684066607003436, Moving Avg Reward: -4.090861090929363, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 120, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 120, Reward: -11.301885209978325, Moving Avg Reward: -4.210480525039225, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 121, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 121, Reward: -11.267465847425608, Moving Avg Reward: -4.329563646861078, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 122, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 122, Reward: -1.5472179775212649, Moving Avg Reward: -4.231690145807509, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 123, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 123, Reward: -11.259128058649985, Moving Avg Reward: -4.350341889941704, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 124, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 124, Reward: 0.7511213220274482, Moving Avg Reward: -4.452954073770379, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 125, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 125, Reward: -11.284047093812466, Moving Avg Reward: -4.581730259754503, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 126, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 126, Reward: 0.5523536891181859, Moving Avg Reward: -4.569139782827734, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 127, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 127, Reward: -11.289409443167054, Moving Avg Reward: -4.68848344229568, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 128, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 128, Reward: -11.243699361862006, Moving Avg Reward: -4.80844229102148, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Episode 129, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 129, Reward: 0.4693381658349366, Moving Avg Reward: -4.810619422291572, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 130, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 130, Reward: 0.6690094962857775, Moving Avg Reward: -4.798371611984582, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 131, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 131, Reward: 0.5421985866422573, Moving Avg Reward: -4.798608044636072, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 132, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 132, Reward: -11.249914758194578, Moving Avg Reward: -4.798117078276506, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 133, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 133, Reward: 0.5998612378266065, Moving Avg Reward: -4.77715985291345, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 134, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 134, Reward: 0.3620408263285467, Moving Avg Reward: -4.780109976011528, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 135, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 135, Reward: 0.727108966604255, Moving Avg Reward: -4.755527754791557, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 136, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 136, Reward: 0.5996289771080945, Moving Avg Reward: -4.755949305216947, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 137, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 137, Reward: 0.6152582650842401, Moving Avg Reward: -4.647476324427817, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 138, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 138, Reward: -11.256881514088878, Moving Avg Reward: -4.647665075240088, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 139, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 139, Reward: -1.9561997829270539, Moving Avg Reward: -4.554780071132713, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 140, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 140, Reward: 0.4780194948237473, Moving Avg Reward: -4.556638605618233, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 141, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 141, Reward: -11.282979128780157, Moving Avg Reward: -4.676522093377544, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 142, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 142, Reward: 0.35842556224804606, Moving Avg Reward: -4.560164018939488, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 143, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 143, Reward: -11.273969977098002, Moving Avg Reward: -4.6799929836232215, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 144, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 144, Reward: -0.005960120494849981, Moving Avg Reward: -4.684500397470971, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 145, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 145, Reward: 0.006362853050231936, Moving Avg Reward: -4.571460756101572, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 146, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 146, Reward: 0.6984451664765818, Moving Avg Reward: -4.571318685713455, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 147, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 147, Reward: -11.235168829619399, Moving Avg Reward: -4.688904686133077, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-26c027611f7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mstate_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs_agent1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monline_policy_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0maction_player1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepsilon_strategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monline_policy_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#DDP Agent\n",
    "import random\n",
    "np.set_printoptions(suppress=True)\n",
    "reload(lh)\n",
    "\n",
    "# Set the environment mode to TRAIN DEFENSE\n",
    "env = lh.LaserHockeyEnv()\n",
    "env.reset(mode=lh.LaserHockeyEnv.TRAIN_DEFENSE)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_bounds = (env.action_space.low, env.action_space.high)\n",
    "epsilon_strategy = EpsilonGreedyStrategy(1.0, 0.01, 0.995, env.action_space)\n",
    "\n",
    "# Goal selection strategy for HERBuffer\n",
    "def sample_goals(episode):\n",
    "    goals = []\n",
    "\n",
    "    for experience in episode:\n",
    "        _, _, reward, next_state, _ = experience\n",
    "        if abs(reward) == 0.2:\n",
    "            goals.append(next_state)\n",
    "\n",
    "    return goals\n",
    "\n",
    "def compute_reward(state, action, goal):\n",
    "    _, _, _, _, info = env.step(action)\n",
    "    \n",
    "    if (info.get('reward_closeness_to_puck', 0) > 0) or \\\n",
    "       (info.get('reward_touch_puck', 0) > 0) or \\\n",
    "       (info.get('reward_puck_direction', 0) > 0):\n",
    "        return 0.2\n",
    "    elif (info.get('reward_closeness_to_puck', 0) < 0) or \\\n",
    "         (info.get('reward_touch_puck', 0) < 0) or \\\n",
    "         (info.get('reward_puck_direction', 0) < 0):\n",
    "        return -0.2\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "her_buffer = HERBuffer(buffer_size=5000, goal_selection_strategy=sample_goals, reward_function=compute_reward)\n",
    "\n",
    "agent = DDPGAgent(state_dim=state_dim, action_dim=action_dim, action_bounds=action_bounds)\n",
    "agent.he_initialization()\n",
    "\n",
    "num_episodes = 10000\n",
    "batch_size = 256\n",
    "train_start = 1000 \n",
    "update_frequency = 1\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "wins = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs = env.reset()\n",
    "    obs_agent1 = env._get_obs()\n",
    "    episode_reward = 0\n",
    "    episode_value_losses = []\n",
    "    episode_policy_losses = []\n",
    "\n",
    "   \n",
    "\n",
    "    episode_trajectory = []\n",
    "\n",
    "    for step in range(80):\n",
    "        state_tensor = torch.FloatTensor(obs_agent1).unsqueeze(0)\n",
    "        agent.online_policy_model.eval()\n",
    "        action_player1 = epsilon_strategy.select_action(agent.online_policy_model, state_tensor, episode)\n",
    "\n",
    "        \n",
    "        agent.online_policy_model.train()\n",
    "\n",
    "        next_obs, reward, done, _, info = env.step(action_player1)\n",
    "        reward += info['winner']\n",
    "        reward += info['reward_closeness_to_puck']\n",
    "        reward += info['reward_touch_puck']\n",
    "        reward += info['reward_puck_direction']\n",
    "        episode_reward += reward\n",
    "\n",
    "        #print(f\"info: {info}\")\n",
    "\n",
    "        obs_agent1 = env._get_obs()\n",
    "        experience = (obs_agent1, action_player1, reward, next_obs, done)\n",
    "        #Store trajectories for HER\n",
    "        episode_trajectory.append(experience)\n",
    "        her_buffer.store(*experience)\n",
    "        \n",
    "\n",
    "        # DDPG Training\n",
    "\n",
    "        if episode > train_start and len(her_buffer) > batch_size:\n",
    "\n",
    "            experiences = her_buffer.sample(batch_size)\n",
    "            states, actions, rewards, next_states, is_terminals = [torch.FloatTensor(x_np).to(device) for x_np in [states_np, actions_np, rewards_np, next_states_np, is_terminals_np]]\n",
    "\n",
    "            \n",
    "            value_loss, policy_loss = agent.optimize_model((states, actions, rewards, next_states, is_terminals))\n",
    "            episode_value_losses.append(value_loss)\n",
    "            episode_policy_losses.append(policy_loss)\n",
    "\n",
    "        if step == 79: \n",
    "            done = True\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    \n",
    "    #Store new trajectories in HER\n",
    "    her_buffer.store_episode(episode_trajectory)\n",
    "\n",
    "    if random.random() < 0.15:\n",
    "        gradient = agent.online_policy_model.input_layer.weight.grad\n",
    "        if gradient is not None:\n",
    "            print(f\"Mean gradient of online policy model's input layer: {gradient.mean()}\")\n",
    "\n",
    "        q_gradient = agent.online_value_model.input_layer.weight.grad\n",
    "        if q_gradient is not None:\n",
    "            print(f\"Mean gradient of online value model's input layer: {q_gradient.mean()}\")\n",
    "\n",
    "    epsilon_strategy.decay_epsilon()\n",
    "    all_rewards.append(episode_reward)\n",
    "    moving_avg_reward = np.mean(all_rewards[-100:])\n",
    "    avg_value_loss = sum(episode_value_losses) / len(episode_value_losses) if episode_value_losses else 0\n",
    "    avg_policy_loss = sum(episode_policy_losses) / len(episode_policy_losses) if episode_policy_losses else 0\n",
    "\n",
    "    print(f\"Episode {episode + 1}, Avg Value Loss: {avg_value_loss}, Avg Policy Loss: {avg_policy_loss}\")\n",
    "    print(f\"Episode {episode + 1}, Reward: {episode_reward}, Moving Avg Reward: {moving_avg_reward}, HER Buffer Size: {len(her_buffer)}\")\n",
    "    print(f\"Current Epsilon: {epsilon_strategy.epsilon}\")\n",
    "\n",
    "    recent_games = wins[-150 * 80:] if episode >= 150 else wins\n",
    "    recent_losses = [1 for i in range(0, len(recent_games), 80) if -1 in recent_games[i:i+80]].count(1)\n",
    "    #loss_rate = recent_losses / (len(recent_games) / 80) if len(recent_games) > 0 else 0\n",
    "    #print(f\"Loss rate over the last {int(len(recent_games)/80)} episodes: {loss_rate:.2f}\")\n",
    "\n",
    "\n",
    "    #if loss_rate > 0.80 and episode > train_start:\n",
    "    #    print(\"Bad Agent. Restarting Training...\")\n",
    "    #    agent = DDPGAgent(state_dim=state_dim, action_dim=action_dim, action_bounds=action_bounds)\n",
    "    #    agent.he_initialization()\n",
    "    #    wins.clear()\n",
    "\n",
    "# Save the models\n",
    "torch.save(agent.online_policy_model.state_dict(), \"online_policy_model_defense.pt\")\n",
    "torch.save(agent.online_value_model.state_dict(), \"online_value_model_defense.pt\")\n",
    "torch.save(agent.target_policy_model.state_dict(), \"target_policy_model_defense.pt\")\n",
    "torch.save(agent.target_value_model.state_dict(), \"target_value_model_defense.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef2a741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experience 1:\n",
      "State shape: [ -6.          -0.00520372   0.00200054   0.999998     0.\n",
      "  -0.27195147   0.10002708   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.           0.72931671\n",
      "   4.61147165 -10.9371109   -3.84231043]\n",
      "Action shape: [ 0.        -0.4070022  0.5639671  0.         0.         0.       ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.          -0.00520372   0.00200054   0.999998     0.\n",
      "  -0.27195147   0.10002708   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.           0.72931671\n",
      "   4.61147165 -10.9371109   -3.84231043]\n",
      "Done: False\n",
      "\n",
      "Experience 2:\n",
      "State shape: [ -6.00000286  -0.01078224   0.00652385   0.99997872   0.\n",
      "  -0.30552006   0.22616762   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.           0.51079369\n",
      "   4.53470182 -10.92617416  -3.83846807]\n",
      "Action shape: [ 0.         -0.05837875  0.71119857  0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00000286  -0.01078224   0.00652385   0.99997872   0.\n",
      "  -0.30552006   0.22616762   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.           0.51079369\n",
      "   4.53470182 -10.92617416  -3.83846807]\n",
      "Done: False\n",
      "\n",
      "Experience 3:\n",
      "State shape: [ -6.00001049  -0.01455975   0.01326959   0.99991196   0.\n",
      "  -0.22849996   0.33730412   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.           0.29248905\n",
      "   4.45800924 -10.91524792  -3.83462977]\n",
      "Action shape: [0.         0.10612337 0.62660354 0.         0.         0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00001049  -0.01455975   0.01326959   0.99991196   0.\n",
      "  -0.22849996   0.33730412   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.           0.29248905\n",
      "   4.45800924 -10.91524792  -3.83462977]\n",
      "Done: False\n",
      "\n",
      "Experience 4:\n",
      "State shape: [ -6.00002575  -0.02696848   0.02103016   0.99977884   0.\n",
      "  -0.66604674   0.38808653   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.           0.07440281\n",
      "   4.38139296 -10.90433311  -3.83079529]\n",
      "Action shape: [ 0.         -0.6616714   0.28631863  0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00002575  -0.02696848   0.02103016   0.99977884   0.\n",
      "  -0.66604674   0.38808653   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.           0.07440281\n",
      "   4.38139296 -10.90433311  -3.83079529]\n",
      "Done: False\n",
      "\n",
      "Experience 5:\n",
      "State shape: [ -6.00004768  -0.04738951   0.02846731   0.99959472   0.\n",
      "  -1.06472135   0.37197256   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -0.143466\n",
      "   4.30485392 -10.8934288   -3.82696462]\n",
      "Action shape: [ 0.         -0.61659193 -0.09085283  0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00004768  -0.04738951   0.02846731   0.99959472   0.\n",
      "  -1.06472135   0.37197256   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -0.143466\n",
      "   4.30485392 -10.8934288   -3.82696462]\n",
      "Done: False\n",
      "\n",
      "Experience 6:\n",
      "State shape: [ -6.00007915  -0.06467295   0.0366356    0.99932869   0.\n",
      "  -0.91216815   0.4086321    6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -0.36111641\n",
      "   4.22839117 -10.88253593  -3.82313776]\n",
      "Action shape: [0.         0.19644172 0.20669185 0.         0.         0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00007915  -0.06467295   0.0366356    0.99932869   0.\n",
      "  -0.91216815   0.4086321    6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -0.36111641\n",
      "   4.22839117 -10.88253593  -3.82313776]\n",
      "Done: False\n",
      "\n",
      "Experience 7:\n",
      "State shape: [ -6.00011921  -0.07086849   0.04507494   0.99898361   0.\n",
      "  -0.35937938   0.42232114   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -0.57854939\n",
      "   4.15200472 -10.87165356  -3.81931472]\n",
      "Action shape: [0.         0.8        0.07718086 0.         0.         0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00011921  -0.07086849   0.04507494   0.99898361   0.\n",
      "  -0.35937938   0.42232114   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -0.57854939\n",
      "   4.15200472 -10.87165356  -3.81931472]\n",
      "Done: False\n",
      "\n",
      "Experience 8:\n",
      "State shape: [ -6.00017834  -0.07684231   0.05510907   0.99848034   0.\n",
      "  -0.35766071   0.50233895   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -0.79576492\n",
      "   4.07569456 -10.86078167  -3.81549549]\n",
      "Action shape: [ 0.         -0.00818478  0.4511521   0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00017834  -0.07684231   0.05510907   0.99848034   0.\n",
      "  -0.35766071   0.50233895   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -0.79576492\n",
      "   4.07569456 -10.86078167  -3.81549549]\n",
      "Done: False\n",
      "\n",
      "Experience 9:\n",
      "State shape: [ -6.00022507  -0.07336855   0.06190887   0.99808181   0.\n",
      "   0.13376276   0.34057441   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -1.01276302\n",
      "   3.9994607  -10.84992123  -3.81168008]\n",
      "Action shape: [ 0.          0.72475827 -0.9120518   0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00022507  -0.07336855   0.06190887   0.99808181   0.\n",
      "   0.13376276   0.34057441   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -1.01276302\n",
      "   3.9994607  -10.84992123  -3.81168008]\n",
      "Done: False\n",
      "\n",
      "Experience 10:\n",
      "State shape: [ -6.00026894  -0.07292509   0.06757169   0.99771442   0.\n",
      "  -0.01109874   0.28373638   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -1.22954464\n",
      "   3.92330313 -10.83907127  -3.80786848]\n",
      "Action shape: [ 0.         -0.21279575 -0.3204611   0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00026894  -0.07292509   0.06757169   0.99771442   0.\n",
      "  -0.01109874   0.28373638   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -1.22954464\n",
      "   3.92330313 -10.83907127  -3.80786848]\n",
      "Done: False\n",
      "\n",
      "Experience 11:\n",
      "State shape: [ -6.0002985   -0.06406975   0.07120126   0.99746197   0.\n",
      "   0.42146409   0.18191758   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -1.44610977\n",
      "   3.84722185 -10.82823277  -3.8040607 ]\n",
      "Action shape: [ 0.          0.6470408  -0.57406914  0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.0002985   -0.06406975   0.07120126   0.99746197   0.\n",
      "   0.42146409   0.18191758   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -1.44610977\n",
      "   3.84722185 -10.82823277  -3.8040607 ]\n",
      "Done: False\n",
      "\n",
      "Experience 12:\n",
      "State shape: [ -6.00030279  -0.05402994   0.07173936   0.99742341   0.\n",
      "   0.49881813   0.02697359   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -1.66245747\n",
      "   3.77121687 -10.81740475  -3.80025673]\n",
      "Action shape: [ 0.          0.1283832  -0.87359655  0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00030279  -0.05402994   0.07173936   0.99742341   0.\n",
      "   0.49881813   0.02697359   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -1.66245747\n",
      "   3.77121687 -10.81740475  -3.80025673]\n",
      "Done: False\n",
      "\n",
      "Experience 13:\n",
      "State shape: [ -6.0003233   -0.05500031   0.07415651   0.99724662   0.\n",
      "  -0.06271636   0.12118024   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -1.87858963\n",
      "   3.69528818 -10.80658722  -3.79645658]\n",
      "Action shape: [ 0.        -0.8254612  0.5311507  0.         0.         0.       ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.0003233   -0.05500031   0.07415651   0.99724662   0.\n",
      "  -0.06271636   0.12118024   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -1.87858963\n",
      "   3.69528818 -10.80658722  -3.79645658]\n",
      "Done: False\n",
      "\n",
      "Experience 14:\n",
      "State shape: [ -6.00035381  -0.06783772   0.07755825   0.99698782   0.\n",
      "  -0.66185504   0.17057872   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -2.09450531\n",
      "   3.61943483 -10.79578114  -3.79266024]\n",
      "Action shape: [ 0.         -0.8985475   0.27851576  0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00035381  -0.06783772   0.07755825   0.99698782   0.\n",
      "  -0.66185504   0.17057872   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -2.09450531\n",
      "   3.61943483 -10.79578114  -3.79266024]\n",
      "Done: False\n",
      "\n",
      "Experience 15:\n",
      "State shape: [ -6.00041199  -0.07815313   0.08366821   0.99649367   0.\n",
      "  -0.55166733   0.30649614   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -2.31020498\n",
      "   3.54365778 -10.78498554  -3.78886771]\n",
      "Action shape: [0.         0.14509621 0.76632214 0.         0.         0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00041199  -0.07815313   0.08366821   0.99649367   0.\n",
      "  -0.55166733   0.30649614   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -2.31020498\n",
      "   3.54365778 -10.78498554  -3.78886771]\n",
      "Done: False\n",
      "\n",
      "Experience 16:\n",
      "State shape: [ -6.00044298  -0.08673334   0.08675183   0.99622995   0.\n",
      "  -0.44711998   0.15474387   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -2.52568913\n",
      "   3.46795607 -10.77420044  -3.785079  ]\n",
      "Action shape: [ 0.          0.13995299 -0.85560125  0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00044298  -0.08673334   0.08675183   0.99622995   0.\n",
      "  -0.44711998   0.15474387   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -2.52568913\n",
      "   3.46795607 -10.77420044  -3.785079  ]\n",
      "Done: False\n",
      "\n",
      "Experience 17:\n",
      "State shape: [ -6.00045252  -0.08674192   0.08766441   0.99615006   0.\n",
      "  -0.00576374   0.0458034    6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -2.74095774\n",
      "   3.39233065 -10.76342678  -3.78129387]\n",
      "Action shape: [ 0.          0.64715004 -0.6142221   0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00045252  -0.08674192   0.08766441   0.99615006   0.\n",
      "  -0.00576374   0.0458034    6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -2.74095774\n",
      "   3.39233065 -10.76342678  -3.78129387]\n",
      "Done: False\n",
      "\n",
      "Experience 18:\n",
      "State shape: [ -6.00048447  -0.09723425   0.09073214   0.99587533   0.\n",
      "  -0.54263312   0.15400004   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -2.95601082\n",
      "   3.31678057 -10.75266361  -3.77751255]\n",
      "Action shape: [ 0.         -0.80365044  0.6100283   0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00048447  -0.09723425   0.09073214   0.99587533   0.\n",
      "  -0.54263312   0.15400004   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -2.95601082\n",
      "   3.31678057 -10.75266361  -3.77751255]\n",
      "Done: False\n",
      "\n",
      "Experience 19:\n",
      "State shape: [ -6.00054264  -0.1057601    0.09601527   0.99537986   0.\n",
      "  -0.45733333   0.26531601   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -3.17084885\n",
      "   3.24130583 -10.74191093  -3.77373505]\n",
      "Action shape: [0.         0.1114175  0.62761545 0.         0.         0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00054264  -0.1057601    0.09601527   0.99537986   0.\n",
      "  -0.45733333   0.26531601   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -3.17084885\n",
      "   3.24130583 -10.74191093  -3.77373505]\n",
      "Done: False\n",
      "\n",
      "Experience 20:\n",
      "State shape: [ -6.0005846   -0.10515404   0.09964504   0.99502305   0.\n",
      "   0.00896457   0.18236393   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -3.3854723\n",
      "   3.16590643 -10.73116875  -3.76996136]\n",
      "Action shape: [ 0.          0.684172   -0.46769586  0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.0005846   -0.10515404   0.09964504   0.99502305   0.\n",
      "   0.00896457   0.18236393   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -3.3854723\n",
      "   3.16590643 -10.73116875  -3.76996136]\n",
      "Done: False\n",
      "\n",
      "Experience 21:\n",
      "State shape: [ -6.00060749  -0.11546373   0.10153169   0.99483231   0.\n",
      "  -0.52657598   0.09481294   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -3.59988117\n",
      "   3.09058237 -10.720438    -3.76619148]\n",
      "Action shape: [ 0.        -0.801221  -0.4936251  0.         0.         0.       ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00060749  -0.11546373   0.10153169   0.99483231   0.\n",
      "  -0.52657598   0.09481294   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -3.59988117\n",
      "   3.09058237 -10.720438    -3.76619148]\n",
      "Done: False\n",
      "\n",
      "Experience 22:\n",
      "State shape: [ -6.00063801  -0.11254835   0.10407786   0.99456915   0.\n",
      "   0.13081241   0.12798679   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -3.81407547\n",
      "   3.01533365 -10.70971775  -3.76242542]\n",
      "Action shape: [0.         0.9680853  0.18703896 0.         0.         0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00063801  -0.11254835   0.10407786   0.99456915   0.\n",
      "   0.13081241   0.12798679   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -3.81407547\n",
      "   3.01533365 -10.70971775  -3.76242542]\n",
      "Done: False\n",
      "\n",
      "Experience 23:\n",
      "State shape: [ -6.00065422  -0.11196136   0.10539386   0.99443056   0.\n",
      "   0.02160983   0.06616423   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -4.02805567\n",
      "   2.94016027 -10.69900799  -3.75866294]\n",
      "Action shape: [ 0.         -0.15951698 -0.3485645   0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00065422  -0.11196136   0.10539386   0.99443056   0.\n",
      "   0.02160983   0.06616423   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -4.02805567\n",
      "   2.94016027 -10.69900799  -3.75866294]\n",
      "Done: False\n",
      "\n",
      "Experience 24:\n",
      "State shape: [ -6.00062943  -0.10178423   0.10337697   0.99464225   0.\n",
      "   0.52069551  -0.10139883   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -4.24182177\n",
      "   2.86506224 -10.68830872  -3.75490427]\n",
      "Action shape: [ 0.          0.7475778  -0.94474477  0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00062943  -0.10178423   0.10337697   0.99464225   0.\n",
      "   0.52069551  -0.10139883   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -4.24182177\n",
      "   2.86506224 -10.68830872  -3.75490427]\n",
      "Done: False\n",
      "\n",
      "Experience 25:\n",
      "State shape: [ -6.00064564  -0.09851122   0.10466641   0.99450739   0.\n",
      "   0.15608174   0.06482332   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -4.45537424\n",
      "   2.79003954 -10.67762089  -3.75114942]\n",
      "Action shape: [ 0.        -0.5300951  0.9371845  0.         0.         0.       ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00064564  -0.09851122   0.10466641   0.99450739   0.\n",
      "   0.15608174   0.06482332   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -4.45537424\n",
      "   2.79003954 -10.67762089  -3.75114942]\n",
      "Done: False\n",
      "\n",
      "Experience 26:\n",
      "State shape: [ -6.00067472  -0.10213232   0.10699831   0.9942592    0.\n",
      "  -0.19477491   0.1172535    6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -4.66871309\n",
      "   2.71509123 -10.66694355  -3.74739838]\n",
      "Action shape: [ 0.         -0.52041984  0.2956089   0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00067472  -0.10213232   0.10699831   0.9942592    0.\n",
      "  -0.19477491   0.1172535    6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -4.66871309\n",
      "   2.71509123 -10.66694355  -3.74739838]\n",
      "Done: False\n",
      "\n",
      "Experience 27:\n",
      "State shape: [ -6.00067139  -0.10834455   0.10675054   0.99428584   0.\n",
      "  -0.30916601  -0.01245953   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -4.8818388\n",
      "   2.64021826 -10.6562767   -3.74365091]\n",
      "Action shape: [ 0.         -0.1770276  -0.73134077  0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00067139  -0.10834455   0.10675054   0.99428584   0.\n",
      "  -0.30916601  -0.01245953   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -4.8818388\n",
      "   2.64021826 -10.6562767   -3.74365091]\n",
      "Done: False\n",
      "\n",
      "Experience 28:\n",
      "State shape: [ -6.00067902  -0.11969614   0.10733718   0.99422268   0.\n",
      "  -0.5710209    0.02950152   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -5.09475136\n",
      "   2.56541967 -10.64562035  -3.73990726]\n",
      "Action shape: [ 0.         -0.4011456   0.23658249  0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00067902  -0.11969614   0.10733718   0.99422268   0.\n",
      "  -0.5710209    0.02950152   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -5.09475136\n",
      "   2.56541967 -10.64562035  -3.73990726]\n",
      "Done: False\n",
      "\n",
      "Experience 29:\n",
      "State shape: [ -6.00072956  -0.12244463   0.11123157   0.99379452   0.\n",
      "  -0.16032365   0.19589266   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -5.30745077\n",
      "   2.49069643 -10.63497448  -3.73616743]\n",
      "Action shape: [0.         0.59755725 0.93813723 0.         0.         0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00072956  -0.12244463   0.11123157   0.99379452   0.\n",
      "  -0.16032365   0.19589266   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -5.30745077\n",
      "   2.49069643 -10.63497448  -3.73616743]\n",
      "Done: False\n",
      "\n",
      "Experience 30:\n",
      "State shape: [ -6.00081444  -0.12856436   0.11753631   0.99306859   0.\n",
      "  -0.34304839   0.31732056   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -5.51993752\n",
      "   2.41604757 -10.62434006  -3.73243141]\n",
      "Action shape: [ 0.         -0.27826446  0.684628    0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00081444  -0.12856436   0.11753631   0.99306859   0.\n",
      "  -0.34304839   0.31732056   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -5.51993752\n",
      "   2.41604757 -10.62434006  -3.73243141]\n",
      "Done: False\n",
      "\n",
      "Experience 31:\n",
      "State shape: [ -6.00092363  -0.12676382   0.12512794   0.99214061   0.\n",
      "   0.04541964   0.3824071    6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -5.73221207\n",
      "   2.34147406 -10.61371613  -3.72869897]\n",
      "Action shape: [0.         0.5711127  0.36696726 0.         0.         0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00092363  -0.12676382   0.12512794   0.99214061   0.\n",
      "   0.04541964   0.3824071    6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -5.73221207\n",
      "   2.34147406 -10.61371613  -3.72869897]\n",
      "Done: False\n",
      "\n",
      "Experience 32:\n",
      "State shape: [ -6.0010004   -0.12936831   0.13022993   0.99148382   0.\n",
      "  -0.16019939   0.25720549   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -5.94427395\n",
      "   2.26697493 -10.60310268  -3.72497034]\n",
      "Action shape: [ 0.        -0.3063697 -0.7059047  0.         0.         0.       ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.0010004   -0.12936831   0.13022993   0.99148382   0.\n",
      "  -0.16019939   0.25720549   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -5.94427395\n",
      "   2.26697493 -10.60310268  -3.72497034]\n",
      "Done: False\n",
      "\n",
      "Experience 33:\n",
      "State shape: [ -6.00107574  -0.12644243   0.13500985   0.99084426   0.\n",
      "   0.11820105   0.24112599   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -6.15612411\n",
      "   2.19255018 -10.59249973  -3.72124553]\n",
      "Action shape: [ 0.          0.41185868 -0.09065855  0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00107574  -0.12644243   0.13500985   0.99084426   0.\n",
      "   0.11820105   0.24112599   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -6.15612411\n",
      "   2.19255018 -10.59249973  -3.72124553]\n",
      "Done: False\n",
      "\n",
      "Experience 34:\n",
      "State shape: [ -6.00110149  -0.11999083   0.1366202    0.9906235    0.\n",
      "   0.31312186   0.08127013   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -6.36776209\n",
      "   2.11819983 -10.58190727  -3.71752429]\n",
      "Action shape: [ 0.          0.29525623 -0.9012904   0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00110149  -0.11999083   0.1366202    0.9906235    0.\n",
      "   0.31312186   0.08127013   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -6.36776209\n",
      "   2.11819983 -10.58190727  -3.71752429]\n",
      "Done: False\n",
      "\n",
      "Experience 35:\n",
      "State shape: [ -6.00110531  -0.12014914   0.13684234   0.99059284   0.\n",
      "  -0.00920021   0.01121257   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -6.57918835\n",
      "   2.04392385 -10.5713253   -3.71380687]\n",
      "Action shape: [ 0.         -0.47301447 -0.39499465  0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00110531  -0.12014914   0.13684234   0.99059284   0.\n",
      "  -0.00920021   0.01121257   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -6.57918835\n",
      "   2.04392385 -10.5713253   -3.71380687]\n",
      "Done: False\n",
      "\n",
      "Experience 36:\n",
      "State shape: [ -6.00115061  -0.11689949   0.13959433   0.99020878   0.\n",
      "   0.14628765   0.13893348   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -6.79040337\n",
      "   1.96972227 -10.56075382  -3.71009302]\n",
      "Action shape: [0.         0.23242757 0.7201089  0.         0.         0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00115061  -0.11689949   0.13959433   0.99020878   0.\n",
      "   0.14628765   0.13893348   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -6.79040337\n",
      "   1.96972227 -10.56075382  -3.71009302]\n",
      "Done: False\n",
      "\n",
      "Experience 37:\n",
      "State shape: [ -6.00121689  -0.10861397   0.14354097   0.98964438   0.\n",
      "   0.39111218   0.19933955   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -7.00140762\n",
      "   1.89559507 -10.55019283  -3.70638299]\n",
      "Action shape: [0.         0.37078276 0.3405782  0.         0.         0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00121689  -0.10861397   0.14354097   0.98964438   0.\n",
      "   0.39111218   0.19933955   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -7.00140762\n",
      "   1.89559507 -10.55019283  -3.70638299]\n",
      "Done: False\n",
      "\n",
      "Experience 38:\n",
      "State shape: [ -6.00132132  -0.09587431   0.14954541   0.98875486   0.\n",
      "   0.60170829   0.30349949   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -7.21220016\n",
      "   1.82154131 -10.53964233  -3.70267677]\n",
      "Action shape: [0.         0.32688466 0.5872687  0.         0.         0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00132132  -0.09587431   0.14954541   0.98875486   0.\n",
      "   0.60170829   0.30349949   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -7.21220016\n",
      "   1.82154131 -10.53964233  -3.70267677]\n",
      "Done: False\n",
      "\n",
      "Experience 39:\n",
      "State shape: [ -6.00146961  -0.08475971   0.15765876   0.98749365   0.\n",
      "   0.50805992   0.41054058   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -7.42278194\n",
      "   1.74756193 -10.52910328  -3.69897413]\n",
      "Action shape: [ 0.        -0.1221437  0.6035131  0.         0.         0.       ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00146961  -0.08475971   0.15765876   0.98749365   0.\n",
      "   0.50805992   0.41054058   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -7.42278194\n",
      "   1.74756193 -10.52910328  -3.69897413]\n",
      "Done: False\n",
      "\n",
      "Experience 40:\n",
      "State shape: [ -6.00169325  -0.07924509   0.16914415   0.98559132   0.\n",
      "   0.20822836   0.5820967    6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -7.63315392\n",
      "   1.67365599 -10.51857471  -3.69527531]\n",
      "Action shape: [ 0.         -0.43352032  0.96725816  0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00169325  -0.07924509   0.16914415   0.98559132   0.\n",
      "   0.20822836   0.5820967    6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -7.63315392\n",
      "   1.67365599 -10.51857471  -3.69527531]\n",
      "Done: False\n",
      "\n",
      "Experience 41:\n",
      "State shape: [ -6.00193024  -0.06813002   0.18052312   0.98357074   0.\n",
      "   0.48889688   0.57785249   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -7.84331512\n",
      "   1.59982443 -10.50805664  -3.69158006]\n",
      "Action shape: [ 0.          0.42628086 -0.02392962  0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00193024  -0.06813002   0.18052312   0.98357074   0.\n",
      "   0.48889688   0.57785249   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -7.84331512\n",
      "   1.59982443 -10.50805664  -3.69158006]\n",
      "Done: False\n",
      "\n",
      "Experience 42:\n",
      "State shape: [ -6.00226021  -0.05209923   0.1951968    0.9807641    0.\n",
      "   0.71532124   0.74699068   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -8.05326557\n",
      "   1.5260663  -10.49754906  -3.68788862]\n",
      "Action shape: [0.         0.35350007 0.9536257  0.         0.         0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00226021  -0.05209923   0.1951968    0.9807641    0.\n",
      "   0.71532124   0.74699068   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -8.05326557\n",
      "   1.5260663  -10.49754906  -3.68788862]\n",
      "Done: True\n",
      "\n",
      "Experience 43:\n",
      "State shape: [ -6.          -0.00887728   0.00066993   0.99999978   0.\n",
      "  -0.4478015    0.03349637   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.           0.48373604\n",
      "   4.66943789 -10.68677425  -3.55238867]\n",
      "Action shape: [ 0.         -0.6701791   0.18885736  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.          -0.00887728   0.00066993   0.99999978   0.\n",
      "  -0.4478015    0.03349637   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.           0.48373604\n",
      "   4.66943789 -10.68677425  -3.55238867]\n",
      "Done: False\n",
      "\n",
      "Experience 44:\n",
      "State shape: [ -6.          -0.01101875  -0.00062701   0.9999998    0.\n",
      "  -0.0994752   -0.06484693   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.           0.27021408\n",
      "   4.59846163 -10.67608738  -3.54883623]\n",
      "Action shape: [ 0.          0.50790113 -0.55447376  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.          -0.01101875  -0.00062701   0.9999998    0.\n",
      "  -0.0994752   -0.06484693   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.           0.27021408\n",
      "   4.59846163 -10.67608738  -3.54883623]\n",
      "Done: False\n",
      "\n",
      "Experience 45:\n",
      "State shape: [ -6.          -0.0126586    0.00023077   0.99999997   0.\n",
      "  -0.08703183   0.04288897   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.           0.05690575\n",
      "   4.52755594 -10.665411    -3.54528737]\n",
      "Action shape: [0.         0.01564525 0.6074306  0.         0.         0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.          -0.0126586    0.00023077   0.99999997   0.\n",
      "  -0.08703183   0.04288897   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.           0.05690575\n",
      "   4.52755594 -10.665411    -3.54528737]\n",
      "Done: False\n",
      "\n",
      "Experience 46:\n",
      "State shape: [ -6.00000048  -0.01183605   0.00277563   0.99999615   0.\n",
      "   0.02618032   0.12724307   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -0.15618896\n",
      "   4.45672083 -10.65474606  -3.54174209]\n",
      "Action shape: [0.         0.16682813 0.47560057 0.         0.         0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00000048  -0.01183605   0.00277563   0.99999615   0.\n",
      "   0.02618032   0.12724307   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -0.15618896\n",
      "   4.45672083 -10.65474606  -3.54174209]\n",
      "Done: False\n",
      "\n",
      "Experience 47:\n",
      "State shape: [ -6.00000095  -0.00226307   0.00377073   0.99999289   0.\n",
      "   0.47280958   0.04975544   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -0.36907101\n",
      "   4.38595724 -10.64409161  -3.53820038]\n",
      "Action shape: [ 0.          0.6692084  -0.43688646  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00000095  -0.00226307   0.00377073   0.99999289   0.\n",
      "   0.47280958   0.04975544   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -0.36907101\n",
      "   4.38595724 -10.64409161  -3.53820038]\n",
      "Done: False\n",
      "\n",
      "Experience 48:\n",
      "State shape: [ -6.           0.00603199   0.00206032   0.99999788   0.\n",
      "   0.42478558  -0.08552083   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -0.58174038\n",
      "   4.31526423 -10.63344765  -3.53466225]\n",
      "Action shape: [ 0.         -0.05772054 -0.7627072   0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.           0.00603199   0.00206032   0.99999788   0.\n",
      "   0.42478558  -0.08552083   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -0.58174038\n",
      "   4.31526423 -10.63344765  -3.53466225]\n",
      "Done: False\n",
      "\n",
      "Experience 49:\n",
      "State shape: [ -6.00000048   0.01680517   0.00303673   0.99999539   0.\n",
      "   0.5329451    0.04882069   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -0.79419708\n",
      "   4.24464178 -10.62281418  -3.53112769]\n",
      "Action shape: [0.         0.17458603 0.7574369  0.         0.         0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00000048   0.01680517   0.00303673   0.99999539   0.\n",
      "   0.5329451    0.04882069   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -0.79419708\n",
      "   4.24464178 -10.62281418  -3.53112769]\n",
      "Done: False\n",
      "\n",
      "Experience 50:\n",
      "State shape: [ -6.           0.03961039   0.00186483   0.99999826   0.\n",
      "   1.14712489  -0.0585952    3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -1.00644112\n",
      "   4.17408991 -10.6121912   -3.52759671]\n",
      "Action shape: [ 0.         0.9351329 -0.6056263  0.         0.         0.       ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.           0.03961039   0.00186483   0.99999826   0.\n",
      "   1.14712489  -0.0585952    3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -1.00644112\n",
      "   4.17408991 -10.6121912   -3.52759671]\n",
      "Done: False\n",
      "\n",
      "Experience 51:\n",
      "State shape: [ -6.           0.05626583  -0.00064998   0.99999979   0.\n",
      "   0.84754324  -0.12574044   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -1.21847248\n",
      "   4.10360861 -10.60157871  -3.52406907]\n",
      "Action shape: [ 0.         -0.4140178  -0.37857458  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.           0.05626583  -0.00064998   0.99999979   0.\n",
      "   0.84754324  -0.12574044   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -1.21847248\n",
      "   4.10360861 -10.60157871  -3.52406907]\n",
      "Done: False\n",
      "\n",
      "Experience 52:\n",
      "State shape: [ -6.           0.06323957  -0.00028411   0.99999996   0.\n",
      "   0.34653586   0.01829323   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -1.43029213\n",
      "   4.03319788 -10.59097767  -3.52054501]\n",
      "Action shape: [ 0.         -0.72443837  0.8120827   0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.           0.06323957  -0.00028411   0.99999996   0.\n",
      "   0.34653586   0.01829323   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -1.43029213\n",
      "   4.03319788 -10.59097767  -3.52054501]\n",
      "Done: False\n",
      "\n",
      "Experience 53:\n",
      "State shape: [ -6.           0.06572104   0.00002096   1.           0.\n",
      "   0.12228625   0.01525364   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -1.64190006\n",
      "   3.96285772 -10.58038712  -3.51702452]\n",
      "Action shape: [ 0.         -0.32523918 -0.01713766  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.           0.06572104   0.00002096   1.           0.\n",
      "   0.12228625   0.01525364   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -1.64190006\n",
      "   3.96285772 -10.58038712  -3.51702452]\n",
      "Done: False\n",
      "\n",
      "Experience 54:\n",
      "State shape: [ -6.           0.06206608   0.0007207    0.99999974   0.\n",
      "  -0.18687809   0.03498679   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -1.85329628\n",
      "   3.89258718 -10.56980705  -3.5135076 ]\n",
      "Action shape: [ 0.         -0.45903474  0.11125833  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.           0.06206608   0.0007207    0.99999974   0.\n",
      "  -0.18687809   0.03498679   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -1.85329628\n",
      "   3.89258718 -10.56980705  -3.5135076 ]\n",
      "Done: False\n",
      "\n",
      "Experience 55:\n",
      "State shape: [ -6.00000048   0.04829741   0.00286144   0.99999591   0.\n",
      "  -0.70099324   0.10703722   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -2.06448078\n",
      "   3.82238722 -10.55923748  -3.50999403]\n",
      "Action shape: [ 0.         -0.7750177   0.40623078  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00000048   0.04829741   0.00286144   0.99999591   0.\n",
      "  -0.70099324   0.10703722   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -2.06448078\n",
      "   3.82238722 -10.55923748  -3.50999403]\n",
      "Done: False\n",
      "\n",
      "Experience 56:\n",
      "State shape: [ -6.00000095   0.02390671   0.00318831   0.99999492   0.\n",
      "  -1.22146428   0.01634392   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -2.27545452\n",
      "   3.75225782 -10.5486784   -3.50648403]\n",
      "Action shape: [ 0.        -0.7999185 -0.5113419  0.         0.         0.       ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00000095   0.02390671   0.00318831   0.99999492   0.\n",
      "  -1.22146428   0.01634392   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -2.27545452\n",
      "   3.75225782 -10.5486784   -3.50648403]\n",
      "Done: False\n",
      "\n",
      "Experience 57:\n",
      "State shape: [ -6.           0.00020313   0.00000315   1.           0.\n",
      "  -1.16646886  -0.15925856   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -2.48621702\n",
      "   3.68219805 -10.53812981  -3.50297761]\n",
      "Action shape: [ 0.          0.04574521 -0.9900722   0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.           0.00020313   0.00000315   1.           0.\n",
      "  -1.16646886  -0.15925856   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -2.48621702\n",
      "   3.68219805 -10.53812981  -3.50297761]\n",
      "Done: False\n",
      "\n",
      "Experience 58:\n",
      "State shape: [ -6.          -0.03347683  -0.00126314   0.9999992    0.\n",
      "  -1.67655611  -0.06331422   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -2.69676876\n",
      "   3.61220884 -10.52759171  -3.49947476]\n",
      "Action shape: [ 0.         -0.7983106   0.54094803  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.          -0.03347683  -0.00126314   0.9999992    0.\n",
      "  -1.67655611  -0.06331422   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -2.69676876\n",
      "   3.61220884 -10.52759171  -3.49947476]\n",
      "Done: False\n",
      "\n",
      "Experience 59:\n",
      "State shape: [ -6.00000095  -0.07734442  -0.0045043    0.99998986   0.\n",
      "  -2.17433929  -0.16205907   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -2.90711021\n",
      "   3.54228926 -10.51706409  -3.49597526]\n",
      "Action shape: [ 0.         -0.79516417 -0.55673766  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00000095  -0.07734442  -0.0045043    0.99998986   0.\n",
      "  -2.17433929  -0.16205907   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -2.90711021\n",
      "   3.54228926 -10.51706409  -3.49597526]\n",
      "Done: False\n",
      "\n",
      "Experience 60:\n",
      "State shape: [ -6.00000429  -0.12973547  -0.00851719   0.99996373   0.\n",
      "  -2.59598351  -0.20064881   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -3.11724091\n",
      "   3.47243929 -10.50654697  -3.49247932]\n",
      "Action shape: [ 0.         -0.69611454 -0.21757452  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00000429  -0.12973547  -0.00851719   0.99996373   0.\n",
      "  -2.59598351  -0.20064881   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -3.11724091\n",
      "   3.47243929 -10.50654697  -3.49247932]\n",
      "Done: False\n",
      "\n",
      "Experience 61:\n",
      "State shape: [ -6.00001335  -0.18462086  -0.01502872   0.99988706   0.\n",
      "  -2.70601463  -0.32559931   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -3.32716179\n",
      "   3.40265989 -10.49604034  -3.48898697]\n",
      "Action shape: [ 0.         -0.24237506 -0.70448893  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00001335  -0.18462086  -0.01502872   0.99988706   0.\n",
      "  -2.70601463  -0.32559931   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -3.32716179\n",
      "   3.40265989 -10.49604034  -3.48898697]\n",
      "Done: False\n",
      "\n",
      "Experience 62:\n",
      "State shape: [ -6.00001907  -0.2402997   -0.01812713   0.99983569   0.\n",
      "  -2.76574302  -0.15494229   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -3.53687286\n",
      "   3.33295012 -10.4855442   -3.48549795]\n",
      "Action shape: [ 0.         -0.17038557  0.96218896  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00001907  -0.2402997   -0.01812713   0.99983569   0.\n",
      "  -2.76574302  -0.15494229   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -3.53687286\n",
      "   3.33295012 -10.4855442   -3.48549795]\n",
      "Done: False\n",
      "\n",
      "Experience 63:\n",
      "State shape: [ -6.00002575  -0.3051219   -0.02093385   0.99978086   0.\n",
      "  -3.22461963  -0.14036241   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -3.74637413\n",
      "   3.26330996 -10.47505856  -3.48201251]\n",
      "Action shape: [ 0.         -0.7695384   0.08220345  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00002575  -0.3051219   -0.02093385   0.99978086   0.\n",
      "  -3.22461963  -0.14036241   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -3.74637413\n",
      "   3.26330996 -10.47505856  -3.48201251]\n",
      "Done: False\n",
      "\n",
      "Experience 64:\n",
      "State shape: [ -6.00002432  -0.35679102  -0.02032796   0.99979337   0.\n",
      "  -2.58702683   0.03030063   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -3.95566559\n",
      "   3.19373941 -10.4645834   -3.47853065]\n",
      "Action shape: [0.        0.8577014 0.9622229 0.        0.        0.       ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00002432  -0.35679102  -0.02032796   0.99979337   0.\n",
      "  -2.58702683   0.03030063   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -3.95566559\n",
      "   3.19373941 -10.4645834   -3.47853065]\n",
      "Done: False\n",
      "\n",
      "Experience 65:\n",
      "State shape: [ -6.00002098  -0.41116333  -0.01902648   0.99981898   0.\n",
      "  -2.72623372   0.06508671   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -4.16474819\n",
      "   3.12423849 -10.45411873  -3.47505212]\n",
      "Action shape: [ 0.         -0.28577158  0.19612893  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00002098  -0.41116333  -0.01902648   0.99981898   0.\n",
      "  -2.72623372   0.06508671   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -4.16474819\n",
      "   3.12423849 -10.45411873  -3.47505212]\n",
      "Done: False\n",
      "\n",
      "Experience 66:\n",
      "State shape: [ -6.00002003  -0.47755098  -0.01852589   0.99982838   0.\n",
      "  -3.32233214   0.02503391   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -4.37362146\n",
      "   3.05480719 -10.44366455  -3.47157717]\n",
      "Action shape: [ 0.         -0.97372156 -0.22582346  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00002003  -0.47755098  -0.01852589   0.99982838   0.\n",
      "  -3.32233214   0.02503391   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -4.37362146\n",
      "   3.05480719 -10.44366455  -3.47157717]\n",
      "Done: False\n",
      "\n",
      "Experience 67:\n",
      "State shape: [ -6.0000267   -0.54841661  -0.0213317    0.99977245   0.\n",
      "  -3.52677274  -0.14031869   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -4.58228588\n",
      "   2.9854455  -10.43322086  -3.46810555]\n",
      "Action shape: [ 0.         -0.40540954 -0.9322818   0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.0000267   -0.54841661  -0.0213317    0.99977245   0.\n",
      "  -3.52677274  -0.14031869   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -4.58228588\n",
      "   2.9854455  -10.43322086  -3.46810555]\n",
      "Done: False\n",
      "\n",
      "Experience 68:\n",
      "State shape: [ -6.00003719  -0.62144899  -0.02524034   0.99968141   0.\n",
      "  -3.62866926  -0.19548507   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -4.79074144\n",
      "   2.91615248 -10.42278767  -3.46463752]\n",
      "Action shape: [ 0.         -0.25806138 -0.31103602  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00003719  -0.62144899  -0.02524034   0.99968141   0.\n",
      "  -3.62866926  -0.19548507   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -4.79074144\n",
      "   2.91615248 -10.42278767  -3.46463752]\n",
      "Done: False\n",
      "\n",
      "Experience 69:\n",
      "State shape: [ -6.00005388  -0.68741941  -0.03027208   0.9995417    0.\n",
      "  -3.26896477  -0.25168425   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -4.99898863\n",
      "   2.84692907 -10.41236496  -3.46117282]\n",
      "Action shape: [ 0.         0.4297202 -0.316859   0.         0.         0.       ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00005388  -0.68741941  -0.03027208   0.9995417    0.\n",
      "  -3.26896477  -0.25168425   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -4.99898863\n",
      "   2.84692907 -10.41236496  -3.46117282]\n",
      "Done: False\n",
      "\n",
      "Experience 70:\n",
      "State shape: [ -6.00006008  -0.76141405  -0.03192552   0.99949025   0.\n",
      "  -3.69002938  -0.08271174   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -5.20702744\n",
      "   2.77777529 -10.40195274  -3.4577117 ]\n",
      "Action shape: [ 0.         -0.72801125  0.95269144  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00006008  -0.76141405  -0.03192552   0.99949025   0.\n",
      "  -3.69002938  -0.08271174   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -5.20702744\n",
      "   2.77777529 -10.40195274  -3.4577117 ]\n",
      "Done: False\n",
      "\n",
      "Experience 71:\n",
      "State shape: [ -6.00006342  -0.83010674  -0.03283724   0.99946071   0.\n",
      "  -3.42928076  -0.04561004   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -5.41485834\n",
      "   2.70869017 -10.39155102  -3.45425391]\n",
      "Action shape: [0.         0.27978647 0.20918474 0.         0.         0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00006342  -0.83010674  -0.03283724   0.99946071   0.\n",
      "  -3.42928076  -0.04561004   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -5.41485834\n",
      "   2.70869017 -10.39155102  -3.45425391]\n",
      "Done: False\n",
      "\n",
      "Experience 72:\n",
      "State shape: [ -6.00006819  -0.90721416  -0.03403333   0.9994207    0.\n",
      "  -3.84832621  -0.05983786   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -5.62248135\n",
      "   2.63967371 -10.38115978  -3.4507997 ]\n",
      "Action shape: [ 0.        -0.7297878 -0.0802185  0.         0.         0.       ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00006819  -0.90721416  -0.03403333   0.9994207    0.\n",
      "  -3.84832621  -0.05983786   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -5.62248135\n",
      "   2.63967371 -10.38115978  -3.4507997 ]\n",
      "Done: False\n",
      "\n",
      "Experience 73:\n",
      "State shape: [ -6.00007677  -0.9886775   -0.0361285    0.99934715   0.\n",
      "  -4.06086826  -0.10482343   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -5.82989693\n",
      "   2.57072687 -10.37077904  -3.44734883]\n",
      "Action shape: [ 0.        -0.4332782 -0.2536351  0.         0.         0.       ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00007677  -0.9886775   -0.0361285    0.99934715   0.\n",
      "  -4.06086826  -0.10482343   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -5.82989693\n",
      "   2.57072687 -10.37077904  -3.44734883]\n",
      "Done: False\n",
      "\n",
      "Experience 74:\n",
      "State shape: [ -6.00008535  -1.06959438  -0.03812754   0.99927288   0.\n",
      "  -4.03410149  -0.10002099   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -6.03710508\n",
      "   2.5018487  -10.36040878  -3.44390154]\n",
      "Action shape: [ 0.         -0.08149058  0.02707684  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00008535  -1.06959438  -0.03812754   0.99927288   0.\n",
      "  -4.03410149  -0.10002099   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -6.03710508\n",
      "   2.5018487  -10.36040878  -3.44390154]\n",
      "Done: False\n",
      "\n",
      "Experience 75:\n",
      "State shape: [ -6.00008488  -1.1596241   -0.03794504   0.99927983   0.\n",
      "  -4.50256252   0.00913166   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -6.24410629\n",
      "   2.43303919 -10.35004807  -3.44045758]\n",
      "Action shape: [ 0.         -0.82184684  0.6154184   0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00008488  -1.1596241   -0.03794504   0.99927983   0.\n",
      "  -4.50256252   0.00913166   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -6.24410629\n",
      "   2.43303919 -10.35004807  -3.44045758]\n",
      "Done: False\n",
      "\n",
      "Experience 76:\n",
      "State shape: [ -6.0000906   -1.23619556  -0.03928711   0.99922796   0.\n",
      "  -3.82068634  -0.06715346   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -6.45090008\n",
      "   2.3642993  -10.33969784  -3.4370172 ]\n",
      "Action shape: [ 0.          0.88572454 -0.4301065   0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.0000906   -1.23619556  -0.03928711   0.99922796   0.\n",
      "  -3.82068634  -0.06715346   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -6.45090008\n",
      "   2.3642993  -10.33969784  -3.4370172 ]\n",
      "Done: False\n",
      "\n",
      "Experience 77:\n",
      "State shape: [ -6.00010729  -1.30953026  -0.04269829   0.99908801   0.\n",
      "  -3.64671659  -0.1707024    3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -6.65748692\n",
      "   2.29562807 -10.3293581   -3.43358016]\n",
      "Action shape: [ 0.          0.14600244 -0.58382386  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00010729  -1.30953026  -0.04269829   0.99908801   0.\n",
      "  -3.64671659  -0.1707024    3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -6.65748692\n",
      "   2.29562807 -10.3293581   -3.43358016]\n",
      "Done: False\n",
      "\n",
      "Experience 78:\n",
      "State shape: [ -6.00013876  -1.3865757   -0.04857736   0.99881942   0.\n",
      "  -3.81772137  -0.29426071   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -6.86386776\n",
      "   2.22702551 -10.31902885  -3.43014669]\n",
      "Action shape: [ 0.         -0.3650789  -0.69663954  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00013876  -1.3865757   -0.04857736   0.99881942   0.\n",
      "  -3.81772137  -0.29426071   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -6.86386776\n",
      "   2.22702551 -10.31902885  -3.43014669]\n",
      "Done: False\n",
      "\n",
      "Experience 79:\n",
      "State shape: [ -6.00015354  -1.45481253  -0.05113591   0.9986917    0.\n",
      "  -3.39678884  -0.12808691   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -7.07004166\n",
      "   2.15849161 -10.3087101   -3.42671657]\n",
      "Action shape: [0.         0.51569545 0.9369119  0.         0.         0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00015354  -1.45481253  -0.05113591   0.9986917    0.\n",
      "  -3.39678884  -0.12808691   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -7.07004166\n",
      "   2.15849161 -10.3087101   -3.42671657]\n",
      "Done: False\n",
      "\n",
      "Experience 80:\n",
      "State shape: [ -6.00018311  -1.51289749  -0.05582082   0.9984408    0.\n",
      "  -2.87673903  -0.23458131   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -7.27601004\n",
      "   2.09002542 -10.29840183  -3.42328978]\n",
      "Action shape: [ 0.         0.6766334 -0.6004308  0.         0.         0.       ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00018311  -1.51289749  -0.05582082   0.9984408    0.\n",
      "  -2.87673903  -0.23458131   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -7.27601004\n",
      "   2.09002542 -10.29840183  -3.42328978]\n",
      "Done: False\n",
      "\n",
      "Experience 81:\n",
      "State shape: [ -6.00021935  -1.56961632  -0.0611183    0.99813053   0.\n",
      "  -2.80480695  -0.26532802   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -7.48177242\n",
      "   2.0216279  -10.2881031   -3.41986656]\n",
      "Action shape: [ 0.          0.02154712 -0.17335446  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00021935  -1.56961632  -0.0611183    0.99813053   0.\n",
      "  -2.80480695  -0.26532802   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -7.48177242\n",
      "   2.0216279  -10.2881031   -3.41986656]\n",
      "Done: False\n",
      "\n",
      "Experience 82:\n",
      "State shape: [ -6.00026417  -1.62678289  -0.06701466   0.99775199   0.\n",
      "  -2.82367754  -0.29542556   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -7.68732834\n",
      "   1.95329905 -10.27781487  -3.41644669]\n",
      "Action shape: [ 0.         -0.11219496 -0.16969432  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00026417  -1.62678289  -0.06701466   0.99775199   0.\n",
      "  -2.82367754  -0.29542556   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -7.68732834\n",
      "   1.95329905 -10.27781487  -3.41644669]\n",
      "Done: False\n",
      "\n",
      "Experience 83:\n",
      "State shape: [ -6.00033951  -1.69291162  -0.0759268    0.99711339   0.\n",
      "  -3.25407124  -0.44675124   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -7.89267921\n",
      "   1.88503885 -10.26753712  -3.41303039]\n",
      "Action shape: [ 0.         -0.72864497 -0.853196    0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00033951  -1.69291162  -0.0759268    0.99711339   0.\n",
      "  -3.25407124  -0.44675124   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -7.89267921\n",
      "   1.88503885 -10.26753712  -3.41303039]\n",
      "Done: True\n",
      "\n",
      "Experience 84:\n",
      "State shape: [ -6.          -0.01000643   0.00047059   0.99999989   0.\n",
      "  -0.50308704   0.02352929   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           3.32406425\n",
      "   0.3952713  -13.58210945   0.89169651]\n",
      "Action shape: [ 0.         -0.75291944  0.13266155  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.          -0.01000643   0.00047059   0.99999989   0.\n",
      "  -0.50308704   0.02352929   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           3.32406425\n",
      "   0.3952713  -13.58210945   0.89169651]\n",
      "Done: False\n",
      "\n",
      "Experience 85:\n",
      "State shape: [ -6.          -0.00806713  -0.00064957   0.99999979   0.\n",
      "   0.1035369   -0.05600805   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           3.05269337\n",
      "   0.41308737 -13.56852722   0.89080483]\n",
      "Action shape: [ 0.         0.8928143 -0.448443   0.         0.         0.       ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.          -0.00806713  -0.00064957   0.99999979   0.\n",
      "   0.1035369   -0.05600805   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           3.05269337\n",
      "   0.41308737 -13.56852722   0.89080483]\n",
      "Done: False\n",
      "\n",
      "Experience 86:\n",
      "State shape: [ -6.          -0.01201725  -0.00101456   0.99999949   0.\n",
      "  -0.19536175  -0.01824923   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           2.78159428\n",
      "   0.43088579 -13.5549593    0.88991404]\n",
      "Action shape: [ 0.         -0.44423229  0.21288966  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.          -0.01201725  -0.00101456   0.99999949   0.\n",
      "  -0.19536175  -0.01824923   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           2.78159428\n",
      "   0.43088579 -13.5549593    0.88991404]\n",
      "Done: False\n",
      "\n",
      "Experience 87:\n",
      "State shape: [ -6.          -0.02218151  -0.00110503   0.99999939   0.\n",
      "  -0.50767958  -0.00452347   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           2.51076603\n",
      "   0.4486661  -13.54140472   0.88902414]\n",
      "Action shape: [ 0.         -0.47326204  0.07738784  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.          -0.02218151  -0.00110503   0.99999939   0.\n",
      "  -0.50767958  -0.00452347   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           2.51076603\n",
      "   0.4486661  -13.54140472   0.88902414]\n",
      "Done: False\n",
      "\n",
      "Experience 88:\n",
      "State shape: [ -6.          -0.03435946   0.00024765   0.99999997   0.\n",
      "  -0.61684561   0.06763408   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           2.24020863\n",
      "   0.46642876 -13.5278635    0.88813514]\n",
      "Action shape: [ 0.         -0.17857361  0.4068347   0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.          -0.03435946   0.00024765   0.99999997   0.\n",
      "  -0.61684561   0.06763408   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           2.24020863\n",
      "   0.46642876 -13.5278635    0.88813514]\n",
      "Done: False\n",
      "\n",
      "Experience 89:\n",
      "State shape: [ -6.          -0.04802513  -0.00149834   0.99999888   0.\n",
      "  -0.67302942  -0.08729985   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           1.96992207\n",
      "   0.48417377 -13.51433563   0.88724703]\n",
      "Action shape: [ 0.         -0.10254806 -0.8735399   0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.          -0.04802513  -0.00149834   0.99999888   0.\n",
      "  -0.67302942  -0.08729985   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           1.96992207\n",
      "   0.48417377 -13.51433563   0.88724703]\n",
      "Done: False\n",
      "\n",
      "Experience 90:\n",
      "State shape: [ -6.          -0.07013559  -0.00239967   0.99999712   0.\n",
      "  -1.10022771  -0.04506632   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           1.6999054\n",
      "   0.50190115 -13.50082111   0.88635981]\n",
      "Action shape: [ 0.         -0.6594895   0.23811878  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.          -0.07013559  -0.00239967   0.99999712   0.\n",
      "  -1.10022771  -0.04506632   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           1.6999054\n",
      "   0.50190115 -13.50082111   0.88635981]\n",
      "Done: False\n",
      "\n",
      "Experience 91:\n",
      "State shape: [ -6.          -0.09969473  -0.00071368   0.99999975   0.\n",
      "  -1.48785841   0.08429936   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           1.43015862\n",
      "   0.5196104  -13.4873209    0.88547349]\n",
      "Action shape: [ 0.         -0.6130596   0.72938234  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.          -0.09969473  -0.00071368   0.99999975   0.\n",
      "  -1.48785841   0.08429936   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           1.43015862\n",
      "   0.5196104  -13.4873209    0.88547349]\n",
      "Done: False\n",
      "\n",
      "Experience 92:\n",
      "State shape: [ -6.00000095  -0.13790512   0.00424936   0.99999097   0.\n",
      "  -1.93968618   0.24815273   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           1.16068172\n",
      "   0.53730202 -13.47383404   0.884588  ]\n",
      "Action shape: [ 0.         -0.72073925  0.9238291   0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00000095  -0.13790512   0.00424936   0.99999097   0.\n",
      "  -1.93968618   0.24815273   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           1.16068172\n",
      "   0.53730202 -13.47383404   0.884588  ]\n",
      "Done: False\n",
      "\n",
      "Experience 93:\n",
      "State shape: [ -6.00000715  -0.17277241   0.01111029   0.99993828   0.\n",
      "  -1.78367889   0.34305727   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           0.89147472\n",
      "   0.55497599 -13.46036053   0.88370341]\n",
      "Action shape: [0.        0.1754217 0.5350855 0.        0.        0.       ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00000715  -0.17277241   0.01111029   0.99993828   0.\n",
      "  -1.78367889   0.34305727   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           0.89147472\n",
      "   0.55497599 -13.46036053   0.88370341]\n",
      "Done: False\n",
      "\n",
      "Experience 94:\n",
      "State shape: [ -6.00002289  -0.19545269   0.01959186   0.99980806   0.\n",
      "  -1.18385029   0.42412972   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           0.62253666\n",
      "   0.57263231 -13.44690037   0.88281971]\n",
      "Action shape: [0.         0.8443138  0.45709822 0.         0.         0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00002289  -0.19545269   0.01959186   0.99980806   0.\n",
      "  -1.18385029   0.42412972   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           0.62253666\n",
      "   0.57263231 -13.44690037   0.88281971]\n",
      "Done: False\n",
      "\n",
      "Experience 95:\n",
      "State shape: [ -6.00005531  -0.22332954   0.03077056   0.99952647   0.\n",
      "  -1.45951927   0.55911547   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           0.35386753\n",
      "   0.590271   -13.43345356   0.88193691]\n",
      "Action shape: [ 0.        -0.4480008  0.761069   0.         0.         0.       ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00005531  -0.22332954   0.03077056   0.99952647   0.\n",
      "  -1.45951927   0.55911547   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           0.35386753\n",
      "   0.590271   -13.43345356   0.88193691]\n",
      "Done: False\n",
      "\n",
      "Experience 96:\n",
      "State shape: [ -6.00008917  -0.25425148   0.03894988   0.99924117   0.\n",
      "  -1.59416091   0.40921569   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           0.08546734\n",
      "   0.60789204 -13.4200201    0.881055  ]\n",
      "Action shape: [ 0.         -0.24519072 -0.8451566   0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00008917  -0.25425148   0.03894988   0.99924117   0.\n",
      "  -1.59416091   0.40921569   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           0.08546734\n",
      "   0.60789204 -13.4200201    0.881055  ]\n",
      "Done: False\n",
      "\n",
      "Experience 97:\n",
      "State shape: [ -6.00013542  -0.29753304   0.04797651   0.99884846   0.\n",
      "  -2.21710992   0.45176014   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -0.18266487\n",
      "   0.62549543 -13.4066       0.88017398]\n",
      "Action shape: [ 0.         -0.9800209   0.23987183  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00013542  -0.29753304   0.04797651   0.99884846   0.\n",
      "  -2.21710992   0.45176014   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -0.18266487\n",
      "   0.62549543 -13.4066       0.88017398]\n",
      "Done: False\n",
      "\n",
      "Experience 98:\n",
      "State shape: [ -6.00019646  -0.34051704   0.05779227   0.99832863   0.\n",
      "  -2.20686984   0.49147731   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -0.4505291\n",
      "   0.64308119 -13.39319324   0.8792938 ]\n",
      "Action shape: [ 0.         -0.0510372   0.22393109  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00019646  -0.34051704   0.05779227   0.99832863   0.\n",
      "  -2.20686984   0.49147731   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -0.4505291\n",
      "   0.64308119 -13.39319324   0.8792938 ]\n",
      "Done: False\n",
      "\n",
      "Experience 99:\n",
      "State shape: [ -6.00027943  -0.37578201   0.06896162   0.99761931   0.\n",
      "  -1.82887077   0.55959558   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -0.71812534\n",
      "   0.6606493  -13.37979984   0.87841451]\n",
      "Action shape: [0.         0.49965712 0.38406065 0.         0.         0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00027943  -0.37578201   0.06896162   0.99761931   0.\n",
      "  -1.82887077   0.55959558   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -0.71812534\n",
      "   0.6606493  -13.37979984   0.87841451]\n",
      "Done: False\n",
      "\n",
      "Experience 100:\n",
      "State shape: [ -6.00040388  -0.41654539   0.08284679   0.9965623    0.\n",
      "  -2.11974621   0.69627267   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -0.98545361\n",
      "   0.67820024 -13.36641979   0.87753612]\n",
      "Action shape: [ 0.         -0.49006557  0.77060527  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00040388  -0.41654539   0.08284679   0.9965623    0.\n",
      "  -2.11974621   0.69627267   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -0.98545361\n",
      "   0.67820024 -13.36641979   0.87753612]\n",
      "Done: False\n",
      "\n",
      "Experience 101:\n",
      "State shape: [ -6.0005312   -0.45279884   0.09500016   0.99547726   0.\n",
      "  -1.8840723    0.61008942   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -1.25251484\n",
      "   0.69573355 -13.35305309   0.87665862]\n",
      "Action shape: [ 0.          0.28926116 -0.48591375  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.0005312   -0.45279884   0.09500016   0.99547726   0.\n",
      "  -1.8840723    0.61008942   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -1.25251484\n",
      "   0.69573355 -13.35305309   0.87665862]\n",
      "Done: False\n",
      "\n",
      "Experience 102:\n",
      "State shape: [ -6.00068617  -0.47884369   0.1079046    0.99416125   0.\n",
      "  -1.37807      0.64857316   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -1.51930904\n",
      "   0.71324921 -13.33969975   0.87578195]\n",
      "Action shape: [0.         0.7008884  0.21697704 0.         0.         0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00068617  -0.47884369   0.1079046    0.99416125   0.\n",
      "  -1.37807      0.64857316   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -1.51930904\n",
      "   0.71324921 -13.33969975   0.87578195]\n",
      "Done: False\n",
      "\n",
      "Experience 103:\n",
      "State shape: [ -6.00086355  -0.51718044   0.12100355   0.99265208   0.\n",
      "  -1.9938066    0.65928471   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -1.78583622\n",
      "   0.73074722 -13.32635975   0.87490618]\n",
      "Action shape: [ 0.         -0.96275896  0.06039337  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00086355  -0.51718044   0.12100355   0.99265208   0.\n",
      "  -1.9938066    0.65928471   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -1.78583622\n",
      "   0.73074722 -13.32635975   0.87490618]\n",
      "Done: False\n",
      "\n",
      "Experience 104:\n",
      "State shape: [ -6.00101185  -0.55900717   0.1309315    0.99139142   0.\n",
      "  -2.14968705   0.50038582   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -2.05209684\n",
      "   0.74822807 -13.3130331    0.87403131]\n",
      "Action shape: [ 0.         -0.29296902 -0.89589477  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00101185  -0.55900717   0.1309315    0.99139142   0.\n",
      "  -2.14968705   0.50038582   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -2.05209684\n",
      "   0.74822807 -13.3130331    0.87403131]\n",
      "Done: False\n",
      "\n",
      "Experience 105:\n",
      "State shape: [ -6.00114679  -0.59800339   0.1393651    0.99024107   0.\n",
      "  -1.99935138   0.42558601   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -2.31809139\n",
      "   0.76569128 -13.29971981   0.87315726]\n",
      "Action shape: [ 0.          0.16064765 -0.42173207  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00114679  -0.59800339   0.1393651    0.99024107   0.\n",
      "  -1.99935138   0.42558601   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -2.31809139\n",
      "   0.76569128 -13.29971981   0.87315726]\n",
      "Done: False\n",
      "\n",
      "Experience 106:\n",
      "State shape: [ -6.0013423   -0.64222765   0.15071214   0.98857769   0.\n",
      "  -2.27787566   0.57341814   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -2.58381987\n",
      "   0.78313684 -13.28641987   0.87228411]\n",
      "Action shape: [ 0.        -0.4766835  0.8334989  0.         0.         0.       ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.0013423   -0.64222765   0.15071214   0.98857769   0.\n",
      "  -2.27787566   0.57341814   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -2.58381987\n",
      "   0.78313684 -13.28641987   0.87228411]\n",
      "Done: False\n",
      "\n",
      "Experience 107:\n",
      "State shape: [ -6.00152349  -0.68840408   0.16050446   0.98703512   0.\n",
      "  -2.36637688   0.49565607   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -2.84928274\n",
      "   0.80056524 -13.27313328   0.87141186]\n",
      "Action shape: [ 0.         -0.20063211 -0.43843368  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00152349  -0.68840408   0.16050446   0.98703512   0.\n",
      "  -2.36637688   0.49565607   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -2.84928274\n",
      "   0.80056524 -13.27313328   0.87141186]\n",
      "Done: False\n",
      "\n",
      "Experience 108:\n",
      "State shape: [ -6.00178146  -0.74648952   0.17346697   0.98483969   0.\n",
      "  -2.98042202   0.65736037   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -3.11448002\n",
      "   0.817976   -13.25986004   0.87054044]\n",
      "Action shape: [ 0.        -0.9898096  0.9117121  0.         0.         0.       ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00178146  -0.74648952   0.17346697   0.98483969   0.\n",
      "  -2.98042202   0.65736037   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -3.11448002\n",
      "   0.817976   -13.25986004   0.87054044]\n",
      "Done: False\n",
      "\n",
      "Experience 109:\n",
      "State shape: [ -6.00209713  -0.79922915   0.18807531   0.98215461   0.\n",
      "  -2.7228024    0.74265975   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -3.37941217\n",
      "   0.83536959 -13.24660015   0.86966991]\n",
      "Action shape: [0.         0.29634333 0.48093027 0.         0.         0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00209713  -0.79922915   0.18807531   0.98215461   0.\n",
      "  -2.7228024    0.74265975   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -3.37941217\n",
      "   0.83536959 -13.24660015   0.86966991]\n",
      "Done: False\n",
      "\n",
      "Experience 110:\n",
      "State shape: [ -6.00247097  -0.85154486   0.20399253   0.97897244   0.\n",
      "  -2.70932555   0.81161875   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -3.64407921\n",
      "   0.85274553 -13.23335361   0.86880028]\n",
      "Action shape: [ 0.         -0.06132945  0.38880065  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00247097  -0.85154486   0.20399253   0.97897244   0.\n",
      "  -2.70932555   0.81161875   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -3.64407921\n",
      "   0.85274553 -13.23335361   0.86880028]\n",
      "Done: False\n",
      "\n",
      "Experience 111:\n",
      "State shape: [ -6.00288677  -0.89211035   0.22029629   0.975433     0.\n",
      "  -2.12403536   0.8341862    8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -3.9084816\n",
      "   0.87010431 -13.22012043   0.86793149]\n",
      "Action shape: [0.         0.7948491  0.12723856 0.         0.         0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00288677  -0.89211035   0.22029629   0.975433     0.\n",
      "  -2.12403536   0.8341862    8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -3.9084816\n",
      "   0.87010431 -13.22012043   0.86793149]\n",
      "Done: False\n",
      "\n",
      "Experience 112:\n",
      "State shape: [ -6.00340414  -0.93627834   0.23895414   0.97103085   0.\n",
      "  -2.31802297   0.95852178   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -4.17261982\n",
      "   0.88744545 -13.2069006    0.86706358]\n",
      "Action shape: [ 0.         -0.35389823  0.701022    0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00340414  -0.93627834   0.23895414   0.97103085   0.\n",
      "  -2.31802297   0.95852178   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -4.17261982\n",
      "   0.88744545 -13.2069006    0.86706358]\n",
      "Done: False\n",
      "\n",
      "Experience 113:\n",
      "State shape: [ -6.00403881  -0.9697051    0.25992671   0.96562835   0.\n",
      "  -1.79457009   1.08288372   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -4.43649387\n",
      "   0.90476942 -13.19369411   0.86619651]\n",
      "Action shape: [0.        0.7140159 0.7011708 0.        0.        0.       ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00403881  -0.9697051    0.25992671   0.96562835   0.\n",
      "  -1.79457009   1.08288372   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -4.43649387\n",
      "   0.90476942 -13.19369411   0.86619651]\n",
      "Done: False\n",
      "\n",
      "Experience 114:\n",
      "State shape: [ -6.00472164  -1.0081358    0.28061183   0.95982134   0.\n",
      "  -2.04306555   1.07425916   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -4.70010376\n",
      "   0.92207623 -13.18050098   0.86533034]\n",
      "Action shape: [ 0.         -0.42561287 -0.04862671  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00472164  -1.0081358    0.28061183   0.95982134   0.\n",
      "  -2.04306555   1.07425916   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -4.70010376\n",
      "   0.92207623 -13.18050098   0.86533034]\n",
      "Done: False\n",
      "\n",
      "Experience 115:\n",
      "State shape: [ -6.00533104  -1.0373826    0.29779488   0.95462988   0.\n",
      "  -1.56327641   0.89752042   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -4.96344995\n",
      "   0.93936539 -13.16732025   0.864465  ]\n",
      "Action shape: [ 0.          0.65689886 -0.9964785   0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00533104  -1.0373826    0.29779488   0.95462988   0.\n",
      "  -1.56327641   0.89752042   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -4.96344995\n",
      "   0.93936539 -13.16732025   0.864465  ]\n",
      "Done: False\n",
      "\n",
      "Experience 116:\n",
      "State shape: [ -6.00607681  -1.07665634   0.31741557   0.94828654   0.\n",
      "  -2.07898879   1.03104842   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -5.22653294\n",
      "   0.95663738 -13.15415287   0.86360055]\n",
      "Action shape: [ 0.         -0.8186065   0.75284994  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00607681  -1.07665634   0.31741557   0.94828654   0.\n",
      "  -2.07898879   1.03104842   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -5.22653294\n",
      "   0.95663738 -13.15415287   0.86360055]\n",
      "Done: False\n",
      "\n",
      "Experience 117:\n",
      "State shape: [ -6.00689793  -1.10972786   0.33757776   0.94129764   0.\n",
      "  -1.77202332   1.0669769    8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -5.4893527\n",
      "   0.97389221 -13.14099884   0.86273694]\n",
      "Action shape: [0.         0.39717606 0.20257005 0.         0.         0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00689793  -1.10972786   0.33757776   0.94129764   0.\n",
      "  -1.77202332   1.0669769    8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -5.4893527\n",
      "   0.97389221 -13.14099884   0.86273694]\n",
      "Done: False\n",
      "\n",
      "Experience 118:\n",
      "State shape: [ -6.00784063  -1.1333971    0.35915892   0.93327642   0.\n",
      "  -1.31027997   1.15120566   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -5.75190973\n",
      "   0.99112988 -13.12785816   0.86187422]\n",
      "Action shape: [0.        0.6380043 0.474894  0.        0.        0.       ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00784063  -1.1333971    0.35915892   0.93327642   0.\n",
      "  -1.31027997   1.15120566   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -5.75190973\n",
      "   0.99112988 -13.12785816   0.86187422]\n",
      "Done: False\n",
      "\n",
      "Experience 119:\n",
      "State shape: [ -6.00878811  -1.16965199   0.37944197   0.92521554   0.\n",
      "  -1.93190587   1.09132838   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -6.01420403\n",
      "   1.0083499  -13.11473083   0.86101234]\n",
      "Action shape: [ 0.         -0.96954364 -0.33759657  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00878811  -1.16965199   0.37944197   0.92521554   0.\n",
      "  -1.93190587   1.09132838   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -6.01420403\n",
      "   1.0083499  -13.11473083   0.86101234]\n",
      "Done: False\n",
      "\n",
      "Experience 120:\n",
      "State shape: [ -6.00966787  -1.21471071   0.39721284   0.91772652   0.\n",
      "  -2.35733747   0.9642365    8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -6.27623653\n",
      "   1.02555275 -13.10161591   0.86015135]\n",
      "Action shape: [ 0.         -0.69452596 -0.7165625   0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00966787  -1.21471071   0.39721284   0.91772652   0.\n",
      "  -2.35733747   0.9642365    8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -6.27623653\n",
      "   1.02555275 -13.10161591   0.86015135]\n",
      "Done: False\n",
      "\n",
      "Experience 121:\n",
      "State shape: [ -6.01054478  -1.26354122   0.41402874   0.91026381   0.\n",
      "  -2.54032636   0.91988569   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -6.53800678\n",
      "   1.04273844 -13.08851433   0.8592912 ]\n",
      "Action shape: [ 0.         -0.34442064 -0.2500562   0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.01054478  -1.26354122   0.41402874   0.91026381   0.\n",
      "  -2.54032636   0.91988569   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -6.53800678\n",
      "   1.04273844 -13.08851433   0.8592912 ]\n",
      "Done: False\n",
      "\n",
      "Experience 122:\n",
      "State shape: [ -6.01152802  -1.31196499   0.43195818   0.90189363   0.\n",
      "  -2.52654982   0.98936498   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -6.79951525\n",
      "   1.05990696 -13.0754261    0.85843194]\n",
      "Action shape: [ 0.         -0.05541917  0.3917341   0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.01152802  -1.31196499   0.43195818   0.90189363   0.\n",
      "  -2.52654982   0.98936498   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -6.79951525\n",
      "   1.05990696 -13.0754261    0.85843194]\n",
      "Done: False\n",
      "\n",
      "Experience 123:\n",
      "State shape: [ -6.01252937  -1.3507967    0.44930782   0.89337701   0.\n",
      "  -2.04350543   0.96637821   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -7.06076241\n",
      "   1.07705832 -13.06235123   0.85757351]\n",
      "Action shape: [ 0.          0.64729905 -0.12960267  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.01252937  -1.3507967    0.44930782   0.89337701   0.\n",
      "  -2.04350543   0.96637821   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -7.06076241\n",
      "   1.07705832 -13.06235123   0.85757351]\n",
      "Done: False\n",
      "\n",
      "Experience 124:\n",
      "State shape: [ -6.01351452  -1.39840221   0.46560843   0.88499084   0.\n",
      "  -2.47604752   0.91657877   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -7.32174778\n",
      "   1.0941925  -13.04928875   0.85671592]\n",
      "Action shape: [ 0.         -0.70850813 -0.28077647  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.01351452  -1.39840221   0.46560843   0.88499084   0.\n",
      "  -2.47604752   0.91657877   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -7.32174778\n",
      "   1.0941925  -13.04928875   0.85671592]\n",
      "Done: False\n",
      "\n",
      "Experience 125:\n",
      "State shape: [ -6.01433849  -1.43555784   0.47869727   0.87798002   0.\n",
      "  -1.93470573   0.74241793   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -7.5824728\n",
      "   1.11130953 -13.03623962   0.85585922]\n",
      "Action shape: [ 0.         0.7360584 -0.9819438  0.         0.         0.       ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.01433849  -1.43555784   0.47869727   0.87798002   0.\n",
      "  -1.93470573   0.74241793   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -7.5824728\n",
      "   1.11130953 -13.03623962   0.85585922]\n",
      "Done: False\n",
      "\n",
      "Experience 126:\n",
      "State shape: [ -6.01510239  -1.47824955   0.49043549   0.8714775    0.\n",
      "  -2.2035358    0.67095429   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -7.84293699\n",
      "   1.12840939 -13.02320385   0.85500336]\n",
      "Action shape: [ 0.         -0.46024013 -0.40292245  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.01510239  -1.47824955   0.49043549   0.8714775    0.\n",
      "  -2.2035358    0.67095429   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -7.84293699\n",
      "   1.12840939 -13.02320385   0.85500336]\n",
      "Done: False\n",
      "\n",
      "Experience 127:\n",
      "State shape: [ -6.01595497  -1.52810383   0.50310993   0.86422243   0.\n",
      "  -2.56720376   0.73020673   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -8.10314083\n",
      "   1.14549255 -13.01018047   0.85414839]\n",
      "Action shape: [ 0.         -0.61022097  0.33407375  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.01595497  -1.52810383   0.50310993   0.86422243   0.\n",
      "  -2.56720376   0.73020673   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -8.10314083\n",
      "   1.14549255 -13.01018047   0.85414839]\n",
      "Done: True\n",
      "\n",
      "Experience 128:\n",
      "State shape: [ -6.           0.00490332  -0.00085504   0.99999963   0.\n",
      "   0.25018677  -0.0427521    3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           4.27040768\n",
      "   3.97807837 -14.54678249  -2.28675818]\n",
      "Action shape: [ 0.          0.37442923 -0.24104251  0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.           0.00490332  -0.00085504   0.99999963   0.\n",
      "   0.25018677  -0.0427521    3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           4.27040768\n",
      "   3.97807837 -14.54678249  -2.28675818]\n",
      "Done: False\n",
      "\n",
      "Experience 129:\n",
      "State shape: [ -6.           0.00490332  -0.00085504   0.99999963   0.\n",
      "   0.25018677  -0.0427521    3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           4.27040768\n",
      "   3.97807837 -14.54678249  -2.28675818]\n",
      "Action shape: [ 0.          0.37442923 -0.24104251  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00490332  -0.00085504   0.99999963   0.\n",
      "   0.25018677  -0.0427521    3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           4.27040768\n",
      "   3.97807837 -14.54678249  -2.28675818]\n",
      "Done: False\n",
      "\n",
      "Experience 130:\n",
      "State shape: [ -6.00000143   0.0017066   -0.00494043   0.9999878    0.\n",
      "  -0.13585913  -0.20427047   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           3.97976303\n",
      "   3.93238878 -14.5322361   -2.28447151]\n",
      "Action shape: [ 0.         -0.57026726 -0.91066384  0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00000143   0.0017066   -0.00494043   0.9999878    0.\n",
      "  -0.13585913  -0.20427047   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           3.97976303\n",
      "   3.93238878 -14.5322361   -2.28447151]\n",
      "Done: False\n",
      "\n",
      "Experience 131:\n",
      "State shape: [ -6.00000143   0.0017066   -0.00494043   0.9999878    0.\n",
      "  -0.13585913  -0.20427047   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           3.97976303\n",
      "   3.93238878 -14.5322361   -2.28447151]\n",
      "Action shape: [ 0.         -0.57026726 -0.91066384  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000143   0.0017066   -0.00494043   0.9999878    0.\n",
      "  -0.13585913  -0.20427047   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           3.97976303\n",
      "   3.93238878 -14.5322361   -2.28447151]\n",
      "Done: False\n",
      "\n",
      "Experience 132:\n",
      "State shape: [ -6.00000286   0.0029397   -0.00697099   0.9999757    0.\n",
      "   0.07358979  -0.10152981   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           3.68940926\n",
      "   3.88674498 -14.51770401  -2.28218699]\n",
      "Action shape: [0.         0.30939445 0.5792666  0.         0.         0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00000286   0.0029397   -0.00697099   0.9999757    0.\n",
      "   0.07358979  -0.10152981   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           3.68940926\n",
      "   3.88674498 -14.51770401  -2.28218699]\n",
      "Done: False\n",
      "\n",
      "Experience 133:\n",
      "State shape: [ -6.00000286   0.0029397   -0.00697099   0.9999757    0.\n",
      "   0.07358979  -0.10152981   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           3.68940926\n",
      "   3.88674498 -14.51770401  -2.28218699]\n",
      "Action shape: [0.         0.30939445 0.5792666  0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000286   0.0029397   -0.00697099   0.9999757    0.\n",
      "   0.07358979  -0.10152981   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           3.68940926\n",
      "   3.88674498 -14.51770401  -2.28218699]\n",
      "Done: False\n",
      "\n",
      "Experience 134:\n",
      "State shape: [ -6.00000191   0.00830173  -0.00616376   0.999981     0.\n",
      "   0.26335678   0.04036262   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           3.3993454\n",
      "   3.84114695 -14.50318623  -2.27990484]\n",
      "Action shape: [0.        0.2862077 0.80001   0.        0.        0.       ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00000191   0.00830173  -0.00616376   0.999981     0.\n",
      "   0.26335678   0.04036262   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           3.3993454\n",
      "   3.84114695 -14.50318623  -2.27990484]\n",
      "Done: False\n",
      "\n",
      "Experience 135:\n",
      "State shape: [ -6.00000191   0.00830173  -0.00616376   0.999981     0.\n",
      "   0.26335678   0.04036262   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           3.3993454\n",
      "   3.84114695 -14.50318623  -2.27990484]\n",
      "Action shape: [0.        0.2862077 0.80001   0.        0.        0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000191   0.00830173  -0.00616376   0.999981     0.\n",
      "   0.26335678   0.04036262   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           3.3993454\n",
      "   3.84114695 -14.50318623  -2.27990484]\n",
      "Done: False\n",
      "\n",
      "Experience 136:\n",
      "State shape: [ -6.           0.00109386  -0.00216176   0.99999766   0.\n",
      "  -0.383899     0.20010161   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           3.10957146\n",
      "   3.79559469 -14.4886837   -2.27762508]\n",
      "Action shape: [ 0.         -0.96079946  0.9006315   0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.           0.00109386  -0.00216176   0.99999766   0.\n",
      "  -0.383899     0.20010161   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           3.10957146\n",
      "   3.79559469 -14.4886837   -2.27762508]\n",
      "Done: False\n",
      "\n",
      "Experience 137:\n",
      "State shape: [ -6.           0.00109386  -0.00216176   0.99999766   0.\n",
      "  -0.383899     0.20010161   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           3.10957146\n",
      "   3.79559469 -14.4886837   -2.27762508]\n",
      "Action shape: [ 0.         -0.96079946  0.9006315   0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00109386  -0.00216176   0.99999766   0.\n",
      "  -0.383899     0.20010161   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           3.10957146\n",
      "   3.79559469 -14.4886837   -2.27762508]\n",
      "Done: False\n",
      "\n",
      "Experience 138:\n",
      "State shape: [ -6.          -0.01820087   0.0004057    0.99999992   0.\n",
      "  -0.97983426   0.128373     3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           2.82008743\n",
      "   3.75008821 -14.47419548  -2.27534747]\n",
      "Action shape: [ 0.        -0.9033668 -0.4044163  0.         0.         0.       ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.          -0.01820087   0.0004057    0.99999992   0.\n",
      "  -0.97983426   0.128373     3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           2.82008743\n",
      "   3.75008821 -14.47419548  -2.27534747]\n",
      "Done: False\n",
      "\n",
      "Experience 139:\n",
      "State shape: [ -6.          -0.01820087   0.0004057    0.99999992   0.\n",
      "  -0.97983426   0.128373     3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           2.82008743\n",
      "   3.75008821 -14.47419548  -2.27534747]\n",
      "Action shape: [ 0.        -0.9033668 -0.4044163  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.          -0.01820087   0.0004057    0.99999992   0.\n",
      "  -0.97983426   0.128373     3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           2.82008743\n",
      "   3.75008821 -14.47419548  -2.27534747]\n",
      "Done: False\n",
      "\n",
      "Experience 140:\n",
      "State shape: [ -6.          -0.03806639   0.00211954   0.99999775   0.\n",
      "  -1.00333393   0.0856922    3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           2.53089333\n",
      "   3.70462656 -14.45972157  -2.27307224]\n",
      "Action shape: [ 0.         -0.06449789 -0.24064049  0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.          -0.03806639   0.00211954   0.99999775   0.\n",
      "  -1.00333393   0.0856922    3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           2.53089333\n",
      "   3.70462656 -14.45972157  -2.27307224]\n",
      "Done: False\n",
      "\n",
      "Experience 141:\n",
      "State shape: [ -6.          -0.03806639   0.00211954   0.99999775   0.\n",
      "  -1.00333393   0.0856922    3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           2.53089333\n",
      "   3.70462656 -14.45972157  -2.27307224]\n",
      "Action shape: [ 0.         -0.06449789 -0.24064049  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.          -0.03806639   0.00211954   0.99999775   0.\n",
      "  -1.00333393   0.0856922    3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           2.53089333\n",
      "   3.70462656 -14.45972157  -2.27307224]\n",
      "Done: False\n",
      "\n",
      "Experience 142:\n",
      "State shape: [ -6.          -0.06471014   0.00132093   0.99999913   0.\n",
      "  -1.32752645  -0.03993037   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           2.24198818\n",
      "   3.65921068 -14.44526196  -2.27079916]\n",
      "Action shape: [ 0.         -0.51521796 -0.7082782   0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.          -0.06471014   0.00132093   0.99999913   0.\n",
      "  -1.32752645  -0.03993037   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           2.24198818\n",
      "   3.65921068 -14.44526196  -2.27079916]\n",
      "Done: False\n",
      "\n",
      "Experience 143:\n",
      "State shape: [ -6.          -0.06471014   0.00132093   0.99999913   0.\n",
      "  -1.32752645  -0.03993037   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           2.24198818\n",
      "   3.65921068 -14.44526196  -2.27079916]\n",
      "Action shape: [ 0.         -0.51521796 -0.7082782   0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.          -0.06471014   0.00132093   0.99999913   0.\n",
      "  -1.32752645  -0.03993037   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           2.24198818\n",
      "   3.65921068 -14.44526196  -2.27079916]\n",
      "Done: False\n",
      "\n",
      "Experience 144:\n",
      "State shape: [ -6.00000095  -0.08175898   0.00327225   0.99999465   0.\n",
      "  -0.86389393   0.09756593   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           1.953372\n",
      "   3.61384058 -14.43081665  -2.26852846]\n",
      "Action shape: [0.         0.6541364  0.77522403 0.         0.         0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00000095  -0.08175898   0.00327225   0.99999465   0.\n",
      "  -0.86389393   0.09756593   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           1.953372\n",
      "   3.61384058 -14.43081665  -2.26852846]\n",
      "Done: False\n",
      "\n",
      "Experience 145:\n",
      "State shape: [ -6.00000095  -0.08175898   0.00327225   0.99999465   0.\n",
      "  -0.86389393   0.09756593   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           1.953372\n",
      "   3.61384058 -14.43081665  -2.26852846]\n",
      "Action shape: [0.         0.6541364  0.77522403 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000095  -0.08175898   0.00327225   0.99999465   0.\n",
      "  -0.86389393   0.09756593   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           1.953372\n",
      "   3.61384058 -14.43081665  -2.26852846]\n",
      "Done: False\n",
      "\n",
      "Experience 146:\n",
      "State shape: [ -6.00000381  -0.08487082   0.00820854   0.99996631   0.\n",
      "  -0.18461421   0.24681903   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           1.66504478\n",
      "   3.5685153  -14.41638565  -2.26625991]\n",
      "Action shape: [0.         0.9907512  0.84151053 0.         0.         0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00000381  -0.08487082   0.00820854   0.99996631   0.\n",
      "  -0.18461421   0.24681903   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           1.66504478\n",
      "   3.5685153  -14.41638565  -2.26625991]\n",
      "Done: False\n",
      "\n",
      "Experience 147:\n",
      "State shape: [ -6.00000381  -0.08487082   0.00820854   0.99996631   0.\n",
      "  -0.18461421   0.24681903   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           1.66504478\n",
      "   3.5685153  -14.41638565  -2.26625991]\n",
      "Action shape: [0.         0.9907512  0.84151053 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000381  -0.08487082   0.00820854   0.99996631   0.\n",
      "  -0.18461421   0.24681903   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           1.66504478\n",
      "   3.5685153  -14.41638565  -2.26625991]\n",
      "Done: False\n",
      "\n",
      "Experience 148:\n",
      "State shape: [ -6.00001431  -0.08739567   0.01552469   0.99987948   0.\n",
      "  -0.16923903   0.36583382   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           1.37700558\n",
      "   3.5232358  -14.40196991  -2.26399374]\n",
      "Action shape: [0.         0.01748461 0.67102265 0.         0.         0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00001431  -0.08739567   0.01552469   0.99987948   0.\n",
      "  -0.16923903   0.36583382   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           1.37700558\n",
      "   3.5232358  -14.40196991  -2.26399374]\n",
      "Done: False\n",
      "\n",
      "Experience 149:\n",
      "State shape: [ -6.00001431  -0.08739567   0.01552469   0.99987948   0.\n",
      "  -0.16923903   0.36583382   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           1.37700558\n",
      "   3.5232358  -14.40196991  -2.26399374]\n",
      "Action shape: [0.         0.01748461 0.67102265 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00001431  -0.08739567   0.01552469   0.99987948   0.\n",
      "  -0.16923903   0.36583382   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           1.37700558\n",
      "   3.5232358  -14.40196991  -2.26399374]\n",
      "Done: False\n",
      "\n",
      "Experience 150:\n",
      "State shape: [ -6.00003815  -0.08225918   0.02541407   0.99967701   0.\n",
      "   0.19871163   0.49457467   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           1.08925438\n",
      "   3.47800112 -14.38756847  -2.26172972]\n",
      "Action shape: [0.        0.5456088 0.7258595 0.        0.        0.       ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00003815  -0.08225918   0.02541407   0.99967701   0.\n",
      "   0.19871163   0.49457467   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           1.08925438\n",
      "   3.47800112 -14.38756847  -2.26172972]\n",
      "Done: False\n",
      "\n",
      "Experience 151:\n",
      "State shape: [ -6.00003815  -0.08225918   0.02541407   0.99967701   0.\n",
      "   0.19871163   0.49457467   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           1.08925438\n",
      "   3.47800112 -14.38756847  -2.26172972]\n",
      "Action shape: [0.        0.5456088 0.7258595 0.        0.        0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00003815  -0.08225918   0.02541407   0.99967701   0.\n",
      "   0.19871163   0.49457467   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           1.08925438\n",
      "   3.47800112 -14.38756847  -2.26172972]\n",
      "Done: False\n",
      "\n",
      "Experience 152:\n",
      "State shape: [ -6.00008202  -0.06730413   0.03739087   0.99930072   0.\n",
      "   0.6773982    0.59913939   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           0.80179119\n",
      "   3.43281221 -14.37318134  -2.25946808]\n",
      "Action shape: [0.        0.7223496 0.5895511 0.        0.        0.       ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00008202  -0.06730413   0.03739087   0.99930072   0.\n",
      "   0.6773982    0.59913939   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           0.80179119\n",
      "   3.43281221 -14.37318134  -2.25946808]\n",
      "Done: False\n",
      "\n",
      "Experience 153:\n",
      "State shape: [ -6.00008202  -0.06730413   0.03739087   0.99930072   0.\n",
      "   0.6773982    0.59913939   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           0.80179119\n",
      "   3.43281221 -14.37318134  -2.25946808]\n",
      "Action shape: [0.        0.7223496 0.5895511 0.        0.        0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00008202  -0.06730413   0.03739087   0.99930072   0.\n",
      "   0.6773982    0.59913939   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           0.80179119\n",
      "   3.43281221 -14.37318134  -2.25946808]\n",
      "Done: False\n",
      "\n",
      "Experience 154:\n",
      "State shape: [ -6.00013971  -0.06034088   0.04872917   0.99881203   0.\n",
      "   0.28154773   0.56744426   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           0.51461506\n",
      "   3.38766813 -14.35880852  -2.25720859]\n",
      "Action shape: [ 0.         -0.5721535  -0.17870176  0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00013971  -0.06034088   0.04872917   0.99881203   0.\n",
      "   0.28154773   0.56744426   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           0.51461506\n",
      "   3.38766813 -14.35880852  -2.25720859]\n",
      "Done: False\n",
      "\n",
      "Experience 155:\n",
      "State shape: [ -6.00013971  -0.06034088   0.04872917   0.99881203   0.\n",
      "   0.28154773   0.56744426   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           0.51461506\n",
      "   3.38766813 -14.35880852  -2.25720859]\n",
      "Action shape: [ 0.         -0.5721535  -0.17870176  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00013971  -0.06034088   0.04872917   0.99881203   0.\n",
      "   0.28154773   0.56744426   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           0.51461506\n",
      "   3.38766813 -14.35880852  -2.25720859]\n",
      "Done: False\n",
      "\n",
      "Experience 156:\n",
      "State shape: [ -6.00021648  -0.05429077   0.06067824   0.99815738   0.\n",
      "   0.23228176   0.59835345   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           0.22772598\n",
      "   3.34256887 -14.34445     -2.25495148]\n",
      "Action shape: [ 0.         -0.0653041   0.17427048  0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00021648  -0.05429077   0.06067824   0.99815738   0.\n",
      "   0.23228176   0.59835345   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           0.22772598\n",
      "   3.34256887 -14.34445     -2.25495148]\n",
      "Done: False\n",
      "\n",
      "Experience 157:\n",
      "State shape: [ -6.00021648  -0.05429077   0.06067824   0.99815738   0.\n",
      "   0.23228176   0.59835345   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           0.22772598\n",
      "   3.34256887 -14.34445     -2.25495148]\n",
      "Action shape: [ 0.         -0.0653041   0.17427048  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00021648  -0.05429077   0.06067824   0.99815738   0.\n",
      "   0.23228176   0.59835345   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           0.22772598\n",
      "   3.34256887 -14.34445     -2.25495148]\n",
      "Done: False\n",
      "\n",
      "Experience 158:\n",
      "State shape: [ -6.00032997  -0.05447674   0.07491772   0.99718972   0.\n",
      "  -0.09296631   0.71362197   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -0.05887604\n",
      "   3.29751539 -14.33010578  -2.25269651]\n",
      "Action shape: [ 0.         -0.47981322  0.64990073  0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00032997  -0.05447674   0.07491772   0.99718972   0.\n",
      "  -0.09296631   0.71362197   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -0.05887604\n",
      "   3.29751539 -14.33010578  -2.25269651]\n",
      "Done: False\n",
      "\n",
      "Experience 159:\n",
      "State shape: [ -6.00032997  -0.05447674   0.07491772   0.99718972   0.\n",
      "  -0.09296631   0.71362197   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -0.05887604\n",
      "   3.29751539 -14.33010578  -2.25269651]\n",
      "Action shape: [ 0.         -0.47981322  0.64990073  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00032997  -0.05447674   0.07491772   0.99718972   0.\n",
      "  -0.09296631   0.71362197   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -0.05887604\n",
      "   3.29751539 -14.33010578  -2.25269651]\n",
      "Done: False\n",
      "\n",
      "Experience 160:\n",
      "State shape: [ -6.00047302  -0.05477285   0.08963785   0.99597442   0.\n",
      "  -0.10129609   0.73851764   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -0.34519196\n",
      "   3.25250673 -14.31577587  -2.25044394]\n",
      "Action shape: [ 0.         -0.01524901  0.14036542  0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00047302  -0.05477285   0.08963785   0.99597442   0.\n",
      "  -0.10129609   0.73851764   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -0.34519196\n",
      "   3.25250673 -14.31577587  -2.25044394]\n",
      "Done: False\n",
      "\n",
      "Experience 161:\n",
      "State shape: [ -6.00047302  -0.05477285   0.08963785   0.99597442   0.\n",
      "  -0.10129609   0.73851764   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -0.34519196\n",
      "   3.25250673 -14.31577587  -2.25044394]\n",
      "Action shape: [ 0.         -0.01524901  0.14036542  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00047302  -0.05477285   0.08963785   0.99597442   0.\n",
      "  -0.10129609   0.73851764   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -0.34519196\n",
      "   3.25250673 -14.31577587  -2.25044394]\n",
      "Done: False\n",
      "\n",
      "Experience 162:\n",
      "State shape: [ -6.00061989  -0.04245186   0.10256479   0.99472633   0.\n",
      "   0.54012018   0.64935672   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -0.63122082\n",
      "   3.2075429  -14.30146027  -2.2481935 ]\n",
      "Action shape: [ 0.          0.9569108  -0.50270206  0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00061989  -0.04245186   0.10256479   0.99472633   0.\n",
      "   0.54012018   0.64935672   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -0.63122082\n",
      "   3.2075429  -14.30146027  -2.2481935 ]\n",
      "Done: False\n",
      "\n",
      "Experience 163:\n",
      "State shape: [ -6.00061989  -0.04245186   0.10256479   0.99472633   0.\n",
      "   0.54012018   0.64935672   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -0.63122082\n",
      "   3.2075429  -14.30146027  -2.2481935 ]\n",
      "Action shape: [ 0.          0.9569108  -0.50270206  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00061989  -0.04245186   0.10256479   0.99472633   0.\n",
      "   0.54012018   0.64935672   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -0.63122082\n",
      "   3.2075429  -14.30146027  -2.2481935 ]\n",
      "Done: False\n",
      "\n",
      "Experience 164:\n",
      "State shape: [ -6.00076199  -0.02529955   0.11368414   0.99351694   0.\n",
      "   0.79226452   0.55924958   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -0.91696358\n",
      "   3.16262388 -14.28715897  -2.24594545]\n",
      "Action shape: [ 0.          0.39352575 -0.5080372   0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00076199  -0.02529955   0.11368414   0.99351694   0.\n",
      "   0.79226452   0.55924958   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -0.91696358\n",
      "   3.16262388 -14.28715897  -2.24594545]\n",
      "Done: False\n",
      "\n",
      "Experience 165:\n",
      "State shape: [ -6.00076199  -0.02529955   0.11368414   0.99351694   0.\n",
      "   0.79226452   0.55924958   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -0.91696358\n",
      "   3.16262388 -14.28715897  -2.24594545]\n",
      "Action shape: [ 0.          0.39352575 -0.5080372   0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00076199  -0.02529955   0.11368414   0.99351694   0.\n",
      "   0.79226452   0.55924958   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -0.91696358\n",
      "   3.16262388 -14.28715897  -2.24594545]\n",
      "Done: False\n",
      "\n",
      "Experience 166:\n",
      "State shape: [ -6.00091648  -0.01605654   0.12465139   0.9922006    0.\n",
      "   0.39772561   0.55230069   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -1.20242119\n",
      "   3.11774969 -14.27287197  -2.24369955]\n",
      "Action shape: [ 0.         -0.56675243 -0.0391789   0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00091648  -0.01605654   0.12465139   0.9922006    0.\n",
      "   0.39772561   0.55230069   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -1.20242119\n",
      "   3.11774969 -14.27287197  -2.24369955]\n",
      "Done: False\n",
      "\n",
      "Experience 167:\n",
      "State shape: [ -6.00091648  -0.01605654   0.12465139   0.9922006    0.\n",
      "   0.39772561   0.55230069   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -1.20242119\n",
      "   3.11774969 -14.27287197  -2.24369955]\n",
      "Action shape: [ 0.         -0.56675243 -0.0391789   0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00091648  -0.01605654   0.12465139   0.9922006    0.\n",
      "   0.39772561   0.55230069   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -1.20242119\n",
      "   3.11774969 -14.27287197  -2.24369955]\n",
      "Done: False\n",
      "\n",
      "Experience 168:\n",
      "State shape: [ -6.00105143  -0.00983286   0.13346565   0.99105344   0.\n",
      "   0.25938919   0.44443136   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -1.4875927\n",
      "   3.07292032 -14.25859928  -2.24145579]\n",
      "Action shape: [ 0.         -0.19512945 -0.60818285  0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00105143  -0.00983286   0.13346565   0.99105344   0.\n",
      "   0.25938919   0.44443136   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -1.4875927\n",
      "   3.07292032 -14.25859928  -2.24145579]\n",
      "Done: False\n",
      "\n",
      "Experience 169:\n",
      "State shape: [ -6.00105143  -0.00983286   0.13346565   0.99105344   0.\n",
      "   0.25938919   0.44443136   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -1.4875927\n",
      "   3.07292032 -14.25859928  -2.24145579]\n",
      "Action shape: [ 0.         -0.19512945 -0.60818285  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00105143  -0.00983286   0.13346565   0.99105344   0.\n",
      "   0.25938919   0.44443136   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -1.4875927\n",
      "   3.07292032 -14.25859928  -2.24145579]\n",
      "Done: False\n",
      "\n",
      "Experience 170:\n",
      "State shape: [ -6.00117111   0.00555468   0.14081333   0.99003616   0.\n",
      "   0.72620589   0.37088922   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -1.77247906\n",
      "   3.02813578 -14.2443409   -2.23921442]\n",
      "Action shape: [ 0.         0.7064013 -0.4146413  0.         0.         0.       ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00117111   0.00555468   0.14081333   0.99003616   0.\n",
      "   0.72620589   0.37088922   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -1.77247906\n",
      "   3.02813578 -14.2443409   -2.23921442]\n",
      "Done: False\n",
      "\n",
      "Experience 171:\n",
      "State shape: [ -6.00117111   0.00555468   0.14081333   0.99003616   0.\n",
      "   0.72620589   0.37088922   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -1.77247906\n",
      "   3.02813578 -14.2443409   -2.23921442]\n",
      "Action shape: [ 0.         0.7064013 -0.4146413  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00117111   0.00555468   0.14081333   0.99003616   0.\n",
      "   0.72620589   0.37088922   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -1.77247906\n",
      "   3.02813578 -14.2443409   -2.23921442]\n",
      "Done: False\n",
      "\n",
      "Experience 172:\n",
      "State shape: [ -6.00127792   0.02919579   0.14707626   0.98912516   0.\n",
      "   1.14527142   0.31644261   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -2.05708122\n",
      "   2.98339605 -14.23009682  -2.23697519]\n",
      "Action shape: [ 0.          0.64890975 -0.3069778   0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00127792   0.02919579   0.14707626   0.98912516   0.\n",
      "   1.14527142   0.31644261   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -2.05708122\n",
      "   2.98339605 -14.23009682  -2.23697519]\n",
      "Done: False\n",
      "\n",
      "Experience 173:\n",
      "State shape: [ -6.00127792   0.02919579   0.14707626   0.98912516   0.\n",
      "   1.14527142   0.31644261   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -2.05708122\n",
      "   2.98339605 -14.23009682  -2.23697519]\n",
      "Action shape: [ 0.          0.64890975 -0.3069778   0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00127792   0.02919579   0.14707626   0.98912516   0.\n",
      "   1.14527142   0.31644261   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -2.05708122\n",
      "   2.98339605 -14.23009682  -2.23697519]\n",
      "Done: False\n",
      "\n",
      "Experience 174:\n",
      "State shape: [ -6.00139093   0.05806732   0.15340943   0.98816271   0.\n",
      "   1.40635288   0.32029462   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -2.34139872\n",
      "   2.93870115 -14.21586704  -2.23473835]\n",
      "Action shape: [0.         0.42501447 0.02171821 0.         0.         0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00139093   0.05806732   0.15340943   0.98816271   0.\n",
      "   1.40635288   0.32029462   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -2.34139872\n",
      "   2.93870115 -14.21586704  -2.23473835]\n",
      "Done: False\n",
      "\n",
      "Experience 175:\n",
      "State shape: [ -6.00139093   0.05806732   0.15340943   0.98816271   0.\n",
      "   1.40635288   0.32029462   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -2.34139872\n",
      "   2.93870115 -14.21586704  -2.23473835]\n",
      "Action shape: [0.         0.42501447 0.02171821 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00139093   0.05806732   0.15340943   0.98816271   0.\n",
      "   1.40635288   0.32029462   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -2.34139872\n",
      "   2.93870115 -14.21586704  -2.23473835]\n",
      "Done: False\n",
      "\n",
      "Experience 176:\n",
      "State shape: [ -6.00154686   0.08801413   0.16173414   0.98683437   0.\n",
      "   1.44842839   0.4215025    3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -2.62543154\n",
      "   2.89405107 -14.20165157  -2.23250365]\n",
      "Action shape: [0.         0.10506507 0.57062465 0.         0.         0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00154686   0.08801413   0.16173414   0.98683437   0.\n",
      "   1.44842839   0.4215025    3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -2.62543154\n",
      "   2.89405107 -14.20165157  -2.23250365]\n",
      "Done: False\n",
      "\n",
      "Experience 177:\n",
      "State shape: [ -6.00154686   0.08801413   0.16173414   0.98683437   0.\n",
      "   1.44842839   0.4215025    3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -2.62543154\n",
      "   2.89405107 -14.20165157  -2.23250365]\n",
      "Action shape: [0.         0.10506507 0.57062465 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00154686   0.08801413   0.16173414   0.98683437   0.\n",
      "   1.44842839   0.4215025    3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -2.62543154\n",
      "   2.89405107 -14.20165157  -2.23250365]\n",
      "Done: False\n",
      "\n",
      "Experience 178:\n",
      "State shape: [ -6.00178051   0.11710548   0.17342806   0.98484654   0.\n",
      "   1.38587332   0.59308684   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -2.90918064\n",
      "   2.84944582 -14.18745041  -2.2302711 ]\n",
      "Action shape: [ 0.         -0.0502656   0.96741706  0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00178051   0.11710548   0.17342806   0.98484654   0.\n",
      "   1.38587332   0.59308684   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -2.90918064\n",
      "   2.84944582 -14.18745041  -2.2302711 ]\n",
      "Done: False\n",
      "\n",
      "Experience 179:\n",
      "State shape: [ -6.00178051   0.11710548   0.17342806   0.98484654   0.\n",
      "   1.38587332   0.59308684   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -2.90918064\n",
      "   2.84944582 -14.18745041  -2.2302711 ]\n",
      "Action shape: [ 0.         -0.0502656   0.96741706  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00178051   0.11710548   0.17342806   0.98484654   0.\n",
      "   1.38587332   0.59308684   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -2.90918064\n",
      "   2.84944582 -14.18745041  -2.2302711 ]\n",
      "Done: False\n",
      "\n",
      "Experience 180:\n",
      "State shape: [ -6.00205803   0.15391493   0.18634616   0.98248415   0.\n",
      "   1.76455331   0.65662086   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -3.19264603\n",
      "   2.80488539 -14.17326355  -2.22804093]\n",
      "Action shape: [0.         0.6082137  0.35821408 0.         0.         0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00205803   0.15391493   0.18634616   0.98248415   0.\n",
      "   1.76455331   0.65662086   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -3.19264603\n",
      "   2.80488539 -14.17326355  -2.22804093]\n",
      "Done: False\n",
      "\n",
      "Experience 181:\n",
      "State shape: [ -6.00205803   0.15391493   0.18634616   0.98248415   0.\n",
      "   1.76455331   0.65662086   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -3.19264603\n",
      "   2.80488539 -14.17326355  -2.22804093]\n",
      "Action shape: [0.         0.6082137  0.35821408 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00205803   0.15391493   0.18634616   0.98248415   0.\n",
      "   1.76455331   0.65662086   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -3.19264603\n",
      "   2.80488539 -14.17326355  -2.22804093]\n",
      "Done: False\n",
      "\n",
      "Experience 182:\n",
      "State shape: [ -6.00243568   0.19350052   0.2025446    0.97927304   0.\n",
      "   1.88410091   0.82569218   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -3.47582769\n",
      "   2.76036882 -14.15909004  -2.22581291]\n",
      "Action shape: [0.         0.23173124 0.9532487  0.         0.         0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00243568   0.19350052   0.2025446    0.97927304   0.\n",
      "   1.88410091   0.82569218   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -3.47582769\n",
      "   2.76036882 -14.15909004  -2.22581291]\n",
      "Done: False\n",
      "\n",
      "Experience 183:\n",
      "State shape: [ -6.00243568   0.19350052   0.2025446    0.97927304   0.\n",
      "   1.88410091   0.82569218   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -3.47582769\n",
      "   2.76036882 -14.15909004  -2.22581291]\n",
      "Action shape: [0.         0.23173124 0.9532487  0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00243568   0.19350052   0.2025446    0.97927304   0.\n",
      "   1.88410091   0.82569218   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -3.47582769\n",
      "   2.76036882 -14.15909004  -2.22581291]\n",
      "Done: False\n",
      "\n",
      "Experience 184:\n",
      "State shape: [ -6.00278378   0.22294188   0.21636907   0.97631164   0.\n",
      "   1.39087224   0.70691127   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -3.75872612\n",
      "   2.71589708 -14.14493084  -2.22358704]\n",
      "Action shape: [ 0.         -0.68177044 -0.6697041   0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00278378   0.22294188   0.21636907   0.97631164   0.\n",
      "   1.39087224   0.70691127   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -3.75872612\n",
      "   2.71589708 -14.14493084  -2.22358704]\n",
      "Done: False\n",
      "\n",
      "Experience 185:\n",
      "State shape: [ -6.00278378   0.22294188   0.21636907   0.97631164   0.\n",
      "   1.39087224   0.70691127   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -3.75872612\n",
      "   2.71589708 -14.14493084  -2.22358704]\n",
      "Action shape: [ 0.         -0.68177044 -0.6697041   0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00278378   0.22294188   0.21636907   0.97631164   0.\n",
      "   1.39087224   0.70691127   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -3.75872612\n",
      "   2.71589708 -14.14493084  -2.22358704]\n",
      "Done: False\n",
      "\n",
      "Experience 186:\n",
      "State shape: [ -6.0031271    0.25592232   0.22915087   0.97339092   0.\n",
      "   1.57391      0.65556788   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -4.04134178\n",
      "   2.67147017 -14.13078594  -2.22136354]\n",
      "Action shape: [ 0.          0.31556562 -0.2894813   0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.0031271    0.25592232   0.22915087   0.97339092   0.\n",
      "   1.57391      0.65556788   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -4.04134178\n",
      "   2.67147017 -14.13078594  -2.22136354]\n",
      "Done: False\n",
      "\n",
      "Experience 187:\n",
      "State shape: [ -6.0031271    0.25592232   0.22915087   0.97339092   0.\n",
      "   1.57391      0.65556788   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -4.04134178\n",
      "   2.67147017 -14.13078594  -2.22136354]\n",
      "Action shape: [ 0.          0.31556562 -0.2894813   0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.0031271    0.25592232   0.22915087   0.97339092   0.\n",
      "   1.57391      0.65556788   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -4.04134178\n",
      "   2.67147017 -14.13078594  -2.22136354]\n",
      "Done: False\n",
      "\n",
      "Experience 188:\n",
      "State shape: [ -6.00350904   0.27884626   0.24255003   0.9701389    0.\n",
      "   1.0674597    0.68941289   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -4.32367468\n",
      "   2.62708712 -14.11665535  -2.2191422 ]\n",
      "Action shape: [ 0.        -0.7108427  0.1908232  0.         0.         0.       ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00350904   0.27884626   0.24255003   0.9701389    0.\n",
      "   1.0674597    0.68941289   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -4.32367468\n",
      "   2.62708712 -14.11665535  -2.2191422 ]\n",
      "Done: False\n",
      "\n",
      "Experience 189:\n",
      "State shape: [ -6.00350904   0.27884626   0.24255003   0.9701389    0.\n",
      "   1.0674597    0.68941289   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -4.32367468\n",
      "   2.62708712 -14.11665535  -2.2191422 ]\n",
      "Action shape: [ 0.        -0.7108427  0.1908232  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00350904   0.27884626   0.24255003   0.9701389    0.\n",
      "   1.0674597    0.68941289   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -4.32367468\n",
      "   2.62708712 -14.11665535  -2.2191422 ]\n",
      "Done: False\n",
      "\n",
      "Experience 190:\n",
      "State shape: [ -6.00396347   0.29767132   0.25753836   0.96626808   0.\n",
      "   0.85318696   0.77401268   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -4.60572529\n",
      "   2.58274889 -14.10253906  -2.216923  ]\n",
      "Action shape: [ 0.        -0.2887292  0.476986   0.         0.         0.       ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00396347   0.29767132   0.25753836   0.96626808   0.\n",
      "   0.85318696   0.77401268   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -4.60572529\n",
      "   2.58274889 -14.10253906  -2.216923  ]\n",
      "Done: False\n",
      "\n",
      "Experience 191:\n",
      "State shape: [ -6.00396347   0.29767132   0.25753836   0.96626808   0.\n",
      "   0.85318696   0.77401268   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -4.60572529\n",
      "   2.58274889 -14.10253906  -2.216923  ]\n",
      "Action shape: [ 0.        -0.2887292  0.476986   0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00396347   0.29767132   0.25753836   0.96626808   0.\n",
      "   0.85318696   0.77401268   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -4.60572529\n",
      "   2.58274889 -14.10253906  -2.216923  ]\n",
      "Done: False\n",
      "\n",
      "Experience 192:\n",
      "State shape: [ -6.00445747   0.32347202   0.27281261   0.96206719   0.\n",
      "   1.20028448   0.79207891   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -4.88749409\n",
      "   2.53845453 -14.08843708  -2.21470618]\n",
      "Action shape: [0.         0.5450033  0.10186002 0.         0.         0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00445747   0.32347202   0.27281261   0.96206719   0.\n",
      "   1.20028448   0.79207891   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -4.88749409\n",
      "   2.53845453 -14.08843708  -2.21470618]\n",
      "Done: False\n",
      "\n",
      "Experience 193:\n",
      "State shape: [ -6.00445747   0.32347202   0.27281261   0.96206719   0.\n",
      "   1.20028448   0.79207891   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -4.88749409\n",
      "   2.53845453 -14.08843708  -2.21470618]\n",
      "Action shape: [0.         0.5450033  0.10186002 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00445747   0.32347202   0.27281261   0.96206719   0.\n",
      "   1.20028448   0.79207891   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -4.88749409\n",
      "   2.53845453 -14.08843708  -2.21470618]\n",
      "Done: False\n",
      "\n",
      "Experience 194:\n",
      "State shape: [ -6.00492287   0.36080265   0.28640923   0.95810738   0.\n",
      "   1.78662813   0.70808029   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -5.16898108\n",
      "   2.494205   -14.07434845  -2.21249151]\n",
      "Action shape: [ 0.          0.9134479  -0.47359627  0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00492287   0.36080265   0.28640923   0.95810738   0.\n",
      "   1.78662813   0.70808029   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -5.16898108\n",
      "   2.494205   -14.07434845  -2.21249151]\n",
      "Done: False\n",
      "\n",
      "Experience 195:\n",
      "State shape: [ -6.00492287   0.36080265   0.28640923   0.95810738   0.\n",
      "   1.78662813   0.70808029   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -5.16898108\n",
      "   2.494205   -14.07434845  -2.21249151]\n",
      "Action shape: [ 0.          0.9134479  -0.47359627  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00492287   0.36080265   0.28640923   0.95810738   0.\n",
      "   1.78662813   0.70808029   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -5.16898108\n",
      "   2.494205   -14.07434845  -2.21249151]\n",
      "Done: False\n",
      "\n",
      "Experience 196:\n",
      "State shape: [ -6.00546646   0.40557909   0.30145201   0.95348135   0.\n",
      "   2.15043139   0.78691      3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -5.45018673\n",
      "   2.44999933 -14.06027412  -2.21027899]\n",
      "Action shape: [0.         0.59794486 0.44445315 0.         0.         0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00546646   0.40557909   0.30145201   0.95348135   0.\n",
      "   2.15043139   0.78691      3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -5.45018673\n",
      "   2.44999933 -14.06027412  -2.21027899]\n",
      "Done: False\n",
      "\n",
      "Experience 197:\n",
      "State shape: [ -6.00546646   0.40557909   0.30145201   0.95348135   0.\n",
      "   2.15043139   0.78691      3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -5.45018673\n",
      "   2.44999933 -14.06027412  -2.21027899]\n",
      "Action shape: [0.         0.59794486 0.44445315 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00546646   0.40557909   0.30145201   0.95348135   0.\n",
      "   2.15043139   0.78691      3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -5.45018673\n",
      "   2.44999933 -14.06027412  -2.21027899]\n",
      "Done: False\n",
      "\n",
      "Experience 198:\n",
      "State shape: [ -6.0059967    0.44009733   0.31536797   0.94896947   0.\n",
      "   1.64416778   0.73146272   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -5.73111105\n",
      "   2.40583754 -14.0462141   -2.20806885]\n",
      "Action shape: [ 0.         -0.6933068  -0.31261992  0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.0059967    0.44009733   0.31536797   0.94896947   0.\n",
      "   1.64416778   0.73146272   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -5.73111105\n",
      "   2.40583754 -14.0462141   -2.20806885]\n",
      "Done: False\n",
      "\n",
      "Experience 199:\n",
      "State shape: [ -6.0059967    0.44009733   0.31536797   0.94896947   0.\n",
      "   1.64416778   0.73146272   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -5.73111105\n",
      "   2.40583754 -14.0462141   -2.20806885]\n",
      "Action shape: [ 0.         -0.6933068  -0.31261992  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.0059967    0.44009733   0.31536797   0.94896947   0.\n",
      "   1.64416778   0.73146272   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -5.73111105\n",
      "   2.40583754 -14.0462141   -2.20806885]\n",
      "Done: False\n",
      "\n",
      "Experience 200:\n",
      "State shape: [ -6.00656128   0.47588968   0.32947774   0.94416334   0.\n",
      "   1.70672131   0.74529886   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -6.01175451\n",
      "   2.36172056 -14.03216839  -2.20586085]\n",
      "Action shape: [0.         0.14283077 0.07801011 0.         0.         0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00656128   0.47588968   0.32947774   0.94416334   0.\n",
      "   1.70672131   0.74529886   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -6.01175451\n",
      "   2.36172056 -14.03216839  -2.20586085]\n",
      "Done: False\n",
      "\n",
      "Experience 201:\n",
      "State shape: [ -6.00656128   0.47588968   0.32947774   0.94416334   0.\n",
      "   1.70672131   0.74529886   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -6.01175451\n",
      "   2.36172056 -14.03216839  -2.20586085]\n",
      "Action shape: [0.         0.14283077 0.07801011 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00656128   0.47588968   0.32947774   0.94416334   0.\n",
      "   1.70672131   0.74529886   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -6.01175451\n",
      "   2.36172056 -14.03216839  -2.20586085]\n",
      "Done: False\n",
      "\n",
      "Experience 202:\n",
      "State shape: [ -6.00713348   0.51371384   0.343117     0.93929267   0.\n",
      "   1.81108212   0.72414893   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -6.29211712\n",
      "   2.31764746 -14.01813602  -2.203655  ]\n",
      "Action shape: [ 0.          0.20727187 -0.11924635  0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00713348   0.51371384   0.343117     0.93929267   0.\n",
      "   1.81108212   0.72414893   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -6.29211712\n",
      "   2.31764746 -14.01813602  -2.203655  ]\n",
      "Done: False\n",
      "\n",
      "Experience 203:\n",
      "State shape: [ -6.00713348   0.51371384   0.343117     0.93929267   0.\n",
      "   1.81108212   0.72414893   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -6.29211712\n",
      "   2.31764746 -14.01813602  -2.203655  ]\n",
      "Action shape: [ 0.          0.20727187 -0.11924635  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00713348   0.51371384   0.343117     0.93929267   0.\n",
      "   1.81108212   0.72414893   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -6.29211712\n",
      "   2.31764746 -14.01813602  -2.203655  ]\n",
      "Done: False\n",
      "\n",
      "Experience 204:\n",
      "State shape: [ -6.00777435   0.53971291   0.35769831   0.9338372    0.\n",
      "   1.21427333   0.778431     3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -6.57219934\n",
      "   2.27361822 -14.00411797  -2.2014513 ]\n",
      "Action shape: [ 0.         -0.83897406  0.3060503   0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00777435   0.53971291   0.35769831   0.9338372    0.\n",
      "   1.21427333   0.778431     3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -6.57219934\n",
      "   2.27361822 -14.00411797  -2.2014513 ]\n",
      "Done: False\n",
      "\n",
      "Experience 205:\n",
      "State shape: [ -6.00777435   0.53971291   0.35769831   0.9338372    0.\n",
      "   1.21427333   0.778431     3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -6.57219934\n",
      "   2.27361822 -14.00411797  -2.2014513 ]\n",
      "Action shape: [ 0.         -0.83897406  0.3060503   0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00777435   0.53971291   0.35769831   0.9338372    0.\n",
      "   1.21427333   0.778431     3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -6.57219934\n",
      "   2.27361822 -14.00411797  -2.2014513 ]\n",
      "Done: False\n",
      "\n",
      "Experience 206:\n",
      "State shape: [ -6.00832605   0.56492186   0.36971346   0.92914582   0.\n",
      "   1.18986034   0.64493257   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -6.85200167\n",
      "   2.22963285 -13.99011421  -2.19924998]\n",
      "Action shape: [ 0.        -0.000191  -0.7526834  0.         0.         0.       ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00832605   0.56492186   0.36971346   0.92914582   0.\n",
      "   1.18986034   0.64493257   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -6.85200167\n",
      "   2.22963285 -13.99011421  -2.19924998]\n",
      "Done: False\n",
      "\n",
      "Experience 207:\n",
      "State shape: [ -6.00832605   0.56492186   0.36971346   0.92914582   0.\n",
      "   1.18986034   0.64493257   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -6.85200167\n",
      "   2.22963285 -13.99011421  -2.19924998]\n",
      "Action shape: [ 0.        -0.000191  -0.7526834  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00832605   0.56492186   0.36971346   0.92914582   0.\n",
      "   1.18986034   0.64493257   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -6.85200167\n",
      "   2.22963285 -13.99011421  -2.19924998]\n",
      "Done: False\n",
      "\n",
      "Experience 208:\n",
      "State shape: [ -6.00890827   0.59874964   0.38193492   0.92418922   0.\n",
      "   1.61959279   0.65942109   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -7.13152409\n",
      "   2.18569231 -13.97612381  -2.19705081]\n",
      "Action shape: [0.         0.678752   0.08168843 0.         0.         0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00890827   0.59874964   0.38193492   0.92418922   0.\n",
      "   1.61959279   0.65942109   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -7.13152409\n",
      "   2.18569231 -13.97612381  -2.19705081]\n",
      "Done: False\n",
      "\n",
      "Experience 209:\n",
      "State shape: [ -6.00890827   0.59874964   0.38193492   0.92418922   0.\n",
      "   1.61959279   0.65942109   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -7.13152409\n",
      "   2.18569231 -13.97612381  -2.19705081]\n",
      "Action shape: [0.         0.678752   0.08168843 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00890827   0.59874964   0.38193492   0.92418922   0.\n",
      "   1.61959279   0.65942109   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -7.13152409\n",
      "   2.18569231 -13.97612381  -2.19705081]\n",
      "Done: False\n",
      "\n",
      "Experience 210:\n",
      "State shape: [ -6.00959396   0.63869429   0.39575979   0.91835407   0.\n",
      "   1.91601002   0.75030029   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -7.41076708\n",
      "   2.14179564 -13.96214771  -2.19485378]\n",
      "Action shape: [0.         0.4920951  0.51239014 0.         0.         0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00959396   0.63869429   0.39575979   0.91835407   0.\n",
      "   1.91601002   0.75030029   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -7.41076708\n",
      "   2.14179564 -13.96214771  -2.19485378]\n",
      "Done: False\n",
      "\n",
      "Experience 211:\n",
      "State shape: [ -6.00959396   0.63869429   0.39575979   0.91835407   0.\n",
      "   1.91601002   0.75030029   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -7.41076708\n",
      "   2.14179564 -13.96214771  -2.19485378]\n",
      "Action shape: [0.         0.4920951  0.51239014 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00959396   0.63869429   0.39575979   0.91835407   0.\n",
      "   1.91601002   0.75030029   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -7.41076708\n",
      "   2.14179564 -13.96214771  -2.19485378]\n",
      "Done: False\n",
      "\n",
      "Experience 212:\n",
      "State shape: [ -6.01031017   0.6901207    0.40961104   0.91226027   0.\n",
      "   2.48991871   0.75662965   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -7.68973064\n",
      "   2.09794283 -13.94818592  -2.1926589 ]\n",
      "Action shape: [0.         0.91626084 0.03568567 0.         0.         0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.01031017   0.6901207    0.40961104   0.91226027   0.\n",
      "   2.48991871   0.75662965   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -7.68973064\n",
      "   2.09794283 -13.94818592  -2.1926589 ]\n",
      "Done: False\n",
      "\n",
      "Experience 213:\n",
      "State shape: [ -6.01031017   0.6901207    0.40961104   0.91226027   0.\n",
      "   2.48991871   0.75662965   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -7.68973064\n",
      "   2.09794283 -13.94818592  -2.1926589 ]\n",
      "Action shape: [0.         0.91626084 0.03568567 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.01031017   0.6901207    0.40961104   0.91226027   0.\n",
      "   2.48991871   0.75662965   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -7.68973064\n",
      "   2.09794283 -13.94818592  -2.1926589 ]\n",
      "Done: False\n",
      "\n",
      "Experience 214:\n",
      "State shape: [ -6.01097965   0.75000906   0.42206494   0.9065656    0.\n",
      "   2.92124724   0.68471074   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -7.58350945\n",
      "   1.89370394   6.42243195 -10.62292671]\n",
      "Action shape: [ 0.         0.7200541 -0.405489   0.         0.         0.       ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.01097965   0.75000906   0.42206494   0.9065656    0.\n",
      "   2.92124724   0.68471074   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -7.58350945\n",
      "   1.89370394   6.42243195 -10.62292671]\n",
      "Done: True\n",
      "\n",
      "Experience 215:\n",
      "State shape: [ -6.01097965   0.75000906   0.42206494   0.9065656    0.\n",
      "   2.92124724   0.68471074   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -7.58350945\n",
      "   1.89370394   6.42243195 -10.62292671]\n",
      "Action shape: [ 0.         0.7200541 -0.405489   0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.01097965   0.75000906   0.42206494   0.9065656    0.\n",
      "   2.92124724   0.68471074   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -7.58350945\n",
      "   1.89370394   6.42243195 -10.62292671]\n",
      "Done: True\n",
      "\n",
      "Experience 216:\n",
      "State shape: [ -6.          -0.00877285   0.00233326   0.99999728   0.\n",
      "  -0.45234597   0.11666287   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           5.54241753\n",
      "   2.14956141 -15.8434267   -4.22105551]\n",
      "Action shape: [ 0.         -0.67698044  0.6577621   0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.          -0.00877285   0.00233326   0.99999728   0.\n",
      "  -0.45234597   0.11666287   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           5.54241753\n",
      "   2.14956141 -15.8434267   -4.22105551]\n",
      "Done: False\n",
      "\n",
      "Experience 217:\n",
      "State shape: [ -6.00000095  -0.00507593   0.00441054   0.99999027   0.\n",
      "   0.17264034   0.10386472   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           5.22586632\n",
      "   2.06522512 -15.82758331  -4.21683455]\n",
      "Action shape: [ 0.          0.92181414 -0.07215782  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00000095  -0.00507593   0.00441054   0.99999027   0.\n",
      "   0.17264034   0.10386472   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           5.22586632\n",
      "   2.06522512 -15.82758331  -4.21683455]\n",
      "Done: False\n",
      "\n",
      "Experience 218:\n",
      "State shape: [ -6.00000286  -0.00128937   0.00735797   0.99997293   0.\n",
      "   0.17202924   0.1473743    5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           4.90963078\n",
      "   1.98097277 -15.81175613  -4.21261787]\n",
      "Action shape: [0.         0.00425291 0.24531333 0.         0.         0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00000286  -0.00128937   0.00735797   0.99997293   0.\n",
      "   0.17202924   0.1473743    5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           4.90963078\n",
      "   1.98097277 -15.81175613  -4.21261787]\n",
      "Done: False\n",
      "\n",
      "Experience 219:\n",
      "State shape: [ -6.00000954   0.01296091   0.0127995    0.99991808   0.\n",
      "   0.68054157   0.27209041   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           4.59371185\n",
      "   1.89680433 -15.79594421  -4.20840549]\n",
      "Action shape: [0.        0.7661881 0.7031673 0.        0.        0.       ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00000954   0.01296091   0.0127995    0.99991808   0.\n",
      "   0.68054157   0.27209041   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           4.59371185\n",
      "   1.89680433 -15.79594421  -4.20840549]\n",
      "Done: False\n",
      "\n",
      "Experience 220:\n",
      "State shape: [ -6.00001955   0.03406048   0.01824435   0.99983356   0.\n",
      "   1.02297664   0.27227601   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           4.2781086\n",
      "   1.81272078 -15.78014851  -4.20419693]\n",
      "Action shape: [0.         0.53285766 0.00104657 0.         0.         0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00001955   0.03406048   0.01824435   0.99983356   0.\n",
      "   1.02297664   0.27227601   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           4.2781086\n",
      "   1.81272078 -15.78014851  -4.20419693]\n",
      "Done: False\n",
      "\n",
      "Experience 221:\n",
      "State shape: [ -6.00003529   0.0536027    0.02445688   0.99970089   0.\n",
      "   0.94062001   0.31069779   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           3.96282101\n",
      "   1.72872114 -15.76436901  -4.19999266]\n",
      "Action shape: [ 0.         -0.09263509  0.21662752  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00003529   0.0536027    0.02445688   0.99970089   0.\n",
      "   0.94062001   0.31069779   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           3.96282101\n",
      "   1.72872114 -15.76436901  -4.19999266]\n",
      "Done: False\n",
      "\n",
      "Experience 222:\n",
      "State shape: [ -6.00006676   0.06210184   0.03366985   0.99943301   0.\n",
      "   0.37084138   0.46084446   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           3.64784908\n",
      "   1.64480543 -15.74860477  -4.19579268]\n",
      "Action shape: [ 0.        -0.8245754  0.8465486  0.         0.         0.       ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00006676   0.06210184   0.03366985   0.99943301   0.\n",
      "   0.37084138   0.46084446   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           3.64784908\n",
      "   1.64480543 -15.74860477  -4.19579268]\n",
      "Done: False\n",
      "\n",
      "Experience 223:\n",
      "State shape: [ -6.00011301   0.06048632   0.04386393   0.99903751   0.\n",
      "  -0.14070176   0.51008993   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           3.33319187\n",
      "   1.56097364 -15.73285675  -4.19159698]\n",
      "Action shape: [ 0.         -0.7544748   0.27765322  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00011301   0.06048632   0.04386393   0.99903751   0.\n",
      "  -0.14070176   0.51008993   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           3.33319187\n",
      "   1.56097364 -15.73285675  -4.19159698]\n",
      "Done: False\n",
      "\n",
      "Experience 224:\n",
      "State shape: [ -6.0001688    0.07146645   0.05358624   0.99856323   0.\n",
      "   0.49191448   0.48669547   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           3.01884937\n",
      "   1.47722578 -15.71712399  -4.18740559]\n",
      "Action shape: [ 0.          0.94256127 -0.13190144  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.0001688    0.07146645   0.05358624   0.99856323   0.\n",
      "   0.49191448   0.48669547   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           3.01884937\n",
      "   1.47722578 -15.71712399  -4.18740559]\n",
      "Done: False\n",
      "\n",
      "Experience 225:\n",
      "State shape: [ -6.00023079   0.08499432   0.06266416   0.99803467   0.\n",
      "   0.62303579   0.45466626   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           2.70482159\n",
      "   1.39356184 -15.70140743  -4.183218  ]\n",
      "Action shape: [ 0.          0.21095988 -0.18058531  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00023079   0.08499432   0.06266416   0.99803467   0.\n",
      "   0.62303579   0.45466626   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           2.70482159\n",
      "   1.39356184 -15.70140743  -4.183218  ]\n",
      "Done: False\n",
      "\n",
      "Experience 226:\n",
      "State shape: [ -6.00031328   0.09466648   0.07298327   0.99733317   0.\n",
      "   0.42297325   0.51714903   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           2.39110756\n",
      "   1.30998135 -15.68570614  -4.17903471]\n",
      "Action shape: [ 0.         -0.28076464  0.35228696  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00031328   0.09466648   0.07298327   0.99733317   0.\n",
      "   0.42297325   0.51714903   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           2.39110756\n",
      "   1.30998135 -15.68570614  -4.17903471]\n",
      "Done: False\n",
      "\n",
      "Experience 227:\n",
      "State shape: [ -6.00041771   0.11686087   0.08425097   0.99644457   0.\n",
      "   1.04354024   0.56513709   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           2.07770729\n",
      "   1.2264843  -15.67002106  -4.17485571]\n",
      "Action shape: [0.         0.94140023 0.2705635  0.         0.         0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00041771   0.11686087   0.08425097   0.99644457   0.\n",
      "   1.04354024   0.56513709   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           2.07770729\n",
      "   1.2264843  -15.67002106  -4.17485571]\n",
      "Done: False\n",
      "\n",
      "Experience 228:\n",
      "State shape: [ -6.0005765    0.12931776   0.0989247    0.99509492   0.\n",
      "   0.53663039   0.7367903    5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           1.76461983\n",
      "   1.1430707  -15.65435123  -4.170681  ]\n",
      "Action shape: [ 0.        -0.7274054  0.9678058  0.         0.         0.       ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.0005765    0.12931776   0.0989247    0.99509492   0.\n",
      "   0.53663039   0.7367903    5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           1.76461983\n",
      "   1.1430707  -15.65435123  -4.170681  ]\n",
      "Done: False\n",
      "\n",
      "Experience 229:\n",
      "State shape: [ -6.00075531   0.15444088   0.11318832   0.99357355   0.\n",
      "   1.17235804   0.71723241   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           1.45184612\n",
      "   1.05974054 -15.63869667  -4.16651058]\n",
      "Action shape: [ 0.          0.9674916  -0.11027028  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00075531   0.15444088   0.11318832   0.99357355   0.\n",
      "   1.17235804   0.71723241   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           1.45184612\n",
      "   1.05974054 -15.63869667  -4.16651058]\n",
      "Done: False\n",
      "\n",
      "Experience 230:\n",
      "State shape: [ -6.00094223   0.16915035   0.12636504   0.99198381   0.\n",
      "   0.65802634   0.6636191    5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           1.13938522\n",
      "   0.97649384 -15.62305832  -4.16234398]\n",
      "Action shape: [ 0.         -0.7346572  -0.30227953  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00094223   0.16915035   0.12636504   0.99198381   0.\n",
      "   0.65802634   0.6636191    5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           1.13938522\n",
      "   0.97649384 -15.62305832  -4.16234398]\n",
      "Done: False\n",
      "\n",
      "Experience 231:\n",
      "State shape: [ -6.00117111   0.18212748   0.14084073   0.99003227   0.\n",
      "   0.56380862   0.73033863   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           0.82723618\n",
      "   0.8933301  -15.60743523  -4.15818167]\n",
      "Action shape: [ 0.        -0.1213102  0.3761744  0.         0.         0.       ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00117111   0.18212748   0.14084073   0.99003227   0.\n",
      "   0.56380862   0.73033863   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           0.82723618\n",
      "   0.8933301  -15.60743523  -4.15818167]\n",
      "Done: False\n",
      "\n",
      "Experience 232:\n",
      "State shape: [ -6.00142527   0.20522785   0.15527122   0.98787188   0.\n",
      "   1.07022786   0.72957218   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           0.51539993\n",
      "   0.81024981 -15.59182835  -4.15402365]\n",
      "Action shape: [ 0.          0.77478236 -0.00432125  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00142527   0.20522785   0.15527122   0.98787188   0.\n",
      "   1.07022786   0.72957218   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           0.51539993\n",
      "   0.81024981 -15.59182835  -4.15402365]\n",
      "Done: False\n",
      "\n",
      "Experience 233:\n",
      "State shape: [ -6.00168467   0.23394966   0.16872098   0.98566385   0.\n",
      "   1.35706985   0.68149459   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           0.20387554\n",
      "   0.72725248 -15.57623672  -4.14986944]\n",
      "Action shape: [ 0.          0.46132144 -0.27106854  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00168467   0.23394966   0.16872098   0.98566385   0.\n",
      "   1.35706985   0.68149459   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           0.20387554\n",
      "   0.72725248 -15.57623672  -4.14986944]\n",
      "Done: False\n",
      "\n",
      "Experience 234:\n",
      "State shape: [ -6.00202036   0.26370478   0.18464229   0.98280579   0.\n",
      "   1.39420891   0.80879939   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -0.10733795\n",
      "   0.64433813 -15.56066036  -4.14571953]\n",
      "Action shape: [0.         0.09620208 0.7177627  0.         0.         0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00202036   0.26370478   0.18464229   0.98280579   0.\n",
      "   1.39420891   0.80879939   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -0.10733795\n",
      "   0.64433813 -15.56066036  -4.14571953]\n",
      "Done: False\n",
      "\n",
      "Experience 235:\n",
      "State shape: [ -6.00239849   0.29602242   0.20101046   0.97958909   0.\n",
      "   1.51969993   0.83407253   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -0.41823959\n",
      "   0.56150675 -15.54510021  -4.14157391]\n",
      "Action shape: [0.         0.229541   0.14249356 0.         0.         0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00239849   0.29602242   0.20101046   0.97958909   0.\n",
      "   1.51969993   0.83407253   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -0.41823959\n",
      "   0.56150675 -15.54510021  -4.14157391]\n",
      "Done: False\n",
      "\n",
      "Experience 236:\n",
      "State shape: [ -6.00272131   0.32744837   0.21396395   0.97684156   0.\n",
      "   1.49519956   0.66208851   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -0.72883034\n",
      "   0.47875834 -15.52955532  -4.13743258]\n",
      "Action shape: [ 0.          0.00882026 -0.96967053  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00272131   0.32744837   0.21396395   0.97684156   0.\n",
      "   1.49519956   0.66208851   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -0.72883034\n",
      "   0.47875834 -15.52955532  -4.13743258]\n",
      "Done: False\n",
      "\n",
      "Experience 237:\n",
      "State shape: [ -6.00298119   0.35259914   0.22382657   0.97462899   0.\n",
      "   1.19961452   0.50539029   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -1.03911114\n",
      "   0.39609241 -15.51402569  -4.13329506]\n",
      "Action shape: [ 0.         -0.39761797 -0.8834874   0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00298119   0.35259914   0.22382657   0.97462899   0.\n",
      "   1.19961452   0.50539029   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -1.03911114\n",
      "   0.39609241 -15.51402569  -4.13329506]\n",
      "Done: False\n",
      "\n",
      "Experience 238:\n",
      "State shape: [ -6.00333118   0.36525488   0.23640894   0.97165365   0.\n",
      "   0.55884415   0.64647245   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -1.34908104\n",
      "   0.31350899 -15.49851227  -4.12916183]\n",
      "Action shape: [ 0.         -0.9230693   0.79544157  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00333118   0.36525488   0.23640894   0.97165365   0.\n",
      "   0.55884415   0.64647245   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -1.34908104\n",
      "   0.31350899 -15.49851227  -4.12916183]\n",
      "Done: False\n",
      "\n",
      "Experience 239:\n",
      "State shape: [ -6.0037365    0.37685537   0.25017621   0.96820032   0.\n",
      "   0.49912271   0.70969403   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -1.658741\n",
      "   0.23100853 -15.48301411  -4.1250329 ]\n",
      "Action shape: [ 0.         -0.07265176  0.35645247  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.0037365    0.37685537   0.25017621   0.96820032   0.\n",
      "   0.49912271   0.70969403   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -1.658741\n",
      "   0.23100853 -15.48301411  -4.1250329 ]\n",
      "Done: False\n",
      "\n",
      "Experience 240:\n",
      "State shape: [ -6.00422049   0.37671995   0.2656002    0.96408326   0.\n",
      "  -0.09738334   0.79820967   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -1.96809196\n",
      "   0.14859056 -15.4675312   -4.12090778]\n",
      "Action shape: [ 0.        -0.8777905  0.499064   0.         0.         0.       ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00422049   0.37671995   0.2656002    0.96408326   0.\n",
      "  -0.09738334   0.79820967   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -1.96809196\n",
      "   0.14859056 -15.4675312   -4.12090778]\n",
      "Done: False\n",
      "\n",
      "Experience 241:\n",
      "State shape: [ -6.00480938   0.37224197   0.2831577    0.95907336   0.\n",
      "  -0.3270424    0.91292673   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -2.27713299\n",
      "   0.06625462 -15.45206356  -4.11678696]\n",
      "Action shape: [ 0.         -0.34662235  0.6467914   0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00480938   0.37224197   0.2831577    0.95907336   0.\n",
      "  -0.3270424    0.91292673   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -2.27713299\n",
      "   0.06625462 -15.45206356  -4.11678696]\n",
      "Done: False\n",
      "\n",
      "Experience 242:\n",
      "State shape: [ -6.00546741   0.36192989   0.3014879    0.95347      0.\n",
      "  -0.62332737   0.95839125   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -2.58586502\n",
      "  -0.01599884 -15.43661213  -4.11267042]\n",
      "Action shape: [ 0.         -0.45320868  0.25633553  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00546741   0.36192989   0.3014879    0.95347      0.\n",
      "  -0.62332737   0.95839125   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -2.58586502\n",
      "  -0.01599884 -15.43661213  -4.11267042]\n",
      "Done: False\n",
      "\n",
      "Experience 243:\n",
      "State shape: [ -6.00619984   0.34386063   0.32052612   0.94723968   0.\n",
      "  -1.01531374   1.00160325   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -2.89428854\n",
      "  -0.0981698  -15.42117596  -4.1085577 ]\n",
      "Action shape: [ 0.        -0.6053037  0.2436352  0.         0.         0.       ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00619984   0.34386063   0.32052612   0.94723968   0.\n",
      "  -1.01531374   1.00160325   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -2.89428854\n",
      "  -0.0981698  -15.42117596  -4.1085577 ]\n",
      "Done: False\n",
      "\n",
      "Experience 244:\n",
      "State shape: [ -6.00694609   0.33490992   0.33871565   0.94088879   0.\n",
      "  -0.55441928   0.96333259   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -3.20240355\n",
      "  -0.18025875 -15.40575504  -4.10444927]\n",
      "Action shape: [ 0.          0.6593837  -0.21577543  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00694609   0.33490992   0.33871565   0.94088879   0.\n",
      "  -0.55441928   0.96333259   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -3.20240355\n",
      "  -0.18025875 -15.40575504  -4.10444927]\n",
      "Done: False\n",
      "\n",
      "Experience 245:\n",
      "State shape: [ -6.00771284   0.3289609    0.35632348   0.93436266   0.\n",
      "  -0.40089247   0.93893045   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -3.51021051\n",
      "  -0.26226568 -15.39034939  -4.10034466]\n",
      "Action shape: [ 0.          0.21317317 -0.13758287  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00771284   0.3289609    0.35632348   0.93436266   0.\n",
      "  -0.40089247   0.93893045   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -3.51021051\n",
      "  -0.26226568 -15.39034939  -4.10034466]\n",
      "Done: False\n",
      "\n",
      "Experience 246:\n",
      "State shape: [ -6.00864697   0.31054354   0.37650577   0.92641427   0.\n",
      "  -1.03943872   1.08457363   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -3.81770992\n",
      "  -0.3441906  -15.37495899  -4.09624434]\n",
      "Action shape: [ 0.        -0.967647   0.8211572  0.         0.         0.       ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00864697   0.31054354   0.37650577   0.92641427   0.\n",
      "  -1.03943872   1.08457363   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -3.81770992\n",
      "  -0.3441906  -15.37495899  -4.09624434]\n",
      "Done: False\n",
      "\n",
      "Experience 247:\n",
      "State shape: [ -6.00974655   0.29562664   0.39875314   0.9170583    0.\n",
      "  -0.8765586    1.20676076   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -4.12490177\n",
      "  -0.4260335  -15.35958385  -4.0921483 ]\n",
      "Action shape: [0.         0.21265379 0.6889087  0.         0.         0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00974655   0.29562664   0.39875314   0.9170583    0.\n",
      "  -0.8765586    1.20676076   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -4.12490177\n",
      "  -0.4260335  -15.35958385  -4.0921483 ]\n",
      "Done: False\n",
      "\n",
      "Experience 248:\n",
      "State shape: [ -6.01106358   0.29394102   0.42360577   0.90584665   0.\n",
      "  -0.23028722   1.36326838   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -4.43178606\n",
      "  -0.50779486 -15.34422493  -4.08805609]\n",
      "Action shape: [0.         0.94097173 0.8824122  0.         0.         0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.01106358   0.29394102   0.42360577   0.90584665   0.\n",
      "  -0.23028722   1.36326838   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -4.43178606\n",
      "  -0.50779486 -15.34422493  -4.08805609]\n",
      "Done: False\n",
      "\n",
      "Experience 249:\n",
      "State shape: [ -6.01257992   0.30315733   0.4501625    0.89294665   0.\n",
      "   0.30476734   1.47625637   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -4.73836374\n",
      "  -0.5894742  -15.32888126  -4.08396816]\n",
      "Action shape: [0.        0.793869  0.6370429 0.        0.        0.       ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.01257992   0.30315733   0.4501625    0.89294665   0.\n",
      "   0.30476734   1.47625637   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -4.73836374\n",
      "  -0.5894742  -15.32888126  -4.08396816]\n",
      "Done: False\n",
      "\n",
      "Experience 250:\n",
      "State shape: [ -6.01424885   0.31458235   0.47729444   0.87874343   0.\n",
      "   0.41183978   1.53129494   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -5.04463482\n",
      "  -0.67107201 -15.31355286  -4.07988405]\n",
      "Action shape: [0.         0.16936676 0.31031522 0.         0.         0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.01424885   0.31458235   0.47729444   0.87874343   0.\n",
      "   0.41183978   1.53129494   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -5.04463482\n",
      "  -0.67107201 -15.31355286  -4.07988405]\n",
      "Done: False\n",
      "\n",
      "Experience 251:\n",
      "State shape: [ -6.01594543   0.33622169   0.50296903   0.86430443   0.\n",
      "   0.93111312   1.47286594   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -5.35059977\n",
      "  -0.75258827 -15.29823971  -4.07580423]\n",
      "Action shape: [ 0.          0.78947103 -0.32943112  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.01594543   0.33622169   0.50296903   0.86430443   0.\n",
      "   0.93111312   1.47286594   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -5.35059977\n",
      "  -0.75258827 -15.29823971  -4.07580423]\n",
      "Done: False\n",
      "\n",
      "Experience 252:\n",
      "State shape: [ -6.01793432   0.36364841   0.53098969   0.84737828   0.\n",
      "   1.20671403   1.63687789   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -5.65625858\n",
      "  -0.834023   -15.28294182  -4.07172871]\n",
      "Action shape: [0.         0.440334   0.92472285 0.         0.         0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.01793432   0.36364841   0.53098969   0.84737828   0.\n",
      "   1.20671403   1.63687789   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -5.65625858\n",
      "  -0.834023   -15.28294182  -4.07172871]\n",
      "Done: False\n",
      "\n",
      "Experience 253:\n",
      "State shape: [ -6.02018738   0.39506531   0.56042116   0.82820778   0.\n",
      "   1.39792955   1.75630713   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -5.96161175\n",
      "  -0.91537619 -15.26765919  -4.06765699]\n",
      "Action shape: [0.         0.32229227 0.6733591  0.         0.         0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.02018738   0.39506531   0.56042116   0.82820778   0.\n",
      "   1.39792955   1.75630713   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -5.96161175\n",
      "  -0.91537619 -15.26765919  -4.06765699]\n",
      "Done: False\n",
      "\n",
      "Experience 254:\n",
      "State shape: [ -6.02251244   0.42610216   0.58860954   0.80841747   0.\n",
      "   1.38620174   1.72217929   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -6.26665974\n",
      "  -0.99664783 -15.25239182  -4.06358957]\n",
      "Action shape: [ 0.          0.02429085 -0.19241749  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.02251244   0.42610216   0.58860954   0.80841747   0.\n",
      "   1.38620174   1.72217929   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -6.26665974\n",
      "  -0.99664783 -15.25239182  -4.06358957]\n",
      "Done: False\n",
      "\n",
      "Experience 255:\n",
      "State shape: [ -6.02499104   0.45930529   0.61653449   0.7873279    0.\n",
      "   1.49609315   1.74978495   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -6.57140255\n",
      "  -1.07783842 -15.2371397   -4.05952597]\n",
      "Action shape: [0.         0.20595504 0.1556446  0.         0.         0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.02499104   0.45930529   0.61653449   0.7873279    0.\n",
      "   1.49609315   1.74978495   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -6.57140255\n",
      "  -1.07783842 -15.2371397   -4.05952597]\n",
      "Done: False\n",
      "\n",
      "Experience 256:\n",
      "State shape: [ -6.02751923   0.4968195    0.64306713   0.76580981   0.\n",
      "   1.71980536   1.70816028   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -6.87584019\n",
      "  -1.15894794 -15.22190285  -4.05546665]\n",
      "Action shape: [ 0.          0.3795884  -0.23468606  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.02751923   0.4968195    0.64306713   0.76580981   0.\n",
      "   1.71980536   1.70816028   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -6.87584019\n",
      "  -1.15894794 -15.22190285  -4.05546665]\n",
      "Done: False\n",
      "\n",
      "Experience 257:\n",
      "State shape: [ -6.0299201    0.53742838   0.66664088   0.74537906   0.\n",
      "   1.89194489   1.55982113   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -7.17997408\n",
      "  -1.23997641 -15.20668125  -4.05141115]\n",
      "Action shape: [ 0.          0.3091008  -0.83635765  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.0299201    0.53742838   0.66664088   0.74537906   0.\n",
      "   1.89194489   1.55982113   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -7.17997408\n",
      "  -1.23997641 -15.20668125  -4.05141115]\n",
      "Done: False\n",
      "\n",
      "Experience 258:\n",
      "State shape: [ -6.03247643   0.56728745   0.69019415   0.72362424   0.\n",
      "   1.35454381   1.60321295   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -7.48380375\n",
      "  -1.32092381 -15.19147491  -4.04735994]\n",
      "Action shape: [ 0.         -0.7476443   0.24464935  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.03247643   0.56728745   0.69019415   0.72362424   0.\n",
      "   1.35454381   1.60321295   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -7.48380375\n",
      "  -1.32092381 -15.19147491  -4.04735994]\n",
      "Done: False\n",
      "\n",
      "Experience 259:\n",
      "State shape: [ -6.0352993    0.59393358   0.71453547   0.69959921   0.\n",
      "   1.18930888   1.71012962   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -7.7873292\n",
      "  -1.40179014 -15.17628384  -4.04331255]\n",
      "Action shape: [ 0.         -0.20674616  0.60281175  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.0352993    0.59393358   0.71453547   0.69959921   0.\n",
      "   1.18930888   1.71012962   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -7.7873292\n",
      "  -1.40179014 -15.17628384  -4.04331255]\n",
      "Done: False\n",
      "\n",
      "Experience 260:\n",
      "State shape: [ -6.03819561   0.62088537   0.73785835   0.6749556    0.\n",
      "   1.21055174   1.69659495   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -8.09055138\n",
      "  -1.48257542 -15.16110802  -4.03926945]\n",
      "Action shape: [ 0.          0.06739037 -0.0763104   0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.03819561   0.62088537   0.73785835   0.6749556    0.\n",
      "   1.21055174   1.69659495   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -8.09055138\n",
      "  -1.48257542 -15.16110802  -4.03926945]\n",
      "Done: True\n",
      "\n",
      "Experience 261:\n",
      "State shape: [ -6.           0.00200891  -0.0008978    0.9999996    0.\n",
      "   0.10571028  -0.04489015   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           3.48105907\n",
      "  -0.53401566 -13.74214745  -0.96949613]\n",
      "Action shape: [ 0.          0.15820588 -0.25309715  0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.           0.00200891  -0.0008978    0.9999996    0.\n",
      "   0.10571028  -0.04489015   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           3.48105907\n",
      "  -0.53401566 -13.74214745  -0.96949613]\n",
      "Done: False\n",
      "\n",
      "Experience 262:\n",
      "State shape: [ -6.           0.01541185  -0.00186871   0.99999825   0.\n",
      "   0.67585421  -0.04854537   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           3.20649147\n",
      "  -0.55338621 -13.728405    -0.96852666]\n",
      "Action shape: [ 0.          0.85644084 -0.02060866  0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.           0.01541185  -0.00186871   0.99999825   0.\n",
      "   0.67585421  -0.04854537   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           3.20649147\n",
      "  -0.55338621 -13.728405    -0.96852666]\n",
      "Done: False\n",
      "\n",
      "Experience 263:\n",
      "State shape: [ -6.           0.03872776   0.00000177   1.           0.\n",
      "   1.15483034   0.09352405   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           2.93219757\n",
      "  -0.57273722 -13.71467686  -0.96755815]\n",
      "Action shape: [0.        0.7370647 0.8010079 0.        0.        0.       ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.           0.03872776   0.00000177   1.           0.\n",
      "   1.15483034   0.09352405   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           2.93219757\n",
      "  -0.57273722 -13.71467686  -0.96755815]\n",
      "Done: False\n",
      "\n",
      "Experience 264:\n",
      "State shape: [ -6.           0.05796146   0.00116327   0.99999932   0.\n",
      "   0.95484281   0.0580751    7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           2.65817833\n",
      "  -0.59206915 -13.70096207  -0.96659058]\n",
      "Action shape: [ 0.         -0.26473483 -0.1998663   0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.           0.05796146   0.00116327   0.99999932   0.\n",
      "   0.95484281   0.0580751    7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           2.65817833\n",
      "  -0.59206915 -13.70096207  -0.96659058]\n",
      "Done: False\n",
      "\n",
      "Experience 265:\n",
      "State shape: [ -6.00000191   0.09012508   0.00542429   0.99998529   0.\n",
      "   1.58314323   0.21305229   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           2.38443279\n",
      "  -0.61138153 -13.68726158  -0.96562403]\n",
      "Action shape: [0.         0.96889406 0.8737838  0.         0.         0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00000191   0.09012508   0.00542429   0.99998529   0.\n",
      "   1.58314323   0.21305229   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           2.38443279\n",
      "  -0.61138153 -13.68726158  -0.96562403]\n",
      "Done: False\n",
      "\n",
      "Experience 266:\n",
      "State shape: [ -6.00000572   0.12705421   0.01014866   0.9999485    0.\n",
      "   1.81870782   0.23622577   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           2.11096096\n",
      "  -0.63067484 -13.67357445  -0.96465844]\n",
      "Action shape: [0.         0.39993218 0.13065545 0.         0.         0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00000572   0.12705421   0.01014866   0.9999485    0.\n",
      "   1.81870782   0.23622577   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           2.11096096\n",
      "  -0.63067484 -13.67357445  -0.96465844]\n",
      "Done: False\n",
      "\n",
      "Experience 267:\n",
      "State shape: [ -6.00001717   0.16994715   0.01720287   0.99985202   0.\n",
      "   2.10320497   0.3527444    7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           1.83776283\n",
      "  -0.6499486  -13.65990067  -0.9636938 ]\n",
      "Action shape: [0.         0.48021555 0.65694875 0.         0.         0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00001717   0.16994715   0.01720287   0.99985202   0.\n",
      "   2.10320497   0.3527444    7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           1.83776283\n",
      "  -0.6499486  -13.65990067  -0.9636938 ]\n",
      "Done: False\n",
      "\n",
      "Experience 268:\n",
      "State shape: [ -6.00003052   0.20930624   0.02269148   0.99974252   0.\n",
      "   1.93569362   0.27448517   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           1.56483841\n",
      "  -0.66920328 -13.64624119  -0.96273011]\n",
      "Action shape: [ 0.         -0.18774429 -0.44123676  0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00003052   0.20930624   0.02269148   0.99974252   0.\n",
      "   1.93569362   0.27448517   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           1.56483841\n",
      "  -0.66920328 -13.64624119  -0.96273011]\n",
      "Done: False\n",
      "\n",
      "Experience 269:\n",
      "State shape: [ -6.00004387   0.23494196   0.02738046   0.99962508   0.\n",
      "   1.25425982   0.23452289   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           1.29218674\n",
      "  -0.68843842 -13.63259506  -0.96176738]\n",
      "Action shape: [ 0.         -0.96189404 -0.2253131   0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00004387   0.23494196   0.02738046   0.99962508   0.\n",
      "   1.25425982   0.23452289   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           1.29218674\n",
      "  -0.68843842 -13.63259506  -0.96176738]\n",
      "Done: False\n",
      "\n",
      "Experience 270:\n",
      "State shape: [ -6.00006771   0.26152182   0.03392348   0.99942443   0.\n",
      "   1.29054511   0.3273052    7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           1.01980782\n",
      "  -0.70765448 -13.61896229  -0.96080559]\n",
      "Action shape: [0.         0.09184697 0.5231199  0.         0.         0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00006771   0.26152182   0.03392348   0.99942443   0.\n",
      "   1.29054511   0.3273052    7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           1.01980782\n",
      "  -0.70765448 -13.61896229  -0.96080559]\n",
      "Done: False\n",
      "\n",
      "Experience 271:\n",
      "State shape: [ -6.00010014   0.29164505   0.04120092   0.99915088   0.\n",
      "   1.4634099    0.36413017   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           0.74770069\n",
      "  -0.72685146 -13.60534382  -0.95984483]\n",
      "Action shape: [0.         0.2973378  0.20762444 0.         0.         0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00010014   0.29164505   0.04120092   0.99915088   0.\n",
      "   1.4634099    0.36413017   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           0.74770069\n",
      "  -0.72685146 -13.60534382  -0.95984483]\n",
      "Done: False\n",
      "\n",
      "Experience 272:\n",
      "State shape: [ -6.00012445   0.32504082   0.04602554   0.99894026   0.\n",
      "   1.6414448    0.24146076   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           0.47586632\n",
      "  -0.74602938 -13.5917387   -0.95888501]\n",
      "Action shape: [ 0.         0.3102496 -0.6916279  0.         0.         0.       ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00012445   0.32504082   0.04602554   0.99894026   0.\n",
      "   1.6414448    0.24146076   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           0.47586632\n",
      "  -0.74602938 -13.5917387   -0.95888501]\n",
      "Done: False\n",
      "\n",
      "Experience 273:\n",
      "State shape: [ -6.00016975   0.34526253   0.05377968   0.99855283   0.\n",
      "   0.96552283   0.38819158   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           0.20430374\n",
      "  -0.76518774 -13.57814693  -0.95792615]\n",
      "Action shape: [ 0.         -0.96245235  0.82728964  0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00016975   0.34526253   0.05377968   0.99855283   0.\n",
      "   0.96552283   0.38819158   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           0.20430374\n",
      "  -0.76518774 -13.57814693  -0.95792615]\n",
      "Done: False\n",
      "\n",
      "Experience 274:\n",
      "State shape: [ -6.00020981   0.35885954   0.05975317   0.99821318   0.\n",
      "   0.64476067   0.29915738   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -0.06698799\n",
      "  -0.78432703 -13.56456852  -0.95696825]\n",
      "Action shape: [ 0.         -0.45115227 -0.5019876   0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00020981   0.35885954   0.05975317   0.99821318   0.\n",
      "   0.64476067   0.29915738   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -0.06698799\n",
      "  -0.78432703 -13.56456852  -0.95696825]\n",
      "Done: False\n",
      "\n",
      "Experience 275:\n",
      "State shape: [ -6.00024176   0.36040449   0.06410644   0.99794307   0.\n",
      "   0.05167326   0.21808258   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -0.33800793\n",
      "  -0.80344725 -13.55100441  -0.9560113 ]\n",
      "Action shape: [ 0.        -0.868315  -0.4571114  0.         0.         0.       ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00024176   0.36040449   0.06410644   0.99794307   0.\n",
      "   0.05167326   0.21808258   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -0.33800793\n",
      "  -0.80344725 -13.55100441  -0.9560113 ]\n",
      "Done: False\n",
      "\n",
      "Experience 276:\n",
      "State shape: [ -6.00027704   0.37190247   0.06864105   0.99764142   0.\n",
      "   0.54827821   0.22723196   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -0.60875702\n",
      "  -0.82254839 -13.53745365  -0.9550553 ]\n",
      "Action shape: [0.        0.744765  0.0515856 0.        0.        0.       ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00027704   0.37190247   0.06864105   0.99764142   0.\n",
      "   0.54827821   0.22723196   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -0.60875702\n",
      "  -0.82254839 -13.53745365  -0.9550553 ]\n",
      "Done: False\n",
      "\n",
      "Experience 277:\n",
      "State shape: [ -6.00034237   0.3709774    0.07626461   0.99708761   0.\n",
      "  -0.0910546    0.38218349   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -0.87923527\n",
      "  -0.84163046 -13.52391624  -0.95410025]\n",
      "Action shape: [ 0.         -0.94041365  0.87363917  0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00034237   0.3709774    0.07626461   0.99708761   0.\n",
      "  -0.0910546    0.38218349   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -0.87923527\n",
      "  -0.84163046 -13.52391624  -0.95410025]\n",
      "Done: False\n",
      "\n",
      "Experience 278:\n",
      "State shape: [ -6.0004406    0.35758591   0.08654048   0.99624834   0.\n",
      "  -0.72995269   0.51550639   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -1.14944267\n",
      "  -0.86069345 -13.51039219  -0.95314616]\n",
      "Action shape: [ 0.         -0.95889956  0.7516939   0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.0004406    0.35758591   0.08654048   0.99624834   0.\n",
      "  -0.72995269   0.51550639   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -1.14944267\n",
      "  -0.86069345 -13.51039219  -0.95314616]\n",
      "Done: False\n",
      "\n",
      "Experience 279:\n",
      "State shape: [ -6.00057793   0.35365391   0.09906905   0.99508056   0.\n",
      "  -0.27022383   0.62914795   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -1.41938019\n",
      "  -0.87973738 -13.49688244  -0.95219302]\n",
      "Action shape: [0.         0.66618073 0.64072746 0.         0.         0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00057793   0.35365391   0.09906905   0.99508056   0.\n",
      "  -0.27022383   0.62914795   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -1.41938019\n",
      "  -0.87973738 -13.49688244  -0.95219302]\n",
      "Done: False\n",
      "\n",
      "Experience 280:\n",
      "State shape: [ -6.00074291   0.35687065   0.11227911   0.99367671   0.\n",
      "   0.08323193   0.66422725   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -1.68904781\n",
      "  -0.89876223 -13.48338604  -0.95124084]\n",
      "Action shape: [0.         0.5208931  0.19778231 0.         0.         0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00074291   0.35687065   0.11227911   0.99367671   0.\n",
      "   0.08323193   0.66422725   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -1.68904781\n",
      "  -0.89876223 -13.48338604  -0.95124084]\n",
      "Done: False\n",
      "\n",
      "Experience 281:\n",
      "State shape: [ -6.00088215   0.35628748   0.12229941   0.99249325   0.\n",
      "  -0.08803872   0.50449914   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -1.95844555\n",
      "  -0.917768   -13.46990299  -0.95028961]\n",
      "Action shape: [ 0.         -0.25383216 -0.9005701   0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00088215   0.35628748   0.12229941   0.99249325   0.\n",
      "  -0.08803872   0.50449914   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -1.95844555\n",
      "  -0.917768   -13.46990299  -0.95028961]\n",
      "Done: False\n",
      "\n",
      "Experience 282:\n",
      "State shape: [ -6.00099182   0.34893847   0.12965574   0.99155907   0.\n",
      "  -0.41064119   0.37077141   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -2.22757435\n",
      "  -0.9367547  -13.4564333   -0.94933933]\n",
      "Action shape: [ 0.         -0.4854416  -0.75397635  0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00099182   0.34893847   0.12965574   0.99155907   0.\n",
      "  -0.41064119   0.37077141   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -2.22757435\n",
      "  -0.9367547  -13.4564333   -0.94933933]\n",
      "Done: False\n",
      "\n",
      "Experience 283:\n",
      "State shape: [ -6.00115299   0.33364391   0.13975916   0.99018553   0.\n",
      "  -0.82409024   0.50981998   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -2.49643373\n",
      "  -0.95572233 -13.44297695  -0.94839001]\n",
      "Action shape: [ 0.         -0.6310587   0.78397584  0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00115299   0.33364391   0.13975916   0.99018553   0.\n",
      "  -0.82409024   0.50981998   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -2.49643373\n",
      "  -0.95572233 -13.44297695  -0.94839001]\n",
      "Done: False\n",
      "\n",
      "Experience 284:\n",
      "State shape: [ -6.00137901   0.32572556   0.15275426   0.9882642    0.\n",
      "  -0.47229353   0.65682292   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -2.76502419\n",
      "  -0.97467136 -13.42953396  -0.94744164]\n",
      "Action shape: [0.        0.5018319 0.8288237 0.        0.        0.       ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00137901   0.32572556   0.15275426   0.9882642    0.\n",
      "  -0.47229353   0.65682292   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -2.76502419\n",
      "  -0.97467136 -13.42953396  -0.94744164]\n",
      "Done: False\n",
      "\n",
      "Experience 285:\n",
      "State shape: [ -6.00169182   0.31019688   0.1690801    0.98560231   0.\n",
      "  -0.87235516   0.82708073   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -3.03334618\n",
      "  -0.99360132 -13.41610432  -0.94649422]\n",
      "Action shape: [ 0.        -0.6128684  0.9599383  0.         0.         0.       ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00169182   0.31019688   0.1690801    0.98560231   0.\n",
      "  -0.87235516   0.82708073   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -3.03334618\n",
      "  -0.99360132 -13.41610432  -0.94649422]\n",
      "Done: False\n",
      "\n",
      "Experience 286:\n",
      "State shape: [ -6.00199699   0.29898214   0.18356755   0.9830071    0.\n",
      "  -0.64584678   0.73590952   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -3.30139971\n",
      "  -1.01251221 -13.40268803  -0.94554776]\n",
      "Action shape: [ 0.         0.3128808 -0.5140363  0.         0.         0.       ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00199699   0.29898214   0.18356755   0.9830071    0.\n",
      "  -0.64584678   0.73590952   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -3.30139971\n",
      "  -1.01251221 -13.40268803  -0.94554776]\n",
      "Done: False\n",
      "\n",
      "Experience 287:\n",
      "State shape: [ -6.00238132   0.28098011   0.20029107   0.97973644   0.\n",
      "  -0.99835277   0.85202718   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -3.56918526\n",
      "  -1.03140402 -13.38928509  -0.94460225]\n",
      "Action shape: [ 0.         -0.54689145  0.6546883   0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00238132   0.28098011   0.20029107   0.97973644   0.\n",
      "  -0.99835277   0.85202718   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -3.56918526\n",
      "  -1.03140402 -13.38928509  -0.94460225]\n",
      "Done: False\n",
      "\n",
      "Experience 288:\n",
      "State shape: [ -6.00275326   0.27622747   0.21520013   0.97656997   0.\n",
      "  -0.3252199    0.76208758   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -3.8367033\n",
      "  -1.05027723 -13.3758955   -0.94365764]\n",
      "Action shape: [ 0.          0.97752714 -0.5070926   0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00275326   0.27622747   0.21520013   0.97656997   0.\n",
      "  -0.3252199    0.76208758   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -3.8367033\n",
      "  -1.05027723 -13.3758955   -0.94365764]\n",
      "Done: False\n",
      "\n",
      "Experience 289:\n",
      "State shape: [ -6.00308323   0.27141905   0.22757569   0.97376039   0.\n",
      "  -0.31313759   0.63452768   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -4.10395384\n",
      "  -1.06913137 -13.36252022  -0.94271398]\n",
      "Action shape: [ 0.          0.00834793 -0.7192011   0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00308323   0.27141905   0.22757569   0.97376039   0.\n",
      "  -0.31313759   0.63452768   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -4.10395384\n",
      "  -1.06913137 -13.36252022  -0.94271398]\n",
      "Done: False\n",
      "\n",
      "Experience 290:\n",
      "State shape: [ -6.00334263   0.26362801   0.2368238    0.97155262   0.\n",
      "  -0.44388953   0.47540116   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -4.37093687\n",
      "  -1.08796692 -13.34915829  -0.94177127]\n",
      "Action shape: [ 0.         -0.20505598 -0.8971784   0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00334263   0.26362801   0.2368238    0.97155262   0.\n",
      "  -0.44388953   0.47540116   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -4.37093687\n",
      "  -1.08796692 -13.34915829  -0.94177127]\n",
      "Done: False\n",
      "\n",
      "Experience 291:\n",
      "State shape: [ -6.00356531   0.24713564   0.24446459   0.96965822   0.\n",
      "  -0.86951435   0.39360747   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -4.63765287\n",
      "  -1.10678339 -13.33580971  -0.94082952]\n",
      "Action shape: [ 0.         -0.65027606 -0.46116465  0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00356531   0.24713564   0.24446459   0.96965822   0.\n",
      "  -0.86951435   0.39360747   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -4.63765287\n",
      "  -1.10678339 -13.33580971  -0.94082952]\n",
      "Done: False\n",
      "\n",
      "Experience 292:\n",
      "State shape: [ -6.00373363   0.238873     0.2500684    0.96822817   0.\n",
      "  -0.44605985   0.28916997   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -4.90410233\n",
      "  -1.12558126 -13.32247448  -0.93988872]\n",
      "Action shape: [ 0.          0.60771525 -0.58883363  0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00373363   0.238873     0.2500684    0.96822817   0.\n",
      "  -0.44605985   0.28916997   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -4.90410233\n",
      "  -1.12558126 -13.32247448  -0.93988872]\n",
      "Done: False\n",
      "\n",
      "Experience 293:\n",
      "State shape: [ -6.00399685   0.22010517   0.25858688   0.96598801   0.\n",
      "  -0.98844308   0.4404071    7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -5.17028522\n",
      "  -1.14436007 -13.3091526   -0.93894881]\n",
      "Action shape: [ 0.        -0.8250816  0.8526967  0.         0.         0.       ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00399685   0.22010517   0.25858688   0.96598801   0.\n",
      "  -0.98844308   0.4404071    7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -5.17028522\n",
      "  -1.14436007 -13.3091526   -0.93894881]\n",
      "Done: False\n",
      "\n",
      "Experience 294:\n",
      "State shape: [ -6.00418997   0.19786501   0.26465516   0.96434312   0.\n",
      "  -1.14766645   0.31436345   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -5.43620205\n",
      "  -1.16312027 -13.29584408  -0.93800986]\n",
      "Action shape: [ 0.         -0.26787966 -0.71065235  0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00418997   0.19786501   0.26465516   0.96434312   0.\n",
      "  -1.14766645   0.31436345   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -5.43620205\n",
      "  -1.16312027 -13.29584408  -0.93800986]\n",
      "Done: False\n",
      "\n",
      "Experience 295:\n",
      "State shape: [ -6.00428343   0.18439627   0.26753272   0.96354878   0.\n",
      "  -0.69034278   0.14925881   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -5.7018528\n",
      "  -1.18186188 -13.28254795  -0.93707186]\n",
      "Action shape: [ 0.          0.65007806 -0.93088377  0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00428343   0.18439627   0.26753272   0.96354878   0.\n",
      "  -0.69034278   0.14925881   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -5.7018528\n",
      "  -1.18186188 -13.28254795  -0.93707186]\n",
      "Done: False\n",
      "\n",
      "Experience 296:\n",
      "State shape: [ -6.00428629   0.17678881   0.26762007   0.96352452   0.\n",
      "  -0.38086358   0.00453277   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -5.96723795\n",
      "  -1.20058441 -13.26926517  -0.93613482]\n",
      "Action shape: [ 0.          0.4425029  -0.81598634  0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00428629   0.17678881   0.26762007   0.96352452   0.\n",
      "  -0.38086358   0.00453277   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -5.96723795\n",
      "  -1.20058441 -13.26926517  -0.93613482]\n",
      "Done: False\n",
      "\n",
      "Experience 297:\n",
      "State shape: [ -6.00419712   0.15927219   0.2648847    0.96428009   0.\n",
      "  -0.85975713  -0.14189078   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -6.23235798\n",
      "  -1.21928835 -13.25599575  -0.93519866]\n",
      "Action shape: [ 0.        -0.7281115 -0.8255571  0.         0.         0.       ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00419712   0.15927219   0.2648847    0.96428009   0.\n",
      "  -0.85975713  -0.14189078   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -6.23235798\n",
      "  -1.21928835 -13.25599575  -0.93519866]\n",
      "Done: False\n",
      "\n",
      "Experience 298:\n",
      "State shape: [ -6.00414371   0.13047504   0.2632082    0.96473906   0.\n",
      "  -1.43000674  -0.08690879   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -6.49721289\n",
      "  -1.23797369 -13.24273968  -0.93426347]\n",
      "Action shape: [ 0.         -0.87916905  0.30999643  0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00414371   0.13047504   0.2632082    0.96473906   0.\n",
      "  -1.43000674  -0.08690879   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -6.49721289\n",
      "  -1.23797369 -13.24273968  -0.93426347]\n",
      "Done: False\n",
      "\n",
      "Experience 299:\n",
      "State shape: [ -6.00402355   0.09931469   0.25944877   0.96575687   0.\n",
      "  -1.53594387  -0.19473812   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -6.76180267\n",
      "  -1.25664043 -13.22949696  -0.93332922]\n",
      "Action shape: [ 0.         -0.20134823 -0.60795736  0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00402355   0.09931469   0.25944877   0.96575687   0.\n",
      "  -1.53594387  -0.19473812   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -6.76180267\n",
      "  -1.25664043 -13.22949696  -0.93332922]\n",
      "Done: False\n",
      "\n",
      "Experience 300:\n",
      "State shape: [ -6.00382233   0.06365728   0.25297111   0.96747383   0.\n",
      "  -1.74479294  -0.33506811   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -7.02612829\n",
      "  -1.27528858 -13.21626759  -0.93239594]\n",
      "Action shape: [ 0.         -0.35853705 -0.7912007   0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00382233   0.06365728   0.25297111   0.96747383   0.\n",
      "  -1.74479294  -0.33506811   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -7.02612829\n",
      "  -1.27528858 -13.21626759  -0.93239594]\n",
      "Done: False\n",
      "\n",
      "Experience 301:\n",
      "State shape: [ -6.00365448   0.02455664   0.24744306   0.96890244   0.\n",
      "  -1.9225527   -0.28548428   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -7.29018927\n",
      "  -1.29391766 -13.20305157  -0.93146354]\n",
      "Action shape: [ 0.        -0.31826    0.2795608  0.         0.         0.       ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00365448   0.02455664   0.24744306   0.96890244   0.\n",
      "  -1.9225527   -0.28548428   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -7.29018927\n",
      "  -1.29391766 -13.20305157  -0.93146354]\n",
      "Done: False\n",
      "\n",
      "Experience 302:\n",
      "State shape: [ -6.00341702  -0.01509428   0.23940448   0.97091992   0.\n",
      "  -1.93532133  -0.41439497   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -7.55398655\n",
      "  -1.31252813 -13.1898489   -0.9305321 ]\n",
      "Action shape: [ 0.         -0.07665524 -0.7268171   0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00341702  -0.01509428   0.23940448   0.97091992   0.\n",
      "  -1.93532133  -0.41439497   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -7.55398655\n",
      "  -1.31252813 -13.1898489   -0.9305321 ]\n",
      "Done: False\n",
      "\n",
      "Experience 303:\n",
      "State shape: [ -6.00326252  -0.06339741   0.23399028   0.97223894   0.\n",
      "  -2.38336229  -0.27862811   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -7.81751919\n",
      "  -1.33112001 -13.17665958  -0.92960155]\n",
      "Action shape: [ 0.         -0.72846556  0.76547325  0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00326252  -0.06339741   0.23399028   0.97223894   0.\n",
      "  -2.38336229  -0.27862811   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -7.81751919\n",
      "  -1.33112001 -13.17665958  -0.92960155]\n",
      "Done: False\n",
      "\n",
      "Experience 304:\n",
      "State shape: [ -6.00308704  -0.10718393   0.22770407   0.97373038   0.\n",
      "  -2.15239787  -0.32303628   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -8.08078957\n",
      "  -1.3496933  -13.16348267  -0.92867196]\n",
      "Action shape: [ 0.          0.2743225  -0.25037974  0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00308704  -0.10718393   0.22770407   0.97373038   0.\n",
      "  -2.15239787  -0.32303628   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -8.08078957\n",
      "  -1.3496933  -13.16348267  -0.92867196]\n",
      "Done: True\n",
      "\n",
      "Experience 305:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.0006036527156829835\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 306:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 307:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 308:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 309:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 310:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 311:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 312:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 313:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 314:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 315:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 316:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 317:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 318:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 319:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 320:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 321:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 322:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 323:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 324:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 325:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 326:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 327:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 328:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 329:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 330:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 331:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 332:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 333:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 334:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 335:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 336:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 337:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 338:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 339:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 340:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 341:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 342:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 343:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 344:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 345:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 346:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 347:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 348:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 349:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 350:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 351:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 352:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 353:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 354:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 355:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 356:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: 0.0006036527156829835\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 357:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 358:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 359:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 360:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 361:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 362:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 363:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 364:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 365:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 366:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 367:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 368:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 369:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 370:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 371:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 372:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 373:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 374:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 375:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 376:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 377:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 378:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 379:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 380:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 381:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 382:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 383:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 384:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 385:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 386:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 387:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 388:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 389:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 390:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 391:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 392:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 393:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 394:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 395:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 396:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 397:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 398:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 399:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 400:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 401:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 402:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 403:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 404:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 405:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 406:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 407:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.0006036527156829835\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 408:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 409:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 410:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 411:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 412:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 413:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 414:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 415:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 416:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 417:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 418:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 419:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 420:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 421:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 422:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 423:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 424:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 425:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 426:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 427:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 428:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 429:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 430:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 431:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 432:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 433:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 434:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 435:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 436:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 437:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 438:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 439:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 440:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 441:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 442:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 443:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 444:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 445:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 446:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 447:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 448:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 449:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 450:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 451:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 452:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 453:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 454:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 455:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 456:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 457:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 458:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.0006036527156829835\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 459:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 460:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 461:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 462:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 463:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 464:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 465:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 466:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 467:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 468:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 469:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 470:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 471:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 472:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 473:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 474:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 475:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 476:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 477:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 478:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 479:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 480:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 481:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 482:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 483:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 484:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 485:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 486:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 487:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 488:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 489:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 490:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 491:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 492:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 493:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 494:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 495:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 496:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 497:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 498:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 499:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 500:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 501:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 502:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 503:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 504:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 505:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 506:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 507:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 508:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 509:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: 0.0006036527156829835\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 510:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 511:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 512:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 513:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 514:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 515:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 516:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 517:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 518:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 519:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 520:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 521:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 522:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 523:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 524:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 525:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 526:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 527:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 528:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 529:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 530:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 531:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 532:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 533:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 534:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 535:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 536:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 537:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 538:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 539:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 540:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 541:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 542:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 543:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 544:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 545:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 546:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 547:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 548:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 549:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 550:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 551:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 552:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 553:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 554:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 555:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 556:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 557:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 558:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 559:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 560:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: 0.0006036527156829835\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 561:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 562:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 563:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 564:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 565:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 566:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 567:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 568:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 569:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 570:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 571:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 572:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 573:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 574:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 575:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 576:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 577:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 578:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 579:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 580:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 581:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 582:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 583:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 584:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 585:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 586:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 587:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 588:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 589:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 590:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 591:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 592:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 593:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 594:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 595:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 596:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 597:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 598:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 599:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 600:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 601:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 602:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 603:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 604:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 605:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 606:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 607:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 608:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 609:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 610:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 611:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: 0.0006036527156829835\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 612:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 613:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 614:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 615:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 616:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 617:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 618:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 619:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 620:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 621:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 622:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 623:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 624:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 625:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 626:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 627:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 628:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 629:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 630:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 631:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 632:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 633:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 634:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 635:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 636:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 637:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 638:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 639:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 640:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 641:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 642:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 643:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 644:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 645:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 646:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 647:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 648:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 649:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 650:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 651:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 652:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 653:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 654:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 655:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 656:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 657:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 658:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 659:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 660:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 661:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 662:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: 0.0006036527156829835\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 663:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 664:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 665:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 666:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 667:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 668:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 669:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 670:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 671:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 672:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 673:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 674:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 675:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 676:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 677:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 678:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 679:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 680:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 681:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 682:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 683:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 684:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 685:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 686:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 687:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 688:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 689:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 690:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 691:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 692:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 693:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 694:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 695:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 696:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 697:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 698:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 699:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 700:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 701:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 702:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 703:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 704:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 705:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 706:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 707:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 708:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 709:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 710:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 711:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 712:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 713:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: 0.0006036527156829835\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 714:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 715:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 716:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 717:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 718:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 719:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 720:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 721:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 722:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 723:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 724:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 725:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 726:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 727:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 728:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 729:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 730:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 731:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 732:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 733:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 734:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 735:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 736:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 737:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 738:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 739:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 740:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 741:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 742:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 743:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 744:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 745:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 746:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 747:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 748:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 749:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 750:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 751:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 752:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 753:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 754:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 755:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 756:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 757:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 758:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 759:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 760:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 761:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 762:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 763:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 764:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: 0.0006036527156829835\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 765:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 766:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 767:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 768:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 769:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 770:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 771:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 772:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 773:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 774:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 775:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 776:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 777:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 778:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 779:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 780:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 781:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 782:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 783:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 784:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 785:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 786:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 787:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 788:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 789:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 790:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 791:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 792:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 793:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 794:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 795:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 796:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 797:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 798:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 799:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 800:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 801:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 802:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 803:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 804:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 805:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 806:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 807:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 808:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 809:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 810:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 811:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 812:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 813:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 814:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 815:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: 0.0006036527156829835\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 816:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 817:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 818:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 819:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 820:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 821:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 822:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 823:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 824:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 825:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 826:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 827:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 828:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 829:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 830:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 831:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 832:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 833:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 834:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 835:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 836:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 837:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 838:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 839:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 840:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 841:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 842:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 843:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 844:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 845:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 846:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 847:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 848:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 849:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 850:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 851:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 852:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 853:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 854:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 855:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 856:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 857:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 858:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 859:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 860:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 861:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 862:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 863:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 864:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 865:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 866:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: 0.0006036527156829835\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 867:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 868:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 869:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 870:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 871:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 872:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 873:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 874:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 875:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 876:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 877:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 878:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 879:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 880:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 881:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 882:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 883:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 884:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 885:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 886:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 887:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 888:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 889:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 890:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 891:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 892:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 893:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 894:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 895:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 896:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 897:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 898:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 899:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 900:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 901:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 902:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 903:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 904:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 905:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 906:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 907:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 908:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 909:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 910:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 911:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 912:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 913:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 914:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 915:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 916:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 917:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: 0.0006036527156829835\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 918:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 919:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 920:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 921:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 922:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 923:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 924:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 925:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 926:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 927:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 928:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 929:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 930:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 931:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 932:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 933:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 934:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 935:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 936:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 937:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 938:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 939:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 940:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 941:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 942:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 943:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 944:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 945:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 946:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 947:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 948:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 949:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 950:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 951:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 952:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 953:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 954:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 955:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 956:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 957:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 958:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 959:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 960:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 961:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 962:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 963:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 964:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 965:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 966:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 967:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 968:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: 0.0006036527156829835\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 969:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 970:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 971:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 972:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 973:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 974:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 975:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 976:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 977:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 978:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 979:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 980:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 981:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 982:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 983:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 984:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 985:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 986:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 987:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 988:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 989:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 990:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 991:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 992:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 993:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 994:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 995:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 996:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 997:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 998:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 999:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 1000:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_her_buffer(buffer, num_entries=1000):\n",
    "    \"\"\"Prints the first num_entries experiences from the HER buffer.\"\"\"\n",
    "    for idx, experience in enumerate(buffer.memory):\n",
    "        if idx >= num_entries:\n",
    "            break\n",
    "        print(f\"Experience {idx + 1}:\")\n",
    "        obs, action, reward, next_obs, done = experience\n",
    "        print(f\"State shape: {np.array(obs)}\")\n",
    "        print(f\"Action shape: {np.array(action)}\")\n",
    "        print(f\"Reward: {reward}\")\n",
    "        print(f\"Next State shape: {np.array(next_obs)}\")\n",
    "        print(f\"Done: {done}\\n\")\n",
    "\n",
    "# Call the function\n",
    "print_her_buffer(her_buffer)\n",
    "\n",
    "\n",
    "\n",
    "def extract_data_from_buffer(buffer, batch_size):\n",
    "    states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "    \n",
    "    sampled_experiences = buffer.sample(batch_size)\n",
    "    \n",
    "    for idx, experience in enumerate(sampled_experiences):\n",
    "        if idx >= batch_size:\n",
    "            break\n",
    "        \n",
    "        print(f\"Experience {idx + 1} Length: {len(experience)}\")\n",
    "        print(experience)\n",
    "        \n",
    "        if len(experience) != 5:\n",
    "            print(f\"Error: Expected length of experience is 5, but got {len(experience)}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            state, action, reward, next_state, done = experience\n",
    "        except ValueError as e:\n",
    "            print(f\"Unexpected error during unpacking: {e}\")\n",
    "            continue\n",
    "        \n",
    "        states.append(np.array(state))\n",
    "        actions.append(np.array(action))\n",
    "        rewards.append(reward)\n",
    "        next_states.append(np.array(next_state))\n",
    "        dones.append(done)\n",
    "    \n",
    "    states_np = np.array(states)\n",
    "    actions_np = np.array(actions)\n",
    "    rewards_np = np.array(rewards)\n",
    "    next_states_np = np.array(next_states)\n",
    "    dones_np = np.array(dones)\n",
    "    \n",
    "    return states_np, actions_np, rewards_np, next_states_np, dones_np\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348362c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Model:\n",
      "Number of Layers: 5\n",
      "Number of Input Neurons: 18\n",
      "Number of Hidden Layers: 2\n",
      "Number of Hidden Neurons in each layer: [256, 6]\n",
      "Number of Output Neurons: 6\n",
      "\n",
      "Value Model:\n",
      "Number of Layers: 5\n",
      "Number of Input Neurons: 24\n",
      "Number of Hidden Layers: 2\n",
      "Number of Hidden Neurons in each layer: [256, 1]\n",
      "Number of Output Neurons: 1\n"
     ]
    }
   ],
   "source": [
    "# Print the number of layers and neurons in the policy model\n",
    "policy_model_layers = list(agent.online_policy_model.children())\n",
    "policy_input_neurons = state_dim\n",
    "policy_hidden_neurons = [layer.out_features for layer in policy_model_layers if isinstance(layer, torch.nn.Linear)]\n",
    "policy_output_neurons = action_dim\n",
    "\n",
    "print(\"Policy Model:\")\n",
    "print(f\"Number of Layers: {len(policy_model_layers)}\")\n",
    "print(f\"Number of Input Neurons: {policy_input_neurons}\")\n",
    "print(f\"Number of Hidden Layers: {len(policy_hidden_neurons)}\")\n",
    "print(f\"Number of Hidden Neurons in each layer: {policy_hidden_neurons}\")\n",
    "print(f\"Number of Output Neurons: {policy_output_neurons}\")\n",
    "\n",
    "# Print the number of layers and neurons in the value model\n",
    "value_model_layers = list(agent.online_value_model.children())\n",
    "value_input_neurons = state_dim + action_dim\n",
    "value_hidden_neurons = [layer.out_features for layer in value_model_layers if isinstance(layer, torch.nn.Linear)]\n",
    "value_output_neurons = 1\n",
    "\n",
    "print(\"\\nValue Model:\")\n",
    "print(f\"Number of Layers: {len(value_model_layers)}\")\n",
    "print(f\"Number of Input Neurons: {value_input_neurons}\")\n",
    "print(f\"Number of Hidden Layers: {len(value_hidden_neurons)}\")\n",
    "print(f\"Number of Hidden Neurons in each layer: {value_hidden_neurons}\")\n",
    "print(f\"Number of Output Neurons: {value_output_neurons}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c879544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 scored\n",
      "Episode 1, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 1, Reward: 0.6496168649744513, Moving Avg Reward: 0.6496168649744513, HER Buffer Size: 4000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 2, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 2, Reward: -11.217263197228286, Moving Avg Reward: -5.2838231661269175, HER Buffer Size: 4045\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 3, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 3, Reward: 11.020645200824736, Moving Avg Reward: 0.15099962285696714, HER Buffer Size: 4529\n",
      "Current Epsilon: 0.990025\n",
      "Player 2 scored\n",
      "Episode 4, Avg Value Loss: 0.0893888481524448, Avg Policy Loss: -0.15893647273959116\n",
      "Episode 4, Reward: -11.272325482621142, Moving Avg Reward: -2.70483165351256, HER Buffer Size: 4572\n",
      "Current Epsilon: 0.985074875\n",
      "Player 1 scored\n",
      "Episode 5, Avg Value Loss: 0.014409171733112163, Avg Policy Loss: -0.18131441102452475\n",
      "Episode 5, Reward: 11.825610327606398, Moving Avg Reward: 0.2012567427112316, HER Buffer Size: 7419\n",
      "Current Epsilon: 0.9801495006250001\n",
      "Player 1 scored\n",
      "Episode 6, Avg Value Loss: 0.013913645173306576, Avg Policy Loss: -0.15734304049983622\n",
      "Episode 6, Reward: 0.3812187117278629, Moving Avg Reward: 0.23125040421400347, HER Buffer Size: 11259\n",
      "Current Epsilon: 0.9752487531218751\n",
      "Player 1 scored\n",
      "Episode 7, Avg Value Loss: 0.011303374357521535, Avg Policy Loss: -0.14081836687400937\n",
      "Episode 7, Reward: 0.5809385050275837, Moving Avg Reward: 0.28120584718737207, HER Buffer Size: 15259\n",
      "Current Epsilon: 0.9703725093562657\n",
      "Player 2 scored\n",
      "Episode 8, Avg Value Loss: 0.007241536816582084, Avg Policy Loss: -0.14765796968713402\n",
      "Episode 8, Reward: 0.8196452931414072, Moving Avg Reward: 0.34851077793162655, HER Buffer Size: 18779\n",
      "Current Epsilon: 0.9655206468094844\n",
      "Player 2 scored\n",
      "Episode 9, Avg Value Loss: 0.0056748761216156365, Avg Policy Loss: -0.1267831176519394\n",
      "Episode 9, Reward: -11.27730241487365, Moving Avg Reward: -0.943246243491182, HER Buffer Size: 18821\n",
      "Current Epsilon: 0.960693043575437\n",
      "Player 2 scored\n",
      "Episode 10, Avg Value Loss: 0.005912312613746021, Avg Policy Loss: -0.12810989382655122\n",
      "Episode 10, Reward: -11.263087518983237, Moving Avg Reward: -1.9752303710403876, HER Buffer Size: 18864\n",
      "Current Epsilon: 0.9558895783575597\n"
     ]
    }
   ],
   "source": [
    "# Latest Initialize DDPG Agent and Environment\n",
    "import random\n",
    "np.set_printoptions(suppress=True)\n",
    "reload(lh)\n",
    "\n",
    "# Set the environment mode to TRAIN DEFENSE\n",
    "env = lh.LaserHockeyEnv()\n",
    "env.reset(mode=lh.LaserHockeyEnv.TRAIN_DEFENSE)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_bounds = (env.action_space.low, env.action_space.high)\n",
    "epsilon_strategy = EpsilonGreedyStrategy(1.0, 0.01, 0.995, env.action_space)\n",
    "\n",
    "def sample_goals(episode):\n",
    "    goals = []\n",
    "    for experience in episode:\n",
    "        _, _, reward, next_state, done, info = experience\n",
    "        # This assumes that you have added 'info' to your stored experiences\n",
    "        if info['reward_closeness_to_puck'] > 0 or info['reward_touch_puck'] > 0 or info['reward_puck_direction'] > 0:\n",
    "            goals.append(next_state)\n",
    "    return goals\n",
    "\n",
    "\n",
    "\n",
    "def compute_reward(state, action, goal):\n",
    "    _, _, _, _, info = env.step(action)\n",
    "    \n",
    "    closeness_reward = info.get('reward_closeness_to_puck', 0)\n",
    "    touch_reward = info.get('reward_touch_puck', 0)\n",
    "    direction_reward = info.get('reward_puck_direction', 0)\n",
    "    game_result = info.get('winner', 0)\n",
    "\n",
    "         \n",
    "    \n",
    "    # Combine rewards, possibly with weights to prioritize certain behaviors\n",
    "    total_reward = 3*closeness_reward + 10*touch_reward + 5*direction_reward\n",
    "\n",
    "    if game_result >= 0:\n",
    "        total_reward += 0.5 \n",
    "    \n",
    "    # If all conditions are met, give a positive reward\n",
    "    if closeness_reward > 0 and touch_reward > 0 and direction_reward > 0:\n",
    "        return 15\n",
    "    \n",
    "    # If any condition is not met, penalize the agent\n",
    "    if closeness_reward <= 0:\n",
    "        return -0.1\n",
    "    if touch_reward <= 0:\n",
    "        return -0.3\n",
    "    if direction_reward <= 0:\n",
    "        return -0.55\n",
    "    \n",
    "    return total_reward  # Return combined reward as a fallback\n",
    "\n",
    "\n",
    "her_buffer = HERBuffer(buffer_size=100000, goal_selection_strategy=sample_goals, reward_function=compute_reward)\n",
    "\n",
    "agent = DDPGAgent(state_dim=state_dim, action_dim=action_dim, action_bounds=action_bounds)\n",
    "agent.he_initialization()\n",
    "\n",
    "num_episodes = 10000\n",
    "batch_size = 512\n",
    "train_start = 200  \n",
    "update_frequency = 1\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "wins = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    obs = env.reset()\n",
    "\n",
    "    obs_agent1 = env._get_obs()\n",
    "    episode_reward = 0\n",
    "    episode_value_losses = []\n",
    "    episode_policy_losses = []\n",
    "\n",
    "    episode_trajectory = []\n",
    "\n",
    "    for step in range(80):\n",
    "        \n",
    "        if episode >= 200:\n",
    "            obs = env.render()\n",
    "\n",
    "        state_tensor = torch.FloatTensor(obs_agent1).unsqueeze(0)\n",
    "        agent.online_policy_model.eval()\n",
    "        action_player1 = epsilon_strategy.select_action(agent.online_policy_model, state_tensor, episode)\n",
    "        \n",
    "        agent.online_policy_model.train()\n",
    "\n",
    "        next_obs, reward, done, _, info = env.step(action_player1)\n",
    "        reward += info['winner']\n",
    "        reward += info['reward_closeness_to_puck']\n",
    "        reward += info['reward_touch_puck']\n",
    "        reward += info['reward_puck_direction']\n",
    "\n",
    "        #print(f\"info: {info}\")\n",
    "        episode_reward += reward\n",
    "\n",
    "        obs_agent1 = env._get_obs()\n",
    "        experience = (obs_agent1, action_player1, reward, next_obs, done, info)  # Added 'info' at the end\n",
    "        episode_trajectory.append(experience)\n",
    "\n",
    "\n",
    "        # DDPG Training\n",
    "        if episode > train_start and len(her_buffer) > batch_size:\n",
    "            experiences = her_buffer.sample(batch_size)\n",
    "            states, actions, rewards, next_states, is_terminals = [torch.FloatTensor(x_np).to(device) for x_np in experiences]\n",
    "            \n",
    "            value_loss, policy_loss = agent.optimize_model((states, actions, rewards, next_states, is_terminals))\n",
    "            episode_value_losses.append(value_loss)\n",
    "            episode_policy_losses.append(policy_loss)\n",
    "\n",
    "        if step == 79: \n",
    "            done = True\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Store trajectories experiences with HER logic, after episode is done\n",
    "    her_goals = sample_goals(episode_trajectory)\n",
    "    for experience in episode_trajectory:\n",
    "        state, action, reward, next_state, done, info = experience\n",
    "        # Store the original experience\n",
    "        her_buffer.store(state, action, reward, next_state, done)\n",
    "\n",
    "        for goal in her_goals:\n",
    "            # Recompute the reward and store the adjusted experience\n",
    "            her_reward = compute_reward(state, action, goal)\n",
    "            her_buffer.store(state, action, her_reward, next_state, done)\n",
    "\n",
    "    epsilon_strategy.decay_epsilon()\n",
    "    all_rewards.append(episode_reward)\n",
    "    moving_avg_reward = np.mean(all_rewards[-100:])\n",
    "    avg_value_loss = sum(episode_value_losses) / len(episode_value_losses) if episode_value_losses else 0\n",
    "    avg_policy_loss = sum(episode_policy_losses) / len(episode_policy_losses) if episode_policy_losses else 0\n",
    "\n",
    "    print(f\"Episode {episode + 1}, Avg Value Loss: {avg_value_loss}, Avg Policy Loss: {avg_policy_loss}\")\n",
    "    print(f\"Episode {episode + 1}, Reward: {episode_reward}, Moving Avg Reward: {moving_avg_reward}, HER Buffer Size: {len(her_buffer)}\")\n",
    "    print(f\"Current Epsilon: {epsilon_strategy.epsilon}\")\n",
    "\n",
    "    # Note: I left out the code regarding \"loss_rate\" because it wasn't completely provided. \n",
    "\n",
    "# Save the models\n",
    "torch.save(agent.online_policy_model.state_dict(), \"online_policy_model_defense.pt\")\n",
    "torch.save(agent.online_value_model.state_dict(), \"online_value_model_defense.pt\")\n",
    "torch.save(agent.target_policy_model.state_dict(), \"target_policy_model_defense.pt\")\n",
    "torch.save(agent.target_value_model.state_dict(), \"target_value_model_defense.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e63d5f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexej\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-d36108bd2232>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_steps_per_game\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m# Nutzen Sie den Agenten, um eine Aktion für player1 basierend auf dem aktuellen Zustand auszuwählen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alexej\\Desktop\\laser-hockey-env\\laserhockey\\hockey_env.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    687\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msurf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m       \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 689\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"render_fps\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    690\u001b[0m       \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"rgb_array\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import laserhockey.hockey_env as h_env\n",
    "import gymnasium as gym\n",
    "from importlib import reload\n",
    "import time\n",
    "\n",
    "env = h_env.HockeyEnv()\n",
    "\n",
    "player1 = h_env.BasicOpponent(weak=False)\n",
    "\n",
    "env.reset(mode=lh.LaserHockeyEnv)\n",
    "\n",
    "# Laden Sie die Gewichte in Ihren Agenten\n",
    "agent.online_policy_model.load_state_dict(torch.load(\"online_policy_model_defense.pt\"))\n",
    "agent.online_value_model.load_state_dict(torch.load(\"online_value_model_defense.pt\"))\n",
    "agent.target_policy_model.load_state_dict(torch.load(\"target_policy_model_defense.pt\"))\n",
    "agent.target_value_model.load_state_dict(torch.load(\"target_value_model_defense.pt\"))\n",
    "\n",
    "# Initialisieren Sie die Umgebung und den Basis-Opponenten\n",
    "player2 = lh.BasicOpponent()\n",
    "\n",
    "num_games = 10\n",
    "max_steps_per_game = 100\n",
    "\n",
    "for game in range(num_games):\n",
    "    obs, info = env.reset()\n",
    "    obs_agent2 = env.obs_agent_two()\n",
    "    \n",
    "    for step in range(max_steps_per_game):\n",
    "        env.render()\n",
    "\n",
    "        # Nutzen Sie den Agenten, um eine Aktion für player1 basierend auf dem aktuellen Zustand auszuwählen\n",
    "        state_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
    "        agent.online_policy_model.eval()\n",
    "        with torch.no_grad():\n",
    "            action_agent1 = np.squeeze(agent.online_policy_model(state_tensor).numpy())\n",
    "        agent.online_policy_model.train()\n",
    "\n",
    "        action_agent2 = player2.act(obs_agent2)\n",
    "        obs, r, d, _, info = env.step(np.hstack([action_agent1, action_agent2]))\n",
    "\n",
    "        obs_agent2 = env.obs_agent_two()\n",
    "\n",
    "        if d:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "83faa776",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h_env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-dd937f329403>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHockeyEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Einstellen der Spieler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplayer1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBasicOpponent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplayer2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHumanOpponent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplayer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'h_env' is not defined"
     ]
    }
   ],
   "source": [
    "env = h_env.HockeyEnv()\n",
    "\n",
    "# Einstellen der Spieler\n",
    "player1 = h_env.BasicOpponent()\n",
    "player2 = h_env.HumanOpponent(env=env, player=2)\n",
    "\n",
    "for game in range(10):\n",
    "    obs, info = env.reset()\n",
    "    obs_agent2 = env.obs_agent_two()\n",
    "\n",
    "    while True:\n",
    "        time.sleep(0.05)\n",
    "        env.render()\n",
    "\n",
    "        a1 = player1.act(obs)\n",
    "        a2 = player2.act(obs_agent2)\n",
    "        \n",
    "        obs, r, d, _, info = env.step(np.hstack([a1, a2]))\n",
    "        obs_agent2 = env.obs_agent_two()\n",
    "        \n",
    "        if d:  # Wenn das Spiel vorbei ist, brechen Sie die innere Schleife ab und starten Sie ein neues Spiel\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc151f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexej\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 2 scored\n",
      "Episode 1, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 1, Reward: -11.34528234995238, Moving Avg Reward: -11.34528234995238, HER Buffer Size: 44\n",
      "Player 2 scored\n",
      "Episode 2, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 2, Reward: -11.314224926309848, Moving Avg Reward: -11.329753638131113, HER Buffer Size: 89\n",
      "Player 2 scored\n",
      "Episode 3, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 3, Reward: -11.34327856418234, Moving Avg Reward: -11.334261946814856, HER Buffer Size: 134\n",
      "Player 2 scored\n",
      "Episode 4, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 4, Reward: -11.433561458543588, Moving Avg Reward: -11.359086824747038, HER Buffer Size: 177\n",
      "Episode 5, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 5, Reward: 0.24532738213854308, Moving Avg Reward: -9.038203983369922, HER Buffer Size: 337\n",
      "Player 1 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Episode 6, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 6, Reward: 11.02567151389122, Moving Avg Reward: -5.694224733826399, HER Buffer Size: 6035\n",
      "Player 1 scored\n",
      "Episode 7, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 7, Reward: 11.933985989865628, Moving Avg Reward: -3.17590891615611, HER Buffer Size: 7572\n",
      "Player 2 scored\n",
      "Episode 8, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 8, Reward: -11.292048153250752, Moving Avg Reward: -4.190426320792939, HER Buffer Size: 7614\n",
      "Player 2 scored\n",
      "Episode 9, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 9, Reward: -11.248027387264838, Moving Avg Reward: -4.974604217067595, HER Buffer Size: 7659\n",
      "Player 2 scored\n",
      "Episode 10, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 10, Reward: -11.357991032655223, Moving Avg Reward: -5.612942898626358, HER Buffer Size: 7702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexej\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 2 scored\n",
      "Episode 11, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 11, Reward: -11.264412706439584, Moving Avg Reward: -6.126712881154833, HER Buffer Size: 7746\n",
      "Episode 12, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 12, Reward: -2.4852056086371235, Moving Avg Reward: -5.823253941778357, HER Buffer Size: 7826\n",
      "Player 1 scored\n",
      "Episode 13, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 13, Reward: -0.9401759301082587, Moving Avg Reward: -5.447632556265272, HER Buffer Size: 11026\n",
      "Player 1 scored\n",
      "Episode 14, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 14, Reward: 11.028643763542176, Moving Avg Reward: -4.270755676279025, HER Buffer Size: 11926\n",
      "Player 2 scored\n",
      "Episode 15, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 15, Reward: -11.385371196566403, Moving Avg Reward: -4.745063377631518, HER Buffer Size: 11969\n",
      "Player 2 scored\n",
      "Episode 16, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 16, Reward: -11.369541840181174, Moving Avg Reward: -5.159093281540872, HER Buffer Size: 12010\n",
      "Player 2 scored\n",
      "Episode 17, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 17, Reward: -11.31852816777033, Moving Avg Reward: -5.521412980730839, HER Buffer Size: 12054\n",
      "Player 2 scored\n",
      "Episode 18, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 18, Reward: -11.38999413749459, Moving Avg Reward: -5.847445267217714, HER Buffer Size: 12099\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-438d5c5ba490>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m             \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[0mstate_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs_agent1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alexej\\Desktop\\laser-hockey-env\\laserhockey\\laser_hockey_env.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    611\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msurf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m             \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 613\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"render_fps\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    614\u001b[0m             \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"rgb_array\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Normal Noise Decay Strategy Latest Initialize DDPG Agent and Environment\n",
    "import numpy as n\n",
    "import random\n",
    "import torch\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "reload(lh)\n",
    "\n",
    "# Set the environment mode to TRAIN DEFENSE\n",
    "env = lh.LaserHockeyEnv()\n",
    "env.reset(mode=lh.LaserHockeyEnv.TRAIN_DEFENSE)\n",
    "initial_state_player2 = env._get_obs()[7:14]  # Capture the initial state of Player 2 after resetting the environment\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_bounds = (env.action_space.low, env.action_space.high)\n",
    "noise_strategy = NormalNoiseStrategy(action_bounds[0], action_bounds[1])\n",
    "\n",
    "\n",
    "def sample_goals(episode):\n",
    "    goals = []\n",
    "    for experience in episode:\n",
    "        _, _, reward, next_state, done, info = experience\n",
    "        # This assumes that you have added 'info' to your stored experiences\n",
    "        if info['reward_closeness_to_puck'] > 0 or info['reward_touch_puck'] > 0 or info['reward_puck_direction'] > 0:\n",
    "            goals.append(next_state)\n",
    "    return goals\n",
    "\n",
    "\n",
    "\n",
    "def compute_reward(state, action, goal):\n",
    "    _, _, _, _, info = env.step(action)\n",
    "    \n",
    "    closeness_reward = info.get('reward_closeness_to_puck', 0)\n",
    "    touch_reward = info.get('reward_touch_puck', 0)\n",
    "    direction_reward = info.get('reward_puck_direction', 0)\n",
    "    game_result = info.get('winner', 0)\n",
    "    Y_max = action_bounds[1][1]\n",
    "    Y_min = action_bounds[0][1]\n",
    "\n",
    "    upper_threshold = Y_max - (1/3) * (Y_max - Y_min)\n",
    "    lower_threshold = Y_min + (1/3) * (Y_max - Y_min)\n",
    "\n",
    "         \n",
    "    total_reward = 0  # Initialization of total_reward\n",
    "    # Combine rewards, possibly with weights to prioritize certain behaviors\n",
    "    \n",
    "\n",
    "    if game_result >= 0:\n",
    "        total_reward += 0.5\n",
    "\n",
    "    else:\n",
    "        total_reward -= 20\n",
    "\n",
    "    \n",
    "    if touch_reward == 1:\n",
    "        total_reward += 100\n",
    "\n",
    "    if closeness_reward >= 0:\n",
    "        total_reward += 20\n",
    "\n",
    "    if direction_reward >= 0:\n",
    "        total_reward += 20\n",
    "\n",
    "    # Add penalty for states that are in the top or bottom third of the field\n",
    "    y_position_player1 = state[1]  # y-position of the defending player\n",
    "\n",
    "    if y_position_player1 > upper_threshold or y_position_player1 < lower_threshold:\n",
    "        total_reward -= 100  # Deduct a penalty from the total reward\n",
    "\n",
    "    \n",
    "    return total_reward  # Return combined reward as a fallback\n",
    "\n",
    "\n",
    "her_buffer = HERBuffer(buffer_size=150000, goal_selection_strategy=sample_goals, reward_function=compute_reward)\n",
    "\n",
    "agent = DDPGAgent(state_dim=state_dim, action_dim=action_dim, action_bounds=action_bounds)\n",
    "agent.he_initialization()\n",
    "\n",
    "num_episodes = 10000\n",
    "batch_size = 1024\n",
    "train_start = 100  \n",
    "update_frequency = 1\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "wins = []\n",
    "\n",
    "\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    obs = env.reset()\n",
    "\n",
    "    obs_agent1 = env._get_obs()\n",
    "    episode_reward = 0\n",
    "    episode_value_losses = []\n",
    "    episode_policy_losses = []\n",
    "\n",
    "    episode_trajectory = []\n",
    "\n",
    "    for step in range(80):\n",
    "        \n",
    "        if episode >= 10:\n",
    "            obs = env.render()\n",
    "\n",
    "        state_tensor = torch.FloatTensor(obs_agent1).unsqueeze(0)\n",
    "        agent.online_policy_model.eval()\n",
    "        action_player1 = noise_strategy.select_action(agent.online_policy_model, state_tensor, episode)\n",
    "\n",
    "        \n",
    "        agent.online_policy_model.train()\n",
    "\n",
    "        next_obs, reward, done, _, info = env.step(action_player1)\n",
    "        next_obs[7:14] = initial_state_player2  # Set the state of Player 2 to its initial state\n",
    "        \n",
    "        reward += info['winner']\n",
    "        reward += info['reward_closeness_to_puck']\n",
    "        reward += info['reward_touch_puck']\n",
    "        reward += info['reward_puck_direction']\n",
    "\n",
    "        #print(f\"info: {info}\")\n",
    "        episode_reward += reward\n",
    "\n",
    "        obs_agent1 = env._get_obs()\n",
    "        experience = (obs_agent1, action_player1, reward, next_obs, done, info)  # Added 'info' at the end\n",
    "        episode_trajectory.append(experience)\n",
    "\n",
    "\n",
    "        # DDPG Training\n",
    "        if episode > train_start and len(her_buffer) > batch_size:\n",
    "            experiences = her_buffer.sample(batch_size)\n",
    "            states, actions, rewards, next_states, is_terminals = [torch.FloatTensor(x_np).to(device) for x_np in experiences]\n",
    "            \n",
    "            value_loss, policy_loss = agent.optimize_model((states, actions, rewards, next_states, is_terminals))\n",
    "            episode_value_losses.append(value_loss)\n",
    "            episode_policy_losses.append(policy_loss)\n",
    "\n",
    "        if step == 79: \n",
    "            done = True\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Store trajectories experiences with HER logic, after episode is done\n",
    "    her_goals = sample_goals(episode_trajectory)\n",
    "    for experience in episode_trajectory:\n",
    "        state, action, reward, next_state, done, info = experience\n",
    "        # Store the original experience\n",
    "        her_buffer.store(state, action, reward, next_state, done)\n",
    "\n",
    "        for goal in her_goals:\n",
    "            # Recompute the reward and store the adjusted experience\n",
    "            her_reward = compute_reward(state, action, goal)\n",
    "            her_buffer.store(state, action, her_reward, next_state, done)\n",
    "\n",
    "    \n",
    "    all_rewards.append(episode_reward)\n",
    "    moving_avg_reward = np.mean(all_rewards[-100:])\n",
    "    avg_value_loss = sum(episode_value_losses) / len(episode_value_losses) if episode_value_losses else 0\n",
    "    avg_policy_loss = sum(episode_policy_losses) / len(episode_policy_losses) if episode_policy_losses else 0\n",
    "\n",
    "    print(f\"Episode {episode + 1}, Avg Value Loss: {avg_value_loss}, Avg Policy Loss: {avg_policy_loss}\")\n",
    "    print(f\"Episode {episode + 1}, Reward: {episode_reward}, Moving Avg Reward: {moving_avg_reward}, HER Buffer Size: {len(her_buffer)}\")\n",
    "    \n",
    "\n",
    "    # Note: I left out the code regarding \"loss_rate\" because it wasn't completely provided. \n",
    "\n",
    "# Save the models\n",
    "torch.save(agent.online_policy_model.state_dict(), \"online_policy_model_defense.pt\")\n",
    "torch.save(agent.online_value_model.state_dict(), \"online_value_model_defense.pt\")\n",
    "torch.save(agent.target_policy_model.state_dict(), \"target_policy_model_defense.pt\")\n",
    "torch.save(agent.target_value_model.state_dict(), \"target_value_model_defense.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c12316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Version 3 Policy\n",
    "\n",
    "class FCDP(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim,\n",
    "                 action_bounds,\n",
    "                 hidden_sizes=(256, 256), \n",
    "                 activation_fc=F.relu,\n",
    "                 out_activation_fc=F.tanh,\n",
    "                 learning_rate=0.0001,\n",
    "                 lr_milestones=[1000, 5000],\n",
    "                 lr_factor=0.5):\n",
    "        super(FCDP, self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "        self.out_activation_fc = out_activation_fc\n",
    "        self.env_min, self.env_max = action_bounds\n",
    "        self.device = device\n",
    "        # Layers\n",
    "        layer_sizes = [input_dim] + list(hidden_sizes) + [len(self.env_max)]\n",
    "        self.layers = nn.ModuleList([nn.Linear(i, o) for i, o in zip(layer_sizes[:-1], layer_sizes[1:])])\n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=lr_milestones, gamma=lr_factor)\n",
    "\n",
    "        # Action scaling\n",
    "        self.env_min = torch.tensor(self.env_min, device=self.device, dtype=torch.float32)\n",
    "        self.env_max = torch.tensor(self.env_max, device=self.device, dtype=torch.float32)\n",
    "        \n",
    "        self.nn_min = self.out_activation_fc(torch.Tensor([float('-inf')])).to(self.device)\n",
    "        self.nn_max = self.out_activation_fc(torch.Tensor([float('inf')])).to(self.device)\n",
    "        self.rescale_fn = lambda x: (x - self.nn_min) * (self.env_max - self.env_min) / (self.nn_max - self.nn_min) + self.env_min\n",
    "\n",
    "    def _format(self, state):\n",
    "        x = state\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, device=self.device, dtype=torch.float32)\n",
    "            x = x.unsqueeze(0)\n",
    "        return x\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self._format(state)\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.activation_fc(layer(x))\n",
    "        x = self.out_activation_fc(self.layers[-1](x))\n",
    "        return self.rescale_fn(x)\n",
    "\n",
    "    def predict(self, state):\n",
    "        state = np.array(state)\n",
    "        with torch.no_grad():\n",
    "            return self.forward(torch.from_numpy(state.astype(np.float32)).to(self.device)).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67280364",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Version 3 Value\n",
    "\n",
    "class FCQV(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 output_dim, \n",
    "                 device,\n",
    "                 learning_rate=0.001,\n",
    "                 lr_milestones=[1000, 5000],\n",
    "                 lr_factor=0.5,\n",
    "                 hidden_sizes=(256, 256), \n",
    "                 activation_fc=F.relu):\n",
    "        super(FCQV, self).__init__()\n",
    "        \n",
    "        self.num_inputs = input_dim\n",
    "        self.n_actions = output_dim\n",
    "        self.activation_fc = activation_fc\n",
    "        self.device = device\n",
    "\n",
    "        # Create layers\n",
    "        layer_sizes = [self.num_inputs + self.n_actions] + list(hidden_sizes) + [1]\n",
    "        self.layers = nn.ModuleList([nn.Linear(i, o) for i, o in zip(layer_sizes[:-1], layer_sizes[1:])])\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=lr_milestones, gamma=lr_factor)\n",
    "        self.loss = torch.nn.MSELoss()\n",
    "\n",
    "    def _format(self, state, action):\n",
    "        x, u = state, action\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, device=self.device, dtype=torch.float32)\n",
    "            x = x.unsqueeze(0)\n",
    "        if not isinstance(u, torch.Tensor):\n",
    "            u = torch.tensor(u, device=self.device, dtype=torch.float32)\n",
    "            u = u.unsqueeze(0)\n",
    "        return x, u\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x, u = self._format(state, action)\n",
    "        x = torch.cat((x, u), dim=1)\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.activation_fc(layer(x))\n",
    "        return self.layers[-1](x)\n",
    "    \n",
    "    def predict(self, state, action):\n",
    "        state = np.array(state)\n",
    "        action = np.array(action)\n",
    "        with torch.no_grad():\n",
    "            return self.forward(torch.from_numpy(state.astype(np.float32)).to(self.device), \n",
    "                                torch.from_numpy(action.astype(np.float32)).to(self.device)).cpu().numpy()\n",
    "\n",
    "    def load(self, experiences):\n",
    "        states, actions, new_states, rewards, is_terminals = experiences\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        actions = torch.from_numpy(actions).float().to(self.device)\n",
    "        new_states = torch.from_numpy(new_states).float().to(self.device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(self.device)\n",
    "        is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n",
    "        return states, actions, new_states, rewards, is_terminals\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gym-RL",
   "language": "python",
   "name": "gym-rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
