{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "308d6939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import laserhockey.laser_hockey_env as lh\n",
    "import gymnasium as gym\n",
    "from importlib import reload\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from copy import deepcopy\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acbaa513",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f73d6669",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Version 1\n",
    "class FCQV(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dims=(256, 256), activation_fc=F.relu):\n",
    "        super(FCQV, self).__init__()\n",
    "\n",
    "        self.activation_fc = activation_fc\n",
    "        self.input_layer = nn.Linear(state_dim + action_dim, hidden_dims[0])\n",
    "        self.bn_input = nn.BatchNorm1d(hidden_dims[0])\n",
    "        \n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.bn_hidden = nn.ModuleList()\n",
    "        \n",
    "        # Add two hidden layers with 128 neurons each\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
    "            bn_layer = nn.BatchNorm1d(hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "            self.bn_hidden.append(bn_layer)\n",
    "        \n",
    "        # Output layer with a single neuron for the Q-value estimate\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        # Combining state and action right at the beginning\n",
    "        x = torch.cat((state, action), dim=1)\n",
    "        x = self.activation_fc(self.bn_input(self.input_layer(x)))\n",
    "        \n",
    "        for hidden_layer, bn_layer in zip(self.hidden_layers, self.bn_hidden):\n",
    "            x = self.activation_fc(bn_layer(hidden_layer(x)))\n",
    "            \n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a139cac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Version 2 \n",
    "class FCQV(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 output_dim, \n",
    "                 hidden_dims=(128,128), \n",
    "                 activation_fc=F.relu):\n",
    "        super(FCQV, self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            in_dim = hidden_dims[i]\n",
    "            if i == 0: \n",
    "                in_dim += output_dim\n",
    "            hidden_layer = nn.Linear(in_dim, hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], 1)\n",
    "\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "        self.device = torch.device(device)\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def _format(self, state, action):\n",
    "        x, u = state, action\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, \n",
    "                             device=self.device, \n",
    "                             dtype=torch.float32)\n",
    "            x = x.unsqueeze(0)\n",
    "        if not isinstance(u, torch.Tensor):\n",
    "            u = torch.tensor(u, \n",
    "                             device=self.device, \n",
    "                             dtype=torch.float32)\n",
    "            u = u.unsqueeze(0)\n",
    "        return x, u\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x, u = self._format(state, action)\n",
    "        x = self.activation_fc(self.input_layer(x))\n",
    "        for i, hidden_layer in enumerate(self.hidden_layers):\n",
    "            if i == 0:\n",
    "                x = torch.cat((x, u), dim=1)\n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        return self.output_layer(x)\n",
    "    \n",
    "    def load(self, experiences):\n",
    "        states, actions, new_states, rewards, is_terminals = experiences\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        actions = torch.from_numpy(actions).float().to(self.device)\n",
    "        new_states = torch.from_numpy(new_states).float().to(self.device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(self.device)\n",
    "        is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n",
    "        return states, actions, new_states, rewards, is_terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "73fea739",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Version 1 Policy\n",
    "class FCDP(nn.Module):\n",
    "    def __init__(self, input_dim, action_bounds, hidden_dims=(256, 256), activation_fc=F.relu, out_activation_fc=F.tanh):\n",
    "        super(FCDP, self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "        self.out_activation_fc = out_activation_fc\n",
    "        self.env_min, self.env_max = action_bounds\n",
    "        \n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.bn_input = nn.BatchNorm1d(hidden_dims[0])\n",
    "        \n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.bn_hidden = nn.ModuleList()\n",
    "        \n",
    "        # Add two hidden layers with 128 neurons each\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
    "            bn_layer = nn.BatchNorm1d(hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "            self.bn_hidden.append(bn_layer)\n",
    "            \n",
    "        # Output layer with the same number of neurons as the action bounds\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], len(self.env_max))\n",
    "        \n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.env_min_tensor = torch.tensor(self.env_min, dtype=torch.float32, device=device)\n",
    "        self.env_max_tensor = torch.tensor(self.env_max, dtype=torch.float32, device=device)\n",
    "        self.rescale_fn = lambda x: (x + 1) / 2 * (self.env_max_tensor - self.env_min_tensor) + self.env_min_tensor\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state  # Assuming state is already formatted\n",
    "        x = self.activation_fc(self.bn_input(self.input_layer(x)))\n",
    "        \n",
    "        for hidden_layer, bn_layer in zip(self.hidden_layers, self.bn_hidden):\n",
    "            x = self.activation_fc(bn_layer(hidden_layer(x)))\n",
    "            \n",
    "        x = self.output_layer(x)\n",
    "        x = self.out_activation_fc(x)\n",
    "        return self.rescale_fn(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a38dc7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Version 2 Policy\n",
    "\n",
    "class FCDP(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim,\n",
    "                 action_bounds,\n",
    "                 hidden_dims=(128,128), \n",
    "                 activation_fc=F.relu,\n",
    "                 out_activation_fc=F.tanh):\n",
    "        super(FCDP, self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "        self.out_activation_fc = out_activation_fc\n",
    "        self.env_min, self.env_max = action_bounds\n",
    "\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], len(self.env_max))\n",
    "\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "        self.device = torch.device(device)\n",
    "        self.to(self.device)\n",
    "        \n",
    "        self.env_min = torch.tensor(self.env_min,\n",
    "                                    device=self.device, \n",
    "                                    dtype=torch.float32)\n",
    "\n",
    "        self.env_max = torch.tensor(self.env_max,\n",
    "                                    device=self.device, \n",
    "                                    dtype=torch.float32)\n",
    "        \n",
    "        self.nn_min = self.out_activation_fc(\n",
    "            torch.Tensor([float('-inf')])).to(self.device)\n",
    "        self.nn_max = self.out_activation_fc(\n",
    "            torch.Tensor([float('inf')])).to(self.device)\n",
    "        self.rescale_fn = lambda x: (x - self.nn_min) * (self.env_max - self.env_min) / \\\n",
    "                                    (self.nn_max - self.nn_min) + self.env_min\n",
    "\n",
    "    def _format(self, state):\n",
    "        x = state\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, \n",
    "                             device=self.device, \n",
    "                             dtype=torch.float32)\n",
    "            x = x.unsqueeze(0)\n",
    "        return x\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self._format(state)\n",
    "        x = self.activation_fc(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        x = self.out_activation_fc(x)\n",
    "        return self.rescale_fn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42fc7c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent:\n",
    "    def __init__(self, state_dim, action_dim, action_bounds, gamma=0.95, lr_value=0.0002, lr_policy=0.0002, value_max_grad_norm=1.0, policy_max_grad_norm=1.0):\n",
    "        self.gamma = gamma\n",
    "        self.tau = 0.005\n",
    "        self.value_max_grad_norm = value_max_grad_norm\n",
    "        self.policy_max_grad_norm = policy_max_grad_norm\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "       \n",
    "        \n",
    "        # Action bounds\n",
    "        self.env_min, self.env_max = action_bounds\n",
    "        \n",
    "        # Action dimension\n",
    "        self.action_dim = action_dim  \n",
    "        \n",
    "        # Network models\n",
    "        self.online_value_model = FCQV(state_dim, action_dim)\n",
    "        self.target_value_model = FCQV(state_dim, action_dim)\n",
    "        self.online_policy_model = FCDP(state_dim, action_bounds)\n",
    "        self.target_policy_model = FCDP(state_dim, action_bounds)\n",
    "        self.target_value_model.load_state_dict(self.online_value_model.state_dict())\n",
    "        self.target_policy_model.load_state_dict(self.online_policy_model.state_dict())\n",
    "        self.value_optimizer = optim.Adam(self.online_value_model.parameters(), lr=lr_value)\n",
    "        self.policy_optimizer = optim.Adam(self.online_policy_model.parameters(), lr=lr_policy)\n",
    "\n",
    "    #def soft_update(self, online_model, target_model):\n",
    "    #    for target_param, online_param in zip(target_model.parameters(), online_model.parameters()):\n",
    "    #        target_param.data.copy_(self.tau * online_param.data + (1.0 - self.tau) * target_param.data)\n",
    "\n",
    "    def soft_update(self, online_model, target_model):\n",
    "        tau = 0.0001\n",
    "        for target, online in zip(self.target_value_model.parameters(), \n",
    "                                  self.online_value_model.parameters()):\n",
    "            target_ratio = (1.0 - tau) * target.data\n",
    "            online_ratio = tau * online.data\n",
    "            mixed_weights = target_ratio + online_ratio\n",
    "            target.data.copy_(mixed_weights)\n",
    "\n",
    "        for target, online in zip(self.target_policy_model.parameters(), \n",
    "                                  self.online_policy_model.parameters()):\n",
    "            target_ratio = (1.0 - tau) * target.data\n",
    "            online_ratio = tau * online.data\n",
    "            mixed_weights = target_ratio + online_ratio\n",
    "            target.data.copy_(mixed_weights)\n",
    "\n",
    "    def optimize_model(self, experiences):\n",
    "        states, actions, rewards, next_states, is_terminals = experiences\n",
    "        batch_size = len(is_terminals)\n",
    "\n",
    "        \n",
    "\n",
    "        argmax_a_q_sp = self.target_policy_model(next_states)\n",
    "        max_a_q_sp = self.target_value_model(next_states, argmax_a_q_sp)\n",
    "\n",
    "        target_q_sa = rewards + self.gamma * max_a_q_sp * (1 - is_terminals)\n",
    "\n",
    "        #L2 Loss\n",
    "        q_sa = self.online_value_model(states, actions)\n",
    "        td_error = q_sa - target_q_sa.detach()\n",
    "        value_loss = td_error.pow(2).mul(0.5).mean()\n",
    "\n",
    "        #Huber Loss\n",
    "        #loss_function = torch.nn.SmoothL1Loss(reduction='mean')\n",
    "        #value_loss = loss_function(q_sa, target_q_sa.detach())\n",
    "\n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.online_value_model.parameters(), self.value_max_grad_norm)\n",
    "        self.value_optimizer.step()\n",
    "        \n",
    "        argmax_a_q_s = self.online_policy_model(states)\n",
    "        max_a_q_s = self.online_value_model(states, argmax_a_q_s)\n",
    "        policy_loss = -max_a_q_s.mean()\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.online_policy_model.parameters(), self.policy_max_grad_norm)\n",
    "        self.policy_optimizer.step()\n",
    "        # Soft Update der Ziel-Netzwerke\n",
    "        self.soft_update(self.online_value_model, self.target_value_model)\n",
    "        self.soft_update(self.online_policy_model, self.target_policy_model)\n",
    "\n",
    "        return value_loss.item(), policy_loss.item()\n",
    "    \n",
    "\n",
    "    \n",
    "    def he_initialization(self):\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            \n",
    "        # Apply He initialization to the network models\n",
    "        self.online_value_model.apply(init_weights)\n",
    "        self.target_value_model.apply(init_weights)\n",
    "        self.online_policy_model.apply(init_weights)\n",
    "        self.target_policy_model.apply(init_weights)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e54043c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalNoiseStrategy():\n",
    "    def __init__(self, low, high, exploration_noise_ratio=0.1):\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "        self.exploration_noise_ratio = exploration_noise_ratio\n",
    "        self.ratio_noise_injected = 0\n",
    "\n",
    "    def _noise_ratio_update(self):\n",
    "        self.exploration_noise_ratio *= 0.9999\n",
    "        return self.exploration_noise_ratio\n",
    "\n",
    "    def select_action(self, model, state, max_exploration=False):\n",
    "        if max_exploration:\n",
    "            noise_scale = self.high\n",
    "        else:\n",
    "            noise_scale = self.exploration_noise_ratio * self.high\n",
    "\n",
    "        with torch.no_grad():\n",
    "            greedy_action = model(state).cpu().detach().data.numpy().squeeze()\n",
    "\n",
    "        noise = np.random.normal(loc=0, scale=noise_scale, size=len(self.high))\n",
    "        noisy_action = greedy_action + noise\n",
    "\n",
    "        # Keep Player 2 static at all times\n",
    "        noisy_action[3] = 0  # Movement in x-direction for Player 2\n",
    "        noisy_action[4] = 0  # Movement in y-direction for Player 2\n",
    "        noisy_action[5] = 0  # Rotation for Player 2\n",
    "\n",
    "        action = np.clip(noisy_action, self.low, self.high)\n",
    "        \n",
    "        self.ratio_noise_injected = np.mean(abs((greedy_action - action) / (self.high - self.low)))\n",
    "        self._noise_ratio_update()\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "61d47e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon Greedy Strategy\n",
    "class EpsilonGreedyStrategy:\n",
    "    def __init__(self, start_epsilon, end_epsilon, decay, action_space):\n",
    "        self.epsilon = start_epsilon\n",
    "        self.end_epsilon = end_epsilon\n",
    "        self.decay = decay\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def select_action(self, model, state_tensor, episode):\n",
    "        if episode < train_start:\n",
    "            # Only allow movements up or down for Player 1\n",
    "            random_action = self.action_space.sample()\n",
    "            random_action[4] = 0 # Zufällige Bewegung in y-Richtung für Spieler 2\n",
    "            random_action[5] = 0  # Zufällige Rotation für Spieler 2\n",
    "            random_action[6] = 0  # Zufällige Rotation für Spieler 2\n",
    "            random_action[7] = 0  # Zufällige Rotation für Spieler 2\n",
    "            self.epsilon = 1.0\n",
    "            return random_action\n",
    "        elif np.random.rand() > self.epsilon:  # Exploitation: Mit Wahrscheinlichkeit 1-epsilon\n",
    "            with torch.no_grad():\n",
    "                model_output = model(state_tensor).cpu().data.numpy().squeeze()\n",
    "                # Resetting indices 4 and beyond to zero for defense \n",
    "                model_output[4:] = 0\n",
    "                return model_output\n",
    "        else:  # Exploration: Mit Wahrscheinlichkeit epsilon\n",
    "            random_action = self.action_space.sample()\n",
    "            #random_action[3] = 0  # Zufällige Bewegung in x-Richtung für Spieler 2\n",
    "            random_action[4] = 0 # Zufällige Bewegung in y-Richtung für Spieler 2\n",
    "            random_action[5] = 0  # Zufällige Rotation für Spieler 2\n",
    "            random_action[6] = 0  # Zufällige Rotation für Spieler 2\n",
    "            random_action[7] = 0  # Zufällige Rotation für Spieler 2\n",
    "            return random_action\n",
    "\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        if self.epsilon > self.end_epsilon:\n",
    "            self.epsilon *= self.decay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replay Buffer for DDPG\n",
    "import random\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def store(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = args\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return zip(*random.sample(self.memory, batch_size))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cddeca53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Draw\n",
      "Episode 1, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 1, Reward: 12.643787730378742, Moving Avg Reward: 12.643787730378742, Replay Buffer Size: 20\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 0 episodes: 0.00\n",
      "Episode 1, Reward: 12.643787730378742, Moving Avg Reward: 12.643787730378742, Replay Buffer Size: 20\n",
      "Current Epsilon: 0.995\n",
      "Episode 2: Won\n",
      "Episode 2, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 2, Reward: 14.771109965632832, Moving Avg Reward: 13.707448848005786, Replay Buffer Size: 41\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 0 episodes: 0.00\n",
      "Episode 2, Reward: 14.771109965632832, Moving Avg Reward: 13.707448848005786, Replay Buffer Size: 41\n",
      "Current Epsilon: 0.995\n",
      "Episode 3: Won\n",
      "Episode 3, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 3, Reward: -22.59487860835752, Moving Avg Reward: 1.6066730292180178, Replay Buffer Size: 112\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 1 episodes: 0.00\n",
      "Episode 3, Reward: -22.59487860835752, Moving Avg Reward: 1.6066730292180178, Replay Buffer Size: 112\n",
      "Current Epsilon: 0.995\n",
      "Episode 4: Won\n",
      "Episode 4, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 4, Reward: -26.283949034694974, Moving Avg Reward: -5.36598248676023, Replay Buffer Size: 192\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 2 episodes: 0.00\n",
      "Episode 4, Reward: -26.283949034694974, Moving Avg Reward: -5.36598248676023, Replay Buffer Size: 192\n",
      "Current Epsilon: 0.995\n",
      "Episode 5: Won\n",
      "Episode 5, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 5, Reward: -23.059958054383152, Moving Avg Reward: -8.904777600284813, Replay Buffer Size: 249\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 3 episodes: 0.00\n",
      "Episode 5, Reward: -23.059958054383152, Moving Avg Reward: -8.904777600284813, Replay Buffer Size: 249\n",
      "Current Epsilon: 0.995\n",
      "Episode 6: Won\n",
      "Episode 6, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 6, Reward: -24.520070850865192, Moving Avg Reward: -11.507326475381545, Replay Buffer Size: 329\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 4 episodes: 0.00\n",
      "Episode 6, Reward: -24.520070850865192, Moving Avg Reward: -11.507326475381545, Replay Buffer Size: 329\n",
      "Current Epsilon: 0.995\n",
      "Episode 7: Won\n",
      "Episode 7, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 7, Reward: -27.61168185187114, Moving Avg Reward: -13.807948672022915, Replay Buffer Size: 341\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 4 episodes: 0.23\n",
      "Episode 7, Reward: -27.61168185187114, Moving Avg Reward: -13.807948672022915, Replay Buffer Size: 341\n",
      "Current Epsilon: 0.995\n",
      "Episode 8: Won\n",
      "Episode 8, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 8, Reward: -20.915906126368974, Moving Avg Reward: -14.696443353816171, Replay Buffer Size: 421\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 5 episodes: 0.19\n",
      "Episode 8, Reward: -20.915906126368974, Moving Avg Reward: -14.696443353816171, Replay Buffer Size: 421\n",
      "Current Epsilon: 0.995\n",
      "Episode 9: Lost\n",
      "Episode 9, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 9, Reward: -27.762028969685183, Moving Avg Reward: -16.14817508891273, Replay Buffer Size: 501\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 6 episodes: 0.16\n",
      "Episode 9, Reward: -27.762028969685183, Moving Avg Reward: -16.14817508891273, Replay Buffer Size: 501\n",
      "Current Epsilon: 0.995\n",
      "Episode 10: Draw\n",
      "Episode 10, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 10, Reward: -65.82676331950695, Moving Avg Reward: -21.116033911972153, Replay Buffer Size: 573\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 7 episodes: 0.28\n",
      "Episode 10, Reward: -65.82676331950695, Moving Avg Reward: -21.116033911972153, Replay Buffer Size: 573\n",
      "Current Epsilon: 0.995\n",
      "Episode 11: Lost\n",
      "Episode 11, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 11, Reward: -24.11083818884948, Moving Avg Reward: -21.388288846233728, Replay Buffer Size: 582\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 7 episodes: 0.27\n",
      "Episode 11, Reward: -24.11083818884948, Moving Avg Reward: -21.388288846233728, Replay Buffer Size: 582\n",
      "Current Epsilon: 0.995\n",
      "Episode 12: Lost\n",
      "Episode 12, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 12, Reward: -6.727373284106342, Moving Avg Reward: -20.16654588272311, Replay Buffer Size: 662\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 8 episodes: 0.24\n",
      "Episode 12, Reward: -6.727373284106342, Moving Avg Reward: -20.16654588272311, Replay Buffer Size: 662\n",
      "Current Epsilon: 0.995\n",
      "Episode 13: Lost\n",
      "Episode 13, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 13, Reward: -35.53481704414787, Moving Avg Reward: -21.348720587448096, Replay Buffer Size: 742\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 9 episodes: 0.22\n",
      "Episode 13, Reward: -35.53481704414787, Moving Avg Reward: -21.348720587448096, Replay Buffer Size: 742\n",
      "Current Epsilon: 0.995\n",
      "Episode 14: Draw\n",
      "Episode 14, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 14, Reward: -3.903477066854858, Moving Avg Reward: -20.10263176454858, Replay Buffer Size: 809\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 10 episodes: 0.20\n",
      "Episode 14, Reward: -3.903477066854858, Moving Avg Reward: -20.10263176454858, Replay Buffer Size: 809\n",
      "Current Epsilon: 0.995\n",
      "Episode 15: Won\n",
      "Episode 15, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 15, Reward: -77.09661713072508, Moving Avg Reward: -23.902230788960345, Replay Buffer Size: 889\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 11 episodes: 0.18\n",
      "Episode 15, Reward: -77.09661713072508, Moving Avg Reward: -23.902230788960345, Replay Buffer Size: 889\n",
      "Current Epsilon: 0.995\n",
      "Episode 16: Won\n",
      "Episode 16, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 16, Reward: 13.671783481662668, Moving Avg Reward: -21.553854897046406, Replay Buffer Size: 915\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 11 episodes: 0.17\n",
      "Episode 16, Reward: 13.671783481662668, Moving Avg Reward: -21.553854897046406, Replay Buffer Size: 915\n",
      "Current Epsilon: 0.995\n",
      "Episode 17: Won\n",
      "Episode 17, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 17, Reward: -25.68391279660203, Moving Avg Reward: -21.79679947937321, Replay Buffer Size: 984\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 12 episodes: 0.24\n",
      "Episode 17, Reward: -25.68391279660203, Moving Avg Reward: -21.79679947937321, Replay Buffer Size: 984\n",
      "Current Epsilon: 0.995\n",
      "Episode 18: Won\n",
      "Episode 18, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 18, Reward: -46.703802456829266, Moving Avg Reward: -23.18052186700966, Replay Buffer Size: 1027\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 12 episodes: 0.23\n",
      "Episode 18, Reward: -46.703802456829266, Moving Avg Reward: -23.18052186700966, Replay Buffer Size: 1027\n",
      "Current Epsilon: 0.995\n",
      "Episode 19: Won\n",
      "Episode 19, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 19, Reward: -23.773223508691892, Moving Avg Reward: -23.211716690256093, Replay Buffer Size: 1035\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 12 episodes: 0.23\n",
      "Episode 19, Reward: -23.773223508691892, Moving Avg Reward: -23.211716690256093, Replay Buffer Size: 1035\n",
      "Current Epsilon: 0.995\n",
      "Episode 20: Won\n",
      "Episode 20, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 20, Reward: -26.276640765514003, Moving Avg Reward: -23.364962894018987, Replay Buffer Size: 1045\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 13 episodes: 0.31\n",
      "Episode 20, Reward: -26.276640765514003, Moving Avg Reward: -23.364962894018987, Replay Buffer Size: 1045\n",
      "Current Epsilon: 0.995\n",
      "Episode 21: Won\n",
      "Episode 21, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 21, Reward: -24.394827523371575, Moving Avg Reward: -23.4140040668453, Replay Buffer Size: 1052\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 13 episodes: 0.30\n",
      "Episode 21, Reward: -24.394827523371575, Moving Avg Reward: -23.4140040668453, Replay Buffer Size: 1052\n",
      "Current Epsilon: 0.995\n",
      "Episode 22: Won\n",
      "Episode 22, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 22, Reward: 12.64854621403764, Moving Avg Reward: -21.774797235896077, Replay Buffer Size: 1090\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 13 episodes: 0.29\n",
      "Episode 22, Reward: 12.64854621403764, Moving Avg Reward: -21.774797235896077, Replay Buffer Size: 1090\n",
      "Current Epsilon: 0.995\n",
      "Episode 23: Won\n",
      "Episode 23, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 23, Reward: -20.465236733656166, Moving Avg Reward: -21.717859822755212, Replay Buffer Size: 1097\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 13 episodes: 0.29\n",
      "Episode 23, Reward: -20.465236733656166, Moving Avg Reward: -21.717859822755212, Replay Buffer Size: 1097\n",
      "Current Epsilon: 0.995\n",
      "Episode 24: Won\n",
      "Episode 24, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 24, Reward: 12.38054169523772, Moving Avg Reward: -20.297093092838836, Replay Buffer Size: 1121\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 14 episodes: 0.29\n",
      "Episode 24, Reward: 12.38054169523772, Moving Avg Reward: -20.297093092838836, Replay Buffer Size: 1121\n",
      "Current Epsilon: 0.995\n",
      "Episode 25: Won\n",
      "Episode 25, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 25, Reward: -31.67826181030145, Moving Avg Reward: -20.752339841537342, Replay Buffer Size: 1134\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 14 episodes: 0.35\n",
      "Episode 25, Reward: -31.67826181030145, Moving Avg Reward: -20.752339841537342, Replay Buffer Size: 1134\n",
      "Current Epsilon: 0.995\n",
      "Episode 26: Won\n",
      "Episode 26, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 26, Reward: -17.48978898291852, Moving Avg Reward: -20.626857116205848, Replay Buffer Size: 1214\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 15 episodes: 0.33\n",
      "Episode 26, Reward: -17.48978898291852, Moving Avg Reward: -20.626857116205848, Replay Buffer Size: 1214\n",
      "Current Epsilon: 0.995\n",
      "Episode 27: Won\n",
      "Episode 27, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 27, Reward: 16.51042656581533, Moving Avg Reward: -19.25140216501988, Replay Buffer Size: 1237\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 15 episodes: 0.32\n",
      "Episode 27, Reward: 16.51042656581533, Moving Avg Reward: -19.25140216501988, Replay Buffer Size: 1237\n",
      "Current Epsilon: 0.995\n",
      "Episode 28: Won\n",
      "Episode 28, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 28, Reward: -21.960506879379825, Moving Avg Reward: -19.348155904818448, Replay Buffer Size: 1244\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 15 episodes: 0.39\n",
      "Episode 28, Reward: -21.960506879379825, Moving Avg Reward: -19.348155904818448, Replay Buffer Size: 1244\n",
      "Current Epsilon: 0.995\n",
      "Episode 29: Won\n",
      "Episode 29, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 29, Reward: -22.070756912712874, Moving Avg Reward: -19.44203869819412, Replay Buffer Size: 1253\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 15 episodes: 0.38\n",
      "Episode 29, Reward: -22.070756912712874, Moving Avg Reward: -19.44203869819412, Replay Buffer Size: 1253\n",
      "Current Epsilon: 0.995\n",
      "Episode 30: Won\n",
      "Episode 30, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 30, Reward: -24.120431115851048, Moving Avg Reward: -19.597985112116017, Replay Buffer Size: 1262\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 15 episodes: 0.38\n",
      "Episode 30, Reward: -24.120431115851048, Moving Avg Reward: -19.597985112116017, Replay Buffer Size: 1262\n",
      "Current Epsilon: 0.995\n",
      "Episode 31: Won\n",
      "Episode 31, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 31, Reward: 10.284851510578441, Moving Avg Reward: -18.634022640416198, Replay Buffer Size: 1340\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 16 episodes: 0.36\n",
      "Episode 31, Reward: 10.284851510578441, Moving Avg Reward: -18.634022640416198, Replay Buffer Size: 1340\n",
      "Current Epsilon: 0.995\n",
      "Episode 32: Won\n",
      "Episode 32, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 32, Reward: -28.62084053591759, Moving Avg Reward: -18.94611069965061, Replay Buffer Size: 1350\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 16 episodes: 0.41\n",
      "Episode 32, Reward: -28.62084053591759, Moving Avg Reward: -18.94611069965061, Replay Buffer Size: 1350\n",
      "Current Epsilon: 0.995\n",
      "Episode 33: Won\n",
      "Episode 33, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 33, Reward: -1.232159481546041, Moving Avg Reward: -18.40932429910199, Replay Buffer Size: 1430\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 17 episodes: 0.39\n",
      "Episode 33, Reward: -1.232159481546041, Moving Avg Reward: -18.40932429910199, Replay Buffer Size: 1430\n",
      "Current Epsilon: 0.995\n",
      "Episode 34: Won\n",
      "Episode 34, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 34, Reward: -25.08016753929971, Moving Avg Reward: -18.605525570872512, Replay Buffer Size: 1440\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 18 episodes: 0.44\n",
      "Episode 34, Reward: -25.08016753929971, Moving Avg Reward: -18.605525570872512, Replay Buffer Size: 1440\n",
      "Current Epsilon: 0.995\n",
      "Episode 35: Won\n",
      "Episode 35, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 35, Reward: 9.591555475056708, Moving Avg Reward: -17.799894683845963, Replay Buffer Size: 1477\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 18 episodes: 0.43\n",
      "Episode 35, Reward: 9.591555475056708, Moving Avg Reward: -17.799894683845963, Replay Buffer Size: 1477\n",
      "Current Epsilon: 0.995\n",
      "Episode 36: Won\n",
      "Episode 36, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 36, Reward: -22.406956007376778, Moving Avg Reward: -17.927868609499594, Replay Buffer Size: 1484\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 18 episodes: 0.49\n",
      "Episode 36, Reward: -22.406956007376778, Moving Avg Reward: -17.927868609499594, Replay Buffer Size: 1484\n",
      "Current Epsilon: 0.995\n",
      "Episode 37: Won\n",
      "Episode 37, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 37, Reward: -19.938495108456816, Moving Avg Reward: -17.982209866228168, Replay Buffer Size: 1564\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 19 episodes: 0.46\n",
      "Episode 37, Reward: -19.938495108456816, Moving Avg Reward: -17.982209866228168, Replay Buffer Size: 1564\n",
      "Current Epsilon: 0.995\n",
      "Episode 38: Won\n",
      "Episode 38, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 38, Reward: -25.34881269760267, Moving Avg Reward: -18.176067835474864, Replay Buffer Size: 1644\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 20 episodes: 0.44\n",
      "Episode 38, Reward: -25.34881269760267, Moving Avg Reward: -18.176067835474864, Replay Buffer Size: 1644\n",
      "Current Epsilon: 0.995\n",
      "Episode 39: Draw\n",
      "Episode 39, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 39, Reward: -23.189916796517508, Moving Avg Reward: -18.30462806524519, Replay Buffer Size: 1651\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 20 episodes: 0.48\n",
      "Episode 39, Reward: -23.189916796517508, Moving Avg Reward: -18.30462806524519, Replay Buffer Size: 1651\n",
      "Current Epsilon: 0.995\n",
      "Episode 40: Lost\n",
      "Episode 40, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 40, Reward: -43.47848741226602, Moving Avg Reward: -18.93397454892071, Replay Buffer Size: 1731\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 21 episodes: 0.46\n",
      "Episode 40, Reward: -43.47848741226602, Moving Avg Reward: -18.93397454892071, Replay Buffer Size: 1731\n",
      "Current Epsilon: 0.995\n",
      "Episode 41: Lost\n",
      "Episode 41, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 41, Reward: -37.713671335061406, Moving Avg Reward: -19.39201593394853, Replay Buffer Size: 1811\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 22 episodes: 0.44\n",
      "Episode 41, Reward: -37.713671335061406, Moving Avg Reward: -19.39201593394853, Replay Buffer Size: 1811\n",
      "Current Epsilon: 0.995\n",
      "Episode 42: Draw\n",
      "Episode 42, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 42, Reward: 10.460373115983135, Moving Avg Reward: -18.681244766093016, Replay Buffer Size: 1840\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 23 episodes: 0.43\n",
      "Episode 42, Reward: 10.460373115983135, Moving Avg Reward: -18.681244766093016, Replay Buffer Size: 1840\n",
      "Current Epsilon: 0.995\n",
      "Episode 43: Won\n",
      "Episode 43, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 43, Reward: -31.16407522560325, Moving Avg Reward: -18.971543148872325, Replay Buffer Size: 1920\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 24 episodes: 0.42\n",
      "Episode 43, Reward: -31.16407522560325, Moving Avg Reward: -18.971543148872325, Replay Buffer Size: 1920\n",
      "Current Epsilon: 0.995\n",
      "Episode 44: Won\n",
      "Episode 44, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 44, Reward: -22.546799285773865, Moving Avg Reward: -19.05279897016554, Replay Buffer Size: 1927\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 24 episodes: 0.46\n",
      "Episode 44, Reward: -22.546799285773865, Moving Avg Reward: -19.05279897016554, Replay Buffer Size: 1927\n",
      "Current Epsilon: 0.995\n",
      "Episode 45: Won\n",
      "Episode 45, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 45, Reward: -47.668713304501836, Moving Avg Reward: -19.688708177595235, Replay Buffer Size: 2007\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 25 episodes: 0.44\n",
      "Episode 45, Reward: -47.668713304501836, Moving Avg Reward: -19.688708177595235, Replay Buffer Size: 2007\n",
      "Current Epsilon: 0.995\n",
      "Episode 46: Lost\n",
      "Episode 46, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 46, Reward: -6.4417156902509, Moving Avg Reward: -19.400730080044273, Replay Buffer Size: 2067\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 25 episodes: 0.43\n",
      "Episode 46, Reward: -6.4417156902509, Moving Avg Reward: -19.400730080044273, Replay Buffer Size: 2067\n",
      "Current Epsilon: 0.995\n",
      "Episode 47: Won\n",
      "Episode 47, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 47, Reward: -25.790850260002024, Moving Avg Reward: -19.53669008387316, Replay Buffer Size: 2075\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 25 episodes: 0.46\n",
      "Episode 47, Reward: -25.790850260002024, Moving Avg Reward: -19.53669008387316, Replay Buffer Size: 2075\n",
      "Current Epsilon: 0.995\n",
      "Episode 48: Won\n",
      "Episode 48, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 48, Reward: -80.52142364168984, Moving Avg Reward: -20.807205366327675, Replay Buffer Size: 2155\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 26 episodes: 0.45\n",
      "Episode 48, Reward: -80.52142364168984, Moving Avg Reward: -20.807205366327675, Replay Buffer Size: 2155\n",
      "Current Epsilon: 0.995\n",
      "Episode 49: Won\n",
      "Episode 49, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 49, Reward: -21.597671832426663, Moving Avg Reward: -20.823337335023574, Replay Buffer Size: 2162\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 27 episodes: 0.48\n",
      "Episode 49, Reward: -21.597671832426663, Moving Avg Reward: -20.823337335023574, Replay Buffer Size: 2162\n",
      "Current Epsilon: 0.995\n",
      "Episode 50: Won\n",
      "Episode 50, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 50, Reward: -19.52657105723241, Moving Avg Reward: -20.79740200946775, Replay Buffer Size: 2221\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 27 episodes: 0.47\n",
      "Episode 50, Reward: -19.52657105723241, Moving Avg Reward: -20.79740200946775, Replay Buffer Size: 2221\n",
      "Current Epsilon: 0.995\n",
      "Episode 51: Won\n",
      "Episode 51, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 51, Reward: -7.749971987144649, Moving Avg Reward: -20.541570048245728, Replay Buffer Size: 2282\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 28 episodes: 0.46\n",
      "Episode 51, Reward: -7.749971987144649, Moving Avg Reward: -20.541570048245728, Replay Buffer Size: 2282\n",
      "Current Epsilon: 0.995\n",
      "Episode 52: Won\n",
      "Episode 52, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 52, Reward: -26.37066550473338, Moving Avg Reward: -20.653668037793565, Replay Buffer Size: 2362\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 29 episodes: 0.44\n",
      "Episode 52, Reward: -26.37066550473338, Moving Avg Reward: -20.653668037793565, Replay Buffer Size: 2362\n",
      "Current Epsilon: 0.995\n",
      "Episode 53: Won\n",
      "Episode 53, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 53, Reward: 24.100440949325517, Moving Avg Reward: -19.809250887093206, Replay Buffer Size: 2385\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 29 episodes: 0.44\n",
      "Episode 53, Reward: 24.100440949325517, Moving Avg Reward: -19.809250887093206, Replay Buffer Size: 2385\n",
      "Current Epsilon: 0.995\n",
      "Episode 54: Won\n",
      "Episode 54, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 54, Reward: -28.469570770751254, Moving Avg Reward: -19.969627181235023, Replay Buffer Size: 2396\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 29 episodes: 0.47\n",
      "Episode 54, Reward: -28.469570770751254, Moving Avg Reward: -19.969627181235023, Replay Buffer Size: 2396\n",
      "Current Epsilon: 0.995\n",
      "Episode 55: Won\n",
      "Episode 55, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 55, Reward: -25.851440197877842, Moving Avg Reward: -20.07656923608307, Replay Buffer Size: 2407\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 30 episodes: 0.50\n",
      "Episode 55, Reward: -25.851440197877842, Moving Avg Reward: -20.07656923608307, Replay Buffer Size: 2407\n",
      "Current Epsilon: 0.995\n",
      "Episode 56: Won\n",
      "Episode 56, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 56, Reward: -8.75607489275939, Moving Avg Reward: -19.874417551380866, Replay Buffer Size: 2487\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 31 episodes: 0.48\n",
      "Episode 56, Reward: -8.75607489275939, Moving Avg Reward: -19.874417551380866, Replay Buffer Size: 2487\n",
      "Current Epsilon: 0.995\n",
      "Episode 57: Won\n",
      "Episode 57, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 57, Reward: -27.93847864359512, Moving Avg Reward: -20.01589230738462, Replay Buffer Size: 2531\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 31 episodes: 0.51\n",
      "Episode 57, Reward: -27.93847864359512, Moving Avg Reward: -20.01589230738462, Replay Buffer Size: 2531\n",
      "Current Epsilon: 0.995\n",
      "Episode 58: Won\n",
      "Episode 58, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 58, Reward: -26.83967641687032, Moving Avg Reward: -20.13354375754817, Replay Buffer Size: 2543\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 31 episodes: 0.50\n",
      "Episode 58, Reward: -26.83967641687032, Moving Avg Reward: -20.13354375754817, Replay Buffer Size: 2543\n",
      "Current Epsilon: 0.995\n",
      "Episode 59: Lost\n",
      "Episode 59, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 59, Reward: -19.8343449326658, Moving Avg Reward: -20.12847259102474, Replay Buffer Size: 2580\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 32 episodes: 0.53\n",
      "Episode 59, Reward: -19.8343449326658, Moving Avg Reward: -20.12847259102474, Replay Buffer Size: 2580\n",
      "Current Epsilon: 0.995\n",
      "Episode 60: Lost\n",
      "Episode 60, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 60, Reward: -21.371493681555524, Moving Avg Reward: -20.149189609200253, Replay Buffer Size: 2587\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 32 episodes: 0.53\n",
      "Episode 60, Reward: -21.371493681555524, Moving Avg Reward: -20.149189609200253, Replay Buffer Size: 2587\n",
      "Current Epsilon: 0.995\n",
      "Episode 61: Lost\n",
      "Episode 61, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 61, Reward: -27.389718553179257, Moving Avg Reward: -20.267886805003187, Replay Buffer Size: 2598\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 32 episodes: 0.52\n",
      "Episode 61, Reward: -27.389718553179257, Moving Avg Reward: -20.267886805003187, Replay Buffer Size: 2598\n",
      "Current Epsilon: 0.995\n",
      "Episode 62: Lost\n",
      "Episode 62, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 62, Reward: -23.15442423666906, Moving Avg Reward: -20.314443860352636, Replay Buffer Size: 2606\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 32 episodes: 0.52\n",
      "Episode 62, Reward: -23.15442423666906, Moving Avg Reward: -20.314443860352636, Replay Buffer Size: 2606\n",
      "Current Epsilon: 0.995\n",
      "Episode 63: Lost\n",
      "Episode 63, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 63, Reward: -25.270309370632155, Moving Avg Reward: -20.393108392261833, Replay Buffer Size: 2686\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 33 episodes: 0.51\n",
      "Episode 63, Reward: -25.270309370632155, Moving Avg Reward: -20.393108392261833, Replay Buffer Size: 2686\n",
      "Current Epsilon: 0.995\n",
      "Episode 64: Lost\n",
      "Episode 64, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 64, Reward: -28.406871043609556, Moving Avg Reward: -20.518323433689144, Replay Buffer Size: 2748\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 34 episodes: 0.49\n",
      "Episode 64, Reward: -28.406871043609556, Moving Avg Reward: -20.518323433689144, Replay Buffer Size: 2748\n",
      "Current Epsilon: 0.995\n",
      "Episode 65: Won\n",
      "Episode 65, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 65, Reward: 15.592791894859177, Moving Avg Reward: -19.96276781324994, Replay Buffer Size: 2769\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 34 episodes: 0.49\n",
      "Episode 65, Reward: 15.592791894859177, Moving Avg Reward: -19.96276781324994, Replay Buffer Size: 2769\n",
      "Current Epsilon: 0.995\n",
      "Episode 66: Won\n",
      "Episode 66, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 66, Reward: -28.801909193884686, Moving Avg Reward: -20.09669419780501, Replay Buffer Size: 2816\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 35 episodes: 0.51\n",
      "Episode 66, Reward: -28.801909193884686, Moving Avg Reward: -20.09669419780501, Replay Buffer Size: 2816\n",
      "Current Epsilon: 0.995\n",
      "Episode 67: Won\n",
      "Episode 67, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 67, Reward: 12.920894173956748, Moving Avg Reward: -19.60389437136081, Replay Buffer Size: 2840\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 35 episodes: 0.51\n",
      "Episode 67, Reward: 12.920894173956748, Moving Avg Reward: -19.60389437136081, Replay Buffer Size: 2840\n",
      "Current Epsilon: 0.995\n",
      "Episode 68: Won\n",
      "Episode 68, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 68, Reward: -68.03614908330417, Moving Avg Reward: -20.31613341124233, Replay Buffer Size: 2920\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 36 episodes: 0.49\n",
      "Episode 68, Reward: -68.03614908330417, Moving Avg Reward: -20.31613341124233, Replay Buffer Size: 2920\n",
      "Current Epsilon: 0.995\n",
      "Episode 69: Won\n",
      "Episode 69, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 69, Reward: 11.387688006848432, Moving Avg Reward: -19.856657738516375, Replay Buffer Size: 2952\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 36 episodes: 0.49\n",
      "Episode 69, Reward: 11.387688006848432, Moving Avg Reward: -19.856657738516375, Replay Buffer Size: 2952\n",
      "Current Epsilon: 0.995\n",
      "Episode 70: Won\n",
      "Episode 70, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 70, Reward: 2.8502028489275046, Moving Avg Reward: -19.532274015838606, Replay Buffer Size: 3022\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 37 episodes: 0.48\n",
      "Episode 70, Reward: 2.8502028489275046, Moving Avg Reward: -19.532274015838606, Replay Buffer Size: 3022\n",
      "Current Epsilon: 0.995\n",
      "Episode 71: Won\n",
      "Episode 71, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 71, Reward: -47.324604396610034, Moving Avg Reward: -19.92371528880722, Replay Buffer Size: 3102\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 38 episodes: 0.46\n",
      "Episode 71, Reward: -47.324604396610034, Moving Avg Reward: -19.92371528880722, Replay Buffer Size: 3102\n",
      "Current Epsilon: 0.995\n",
      "Episode 72: Won\n",
      "Episode 72, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 72, Reward: -0.5228801710383557, Moving Avg Reward: -19.654259245504868, Replay Buffer Size: 3156\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 39 episodes: 0.46\n",
      "Episode 72, Reward: -0.5228801710383557, Moving Avg Reward: -19.654259245504868, Replay Buffer Size: 3156\n",
      "Current Epsilon: 0.995\n",
      "Episode 73: Won\n",
      "Episode 73, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 73, Reward: -26.279313898915298, Moving Avg Reward: -19.745013418839257, Replay Buffer Size: 3197\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 39 episodes: 0.48\n",
      "Episode 73, Reward: -26.279313898915298, Moving Avg Reward: -19.745013418839257, Replay Buffer Size: 3197\n",
      "Current Epsilon: 0.995\n",
      "Episode 74: Won\n",
      "Episode 74, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 74, Reward: -26.243823276911463, Moving Avg Reward: -19.83283517367807, Replay Buffer Size: 3255\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 40 episodes: 0.47\n",
      "Episode 74, Reward: -26.243823276911463, Moving Avg Reward: -19.83283517367807, Replay Buffer Size: 3255\n",
      "Current Epsilon: 0.995\n",
      "Episode 75: Won\n",
      "Episode 75, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 75, Reward: -24.69578994124476, Moving Avg Reward: -19.89767457057896, Replay Buffer Size: 3263\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 40 episodes: 0.49\n",
      "Episode 75, Reward: -24.69578994124476, Moving Avg Reward: -19.89767457057896, Replay Buffer Size: 3263\n",
      "Current Epsilon: 0.995\n",
      "Episode 76: Won\n",
      "Episode 76, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 76, Reward: 18.464379250324413, Moving Avg Reward: -19.392910704514442, Replay Buffer Size: 3281\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 41 episodes: 0.49\n",
      "Episode 76, Reward: 18.464379250324413, Moving Avg Reward: -19.392910704514442, Replay Buffer Size: 3281\n",
      "Current Epsilon: 0.995\n",
      "Episode 77: Won\n",
      "Episode 77, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 77, Reward: -50.751702546336865, Moving Avg Reward: -19.800167741421227, Replay Buffer Size: 3361\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 42 episodes: 0.48\n",
      "Episode 77, Reward: -50.751702546336865, Moving Avg Reward: -19.800167741421227, Replay Buffer Size: 3361\n",
      "Current Epsilon: 0.995\n",
      "Episode 78: Won\n",
      "Episode 78, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 78, Reward: 16.018204001179736, Moving Avg Reward: -19.340957847285317, Replay Buffer Size: 3388\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 42 episodes: 0.47\n",
      "Episode 78, Reward: 16.018204001179736, Moving Avg Reward: -19.340957847285317, Replay Buffer Size: 3388\n",
      "Current Epsilon: 0.995\n",
      "Episode 79: Won\n",
      "Episode 79, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 79, Reward: -10.61048076826379, Moving Avg Reward: -19.230445479196437, Replay Buffer Size: 3449\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 43 episodes: 0.46\n",
      "Episode 79, Reward: -10.61048076826379, Moving Avg Reward: -19.230445479196437, Replay Buffer Size: 3449\n",
      "Current Epsilon: 0.995\n",
      "Episode 80: Won\n",
      "Episode 80, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 80, Reward: -32.66311942971001, Moving Avg Reward: -19.398353903577863, Replay Buffer Size: 3529\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 44 episodes: 0.45\n",
      "Episode 80, Reward: -32.66311942971001, Moving Avg Reward: -19.398353903577863, Replay Buffer Size: 3529\n",
      "Current Epsilon: 0.995\n",
      "Episode 81: Won\n",
      "Episode 81, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 81, Reward: -18.463862685781457, Moving Avg Reward: -19.38681697496309, Replay Buffer Size: 3578\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 44 episodes: 0.45\n",
      "Episode 81, Reward: -18.463862685781457, Moving Avg Reward: -19.38681697496309, Replay Buffer Size: 3578\n",
      "Current Epsilon: 0.995\n",
      "Episode 82: Won\n",
      "Episode 82, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 82, Reward: -25.183482307651758, Moving Avg Reward: -19.457508015605633, Replay Buffer Size: 3586\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 44 episodes: 0.47\n",
      "Episode 82, Reward: -25.183482307651758, Moving Avg Reward: -19.457508015605633, Replay Buffer Size: 3586\n",
      "Current Epsilon: 0.995\n",
      "Episode 83: Won\n",
      "Episode 83, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 83, Reward: -25.417650856656024, Moving Avg Reward: -19.52931696549781, Replay Buffer Size: 3598\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 44 episodes: 0.47\n",
      "Episode 83, Reward: -25.417650856656024, Moving Avg Reward: -19.52931696549781, Replay Buffer Size: 3598\n",
      "Current Epsilon: 0.995\n",
      "Episode 84: Won\n",
      "Episode 84, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 84, Reward: 15.663660494714966, Moving Avg Reward: -19.110352948114322, Replay Buffer Size: 3620\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 45 episodes: 0.46\n",
      "Episode 84, Reward: 15.663660494714966, Moving Avg Reward: -19.110352948114322, Replay Buffer Size: 3620\n",
      "Current Epsilon: 0.995\n",
      "Episode 85: Won\n",
      "Episode 85, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 85, Reward: 17.170025730130774, Moving Avg Reward: -18.68352496366438, Replay Buffer Size: 3642\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 45 episodes: 0.46\n",
      "Episode 85, Reward: 17.170025730130774, Moving Avg Reward: -18.68352496366438, Replay Buffer Size: 3642\n",
      "Current Epsilon: 0.995\n",
      "Episode 86: Won\n",
      "Episode 86, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 86, Reward: -60.412684463664995, Moving Avg Reward: -19.168747748548107, Replay Buffer Size: 3722\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 46 episodes: 0.45\n",
      "Episode 86, Reward: -60.412684463664995, Moving Avg Reward: -19.168747748548107, Replay Buffer Size: 3722\n",
      "Current Epsilon: 0.995\n",
      "Episode 87: Won\n",
      "Episode 87, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 87, Reward: -26.126892165139374, Moving Avg Reward: -19.24872642000318, Replay Buffer Size: 3732\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 46 episodes: 0.47\n",
      "Episode 87, Reward: -26.126892165139374, Moving Avg Reward: -19.24872642000318, Replay Buffer Size: 3732\n",
      "Current Epsilon: 0.995\n",
      "Episode 88: Won\n",
      "Episode 88, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 88, Reward: 4.179498288609704, Moving Avg Reward: -18.982496593768943, Replay Buffer Size: 3812\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 47 episodes: 0.46\n",
      "Episode 88, Reward: 4.179498288609704, Moving Avg Reward: -18.982496593768943, Replay Buffer Size: 3812\n",
      "Current Epsilon: 0.995\n",
      "Episode 89: Lost\n",
      "Episode 89, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 89, Reward: 16.957167971955478, Moving Avg Reward: -18.57868013797429, Replay Buffer Size: 3833\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 47 episodes: 0.46\n",
      "Episode 89, Reward: 16.957167971955478, Moving Avg Reward: -18.57868013797429, Replay Buffer Size: 3833\n",
      "Current Epsilon: 0.995\n",
      "Episode 90: Won\n",
      "Episode 90, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 90, Reward: -21.653537263644985, Moving Avg Reward: -18.612845217148408, Replay Buffer Size: 3840\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 48 episodes: 0.48\n",
      "Episode 90, Reward: -21.653537263644985, Moving Avg Reward: -18.612845217148408, Replay Buffer Size: 3840\n",
      "Current Epsilon: 0.995\n",
      "Episode 91: Won\n",
      "Episode 91, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 91, Reward: -25.185367384928888, Moving Avg Reward: -18.685070735475666, Replay Buffer Size: 3852\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 48 episodes: 0.50\n",
      "Episode 91, Reward: -25.185367384928888, Moving Avg Reward: -18.685070735475666, Replay Buffer Size: 3852\n",
      "Current Epsilon: 0.995\n",
      "Episode 92: Won\n",
      "Episode 92, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 92, Reward: -25.7758884159889, Moving Avg Reward: -18.762144840698635, Replay Buffer Size: 3932\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 49 episodes: 0.49\n",
      "Episode 92, Reward: -25.7758884159889, Moving Avg Reward: -18.762144840698635, Replay Buffer Size: 3932\n",
      "Current Epsilon: 0.995\n",
      "Episode 93: Won\n",
      "Episode 93, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 93, Reward: 10.669092498642245, Moving Avg Reward: -18.445679923071314, Replay Buffer Size: 3959\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 49 episodes: 0.48\n",
      "Episode 93, Reward: 10.669092498642245, Moving Avg Reward: -18.445679923071314, Replay Buffer Size: 3959\n",
      "Current Epsilon: 0.995\n",
      "Episode 94: Won\n",
      "Episode 94, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 94, Reward: -48.08110482309163, Moving Avg Reward: -18.760950400731105, Replay Buffer Size: 4039\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 50 episodes: 0.48\n",
      "Episode 94, Reward: -48.08110482309163, Moving Avg Reward: -18.760950400731105, Replay Buffer Size: 4039\n",
      "Current Epsilon: 0.995\n",
      "Episode 95: Won\n",
      "Episode 95, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 95, Reward: -27.267871991697575, Moving Avg Reward: -18.85049694379391, Replay Buffer Size: 4050\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 50 episodes: 0.49\n",
      "Episode 95, Reward: -27.267871991697575, Moving Avg Reward: -18.85049694379391, Replay Buffer Size: 4050\n",
      "Current Epsilon: 0.995\n",
      "Episode 96: Won\n",
      "Episode 96, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 96, Reward: -40.14914676761727, Moving Avg Reward: -19.072357879458735, Replay Buffer Size: 4130\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 51 episodes: 0.48\n",
      "Episode 96, Reward: -40.14914676761727, Moving Avg Reward: -19.072357879458735, Replay Buffer Size: 4130\n",
      "Current Epsilon: 0.995\n",
      "Episode 97: Lost\n",
      "Episode 97, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 97, Reward: -37.650331511113286, Moving Avg Reward: -19.263883380816, Replay Buffer Size: 4210\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 52 episodes: 0.48\n",
      "Episode 97, Reward: -37.650331511113286, Moving Avg Reward: -19.263883380816, Replay Buffer Size: 4210\n",
      "Current Epsilon: 0.995\n",
      "Episode 98: Draw\n",
      "Episode 98, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 98, Reward: -24.31212508702199, Moving Avg Reward: -19.31539605128749, Replay Buffer Size: 4220\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 52 episodes: 0.49\n",
      "Episode 98, Reward: -24.31212508702199, Moving Avg Reward: -19.31539605128749, Replay Buffer Size: 4220\n",
      "Current Epsilon: 0.995\n",
      "Episode 99: Lost\n",
      "Episode 99, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 99, Reward: 18.14260290941098, Moving Avg Reward: -18.937032425421847, Replay Buffer Size: 4237\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 52 episodes: 0.49\n",
      "Episode 99, Reward: 18.14260290941098, Moving Avg Reward: -18.937032425421847, Replay Buffer Size: 4237\n",
      "Current Epsilon: 0.995\n",
      "Episode 100: Won\n",
      "Episode 100, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 100, Reward: 16.082845335839217, Moving Avg Reward: -18.586833647809236, Replay Buffer Size: 4265\n",
      "Current Epsilon: 0.995\n",
      "Loss rate over the last 53 episodes: 0.49\n",
      "Episode 100, Reward: 16.082845335839217, Moving Avg Reward: -18.586833647809236, Replay Buffer Size: 4265\n",
      "Current Epsilon: 0.995\n",
      "Episode 101: Won\n",
      "Episode 101, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 101, Reward: -31.258969379789058, Moving Avg Reward: -19.025861218910915, Replay Buffer Size: 4277\n",
      "Current Epsilon: 0.990025\n",
      "Loss rate over the last 53 episodes: 0.51\n",
      "Episode 101, Reward: -31.258969379789058, Moving Avg Reward: -19.025861218910915, Replay Buffer Size: 4277\n",
      "Current Epsilon: 0.990025\n",
      "Episode 102: Won\n",
      "Episode 102, Avg Value Loss: 6.896178805828095, Avg Policy Loss: 5.769881248474121\n",
      "Episode 102, Reward: -20.229574595616793, Moving Avg Reward: -19.37586806452341, Replay Buffer Size: 4357\n",
      "Current Epsilon: 0.985074875\n",
      "Loss rate over the last 54 episodes: 0.50\n",
      "Episode 102, Reward: -20.229574595616793, Moving Avg Reward: -19.37586806452341, Replay Buffer Size: 4357\n",
      "Current Epsilon: 0.985074875\n",
      "Episode 103: Won\n",
      "Episode 103, Avg Value Loss: 7.195157527923584, Avg Policy Loss: 5.261714935302734\n",
      "Episode 103, Reward: 9.99, Moving Avg Reward: -19.050019278439834, Replay Buffer Size: 4358\n",
      "Current Epsilon: 0.9801495006250001\n",
      "Loss rate over the last 54 episodes: 0.50\n",
      "Episode 103, Reward: 9.99, Moving Avg Reward: -19.050019278439834, Replay Buffer Size: 4358\n",
      "Current Epsilon: 0.9801495006250001\n",
      "Episode 104: Won\n",
      "Episode 104, Avg Value Loss: 4.057867010434468, Avg Policy Loss: 5.231378263897366\n",
      "Episode 104, Reward: -34.56638556025539, Moving Avg Reward: -19.132843643695438, Replay Buffer Size: 4376\n",
      "Current Epsilon: 0.9752487531218751\n",
      "Loss rate over the last 54 episodes: 0.51\n",
      "Episode 104, Reward: -34.56638556025539, Moving Avg Reward: -19.132843643695438, Replay Buffer Size: 4376\n",
      "Current Epsilon: 0.9752487531218751\n",
      "Episode 105: Won\n",
      "Episode 105, Avg Value Loss: 5.08911550962008, Avg Policy Loss: 4.798627009758582\n",
      "Episode 105, Reward: -29.60212708807704, Moving Avg Reward: -19.198265334032378, Replay Buffer Size: 4389\n",
      "Current Epsilon: 0.9703725093562657\n",
      "Loss rate over the last 54 episodes: 0.51\n",
      "Episode 105, Reward: -29.60212708807704, Moving Avg Reward: -19.198265334032378, Replay Buffer Size: 4389\n",
      "Current Epsilon: 0.9703725093562657\n",
      "Episode 106: Won\n",
      "Episode 106, Avg Value Loss: 4.637048703432083, Avg Policy Loss: 4.9061590194702145\n",
      "Episode 106, Reward: 18.710640912446465, Moving Avg Reward: -18.76595821639926, Replay Buffer Size: 4409\n",
      "Current Epsilon: 0.9655206468094844\n",
      "Loss rate over the last 55 episodes: 0.51\n",
      "Episode 106, Reward: 18.710640912446465, Moving Avg Reward: -18.76595821639926, Replay Buffer Size: 4409\n",
      "Current Epsilon: 0.9655206468094844\n",
      "Episode 107: Won\n",
      "Episode 107, Avg Value Loss: 4.154530737135145, Avg Policy Loss: 5.009884834289551\n",
      "Episode 107, Reward: -22.951823615037462, Moving Avg Reward: -18.719359634030923, Replay Buffer Size: 4418\n",
      "Current Epsilon: 0.960693043575437\n",
      "Loss rate over the last 55 episodes: 0.53\n",
      "Episode 107, Reward: -22.951823615037462, Moving Avg Reward: -18.719359634030923, Replay Buffer Size: 4418\n",
      "Current Epsilon: 0.960693043575437\n",
      "Episode 108: Won\n",
      "Episode 108, Avg Value Loss: 3.936979115009308, Avg Policy Loss: 5.082425534725189\n",
      "Episode 108, Reward: -22.499930970964055, Moving Avg Reward: -18.735199882476874, Replay Buffer Size: 4426\n",
      "Current Epsilon: 0.9558895783575597\n",
      "Loss rate over the last 55 episodes: 0.52\n",
      "Episode 108, Reward: -22.499930970964055, Moving Avg Reward: -18.735199882476874, Replay Buffer Size: 4426\n",
      "Current Epsilon: 0.9558895783575597\n",
      "Episode 109: Won\n",
      "Episode 109, Avg Value Loss: 4.19160453826189, Avg Policy Loss: 5.247106415033341\n",
      "Episode 109, Reward: 3.6731351409466875, Moving Avg Reward: -18.420848241370553, Replay Buffer Size: 4506\n",
      "Current Epsilon: 0.9511101304657719\n",
      "Loss rate over the last 56 episodes: 0.51\n",
      "Episode 109, Reward: 3.6731351409466875, Moving Avg Reward: -18.420848241370553, Replay Buffer Size: 4506\n",
      "Current Epsilon: 0.9511101304657719\n",
      "Episode 110: Won\n",
      "Episode 110, Avg Value Loss: 2.4980493411421776, Avg Policy Loss: 4.643403768539429\n",
      "Episode 110, Reward: -22.689576637631376, Moving Avg Reward: -17.9894763745518, Replay Buffer Size: 4514\n",
      "Current Epsilon: 0.946354579813443\n",
      "Loss rate over the last 56 episodes: 0.53\n",
      "Episode 110, Reward: -22.689576637631376, Moving Avg Reward: -17.9894763745518, Replay Buffer Size: 4514\n",
      "Current Epsilon: 0.946354579813443\n",
      "Episode 111: Won\n",
      "Episode 111, Avg Value Loss: 3.593456688680147, Avg Policy Loss: 5.344463498968827\n",
      "Episode 111, Reward: 18.509631332241298, Moving Avg Reward: -17.563271679340893, Replay Buffer Size: 4533\n",
      "Current Epsilon: 0.9416228069143757\n",
      "Loss rate over the last 56 episodes: 0.53\n",
      "Episode 111, Reward: 18.509631332241298, Moving Avg Reward: -17.563271679340893, Replay Buffer Size: 4533\n",
      "Current Epsilon: 0.9416228069143757\n",
      "Episode 112: Won\n",
      "Episode 112, Avg Value Loss: 4.344613085027601, Avg Policy Loss: 4.763991656850596\n",
      "Episode 112, Reward: -7.340961389595664, Moving Avg Reward: -17.56940756039579, Replay Buffer Size: 4594\n",
      "Current Epsilon: 0.9369146928798039\n",
      "Loss rate over the last 57 episodes: 0.52\n",
      "Episode 112, Reward: -7.340961389595664, Moving Avg Reward: -17.56940756039579, Replay Buffer Size: 4594\n",
      "Current Epsilon: 0.9369146928798039\n",
      "Episode 113: Won\n",
      "Episode 113, Avg Value Loss: 3.920748518063472, Avg Policy Loss: 4.7884126626528225\n",
      "Episode 113, Reward: -27.42601815479499, Moving Avg Reward: -17.488319571502256, Replay Buffer Size: 4607\n",
      "Current Epsilon: 0.9322301194154049\n",
      "Loss rate over the last 57 episodes: 0.54\n",
      "Episode 113, Reward: -27.42601815479499, Moving Avg Reward: -17.488319571502256, Replay Buffer Size: 4607\n",
      "Current Epsilon: 0.9322301194154049\n",
      "Episode 114: Won\n",
      "Episode 114, Avg Value Loss: 3.955947480504475, Avg Policy Loss: 4.696719502645825\n",
      "Episode 114, Reward: -33.65662868955667, Moving Avg Reward: -17.785851087729274, Replay Buffer Size: 4670\n",
      "Current Epsilon: 0.9275689688183278\n",
      "Loss rate over the last 58 episodes: 0.53\n",
      "Episode 114, Reward: -33.65662868955667, Moving Avg Reward: -17.785851087729274, Replay Buffer Size: 4670\n",
      "Current Epsilon: 0.9275689688183278\n",
      "Episode 115: Won\n",
      "Episode 115, Avg Value Loss: 3.80339883649072, Avg Policy Loss: 4.5841741950012915\n",
      "Episode 115, Reward: -29.40294445213801, Moving Avg Reward: -17.3089143609434, Replay Buffer Size: 4713\n",
      "Current Epsilon: 0.9229311239742362\n",
      "Loss rate over the last 58 episodes: 0.54\n",
      "Episode 115, Reward: -29.40294445213801, Moving Avg Reward: -17.3089143609434, Replay Buffer Size: 4713\n",
      "Current Epsilon: 0.9229311239742362\n",
      "Episode 116: Won\n",
      "Episode 116, Avg Value Loss: 3.8672802433371545, Avg Policy Loss: 4.625284194946289\n",
      "Episode 116, Reward: -4.732538413083356, Moving Avg Reward: -17.492957579890867, Replay Buffer Size: 4793\n",
      "Current Epsilon: 0.918316468354365\n",
      "Loss rate over the last 59 episodes: 0.53\n",
      "Episode 116, Reward: -4.732538413083356, Moving Avg Reward: -17.492957579890867, Replay Buffer Size: 4793\n",
      "Current Epsilon: 0.918316468354365\n",
      "Episode 117: Won\n",
      "Episode 117, Avg Value Loss: 4.340249697367351, Avg Policy Loss: 4.4265627066294355\n",
      "Episode 117, Reward: -18.88943477983251, Moving Avg Reward: -17.425012799723174, Replay Buffer Size: 4799\n",
      "Current Epsilon: 0.9137248860125932\n",
      "Loss rate over the last 59 episodes: 0.55\n",
      "Episode 117, Reward: -18.88943477983251, Moving Avg Reward: -17.425012799723174, Replay Buffer Size: 4799\n",
      "Current Epsilon: 0.9137248860125932\n",
      "Episode 118: Won\n",
      "Episode 118, Avg Value Loss: 4.229713749317896, Avg Policy Loss: 4.357257894107273\n",
      "Episode 118, Reward: -74.06302013557527, Moving Avg Reward: -17.698604976510627, Replay Buffer Size: 4841\n",
      "Current Epsilon: 0.9091562615825302\n",
      "Loss rate over the last 60 episodes: 0.56\n",
      "Episode 118, Reward: -74.06302013557527, Moving Avg Reward: -17.698604976510627, Replay Buffer Size: 4841\n",
      "Current Epsilon: 0.9091562615825302\n",
      "Episode 119: Lost\n",
      "Episode 119, Avg Value Loss: 3.8063325773585928, Avg Policy Loss: 4.31903321092779\n",
      "Episode 119, Reward: -29.619987690099514, Moving Avg Reward: -17.757072618324706, Replay Buffer Size: 4852\n",
      "Current Epsilon: 0.9046104802746175\n",
      "Loss rate over the last 60 episodes: 0.56\n",
      "Episode 119, Reward: -29.619987690099514, Moving Avg Reward: -17.757072618324706, Replay Buffer Size: 4852\n",
      "Current Epsilon: 0.9046104802746175\n",
      "Episode 120: Lost\n",
      "Episode 120, Avg Value Loss: 3.156738966703415, Avg Policy Loss: 4.316280037164688\n",
      "Episode 120, Reward: -26.443742240890415, Moving Avg Reward: -17.758743633078474, Replay Buffer Size: 4860\n",
      "Current Epsilon: 0.9000874278732445\n",
      "Loss rate over the last 60 episodes: 0.56\n",
      "Episode 120, Reward: -26.443742240890415, Moving Avg Reward: -17.758743633078474, Replay Buffer Size: 4860\n",
      "Current Epsilon: 0.9000874278732445\n",
      "Episode 121: Lost\n",
      "Episode 121, Avg Value Loss: 3.55750923543363, Avg Policy Loss: 4.299496264071078\n",
      "Episode 121, Reward: 4.492952653523778, Moving Avg Reward: -17.469865831309516, Replay Buffer Size: 4934\n",
      "Current Epsilon: 0.8955869907338783\n",
      "Loss rate over the last 61 episodes: 0.55\n",
      "Episode 121, Reward: 4.492952653523778, Moving Avg Reward: -17.469865831309516, Replay Buffer Size: 4934\n",
      "Current Epsilon: 0.8955869907338783\n",
      "Episode 122: Won\n",
      "Episode 122, Avg Value Loss: 3.5744856232303683, Avg Policy Loss: 4.224774368738724\n",
      "Episode 122, Reward: -22.018331246939248, Moving Avg Reward: -17.816534605919284, Replay Buffer Size: 4993\n",
      "Current Epsilon: 0.8911090557802088\n",
      "Loss rate over the last 62 episodes: 0.54\n",
      "Episode 122, Reward: -22.018331246939248, Moving Avg Reward: -17.816534605919284, Replay Buffer Size: 4993\n",
      "Current Epsilon: 0.8911090557802088\n",
      "Episode 123: Won\n",
      "Episode 123, Avg Value Loss: 3.5164736029894454, Avg Policy Loss: 4.249112320982891\n",
      "Episode 123, Reward: 13.999106567589818, Moving Avg Reward: -17.471891172906826, Replay Buffer Size: 5039\n",
      "Current Epsilon: 0.8866535105013078\n",
      "Loss rate over the last 62 episodes: 0.54\n",
      "Episode 123, Reward: 13.999106567589818, Moving Avg Reward: -17.471891172906826, Replay Buffer Size: 5039\n",
      "Current Epsilon: 0.8866535105013078\n",
      "Episode 124: Won\n",
      "Episode 124, Avg Value Loss: 3.5402674133127388, Avg Policy Loss: 4.084491968154907\n",
      "Episode 124, Reward: -26.650184737546475, Moving Avg Reward: -17.862198437234664, Replay Buffer Size: 5050\n",
      "Current Epsilon: 0.8822202429488013\n",
      "Loss rate over the last 63 episodes: 0.55\n",
      "Episode 124, Reward: -26.650184737546475, Moving Avg Reward: -17.862198437234664, Replay Buffer Size: 5050\n",
      "Current Epsilon: 0.8822202429488013\n",
      "Episode 125: Won\n",
      "Episode 125, Avg Value Loss: 3.7347341125661675, Avg Policy Loss: 4.07508997483687\n",
      "Episode 125, Reward: -23.318086730336347, Moving Avg Reward: -17.778596686435016, Replay Buffer Size: 5061\n",
      "Current Epsilon: 0.8778091417340573\n",
      "Loss rate over the last 63 episodes: 0.55\n",
      "Episode 125, Reward: -23.318086730336347, Moving Avg Reward: -17.778596686435016, Replay Buffer Size: 5061\n",
      "Current Epsilon: 0.8778091417340573\n",
      "Episode 126: Won\n",
      "Episode 126, Avg Value Loss: 3.6700097188353538, Avg Policy Loss: 4.042068442702293\n",
      "Episode 126, Reward: -11.765446860257457, Moving Avg Reward: -17.721353265208407, Replay Buffer Size: 5141\n",
      "Current Epsilon: 0.8734200960253871\n",
      "Loss rate over the last 64 episodes: 0.54\n",
      "Episode 126, Reward: -11.765446860257457, Moving Avg Reward: -17.721353265208407, Replay Buffer Size: 5141\n",
      "Current Epsilon: 0.8734200960253871\n",
      "Episode 127: Won\n",
      "Episode 127, Avg Value Loss: 3.590246619284153, Avg Policy Loss: 3.9168824791908263\n",
      "Episode 127, Reward: -0.8000000000000005, Moving Avg Reward: -17.894457530866553, Replay Buffer Size: 5221\n",
      "Current Epsilon: 0.8690529955452602\n",
      "Loss rate over the last 65 episodes: 0.54\n",
      "Episode 127, Reward: -0.8000000000000005, Moving Avg Reward: -17.894457530866553, Replay Buffer Size: 5221\n",
      "Current Epsilon: 0.8690529955452602\n",
      "Episode 128: Draw\n",
      "Episode 128, Avg Value Loss: 4.17773421605428, Avg Policy Loss: 3.7627756463156805\n",
      "Episode 128, Reward: 18.515133125363082, Moving Avg Reward: -17.489701130819128, Replay Buffer Size: 5239\n",
      "Current Epsilon: 0.8647077305675338\n",
      "Loss rate over the last 65 episodes: 0.53\n",
      "Episode 128, Reward: 18.515133125363082, Moving Avg Reward: -17.489701130819128, Replay Buffer Size: 5239\n",
      "Current Epsilon: 0.8647077305675338\n",
      "Episode 129: Won\n",
      "Episode 129, Avg Value Loss: 3.835326083244816, Avg Policy Loss: 3.859609184726592\n",
      "Episode 129, Reward: -9.300095953789183, Moving Avg Reward: -17.361994521229896, Replay Buffer Size: 5301\n",
      "Current Epsilon: 0.8603841919146962\n",
      "Loss rate over the last 66 episodes: 0.53\n",
      "Episode 129, Reward: -9.300095953789183, Moving Avg Reward: -17.361994521229896, Replay Buffer Size: 5301\n",
      "Current Epsilon: 0.8603841919146962\n",
      "Episode 130: Won\n",
      "Episode 130, Avg Value Loss: 3.4670974922180178, Avg Policy Loss: 3.918871030807495\n",
      "Episode 130, Reward: -32.49146014306378, Moving Avg Reward: -17.445704811502015, Replay Buffer Size: 5326\n",
      "Current Epsilon: 0.8560822709551227\n",
      "Loss rate over the last 66 episodes: 0.54\n",
      "Episode 130, Reward: -32.49146014306378, Moving Avg Reward: -17.445704811502015, Replay Buffer Size: 5326\n",
      "Current Epsilon: 0.8560822709551227\n",
      "Episode 131: Won\n",
      "Episode 131, Avg Value Loss: 3.5275425001978875, Avg Policy Loss: 3.75061312019825\n",
      "Episode 131, Reward: -18.45827571946357, Moving Avg Reward: -17.73313608380244, Replay Buffer Size: 5406\n",
      "Current Epsilon: 0.851801859600347\n",
      "Loss rate over the last 67 episodes: 0.53\n",
      "Episode 131, Reward: -18.45827571946357, Moving Avg Reward: -17.73313608380244, Replay Buffer Size: 5406\n",
      "Current Epsilon: 0.851801859600347\n",
      "Episode 132: Won\n",
      "Episode 132, Avg Value Loss: 3.2114588847527137, Avg Policy Loss: 3.635149075434758\n",
      "Episode 132, Reward: -26.673401623529436, Moving Avg Reward: -17.713661694678557, Replay Buffer Size: 5419\n",
      "Current Epsilon: 0.8475428503023453\n",
      "Loss rate over the last 67 episodes: 0.55\n",
      "Episode 132, Reward: -26.673401623529436, Moving Avg Reward: -17.713661694678557, Replay Buffer Size: 5419\n",
      "Current Epsilon: 0.8475428503023453\n",
      "Episode 133: Won\n",
      "Episode 133, Avg Value Loss: 3.4774825870990753, Avg Policy Loss: 3.9458344876766205\n",
      "Episode 133, Reward: -26.388520301998934, Moving Avg Reward: -17.96522530288308, Replay Buffer Size: 5427\n",
      "Current Epsilon: 0.8433051360508336\n",
      "Loss rate over the last 67 episodes: 0.55\n",
      "Episode 133, Reward: -26.388520301998934, Moving Avg Reward: -17.96522530288308, Replay Buffer Size: 5427\n",
      "Current Epsilon: 0.8433051360508336\n",
      "Episode 134: Won\n",
      "Episode 134, Avg Value Loss: 3.5155566625297068, Avg Policy Loss: 3.6092031061649323\n",
      "Episode 134, Reward: -23.160622109647182, Moving Avg Reward: -17.94602984858656, Replay Buffer Size: 5507\n",
      "Current Epsilon: 0.8390886103705794\n",
      "Loss rate over the last 68 episodes: 0.54\n",
      "Episode 134, Reward: -23.160622109647182, Moving Avg Reward: -17.94602984858656, Replay Buffer Size: 5507\n",
      "Current Epsilon: 0.8390886103705794\n",
      "Episode 135: Lost\n",
      "Episode 135, Avg Value Loss: 3.5590817637559846, Avg Policy Loss: 3.5293774546646492\n",
      "Episode 135, Reward: 10.549882574793813, Moving Avg Reward: -17.936446577589187, Replay Buffer Size: 5548\n",
      "Current Epsilon: 0.8348931673187264\n",
      "Loss rate over the last 69 episodes: 0.53\n",
      "Episode 135, Reward: 10.549882574793813, Moving Avg Reward: -17.936446577589187, Replay Buffer Size: 5548\n",
      "Current Epsilon: 0.8348931673187264\n",
      "Episode 136: Won\n",
      "Episode 136, Avg Value Loss: 2.8471587234073215, Avg Policy Loss: 3.590905533896552\n",
      "Episode 136, Reward: -27.225215428572135, Moving Avg Reward: -17.984629171801142, Replay Buffer Size: 5557\n",
      "Current Epsilon: 0.8307187014821328\n",
      "Loss rate over the last 69 episodes: 0.55\n",
      "Episode 136, Reward: -27.225215428572135, Moving Avg Reward: -17.984629171801142, Replay Buffer Size: 5557\n",
      "Current Epsilon: 0.8307187014821328\n",
      "Episode 137: Won\n",
      "Episode 137, Avg Value Loss: 3.402947733276769, Avg Policy Loss: 3.622064188907021\n",
      "Episode 137, Reward: 16.505683718985747, Moving Avg Reward: -17.62018738352672, Replay Buffer Size: 5576\n",
      "Current Epsilon: 0.8265651079747222\n",
      "Loss rate over the last 69 episodes: 0.55\n",
      "Episode 137, Reward: 16.505683718985747, Moving Avg Reward: -17.62018738352672, Replay Buffer Size: 5576\n",
      "Current Epsilon: 0.8265651079747222\n",
      "Episode 138: Won\n",
      "Episode 138, Avg Value Loss: 3.802737981665368, Avg Policy Loss: 3.4895906401615515\n",
      "Episode 138, Reward: -49.23488172561928, Moving Avg Reward: -17.859048073806886, Replay Buffer Size: 5627\n",
      "Current Epsilon: 0.8224322824348486\n",
      "Loss rate over the last 70 episodes: 0.55\n",
      "Episode 138, Reward: -49.23488172561928, Moving Avg Reward: -17.859048073806886, Replay Buffer Size: 5627\n",
      "Current Epsilon: 0.8224322824348486\n",
      "Episode 139: Won\n",
      "Episode 139, Avg Value Loss: 3.8728529539975254, Avg Policy Loss: 3.3642781647768887\n",
      "Episode 139, Reward: -31.381638157661563, Moving Avg Reward: -17.940965287418326, Replay Buffer Size: 5638\n",
      "Current Epsilon: 0.8183201210226743\n",
      "Loss rate over the last 70 episodes: 0.55\n",
      "Episode 139, Reward: -31.381638157661563, Moving Avg Reward: -17.940965287418326, Replay Buffer Size: 5638\n",
      "Current Epsilon: 0.8183201210226743\n",
      "Episode 140: Won\n",
      "Episode 140, Avg Value Loss: 3.6751444462018137, Avg Policy Loss: 3.3620677422254515\n",
      "Episode 140, Reward: -27.486831361802274, Moving Avg Reward: -17.781048726913692, Replay Buffer Size: 5677\n",
      "Current Epsilon: 0.8142285204175609\n",
      "Loss rate over the last 70 episodes: 0.55\n",
      "Episode 140, Reward: -27.486831361802274, Moving Avg Reward: -17.781048726913692, Replay Buffer Size: 5677\n",
      "Current Epsilon: 0.8142285204175609\n",
      "Episode 141: Won\n",
      "Episode 141, Avg Value Loss: 3.411436163860819, Avg Policy Loss: 3.375795877498129\n",
      "Episode 141, Reward: -47.16802927315226, Moving Avg Reward: -17.875592306294596, Replay Buffer Size: 5723\n",
      "Current Epsilon: 0.810157377815473\n",
      "Loss rate over the last 71 episodes: 0.56\n",
      "Episode 141, Reward: -47.16802927315226, Moving Avg Reward: -17.875592306294596, Replay Buffer Size: 5723\n",
      "Current Epsilon: 0.810157377815473\n",
      "Episode 142: Won\n",
      "Episode 142, Avg Value Loss: 3.5534075668879916, Avg Policy Loss: 3.383864062173026\n",
      "Episode 142, Reward: -20.466283011070768, Moving Avg Reward: -18.184858867565136, Replay Buffer Size: 5730\n",
      "Current Epsilon: 0.8061065909263957\n",
      "Loss rate over the last 71 episodes: 0.56\n",
      "Episode 142, Reward: -20.466283011070768, Moving Avg Reward: -18.184858867565136, Replay Buffer Size: 5730\n",
      "Current Epsilon: 0.8061065909263957\n",
      "Episode 143: Lost\n",
      "Episode 143, Avg Value Loss: 3.5864108234643934, Avg Policy Loss: 3.4481759339571\n",
      "Episode 143, Reward: -18.486228022230577, Moving Avg Reward: -18.058080395531405, Replay Buffer Size: 5810\n",
      "Current Epsilon: 0.8020760579717637\n",
      "Loss rate over the last 72 episodes: 0.55\n",
      "Episode 143, Reward: -18.486228022230577, Moving Avg Reward: -18.058080395531405, Replay Buffer Size: 5810\n",
      "Current Epsilon: 0.8020760579717637\n",
      "Episode 144: Lost\n",
      "Episode 144, Avg Value Loss: 2.6237897210650973, Avg Policy Loss: 3.415408452351888\n",
      "Episode 144, Reward: -26.070163704768976, Moving Avg Reward: -18.09331403972136, Replay Buffer Size: 5819\n",
      "Current Epsilon: 0.798065677681905\n",
      "Loss rate over the last 72 episodes: 0.56\n",
      "Episode 144, Reward: -26.070163704768976, Moving Avg Reward: -18.09331403972136, Replay Buffer Size: 5819\n",
      "Current Epsilon: 0.798065677681905\n",
      "Episode 145: Lost\n",
      "Episode 145, Avg Value Loss: 3.196592539548874, Avg Policy Loss: 3.2452278435230255\n",
      "Episode 145, Reward: -21.854428891293793, Moving Avg Reward: -17.83517119558928, Replay Buffer Size: 5827\n",
      "Current Epsilon: 0.7940753492934954\n",
      "Loss rate over the last 72 episodes: 0.56\n",
      "Episode 145, Reward: -21.854428891293793, Moving Avg Reward: -17.83517119558928, Replay Buffer Size: 5827\n",
      "Current Epsilon: 0.7940753492934954\n",
      "Episode 146: Lost\n",
      "Episode 146, Avg Value Loss: 3.342648905515671, Avg Policy Loss: 3.341535452008247\n",
      "Episode 146, Reward: -52.69839112037606, Moving Avg Reward: -18.29773794989053, Replay Buffer Size: 5907\n",
      "Current Epsilon: 0.7901049725470279\n",
      "Loss rate over the last 73 episodes: 0.57\n",
      "Episode 146, Reward: -52.69839112037606, Moving Avg Reward: -18.29773794989053, Replay Buffer Size: 5907\n",
      "Current Epsilon: 0.7901049725470279\n",
      "Episode 147: Lost\n",
      "Episode 147, Avg Value Loss: 3.2688879907131194, Avg Policy Loss: 3.309395396709442\n",
      "Episode 147, Reward: -47.25482085375865, Moving Avg Reward: -18.512377655828097, Replay Buffer Size: 5987\n",
      "Current Epsilon: 0.7861544476842928\n",
      "Loss rate over the last 74 episodes: 0.56\n",
      "Episode 147, Reward: -47.25482085375865, Moving Avg Reward: -18.512377655828097, Replay Buffer Size: 5987\n",
      "Current Epsilon: 0.7861544476842928\n",
      "Episode 148: Lost\n",
      "Episode 148, Avg Value Loss: 3.2871484536873665, Avg Policy Loss: 3.176642966897864\n",
      "Episode 148, Reward: -77.19796514221076, Moving Avg Reward: -18.479143070833306, Replay Buffer Size: 6063\n",
      "Current Epsilon: 0.7822236754458713\n",
      "Loss rate over the last 75 episodes: 0.57\n",
      "Episode 148, Reward: -77.19796514221076, Moving Avg Reward: -18.479143070833306, Replay Buffer Size: 6063\n",
      "Current Epsilon: 0.7822236754458713\n",
      "Episode 149: Lost\n",
      "Episode 149, Avg Value Loss: 3.627702902192655, Avg Policy Loss: 3.1827005614405093\n",
      "Episode 149, Reward: -51.55656043939476, Moving Avg Reward: -18.77873195690299, Replay Buffer Size: 6109\n",
      "Current Epsilon: 0.778312557068642\n",
      "Loss rate over the last 76 episodes: 0.58\n",
      "Episode 149, Reward: -51.55656043939476, Moving Avg Reward: -18.77873195690299, Replay Buffer Size: 6109\n",
      "Current Epsilon: 0.778312557068642\n",
      "Episode 150: Lost\n",
      "Episode 150, Avg Value Loss: 3.5434984751045704, Avg Policy Loss: 3.1719001859426497\n",
      "Episode 150, Reward: -55.93714132924931, Moving Avg Reward: -19.142837659623158, Replay Buffer Size: 6189\n",
      "Current Epsilon: 0.7744209942832988\n",
      "Loss rate over the last 77 episodes: 0.57\n",
      "Episode 150, Reward: -55.93714132924931, Moving Avg Reward: -19.142837659623158, Replay Buffer Size: 6189\n",
      "Current Epsilon: 0.7744209942832988\n",
      "Episode 151: Lost\n",
      "Episode 151, Avg Value Loss: 3.4949073410696454, Avg Policy Loss: 3.0295570426517062\n",
      "Episode 151, Reward: 18.08847543342113, Moving Avg Reward: -18.8844531854175, Replay Buffer Size: 6225\n",
      "Current Epsilon: 0.7705488893118823\n",
      "Loss rate over the last 77 episodes: 0.57\n",
      "Episode 151, Reward: 18.08847543342113, Moving Avg Reward: -18.8844531854175, Replay Buffer Size: 6225\n",
      "Current Epsilon: 0.7705488893118823\n",
      "Episode 152: Won\n",
      "Episode 152, Avg Value Loss: 3.523605103045702, Avg Policy Loss: 3.098549652099609\n",
      "Episode 152, Reward: -3.278392006604463, Moving Avg Reward: -18.653530450436207, Replay Buffer Size: 6305\n",
      "Current Epsilon: 0.7666961448653229\n",
      "Loss rate over the last 78 episodes: 0.56\n",
      "Episode 152, Reward: -3.278392006604463, Moving Avg Reward: -18.653530450436207, Replay Buffer Size: 6305\n",
      "Current Epsilon: 0.7666961448653229\n",
      "Episode 153: Won\n",
      "Episode 153, Avg Value Loss: 3.6850753245146377, Avg Policy Loss: 3.1258949818818467\n",
      "Episode 153, Reward: -40.709923410989695, Moving Avg Reward: -19.30163409403936, Replay Buffer Size: 6328\n",
      "Current Epsilon: 0.7628626641409962\n",
      "Loss rate over the last 79 episodes: 0.57\n",
      "Episode 153, Reward: -40.709923410989695, Moving Avg Reward: -19.30163409403936, Replay Buffer Size: 6328\n",
      "Current Epsilon: 0.7628626641409962\n",
      "Episode 154: Won\n",
      "Episode 154, Avg Value Loss: 3.4367362345967973, Avg Policy Loss: 3.3298031943184987\n",
      "Episode 154, Reward: -19.765900690049705, Moving Avg Reward: -19.214597393232346, Replay Buffer Size: 6335\n",
      "Current Epsilon: 0.7590483508202912\n",
      "Loss rate over the last 79 episodes: 0.57\n",
      "Episode 154, Reward: -19.765900690049705, Moving Avg Reward: -19.214597393232346, Replay Buffer Size: 6335\n",
      "Current Epsilon: 0.7590483508202912\n",
      "Episode 155: Won\n",
      "Episode 155, Avg Value Loss: 3.559758272767067, Avg Policy Loss: 3.019903412461281\n",
      "Episode 155, Reward: -0.13035430305982443, Moving Avg Reward: -18.95738653428417, Replay Buffer Size: 6415\n",
      "Current Epsilon: 0.7552531090661897\n",
      "Loss rate over the last 80 episodes: 0.56\n",
      "Episode 155, Reward: -0.13035430305982443, Moving Avg Reward: -18.95738653428417, Replay Buffer Size: 6415\n",
      "Current Epsilon: 0.7552531090661897\n",
      "Episode 156: Lost\n",
      "Episode 156, Avg Value Loss: 3.0010443910172113, Avg Policy Loss: 3.1095667387309827\n",
      "Episode 156, Reward: -29.362644027697762, Moving Avg Reward: -19.16345222563355, Replay Buffer Size: 6453\n",
      "Current Epsilon: 0.7514768435208588\n",
      "Loss rate over the last 80 episodes: 0.57\n",
      "Episode 156, Reward: -29.362644027697762, Moving Avg Reward: -19.16345222563355, Replay Buffer Size: 6453\n",
      "Current Epsilon: 0.7514768435208588\n",
      "Episode 157: Lost\n",
      "Episode 157, Avg Value Loss: 3.202382140689426, Avg Policy Loss: 2.9897115230560303\n",
      "Episode 157, Reward: -23.53952529102829, Moving Avg Reward: -19.11946269210788, Replay Buffer Size: 6462\n",
      "Current Epsilon: 0.7477194593032545\n",
      "Loss rate over the last 80 episodes: 0.57\n",
      "Episode 157, Reward: -23.53952529102829, Moving Avg Reward: -19.11946269210788, Replay Buffer Size: 6462\n",
      "Current Epsilon: 0.7477194593032545\n",
      "Episode 158: Lost\n",
      "Episode 158, Avg Value Loss: 3.613381017338146, Avg Policy Loss: 3.1177217960357666\n",
      "Episode 158, Reward: -23.44167483667743, Moving Avg Reward: -19.085482676305954, Replay Buffer Size: 6473\n",
      "Current Epsilon: 0.7439808620067382\n",
      "Loss rate over the last 80 episodes: 0.57\n",
      "Episode 158, Reward: -23.44167483667743, Moving Avg Reward: -19.085482676305954, Replay Buffer Size: 6473\n",
      "Current Epsilon: 0.7439808620067382\n",
      "Episode 159: Lost\n",
      "Episode 159, Avg Value Loss: 3.9423763751983643, Avg Policy Loss: 3.069862206776937\n",
      "Episode 159, Reward: -28.241069417330408, Moving Avg Reward: -19.169549921152598, Replay Buffer Size: 6482\n",
      "Current Epsilon: 0.7402609576967045\n",
      "Loss rate over the last 81 episodes: 0.58\n",
      "Episode 159, Reward: -28.241069417330408, Moving Avg Reward: -19.169549921152598, Replay Buffer Size: 6482\n",
      "Current Epsilon: 0.7402609576967045\n",
      "Episode 160: Lost\n",
      "Episode 160, Avg Value Loss: 3.2050527984445747, Avg Policy Loss: 3.077274788509716\n",
      "Episode 160, Reward: 17.82710406420894, Moving Avg Reward: -18.777563943694954, Replay Buffer Size: 6504\n",
      "Current Epsilon: 0.736559652908221\n",
      "Loss rate over the last 81 episodes: 0.58\n",
      "Episode 160, Reward: 17.82710406420894, Moving Avg Reward: -18.777563943694954, Replay Buffer Size: 6504\n",
      "Current Epsilon: 0.736559652908221\n",
      "Episode 161: Won\n",
      "Episode 161, Avg Value Loss: 3.1199337906307645, Avg Policy Loss: 3.099026017718845\n",
      "Episode 161, Reward: -25.95314585483061, Moving Avg Reward: -18.76319821671147, Replay Buffer Size: 6513\n",
      "Current Epsilon: 0.7328768546436799\n",
      "Loss rate over the last 81 episodes: 0.58\n",
      "Episode 161, Reward: -25.95314585483061, Moving Avg Reward: -18.76319821671147, Replay Buffer Size: 6513\n",
      "Current Epsilon: 0.7328768546436799\n",
      "Episode 162: Won\n",
      "Episode 162, Avg Value Loss: 3.694753110408783, Avg Policy Loss: 2.7644503355026244\n",
      "Episode 162, Reward: -24.452848310769276, Moving Avg Reward: -18.776182457452467, Replay Buffer Size: 6523\n",
      "Current Epsilon: 0.7292124703704616\n",
      "Loss rate over the last 81 episodes: 0.58\n",
      "Episode 162, Reward: -24.452848310769276, Moving Avg Reward: -18.776182457452467, Replay Buffer Size: 6523\n",
      "Current Epsilon: 0.7292124703704616\n",
      "Episode 163: Won\n",
      "Episode 163, Avg Value Loss: 3.230557014112887, Avg Policy Loss: 2.975275723830513\n",
      "Episode 163, Reward: -30.26591242235179, Moving Avg Reward: -18.826138487969665, Replay Buffer Size: 6569\n",
      "Current Epsilon: 0.7255664080186093\n",
      "Loss rate over the last 82 episodes: 0.58\n",
      "Episode 163, Reward: -30.26591242235179, Moving Avg Reward: -18.826138487969665, Replay Buffer Size: 6569\n",
      "Current Epsilon: 0.7255664080186093\n",
      "Episode 164: Won\n",
      "Episode 164, Avg Value Loss: 3.313641091187795, Avg Policy Loss: 2.9152462681134543\n",
      "Episode 164, Reward: -24.08514779952513, Moving Avg Reward: -18.782921255528823, Replay Buffer Size: 6581\n",
      "Current Epsilon: 0.7219385759785162\n",
      "Loss rate over the last 82 episodes: 0.58\n",
      "Episode 164, Reward: -24.08514779952513, Moving Avg Reward: -18.782921255528823, Replay Buffer Size: 6581\n",
      "Current Epsilon: 0.7219385759785162\n",
      "Episode 165: Won\n",
      "Episode 165, Avg Value Loss: 3.79407356002114, Avg Policy Loss: 3.1238860433751885\n",
      "Episode 165, Reward: -26.66483595214518, Moving Avg Reward: -19.205497533998866, Replay Buffer Size: 6592\n",
      "Current Epsilon: 0.7183288830986236\n",
      "Loss rate over the last 82 episodes: 0.58\n",
      "Episode 165, Reward: -26.66483595214518, Moving Avg Reward: -19.205497533998866, Replay Buffer Size: 6592\n",
      "Current Epsilon: 0.7183288830986236\n",
      "Episode 166: Won\n",
      "Episode 166, Avg Value Loss: 3.1609101825290256, Avg Policy Loss: 3.061518132686615\n",
      "Episode 166, Reward: -23.219366497091993, Moving Avg Reward: -19.14967210703094, Replay Buffer Size: 6628\n",
      "Current Epsilon: 0.7147372386831305\n",
      "Loss rate over the last 82 episodes: 0.58\n",
      "Episode 166, Reward: -23.219366497091993, Moving Avg Reward: -19.14967210703094, Replay Buffer Size: 6628\n",
      "Current Epsilon: 0.7147372386831305\n",
      "Episode 167: Won\n",
      "Episode 167, Avg Value Loss: 3.477580374479294, Avg Policy Loss: 3.009646010398865\n",
      "Episode 167, Reward: -8.65491870917494, Moving Avg Reward: -19.36543023586226, Replay Buffer Size: 6708\n",
      "Current Epsilon: 0.7111635524897149\n",
      "Loss rate over the last 83 episodes: 0.57\n",
      "Episode 167, Reward: -8.65491870917494, Moving Avg Reward: -19.36543023586226, Replay Buffer Size: 6708\n",
      "Current Epsilon: 0.7111635524897149\n",
      "Episode 168: Lost\n",
      "Episode 168, Avg Value Loss: 3.6501751564443112, Avg Policy Loss: 2.9994762927293777\n",
      "Episode 168, Reward: -116.92808351136445, Moving Avg Reward: -19.854349580142863, Replay Buffer Size: 6788\n",
      "Current Epsilon: 0.7076077347272662\n",
      "Loss rate over the last 84 episodes: 0.57\n",
      "Episode 168, Reward: -116.92808351136445, Moving Avg Reward: -19.854349580142863, Replay Buffer Size: 6788\n",
      "Current Epsilon: 0.7076077347272662\n",
      "Episode 169: Draw\n",
      "Episode 169, Avg Value Loss: 3.7236268699169157, Avg Policy Loss: 2.9142864644527435\n",
      "Episode 169, Reward: -28.083513106123423, Moving Avg Reward: -20.24906159127258, Replay Buffer Size: 6868\n",
      "Current Epsilon: 0.7040696960536299\n",
      "Loss rate over the last 85 episodes: 0.56\n",
      "Episode 169, Reward: -28.083513106123423, Moving Avg Reward: -20.24906159127258, Replay Buffer Size: 6868\n",
      "Current Epsilon: 0.7040696960536299\n",
      "Episode 170: Draw\n",
      "Episode 170, Avg Value Loss: 3.6868217097861424, Avg Policy Loss: 2.783352004630225\n",
      "Episode 170, Reward: 12.72667709277869, Moving Avg Reward: -20.150296848834067, Replay Buffer Size: 6924\n",
      "Current Epsilon: 0.7005493475733617\n",
      "Loss rate over the last 86 episodes: 0.55\n",
      "Episode 170, Reward: 12.72667709277869, Moving Avg Reward: -20.150296848834067, Replay Buffer Size: 6924\n",
      "Current Epsilon: 0.7005493475733617\n",
      "Episode 171: Won\n",
      "Episode 171, Avg Value Loss: 3.441399047374725, Avg Policy Loss: 2.9389761114120483\n",
      "Episode 171, Reward: -36.5031556075699, Moving Avg Reward: -20.042082360943663, Replay Buffer Size: 6974\n",
      "Current Epsilon: 0.697046600835495\n",
      "Loss rate over the last 87 episodes: 0.56\n",
      "Episode 171, Reward: -36.5031556075699, Moving Avg Reward: -20.042082360943663, Replay Buffer Size: 6974\n",
      "Current Epsilon: 0.697046600835495\n",
      "Episode 172: Won\n",
      "Episode 172, Avg Value Loss: 3.3003997314721345, Avg Policy Loss: 2.8303992569446565\n",
      "Episode 172, Reward: -17.463440767885892, Moving Avg Reward: -20.211487966912138, Replay Buffer Size: 7054\n",
      "Current Epsilon: 0.6935613678313175\n",
      "Loss rate over the last 88 episodes: 0.56\n",
      "Episode 172, Reward: -17.463440767885892, Moving Avg Reward: -20.211487966912138, Replay Buffer Size: 7054\n",
      "Current Epsilon: 0.6935613678313175\n",
      "Episode 173: Won\n",
      "Episode 173, Avg Value Loss: 3.6047901540994642, Avg Policy Loss: 2.8365618392825125\n",
      "Episode 173, Reward: -146.55139842703818, Moving Avg Reward: -21.414208812193365, Replay Buffer Size: 7134\n",
      "Current Epsilon: 0.6900935609921609\n",
      "Loss rate over the last 89 episodes: 0.55\n",
      "Episode 173, Reward: -146.55139842703818, Moving Avg Reward: -21.414208812193365, Replay Buffer Size: 7134\n",
      "Current Epsilon: 0.6900935609921609\n",
      "Episode 174: Draw\n",
      "Episode 174, Avg Value Loss: 3.4146753042936324, Avg Policy Loss: 2.8678976386785506\n",
      "Episode 174, Reward: -30.418983204861796, Moving Avg Reward: -21.45596041147287, Replay Buffer Size: 7214\n",
      "Current Epsilon: 0.6866430931872001\n",
      "Loss rate over the last 90 episodes: 0.54\n",
      "Episode 174, Reward: -30.418983204861796, Moving Avg Reward: -21.45596041147287, Replay Buffer Size: 7214\n",
      "Current Epsilon: 0.6866430931872001\n",
      "Episode 175: Draw\n",
      "Episode 175, Avg Value Loss: 2.626098155975342, Avg Policy Loss: 2.9821268830980574\n",
      "Episode 175, Reward: -21.0480680352037, Moving Avg Reward: -21.419483192412464, Replay Buffer Size: 7221\n",
      "Current Epsilon: 0.6832098777212641\n",
      "Loss rate over the last 90 episodes: 0.55\n",
      "Episode 175, Reward: -21.0480680352037, Moving Avg Reward: -21.419483192412464, Replay Buffer Size: 7221\n",
      "Current Epsilon: 0.6832098777212641\n",
      "Episode 176: Lost\n",
      "Episode 176, Avg Value Loss: 3.26516550465634, Avg Policy Loss: 2.9095290836535\n",
      "Episode 176, Reward: 16.968455152029577, Moving Avg Reward: -21.43444243339541, Replay Buffer Size: 7240\n",
      "Current Epsilon: 0.6797938283326578\n",
      "Loss rate over the last 90 episodes: 0.55\n",
      "Episode 176, Reward: 16.968455152029577, Moving Avg Reward: -21.43444243339541, Replay Buffer Size: 7240\n",
      "Current Epsilon: 0.6797938283326578\n",
      "Episode 177: Won\n",
      "Episode 177, Avg Value Loss: 3.455885761976242, Avg Policy Loss: 2.8208039939403533\n",
      "Episode 177, Reward: -103.65653881861378, Moving Avg Reward: -21.96349079611818, Replay Buffer Size: 7320\n",
      "Current Epsilon: 0.6763948591909945\n",
      "Loss rate over the last 91 episodes: 0.55\n",
      "Episode 177, Reward: -103.65653881861378, Moving Avg Reward: -21.96349079611818, Replay Buffer Size: 7320\n",
      "Current Epsilon: 0.6763948591909945\n",
      "Episode 178: Won\n",
      "Episode 178, Avg Value Loss: 3.502423162971224, Avg Policy Loss: 2.5869248594556535\n",
      "Episode 178, Reward: -36.57430019165164, Moving Avg Reward: -22.48941583804649, Replay Buffer Size: 7334\n",
      "Current Epsilon: 0.6730128848950395\n",
      "Loss rate over the last 91 episodes: 0.56\n",
      "Episode 178, Reward: -36.57430019165164, Moving Avg Reward: -22.48941583804649, Replay Buffer Size: 7334\n",
      "Current Epsilon: 0.6730128848950395\n",
      "Episode 179: Won\n",
      "Episode 179, Avg Value Loss: 3.3083818785846235, Avg Policy Loss: 2.8510286271572114\n",
      "Episode 179, Reward: -62.816252377325995, Moving Avg Reward: -23.011473554137112, Replay Buffer Size: 7414\n",
      "Current Epsilon: 0.6696478204705644\n",
      "Loss rate over the last 92 episodes: 0.55\n",
      "Episode 179, Reward: -62.816252377325995, Moving Avg Reward: -23.011473554137112, Replay Buffer Size: 7414\n",
      "Current Epsilon: 0.6696478204705644\n",
      "Episode 180: Lost\n",
      "Episode 180, Avg Value Loss: 3.593564366300901, Avg Policy Loss: 2.7170720224579177\n",
      "Episode 180, Reward: -43.7601149416489, Moving Avg Reward: -23.122443509256495, Replay Buffer Size: 7462\n",
      "Current Epsilon: 0.6662995813682115\n",
      "Loss rate over the last 93 episodes: 0.56\n",
      "Episode 180, Reward: -43.7601149416489, Moving Avg Reward: -23.122443509256495, Replay Buffer Size: 7462\n",
      "Current Epsilon: 0.6662995813682115\n",
      "Episode 181: Lost\n",
      "Episode 181, Avg Value Loss: 3.4874097967147826, Avg Policy Loss: 2.9074388122558594\n",
      "Episode 181, Reward: 13.037601155669057, Moving Avg Reward: -22.807428870841992, Replay Buffer Size: 7487\n",
      "Current Epsilon: 0.6629680834613705\n",
      "Loss rate over the last 93 episodes: 0.56\n",
      "Episode 181, Reward: 13.037601155669057, Moving Avg Reward: -22.807428870841992, Replay Buffer Size: 7487\n",
      "Current Epsilon: 0.6629680834613705\n",
      "Episode 182: Won\n",
      "Episode 182, Avg Value Loss: 2.9779897928237915, Avg Policy Loss: 3.0138988196849823\n",
      "Episode 182, Reward: -23.450352951841484, Moving Avg Reward: -22.79009757728389, Replay Buffer Size: 7495\n",
      "Current Epsilon: 0.6596532430440636\n",
      "Loss rate over the last 93 episodes: 0.56\n",
      "Episode 182, Reward: -23.450352951841484, Moving Avg Reward: -22.79009757728389, Replay Buffer Size: 7495\n",
      "Current Epsilon: 0.6596532430440636\n",
      "Episode 183: Won\n",
      "Episode 183, Avg Value Loss: 3.282465396892457, Avg Policy Loss: 2.7906803517114547\n",
      "Episode 183, Reward: -42.00894879108446, Moving Avg Reward: -22.95601055662818, Replay Buffer Size: 7537\n",
      "Current Epsilon: 0.6563549768288433\n",
      "Loss rate over the last 94 episodes: 0.56\n",
      "Episode 183, Reward: -42.00894879108446, Moving Avg Reward: -22.95601055662818, Replay Buffer Size: 7537\n",
      "Current Epsilon: 0.6563549768288433\n",
      "Episode 184: Won\n",
      "Episode 184, Avg Value Loss: 3.121830170804804, Avg Policy Loss: 2.7589564973657783\n",
      "Episode 184, Reward: -24.993997957870278, Moving Avg Reward: -23.362587141154037, Replay Buffer Size: 7548\n",
      "Current Epsilon: 0.653073201944699\n",
      "Loss rate over the last 94 episodes: 0.56\n",
      "Episode 184, Reward: -24.993997957870278, Moving Avg Reward: -23.362587141154037, Replay Buffer Size: 7548\n",
      "Current Epsilon: 0.653073201944699\n",
      "Episode 185: Won\n",
      "Episode 185, Avg Value Loss: 3.5647414922714233, Avg Policy Loss: 2.9093534350395203\n",
      "Episode 185, Reward: -31.95043079500214, Moving Avg Reward: -23.85379170640536, Replay Buffer Size: 7560\n",
      "Current Epsilon: 0.6498078359349755\n",
      "Loss rate over the last 94 episodes: 0.56\n",
      "Episode 185, Reward: -31.95043079500214, Moving Avg Reward: -23.85379170640536, Replay Buffer Size: 7560\n",
      "Current Epsilon: 0.6498078359349755\n",
      "Episode 186: Won\n",
      "Episode 186, Avg Value Loss: 3.3680324980190823, Avg Policy Loss: 2.868475699911312\n",
      "Episode 186, Reward: -45.17014831713068, Moving Avg Reward: -23.701366344940013, Replay Buffer Size: 7609\n",
      "Current Epsilon: 0.6465587967553006\n",
      "Loss rate over the last 95 episodes: 0.57\n",
      "Episode 186, Reward: -45.17014831713068, Moving Avg Reward: -23.701366344940013, Replay Buffer Size: 7609\n",
      "Current Epsilon: 0.6465587967553006\n",
      "Episode 187: Won\n",
      "Episode 187, Avg Value Loss: 2.8647315068678423, Avg Policy Loss: 2.801300287246704\n",
      "Episode 187, Reward: -31.430533588365947, Moving Avg Reward: -23.75440275917228, Replay Buffer Size: 7620\n",
      "Current Epsilon: 0.6433260027715241\n",
      "Loss rate over the last 95 episodes: 0.57\n",
      "Episode 187, Reward: -31.430533588365947, Moving Avg Reward: -23.75440275917228, Replay Buffer Size: 7620\n",
      "Current Epsilon: 0.6433260027715241\n",
      "Episode 188: Won\n",
      "Episode 188, Avg Value Loss: 3.6289263367652893, Avg Policy Loss: 2.7814812660217285\n",
      "Episode 188, Reward: -17.90652601097416, Moving Avg Reward: -23.975263002168116, Replay Buffer Size: 7626\n",
      "Current Epsilon: 0.6401093727576664\n",
      "Loss rate over the last 95 episodes: 0.57\n",
      "Episode 188, Reward: -17.90652601097416, Moving Avg Reward: -23.975263002168116, Replay Buffer Size: 7626\n",
      "Current Epsilon: 0.6401093727576664\n",
      "Episode 189: Won\n",
      "Episode 189, Avg Value Loss: 3.1978908494114875, Avg Policy Loss: 2.8423535019159316\n",
      "Episode 189, Reward: -25.41077455904758, Moving Avg Reward: -24.398942427478154, Replay Buffer Size: 7706\n",
      "Current Epsilon: 0.6369088258938781\n",
      "Loss rate over the last 96 episodes: 0.56\n",
      "Episode 189, Reward: -25.41077455904758, Moving Avg Reward: -24.398942427478154, Replay Buffer Size: 7706\n",
      "Current Epsilon: 0.6369088258938781\n",
      "Episode 190: Lost\n",
      "Episode 190, Avg Value Loss: 4.0237115859985355, Avg Policy Loss: 2.936289429664612\n",
      "Episode 190, Reward: -25.08447529798251, Moving Avg Reward: -24.433251807821524, Replay Buffer Size: 7716\n",
      "Current Epsilon: 0.6337242817644086\n",
      "Loss rate over the last 96 episodes: 0.57\n",
      "Episode 190, Reward: -25.08447529798251, Moving Avg Reward: -24.433251807821524, Replay Buffer Size: 7716\n",
      "Current Epsilon: 0.6337242817644086\n",
      "Episode 191: Lost\n",
      "Episode 191, Avg Value Loss: 3.458001876728875, Avg Policy Loss: 2.9263449771063668\n",
      "Episode 191, Reward: -94.53199825931708, Moving Avg Reward: -25.126718116565407, Replay Buffer Size: 7786\n",
      "Current Epsilon: 0.6305556603555866\n",
      "Loss rate over the last 97 episodes: 0.58\n",
      "Episode 191, Reward: -94.53199825931708, Moving Avg Reward: -25.126718116565407, Replay Buffer Size: 7786\n",
      "Current Epsilon: 0.6305556603555866\n",
      "Episode 192: Lost\n",
      "Episode 192, Avg Value Loss: 3.5708342218399047, Avg Policy Loss: 2.7800362586975096\n",
      "Episode 192, Reward: 10.397511585308353, Moving Avg Reward: -24.764984116552437, Replay Buffer Size: 7836\n",
      "Current Epsilon: 0.6274028820538087\n",
      "Loss rate over the last 97 episodes: 0.57\n",
      "Episode 192, Reward: 10.397511585308353, Moving Avg Reward: -24.764984116552437, Replay Buffer Size: 7836\n",
      "Current Epsilon: 0.6274028820538087\n",
      "Episode 193: Won\n",
      "Episode 193, Avg Value Loss: 3.5545786089367337, Avg Policy Loss: 2.723351001739502\n",
      "Episode 193, Reward: -28.90352765184274, Moving Avg Reward: -25.16071031805728, Replay Buffer Size: 7845\n",
      "Current Epsilon: 0.6242658676435396\n",
      "Loss rate over the last 98 episodes: 0.58\n",
      "Episode 193, Reward: -28.90352765184274, Moving Avg Reward: -25.16071031805728, Replay Buffer Size: 7845\n",
      "Current Epsilon: 0.6242658676435396\n",
      "Episode 194: Won\n",
      "Episode 194, Avg Value Loss: 3.210904611991002, Avg Policy Loss: 2.88425888465001\n",
      "Episode 194, Reward: 15.2254276344185, Moving Avg Reward: -24.527644993482184, Replay Buffer Size: 7871\n",
      "Current Epsilon: 0.6211445383053219\n",
      "Loss rate over the last 98 episodes: 0.58\n",
      "Episode 194, Reward: 15.2254276344185, Moving Avg Reward: -24.527644993482184, Replay Buffer Size: 7871\n",
      "Current Epsilon: 0.6211445383053219\n",
      "Episode 195: Won\n",
      "Episode 195, Avg Value Loss: 3.251201616972685, Avg Policy Loss: 2.897763592004776\n",
      "Episode 195, Reward: -35.910649196208155, Moving Avg Reward: -24.61407276552729, Replay Buffer Size: 7951\n",
      "Current Epsilon: 0.6180388156137953\n",
      "Loss rate over the last 99 episodes: 0.57\n",
      "Episode 195, Reward: -35.910649196208155, Moving Avg Reward: -24.61407276552729, Replay Buffer Size: 7951\n",
      "Current Epsilon: 0.6180388156137953\n",
      "Episode 196: Won\n",
      "Episode 196, Avg Value Loss: 3.375737504525618, Avg Policy Loss: 2.9963680614124644\n",
      "Episode 196, Reward: -24.361955880765777, Moving Avg Reward: -24.456200856658775, Replay Buffer Size: 7962\n",
      "Current Epsilon: 0.6149486215357263\n",
      "Loss rate over the last 99 episodes: 0.58\n",
      "Episode 196, Reward: -24.361955880765777, Moving Avg Reward: -24.456200856658775, Replay Buffer Size: 7962\n",
      "Current Epsilon: 0.6149486215357263\n",
      "Episode 197: Won\n",
      "Episode 197, Avg Value Loss: 3.716447059924786, Avg Policy Loss: 2.80459686426016\n",
      "Episode 197, Reward: -27.531451962564972, Moving Avg Reward: -24.355012061173294, Replay Buffer Size: 7975\n",
      "Current Epsilon: 0.6118738784280476\n",
      "Loss rate over the last 99 episodes: 0.58\n",
      "Episode 197, Reward: -27.531451962564972, Moving Avg Reward: -24.355012061173294, Replay Buffer Size: 7975\n",
      "Current Epsilon: 0.6118738784280476\n",
      "Episode 198: Won\n",
      "Episode 198, Avg Value Loss: 2.7739349484443663, Avg Policy Loss: 2.9429033994674683\n",
      "Episode 198, Reward: -26.18256381432436, Moving Avg Reward: -24.373716448446316, Replay Buffer Size: 7985\n",
      "Current Epsilon: 0.6088145090359074\n",
      "Loss rate over the last 99 episodes: 0.58\n",
      "Episode 198, Reward: -26.18256381432436, Moving Avg Reward: -24.373716448446316, Replay Buffer Size: 7985\n",
      "Current Epsilon: 0.6088145090359074\n",
      "Episode 199: Won\n",
      "Episode 199, Avg Value Loss: 3.1326338811354204, Avg Policy Loss: 2.9496347687461157\n",
      "Episode 199, Reward: -23.89767146855987, Moving Avg Reward: -24.79411919222603, Replay Buffer Size: 7996\n",
      "Current Epsilon: 0.6057704364907278\n",
      "Loss rate over the last 99 episodes: 0.58\n",
      "Episode 199, Reward: -23.89767146855987, Moving Avg Reward: -24.79411919222603, Replay Buffer Size: 7996\n",
      "Current Epsilon: 0.6057704364907278\n",
      "Episode 200: Won\n",
      "Episode 200, Avg Value Loss: 3.693031168998556, Avg Policy Loss: 2.830553176555228\n",
      "Episode 200, Reward: -54.682966936800014, Moving Avg Reward: -25.50177731495242, Replay Buffer Size: 8043\n",
      "Current Epsilon: 0.6027415843082742\n",
      "Loss rate over the last 100 episodes: 0.59\n",
      "Episode 200, Reward: -54.682966936800014, Moving Avg Reward: -25.50177731495242, Replay Buffer Size: 8043\n",
      "Current Epsilon: 0.6027415843082742\n",
      "Episode 201: Lost\n",
      "Episode 201, Avg Value Loss: 3.46227832203326, Avg Policy Loss: 2.8574968939242154\n",
      "Episode 201, Reward: 10.794342960866503, Moving Avg Reward: -25.081244191545867, Replay Buffer Size: 8089\n",
      "Current Epsilon: 0.5997278763867329\n",
      "Loss rate over the last 101 episodes: 0.58\n",
      "Episode 201, Reward: 10.794342960866503, Moving Avg Reward: -25.081244191545867, Replay Buffer Size: 8089\n",
      "Current Epsilon: 0.5997278763867329\n",
      "Episode 202: Won\n",
      "Episode 202, Avg Value Loss: 3.850416425954212, Avg Policy Loss: 2.7611475559798153\n",
      "Episode 202, Reward: -40.19826876051948, Moving Avg Reward: -25.280931133194894, Replay Buffer Size: 8133\n",
      "Current Epsilon: 0.5967292370047992\n",
      "Loss rate over the last 101 episodes: 0.59\n",
      "Episode 202, Reward: -40.19826876051948, Moving Avg Reward: -25.280931133194894, Replay Buffer Size: 8133\n",
      "Current Epsilon: 0.5967292370047992\n",
      "Episode 203: Won\n",
      "Episode 203, Avg Value Loss: 3.156180490146984, Avg Policy Loss: 2.893697500228882\n",
      "Episode 203, Reward: -28.098191984719328, Moving Avg Reward: -25.66181305304209, Replay Buffer Size: 8144\n",
      "Current Epsilon: 0.5937455908197752\n",
      "Loss rate over the last 101 episodes: 0.59\n",
      "Episode 203, Reward: -28.098191984719328, Moving Avg Reward: -25.66181305304209, Replay Buffer Size: 8144\n",
      "Current Epsilon: 0.5937455908197752\n",
      "Episode 204: Won\n",
      "Episode 204, Avg Value Loss: 3.5842817083001135, Avg Policy Loss: 2.8537570416927336\n",
      "Episode 204, Reward: 7.010377492885028, Moving Avg Reward: -25.24604542251068, Replay Buffer Size: 8224\n",
      "Current Epsilon: 0.5907768628656763\n",
      "Loss rate over the last 102 episodes: 0.58\n",
      "Episode 204, Reward: 7.010377492885028, Moving Avg Reward: -25.24604542251068, Replay Buffer Size: 8224\n",
      "Current Epsilon: 0.5907768628656763\n",
      "Episode 205: Won\n",
      "Episode 205, Avg Value Loss: 3.5742189619276257, Avg Policy Loss: 2.750305864546034\n",
      "Episode 205, Reward: -24.34887976531563, Moving Avg Reward: -25.19351294928307, Replay Buffer Size: 8233\n",
      "Current Epsilon: 0.5878229785513479\n",
      "Loss rate over the last 102 episodes: 0.59\n",
      "Episode 205, Reward: -24.34887976531563, Moving Avg Reward: -25.19351294928307, Replay Buffer Size: 8233\n",
      "Current Epsilon: 0.5878229785513479\n",
      "Episode 206: Won\n",
      "Episode 206, Avg Value Loss: 3.302884922362864, Avg Policy Loss: 2.887143686413765\n",
      "Episode 206, Reward: -39.525169361747835, Moving Avg Reward: -25.77587105202501, Replay Buffer Size: 8297\n",
      "Current Epsilon: 0.5848838636585911\n",
      "Loss rate over the last 103 episodes: 0.59\n",
      "Episode 206, Reward: -39.525169361747835, Moving Avg Reward: -25.77587105202501, Replay Buffer Size: 8297\n",
      "Current Epsilon: 0.5848838636585911\n",
      "Episode 207: Won\n",
      "Episode 207, Avg Value Loss: 3.5942909345030785, Avg Policy Loss: 2.8059390604496004\n",
      "Episode 207, Reward: -28.516295410754463, Moving Avg Reward: -25.83151576998218, Replay Buffer Size: 8377\n",
      "Current Epsilon: 0.5819594443402982\n",
      "Loss rate over the last 104 episodes: 0.58\n",
      "Episode 207, Reward: -28.516295410754463, Moving Avg Reward: -25.83151576998218, Replay Buffer Size: 8377\n",
      "Current Epsilon: 0.5819594443402982\n",
      "Episode 208: Won\n",
      "Episode 208, Avg Value Loss: 3.300286889076233, Avg Policy Loss: 2.8973367479112415\n",
      "Episode 208, Reward: -33.47391740689084, Moving Avg Reward: -25.941255634341445, Replay Buffer Size: 8386\n",
      "Current Epsilon: 0.5790496471185967\n",
      "Loss rate over the last 104 episodes: 0.59\n",
      "Episode 208, Reward: -33.47391740689084, Moving Avg Reward: -25.941255634341445, Replay Buffer Size: 8386\n",
      "Current Epsilon: 0.5790496471185967\n",
      "Episode 209: Won\n",
      "Episode 209, Avg Value Loss: 3.156736368133176, Avg Policy Loss: 2.8870177038254274\n",
      "Episode 209, Reward: 16.421684824094168, Moving Avg Reward: -25.813770137509977, Replay Buffer Size: 8417\n",
      "Current Epsilon: 0.5761543988830038\n",
      "Loss rate over the last 105 episodes: 0.59\n",
      "Episode 209, Reward: 16.421684824094168, Moving Avg Reward: -25.813770137509977, Replay Buffer Size: 8417\n",
      "Current Epsilon: 0.5761543988830038\n",
      "Episode 210: Won\n",
      "Episode 210, Avg Value Loss: 3.503107117116451, Avg Policy Loss: 2.879226490855217\n",
      "Episode 210, Reward: -20.06923797877945, Moving Avg Reward: -25.78756675092145, Replay Buffer Size: 8497\n",
      "Current Epsilon: 0.5732736268885887\n",
      "Loss rate over the last 106 episodes: 0.58\n",
      "Episode 210, Reward: -20.06923797877945, Moving Avg Reward: -25.78756675092145, Replay Buffer Size: 8497\n",
      "Current Epsilon: 0.5732736268885887\n",
      "Episode 211: Won\n",
      "Episode 211, Avg Value Loss: 3.4209809832274916, Avg Policy Loss: 2.899733844399452\n",
      "Episode 211, Reward: -92.53828510312047, Moving Avg Reward: -26.89804591527507, Replay Buffer Size: 8577\n",
      "Current Epsilon: 0.5704072587541458\n",
      "Loss rate over the last 107 episodes: 0.58\n",
      "Episode 211, Reward: -92.53828510312047, Moving Avg Reward: -26.89804591527507, Replay Buffer Size: 8577\n",
      "Current Epsilon: 0.5704072587541458\n",
      "Episode 212: Draw\n",
      "Episode 212, Avg Value Loss: 3.864390108320448, Avg Policy Loss: 2.736771821975708\n",
      "Episode 212, Reward: -22.97920067932371, Moving Avg Reward: -27.054428308172348, Replay Buffer Size: 8586\n",
      "Current Epsilon: 0.567555222460375\n",
      "Loss rate over the last 107 episodes: 0.59\n",
      "Episode 212, Reward: -22.97920067932371, Moving Avg Reward: -27.054428308172348, Replay Buffer Size: 8586\n",
      "Current Epsilon: 0.567555222460375\n",
      "Episode 213: Lost\n",
      "Episode 213, Avg Value Loss: 3.4188146163587985, Avg Policy Loss: 2.8916947323343027\n",
      "Episode 213, Reward: -41.305307998650505, Moving Avg Reward: -27.193221206610907, Replay Buffer Size: 8632\n",
      "Current Epsilon: 0.5647174463480732\n",
      "Loss rate over the last 107 episodes: 0.58\n",
      "Episode 213, Reward: -41.305307998650505, Moving Avg Reward: -27.193221206610907, Replay Buffer Size: 8632\n",
      "Current Epsilon: 0.5647174463480732\n",
      "Episode 214: Lost\n",
      "Episode 214, Avg Value Loss: 3.724091511964798, Avg Policy Loss: 2.7434150052070616\n",
      "Episode 214, Reward: -63.91506747594353, Moving Avg Reward: -27.495805594474774, Replay Buffer Size: 8682\n",
      "Current Epsilon: 0.5618938591163328\n",
      "Loss rate over the last 108 episodes: 0.59\n",
      "Episode 214, Reward: -63.91506747594353, Moving Avg Reward: -27.495805594474774, Replay Buffer Size: 8682\n",
      "Current Epsilon: 0.5618938591163328\n",
      "Episode 215: Lost\n",
      "Episode 215, Avg Value Loss: 3.4770535425706344, Avg Policy Loss: 2.8512978943911467\n",
      "Episode 215, Reward: -35.96812317887503, Moving Avg Reward: -27.561457381742137, Replay Buffer Size: 8737\n",
      "Current Epsilon: 0.5590843898207511\n",
      "Loss rate over the last 109 episodes: 0.60\n",
      "Episode 215, Reward: -35.96812317887503, Moving Avg Reward: -27.561457381742137, Replay Buffer Size: 8737\n",
      "Current Epsilon: 0.5590843898207511\n",
      "Episode 216: Lost\n",
      "Episode 216, Avg Value Loss: 2.6436822175979615, Avg Policy Loss: 3.0261608123779298\n",
      "Episode 216, Reward: -27.43698117649193, Moving Avg Reward: -27.78850180937622, Replay Buffer Size: 8747\n",
      "Current Epsilon: 0.5562889678716474\n",
      "Loss rate over the last 109 episodes: 0.59\n",
      "Episode 216, Reward: -27.43698117649193, Moving Avg Reward: -27.78850180937622, Replay Buffer Size: 8747\n",
      "Current Epsilon: 0.5562889678716474\n",
      "Episode 217: Lost\n",
      "Episode 217, Avg Value Loss: 4.149403512477875, Avg Policy Loss: 2.7252460718154907\n",
      "Episode 217, Reward: -25.665080596105394, Moving Avg Reward: -27.85625826753895, Replay Buffer Size: 8755\n",
      "Current Epsilon: 0.5535075230322891\n",
      "Loss rate over the last 109 episodes: 0.59\n",
      "Episode 217, Reward: -25.665080596105394, Moving Avg Reward: -27.85625826753895, Replay Buffer Size: 8755\n",
      "Current Epsilon: 0.5535075230322891\n",
      "Episode 218: Lost\n",
      "Episode 218, Avg Value Loss: 3.9480448842048643, Avg Policy Loss: 2.87966902256012\n",
      "Episode 218, Reward: -28.821482775538783, Moving Avg Reward: -27.40384289393859, Replay Buffer Size: 8765\n",
      "Current Epsilon: 0.5507399854171277\n",
      "Loss rate over the last 109 episodes: 0.59\n",
      "Episode 218, Reward: -28.821482775538783, Moving Avg Reward: -27.40384289393859, Replay Buffer Size: 8765\n",
      "Current Epsilon: 0.5507399854171277\n",
      "Episode 219: Lost\n",
      "Episode 219, Avg Value Loss: 3.4247025576504795, Avg Policy Loss: 2.6910249970176\n",
      "Episode 219, Reward: -22.444109091121106, Moving Avg Reward: -27.332084107948802, Replay Buffer Size: 8776\n",
      "Current Epsilon: 0.547986285490042\n",
      "Loss rate over the last 109 episodes: 0.59\n",
      "Episode 219, Reward: -22.444109091121106, Moving Avg Reward: -27.332084107948802, Replay Buffer Size: 8776\n",
      "Current Epsilon: 0.547986285490042\n",
      "Episode 220: Lost\n",
      "Episode 220, Avg Value Loss: 3.434426870610979, Avg Policy Loss: 2.8314898014068604\n",
      "Episode 220, Reward: 19.16906935247436, Moving Avg Reward: -26.87595599201516, Replay Buffer Size: 8794\n",
      "Current Epsilon: 0.5452463540625918\n",
      "Loss rate over the last 109 episodes: 0.59\n",
      "Episode 220, Reward: 19.16906935247436, Moving Avg Reward: -26.87595599201516, Replay Buffer Size: 8794\n",
      "Current Epsilon: 0.5452463540625918\n",
      "Episode 221: Won\n",
      "Episode 221, Avg Value Loss: 3.551787681132555, Avg Policy Loss: 2.9055869579315186\n",
      "Episode 221, Reward: -97.53822330407618, Moving Avg Reward: -27.896267751591157, Replay Buffer Size: 8874\n",
      "Current Epsilon: 0.5425201222922789\n",
      "Loss rate over the last 110 episodes: 0.59\n",
      "Episode 221, Reward: -97.53822330407618, Moving Avg Reward: -27.896267751591157, Replay Buffer Size: 8874\n",
      "Current Epsilon: 0.5425201222922789\n",
      "Episode 222: Won\n",
      "Episode 222, Avg Value Loss: 3.5714859870763926, Avg Policy Loss: 2.8272931575775146\n",
      "Episode 222, Reward: -34.581718896976724, Moving Avg Reward: -28.021901628091538, Replay Buffer Size: 8887\n",
      "Current Epsilon: 0.5398075216808175\n",
      "Loss rate over the last 111 episodes: 0.59\n",
      "Episode 222, Reward: -34.581718896976724, Moving Avg Reward: -28.021901628091538, Replay Buffer Size: 8887\n",
      "Current Epsilon: 0.5398075216808175\n",
      "Episode 223: Won\n",
      "Episode 223, Avg Value Loss: 3.8409776240587234, Avg Policy Loss: 2.803701728582382\n",
      "Episode 223, Reward: -25.606064942329354, Moving Avg Reward: -28.417953343190728, Replay Buffer Size: 8895\n",
      "Current Epsilon: 0.5371084840724134\n",
      "Loss rate over the last 111 episodes: 0.59\n",
      "Episode 223, Reward: -25.606064942329354, Moving Avg Reward: -28.417953343190728, Replay Buffer Size: 8895\n",
      "Current Epsilon: 0.5371084840724134\n",
      "Episode 224: Won\n",
      "Episode 224, Avg Value Loss: 3.6346026442945005, Avg Policy Loss: 2.8548358559608458\n",
      "Episode 224, Reward: -79.72153104031902, Moving Avg Reward: -28.94866680621845, Replay Buffer Size: 8975\n",
      "Current Epsilon: 0.5344229416520513\n",
      "Loss rate over the last 112 episodes: 0.59\n",
      "Episode 224, Reward: -79.72153104031902, Moving Avg Reward: -28.94866680621845, Replay Buffer Size: 8975\n",
      "Current Epsilon: 0.5344229416520513\n",
      "Episode 225: Lost\n",
      "Episode 225, Avg Value Loss: 3.776778120260972, Avg Policy Loss: 2.7494449065281796\n",
      "Episode 225, Reward: -36.01663283348154, Moving Avg Reward: -29.075652267249904, Replay Buffer Size: 8988\n",
      "Current Epsilon: 0.531750826943791\n",
      "Loss rate over the last 112 episodes: 0.60\n",
      "Episode 225, Reward: -36.01663283348154, Moving Avg Reward: -29.075652267249904, Replay Buffer Size: 8988\n",
      "Current Epsilon: 0.531750826943791\n",
      "Episode 226: Lost\n",
      "Episode 226, Avg Value Loss: 3.0322734779781766, Avg Policy Loss: 2.792453130086263\n",
      "Episode 226, Reward: -28.9732760442805, Moving Avg Reward: -29.247730559090133, Replay Buffer Size: 8997\n",
      "Current Epsilon: 0.5290920728090721\n",
      "Loss rate over the last 112 episodes: 0.60\n",
      "Episode 226, Reward: -28.9732760442805, Moving Avg Reward: -29.247730559090133, Replay Buffer Size: 8997\n",
      "Current Epsilon: 0.5290920728090721\n",
      "Episode 227: Lost\n",
      "Episode 227, Avg Value Loss: 3.34759676232934, Avg Policy Loss: 3.0153040319681166\n",
      "Episode 227, Reward: -16.32686574380518, Moving Avg Reward: -29.402999216528183, Replay Buffer Size: 9077\n",
      "Current Epsilon: 0.5264466124450268\n",
      "Loss rate over the last 113 episodes: 0.59\n",
      "Episode 227, Reward: -16.32686574380518, Moving Avg Reward: -29.402999216528183, Replay Buffer Size: 9077\n",
      "Current Epsilon: 0.5264466124450268\n",
      "Episode 228: Lost\n",
      "Episode 228, Avg Value Loss: 3.0807585386519736, Avg Policy Loss: 2.850621761159694\n",
      "Episode 228, Reward: -47.688758639884064, Moving Avg Reward: -30.065038134180654, Replay Buffer Size: 9124\n",
      "Current Epsilon: 0.5238143793828016\n",
      "Loss rate over the last 114 episodes: 0.60\n",
      "Episode 228, Reward: -47.688758639884064, Moving Avg Reward: -30.065038134180654, Replay Buffer Size: 9124\n",
      "Current Epsilon: 0.5238143793828016\n",
      "Episode 229: Lost\n",
      "Episode 229, Avg Value Loss: 2.7666455677577426, Avg Policy Loss: 2.8902935641152516\n",
      "Episode 229, Reward: -21.586325484197182, Moving Avg Reward: -30.187900429484735, Replay Buffer Size: 9131\n",
      "Current Epsilon: 0.5211953074858876\n",
      "Loss rate over the last 114 episodes: 0.60\n",
      "Episode 229, Reward: -21.586325484197182, Moving Avg Reward: -30.187900429484735, Replay Buffer Size: 9131\n",
      "Current Epsilon: 0.5211953074858876\n",
      "Episode 230: Lost\n",
      "Episode 230, Avg Value Loss: 3.5662719309329987, Avg Policy Loss: 2.808012330532074\n",
      "Episode 230, Reward: -42.14426286460493, Moving Avg Reward: -30.284428456700148, Replay Buffer Size: 9191\n",
      "Current Epsilon: 0.5185893309484582\n",
      "Loss rate over the last 114 episodes: 0.59\n",
      "Episode 230, Reward: -42.14426286460493, Moving Avg Reward: -30.284428456700148, Replay Buffer Size: 9191\n",
      "Current Epsilon: 0.5185893309484582\n",
      "Episode 231: Won\n",
      "Episode 231, Avg Value Loss: 3.2633126616477965, Avg Policy Loss: 2.9941787719726562\n",
      "Episode 231, Reward: -32.70258313866271, Moving Avg Reward: -30.42687153089214, Replay Buffer Size: 9201\n",
      "Current Epsilon: 0.5159963842937159\n",
      "Loss rate over the last 115 episodes: 0.60\n",
      "Episode 231, Reward: -32.70258313866271, Moving Avg Reward: -30.42687153089214, Replay Buffer Size: 9201\n",
      "Current Epsilon: 0.5159963842937159\n",
      "Episode 232: Won\n",
      "Episode 232, Avg Value Loss: 3.2705690264701843, Avg Policy Loss: 3.031685491402944\n",
      "Episode 232, Reward: -33.56564036079742, Moving Avg Reward: -30.495793918264816, Replay Buffer Size: 9213\n",
      "Current Epsilon: 0.5134164023722473\n",
      "Loss rate over the last 115 episodes: 0.60\n",
      "Episode 232, Reward: -33.56564036079742, Moving Avg Reward: -30.495793918264816, Replay Buffer Size: 9213\n",
      "Current Epsilon: 0.5134164023722473\n",
      "Episode 233: Won\n",
      "Episode 233, Avg Value Loss: 3.7397598028182983, Avg Policy Loss: 2.925241913114275\n",
      "Episode 233, Reward: 17.197023518778277, Moving Avg Reward: -30.05993848005705, Replay Buffer Size: 9234\n",
      "Current Epsilon: 0.510849320360386\n",
      "Loss rate over the last 115 episodes: 0.60\n",
      "Episode 233, Reward: 17.197023518778277, Moving Avg Reward: -30.05993848005705, Replay Buffer Size: 9234\n",
      "Current Epsilon: 0.510849320360386\n",
      "Episode 234: Won\n",
      "Episode 234, Avg Value Loss: 4.044820745786031, Avg Policy Loss: 2.6408037741978965\n",
      "Episode 234, Reward: -27.299530241200376, Moving Avg Reward: -30.10132756137258, Replay Buffer Size: 9246\n",
      "Current Epsilon: 0.5082950737585841\n",
      "Loss rate over the last 115 episodes: 0.60\n",
      "Episode 234, Reward: -27.299530241200376, Moving Avg Reward: -30.10132756137258, Replay Buffer Size: 9246\n",
      "Current Epsilon: 0.5082950737585841\n",
      "Episode 235: Won\n",
      "Episode 235, Avg Value Loss: 4.30567854642868, Avg Policy Loss: 2.698518896102905\n",
      "Episode 235, Reward: 16.472384915127392, Moving Avg Reward: -30.042102537969246, Replay Buffer Size: 9266\n",
      "Current Epsilon: 0.5057535983897912\n",
      "Loss rate over the last 115 episodes: 0.60\n",
      "Episode 235, Reward: 16.472384915127392, Moving Avg Reward: -30.042102537969246, Replay Buffer Size: 9266\n",
      "Current Epsilon: 0.5057535983897912\n",
      "Episode 236: Won\n",
      "Episode 236, Avg Value Loss: 3.3739975690841675, Avg Policy Loss: 3.008976839206837\n",
      "Episode 236, Reward: 15.77721120947154, Moving Avg Reward: -29.612078271588807, Replay Buffer Size: 9293\n",
      "Current Epsilon: 0.5032248303978422\n",
      "Loss rate over the last 116 episodes: 0.59\n",
      "Episode 236, Reward: 15.77721120947154, Moving Avg Reward: -29.612078271588807, Replay Buffer Size: 9293\n",
      "Current Epsilon: 0.5032248303978422\n",
      "Episode 237: Won\n",
      "Episode 237, Avg Value Loss: 2.9235159158706665, Avg Policy Loss: 3.0076909959316254\n",
      "Episode 237, Reward: -27.152083283996262, Moving Avg Reward: -30.048655941618627, Replay Buffer Size: 9301\n",
      "Current Epsilon: 0.500708706245853\n",
      "Loss rate over the last 116 episodes: 0.60\n",
      "Episode 237, Reward: -27.152083283996262, Moving Avg Reward: -30.048655941618627, Replay Buffer Size: 9301\n",
      "Current Epsilon: 0.500708706245853\n",
      "Episode 238: Won\n",
      "Episode 238, Avg Value Loss: 3.8311599933582805, Avg Policy Loss: 2.8935277824816494\n",
      "Episode 238, Reward: -40.39825348794031, Moving Avg Reward: -29.960289659241838, Replay Buffer Size: 9347\n",
      "Current Epsilon: 0.4982051627146237\n",
      "Loss rate over the last 116 episodes: 0.60\n",
      "Episode 238, Reward: -40.39825348794031, Moving Avg Reward: -29.960289659241838, Replay Buffer Size: 9347\n",
      "Current Epsilon: 0.4982051627146237\n",
      "Episode 239: Won\n",
      "Episode 239, Avg Value Loss: 4.37079576083592, Avg Policy Loss: 2.8206094673701694\n",
      "Episode 239, Reward: -22.608988714558073, Moving Avg Reward: -29.872563164810803, Replay Buffer Size: 9354\n",
      "Current Epsilon: 0.49571413690105054\n",
      "Loss rate over the last 116 episodes: 0.60\n",
      "Episode 239, Reward: -22.608988714558073, Moving Avg Reward: -29.872563164810803, Replay Buffer Size: 9354\n",
      "Current Epsilon: 0.49571413690105054\n",
      "Episode 240: Won\n",
      "Episode 240, Avg Value Loss: 2.9905212372541428, Avg Policy Loss: 2.8648831844329834\n",
      "Episode 240, Reward: -23.901469908499152, Moving Avg Reward: -29.83670955027777, Replay Buffer Size: 9362\n",
      "Current Epsilon: 0.4932355662165453\n",
      "Loss rate over the last 117 episodes: 0.61\n",
      "Episode 240, Reward: -23.901469908499152, Moving Avg Reward: -29.83670955027777, Replay Buffer Size: 9362\n",
      "Current Epsilon: 0.4932355662165453\n",
      "Episode 241: Won\n",
      "Episode 241, Avg Value Loss: 3.1508031541650947, Avg Policy Loss: 3.0818063129078257\n",
      "Episode 241, Reward: -26.208611606709002, Moving Avg Reward: -29.627115373613336, Replay Buffer Size: 9373\n",
      "Current Epsilon: 0.4907693883854626\n",
      "Loss rate over the last 117 episodes: 0.61\n",
      "Episode 241, Reward: -26.208611606709002, Moving Avg Reward: -29.627115373613336, Replay Buffer Size: 9373\n",
      "Current Epsilon: 0.4907693883854626\n",
      "Episode 242: Won\n",
      "Episode 242, Avg Value Loss: 3.8056505067007884, Avg Policy Loss: 2.766100917543684\n",
      "Episode 242, Reward: -23.339062441443215, Moving Avg Reward: -29.655843167917066, Replay Buffer Size: 9380\n",
      "Current Epsilon: 0.4883155414435353\n",
      "Loss rate over the last 117 episodes: 0.61\n",
      "Episode 242, Reward: -23.339062441443215, Moving Avg Reward: -29.655843167917066, Replay Buffer Size: 9380\n",
      "Current Epsilon: 0.4883155414435353\n",
      "Episode 243: Won\n",
      "Episode 243, Avg Value Loss: 3.341834200753106, Avg Policy Loss: 3.028034289677938\n",
      "Episode 243, Reward: -24.29961488421292, Moving Avg Reward: -29.71397703653689, Replay Buffer Size: 9389\n",
      "Current Epsilon: 0.4858739637363176\n",
      "Loss rate over the last 117 episodes: 0.60\n",
      "Episode 243, Reward: -24.29961488421292, Moving Avg Reward: -29.71397703653689, Replay Buffer Size: 9389\n",
      "Current Epsilon: 0.4858739637363176\n",
      "Episode 244: Won\n",
      "Episode 244, Avg Value Loss: 3.191005335913764, Avg Policy Loss: 3.0348764613822654\n",
      "Episode 244, Reward: -44.67599215615495, Moving Avg Reward: -29.900035321050744, Replay Buffer Size: 9416\n",
      "Current Epsilon: 0.483444593917636\n",
      "Loss rate over the last 117 episodes: 0.60\n",
      "Episode 244, Reward: -44.67599215615495, Moving Avg Reward: -29.900035321050744, Replay Buffer Size: 9416\n",
      "Current Epsilon: 0.483444593917636\n",
      "Episode 245: Won\n",
      "Episode 245, Avg Value Loss: 3.663690587748652, Avg Policy Loss: 2.9161645744157876\n",
      "Episode 245, Reward: -66.76278276924135, Moving Avg Reward: -30.34911885983022, Replay Buffer Size: 9462\n",
      "Current Epsilon: 0.4810273709480478\n",
      "Loss rate over the last 118 episodes: 0.61\n",
      "Episode 245, Reward: -66.76278276924135, Moving Avg Reward: -30.34911885983022, Replay Buffer Size: 9462\n",
      "Current Epsilon: 0.4810273709480478\n",
      "Episode 246: Lost\n",
      "Episode 246, Avg Value Loss: 3.001341744473106, Avg Policy Loss: 3.11248967522069\n",
      "Episode 246, Reward: 18.070732139155787, Moving Avg Reward: -29.641427627234897, Replay Buffer Size: 9481\n",
      "Current Epsilon: 0.47862223409330756\n",
      "Loss rate over the last 118 episodes: 0.61\n",
      "Episode 246, Reward: 18.070732139155787, Moving Avg Reward: -29.641427627234897, Replay Buffer Size: 9481\n",
      "Current Epsilon: 0.47862223409330756\n",
      "Episode 247: Won\n",
      "Episode 247, Avg Value Loss: 3.6862905025482178, Avg Policy Loss: 3.0833631583622525\n",
      "Episode 247, Reward: -24.044120335034883, Moving Avg Reward: -29.409320622047662, Replay Buffer Size: 9488\n",
      "Current Epsilon: 0.47622912292284103\n",
      "Loss rate over the last 118 episodes: 0.61\n",
      "Episode 247, Reward: -24.044120335034883, Moving Avg Reward: -29.409320622047662, Replay Buffer Size: 9488\n",
      "Current Epsilon: 0.47622912292284103\n",
      "Episode 248: Won\n",
      "Episode 248, Avg Value Loss: 4.469289806154039, Avg Policy Loss: 2.643609126408895\n",
      "Episode 248, Reward: -28.055469658983363, Moving Avg Reward: -28.917895667215383, Replay Buffer Size: 9497\n",
      "Current Epsilon: 0.4738479773082268\n",
      "Loss rate over the last 118 episodes: 0.61\n",
      "Episode 248, Reward: -28.055469658983363, Moving Avg Reward: -28.917895667215383, Replay Buffer Size: 9497\n",
      "Current Epsilon: 0.4738479773082268\n",
      "Episode 249: Won\n",
      "Episode 249, Avg Value Loss: 3.0560134291648864, Avg Policy Loss: 2.9836426734924317\n",
      "Episode 249, Reward: -27.02792785784647, Moving Avg Reward: -28.672609341399905, Replay Buffer Size: 9507\n",
      "Current Epsilon: 0.47147873742168567\n",
      "Loss rate over the last 118 episodes: 0.61\n",
      "Episode 249, Reward: -27.02792785784647, Moving Avg Reward: -28.672609341399905, Replay Buffer Size: 9507\n",
      "Current Epsilon: 0.47147873742168567\n",
      "Episode 250: Won\n",
      "Episode 250, Avg Value Loss: 3.3726685597346377, Avg Policy Loss: 2.972977876663208\n",
      "Episode 250, Reward: -28.99119531774429, Moving Avg Reward: -28.403149881284854, Replay Buffer Size: 9520\n",
      "Current Epsilon: 0.46912134373457726\n",
      "Loss rate over the last 119 episodes: 0.61\n",
      "Episode 250, Reward: -28.99119531774429, Moving Avg Reward: -28.403149881284854, Replay Buffer Size: 9520\n",
      "Current Epsilon: 0.46912134373457726\n",
      "Episode 251: Won\n",
      "Episode 251, Avg Value Loss: 3.1891511671245096, Avg Policy Loss: 3.015837147831917\n",
      "Episode 251, Reward: -53.97636076112254, Moving Avg Reward: -29.123798243230297, Replay Buffer Size: 9600\n",
      "Current Epsilon: 0.46677573701590436\n",
      "Loss rate over the last 120 episodes: 0.60\n",
      "Episode 251, Reward: -53.97636076112254, Moving Avg Reward: -29.123798243230297, Replay Buffer Size: 9600\n",
      "Current Epsilon: 0.46677573701590436\n",
      "Episode 252: Won\n",
      "Episode 252, Avg Value Loss: 3.239308935403824, Avg Policy Loss: 3.007851576805115\n",
      "Episode 252, Reward: -25.733764051354115, Moving Avg Reward: -29.348351963677793, Replay Buffer Size: 9610\n",
      "Current Epsilon: 0.46444185833082485\n",
      "Loss rate over the last 120 episodes: 0.61\n",
      "Episode 252, Reward: -25.733764051354115, Moving Avg Reward: -29.348351963677793, Replay Buffer Size: 9610\n",
      "Current Epsilon: 0.46444185833082485\n",
      "Episode 253: Won\n",
      "Episode 253, Avg Value Loss: 3.2092127203941345, Avg Policy Loss: 3.2130877839194403\n",
      "Episode 253, Reward: 18.26031759494801, Moving Avg Reward: -28.758649553618415, Replay Buffer Size: 9628\n",
      "Current Epsilon: 0.46211964903917074\n",
      "Loss rate over the last 120 episodes: 0.61\n",
      "Episode 253, Reward: 18.26031759494801, Moving Avg Reward: -28.758649553618415, Replay Buffer Size: 9628\n",
      "Current Epsilon: 0.46211964903917074\n",
      "Episode 254: Won\n",
      "Episode 254, Avg Value Loss: 3.1290190390178134, Avg Policy Loss: 3.0133675507136752\n",
      "Episode 254, Reward: -21.007789396957605, Moving Avg Reward: -28.771068440687497, Replay Buffer Size: 9635\n",
      "Current Epsilon: 0.4598090507939749\n",
      "Loss rate over the last 120 episodes: 0.61\n",
      "Episode 254, Reward: -21.007789396957605, Moving Avg Reward: -28.771068440687497, Replay Buffer Size: 9635\n",
      "Current Epsilon: 0.4598090507939749\n",
      "Episode 255: Won\n",
      "Episode 255, Avg Value Loss: 3.5056850016117096, Avg Policy Loss: 2.844216912984848\n",
      "Episode 255, Reward: 9.92, Moving Avg Reward: -28.670564897656895, Replay Buffer Size: 9643\n",
      "Current Epsilon: 0.457510005540005\n",
      "Loss rate over the last 120 episodes: 0.61\n",
      "Episode 255, Reward: 9.92, Moving Avg Reward: -28.670564897656895, Replay Buffer Size: 9643\n",
      "Current Epsilon: 0.457510005540005\n",
      "Episode 256: Won\n",
      "Episode 256, Avg Value Loss: 3.71619433760643, Avg Policy Loss: 3.026738753914833\n",
      "Episode 256, Reward: -78.35051304405323, Moving Avg Reward: -29.16044358782045, Replay Buffer Size: 9723\n",
      "Current Epsilon: 0.45522245551230495\n",
      "Loss rate over the last 121 episodes: 0.60\n",
      "Episode 256, Reward: -78.35051304405323, Moving Avg Reward: -29.16044358782045, Replay Buffer Size: 9723\n",
      "Current Epsilon: 0.45522245551230495\n",
      "Episode 257: Won\n",
      "Episode 257, Avg Value Loss: 3.613258567349664, Avg Policy Loss: 2.8685850110547295\n",
      "Episode 257, Reward: 16.549878235623446, Moving Avg Reward: -28.75954955255393, Replay Buffer Size: 9752\n",
      "Current Epsilon: 0.4529463432347434\n",
      "Loss rate over the last 121 episodes: 0.60\n",
      "Episode 257, Reward: 16.549878235623446, Moving Avg Reward: -28.75954955255393, Replay Buffer Size: 9752\n",
      "Current Epsilon: 0.4529463432347434\n",
      "Episode 258: Won\n",
      "Episode 258, Avg Value Loss: 3.626001188158989, Avg Policy Loss: 2.9939479500055315\n",
      "Episode 258, Reward: -82.19380855111801, Moving Avg Reward: -29.34707088969834, Replay Buffer Size: 9832\n",
      "Current Epsilon: 0.4506816115185697\n",
      "Loss rate over the last 122 episodes: 0.59\n",
      "Episode 258, Reward: -82.19380855111801, Moving Avg Reward: -29.34707088969834, Replay Buffer Size: 9832\n",
      "Current Epsilon: 0.4506816115185697\n",
      "Episode 259: Won\n",
      "Episode 259, Avg Value Loss: 2.9563655853271484, Avg Policy Loss: 2.920348515877357\n",
      "Episode 259, Reward: -29.72243503114384, Moving Avg Reward: -29.36188454583647, Replay Buffer Size: 9845\n",
      "Current Epsilon: 0.4484282034609769\n",
      "Loss rate over the last 123 episodes: 0.60\n",
      "Episode 259, Reward: -29.72243503114384, Moving Avg Reward: -29.36188454583647, Replay Buffer Size: 9845\n",
      "Current Epsilon: 0.4484282034609769\n",
      "Episode 260: Won\n",
      "Episode 260, Avg Value Loss: 3.532602018001033, Avg Policy Loss: 3.057515228495878\n",
      "Episode 260, Reward: -36.175865310540196, Moving Avg Reward: -29.901914239583967, Replay Buffer Size: 9896\n",
      "Current Epsilon: 0.446186062443672\n",
      "Loss rate over the last 123 episodes: 0.60\n",
      "Episode 260, Reward: -36.175865310540196, Moving Avg Reward: -29.901914239583967, Replay Buffer Size: 9896\n",
      "Current Epsilon: 0.446186062443672\n",
      "Episode 261: Won\n",
      "Episode 261, Avg Value Loss: 3.771195424290804, Avg Policy Loss: 2.9828012448090773\n",
      "Episode 261, Reward: -41.52434388607388, Moving Avg Reward: -30.057626219896395, Replay Buffer Size: 9948\n",
      "Current Epsilon: 0.4439551321314536\n",
      "Loss rate over the last 124 episodes: 0.60\n",
      "Episode 261, Reward: -41.52434388607388, Moving Avg Reward: -30.057626219896395, Replay Buffer Size: 9948\n",
      "Current Epsilon: 0.4439551321314536\n",
      "Episode 262: Lost\n",
      "Episode 262, Avg Value Loss: 3.4085869789123535, Avg Policy Loss: 3.2164962821536593\n",
      "Episode 262, Reward: -27.291992975900076, Moving Avg Reward: -30.086017666547708, Replay Buffer Size: 9957\n",
      "Current Epsilon: 0.4417353564707963\n",
      "Loss rate over the last 124 episodes: 0.60\n",
      "Episode 262, Reward: -27.291992975900076, Moving Avg Reward: -30.086017666547708, Replay Buffer Size: 9957\n",
      "Current Epsilon: 0.4417353564707963\n",
      "Episode 263: Lost\n",
      "Episode 263, Avg Value Loss: 3.9263640897614613, Avg Policy Loss: 2.9927012579781667\n",
      "Episode 263, Reward: -30.386645762408534, Moving Avg Reward: -30.087224999948273, Replay Buffer Size: 9971\n",
      "Current Epsilon: 0.43952667968844233\n",
      "Loss rate over the last 124 episodes: 0.60\n",
      "Episode 263, Reward: -30.386645762408534, Moving Avg Reward: -30.087224999948273, Replay Buffer Size: 9971\n",
      "Current Epsilon: 0.43952667968844233\n",
      "Episode 264: Lost\n",
      "Episode 264, Avg Value Loss: 3.681244044005871, Avg Policy Loss: 2.9960898876190187\n",
      "Episode 264, Reward: -29.490668809435395, Moving Avg Reward: -30.141280210047377, Replay Buffer Size: 10051\n",
      "Current Epsilon: 0.43732904629000013\n",
      "Loss rate over the last 125 episodes: 0.60\n",
      "Episode 264, Reward: -29.490668809435395, Moving Avg Reward: -30.141280210047377, Replay Buffer Size: 10051\n",
      "Current Epsilon: 0.43732904629000013\n",
      "Episode 265: Lost\n",
      "Episode 265, Avg Value Loss: 3.665355627353375, Avg Policy Loss: 2.981463780769935\n",
      "Episode 265, Reward: -32.01450291486796, Moving Avg Reward: -30.1947768796746, Replay Buffer Size: 10064\n",
      "Current Epsilon: 0.4351424010585501\n",
      "Loss rate over the last 125 episodes: 0.60\n",
      "Episode 265, Reward: -32.01450291486796, Moving Avg Reward: -30.1947768796746, Replay Buffer Size: 10064\n",
      "Current Epsilon: 0.4351424010585501\n",
      "Episode 266: Lost\n",
      "Episode 266, Avg Value Loss: 4.538115903735161, Avg Policy Loss: 2.584670126438141\n",
      "Episode 266, Reward: -27.254728095590114, Moving Avg Reward: -30.235130495659583, Replay Buffer Size: 10072\n",
      "Current Epsilon: 0.43296668905325736\n",
      "Loss rate over the last 125 episodes: 0.60\n",
      "Episode 266, Reward: -27.254728095590114, Moving Avg Reward: -30.235130495659583, Replay Buffer Size: 10072\n",
      "Current Epsilon: 0.43296668905325736\n",
      "Episode 267: Lost\n",
      "Episode 267, Avg Value Loss: 3.667779838166586, Avg Policy Loss: 3.0305392335100874\n",
      "Episode 267, Reward: -31.97967586030552, Moving Avg Reward: -30.46837806717089, Replay Buffer Size: 10113\n",
      "Current Epsilon: 0.43080185560799106\n",
      "Loss rate over the last 126 episodes: 0.61\n",
      "Episode 267, Reward: -31.97967586030552, Moving Avg Reward: -30.46837806717089, Replay Buffer Size: 10113\n",
      "Current Epsilon: 0.43080185560799106\n",
      "Episode 268: Lost\n",
      "Episode 268, Avg Value Loss: 3.8153003260493277, Avg Policy Loss: 2.986812284588814\n",
      "Episode 268, Reward: -75.2981247811298, Moving Avg Reward: -30.052078479868545, Replay Buffer Size: 10193\n",
      "Current Epsilon: 0.4286478463299511\n",
      "Loss rate over the last 127 episodes: 0.60\n",
      "Episode 268, Reward: -75.2981247811298, Moving Avg Reward: -30.052078479868545, Replay Buffer Size: 10193\n",
      "Current Epsilon: 0.4286478463299511\n",
      "Episode 269: Lost\n",
      "Episode 269, Avg Value Loss: 3.6556607607083444, Avg Policy Loss: 3.013965050379435\n",
      "Episode 269, Reward: -145.36853601892273, Moving Avg Reward: -31.22492870899654, Replay Buffer Size: 10271\n",
      "Current Epsilon: 0.42650460709830135\n",
      "Loss rate over the last 128 episodes: 0.61\n",
      "Episode 269, Reward: -145.36853601892273, Moving Avg Reward: -31.22492870899654, Replay Buffer Size: 10271\n",
      "Current Epsilon: 0.42650460709830135\n",
      "Episode 270: Lost\n",
      "Episode 270, Avg Value Loss: 3.6811020832795363, Avg Policy Loss: 2.95102150623615\n",
      "Episode 270, Reward: -35.22895745020013, Moving Avg Reward: -31.704485054426332, Replay Buffer Size: 10284\n",
      "Current Epsilon: 0.42437208406280985\n",
      "Loss rate over the last 128 episodes: 0.61\n",
      "Episode 270, Reward: -35.22895745020013, Moving Avg Reward: -31.704485054426332, Replay Buffer Size: 10284\n",
      "Current Epsilon: 0.42437208406280985\n",
      "Episode 271: Lost\n",
      "Episode 271, Avg Value Loss: 3.493691609455989, Avg Policy Loss: 2.8887232083540697\n",
      "Episode 271, Reward: -36.00287581499071, Moving Avg Reward: -31.699482256500538, Replay Buffer Size: 10297\n",
      "Current Epsilon: 0.4222502236424958\n",
      "Loss rate over the last 128 episodes: 0.61\n",
      "Episode 271, Reward: -36.00287581499071, Moving Avg Reward: -31.699482256500538, Replay Buffer Size: 10297\n",
      "Current Epsilon: 0.4222502236424958\n",
      "Episode 272: Lost\n",
      "Episode 272, Avg Value Loss: 3.4921993911266327, Avg Policy Loss: 3.41596382856369\n",
      "Episode 272, Reward: -26.53127883698548, Moving Avg Reward: -31.790160637191534, Replay Buffer Size: 10305\n",
      "Current Epsilon: 0.42013897252428334\n",
      "Loss rate over the last 128 episodes: 0.61\n",
      "Episode 272, Reward: -26.53127883698548, Moving Avg Reward: -31.790160637191534, Replay Buffer Size: 10305\n",
      "Current Epsilon: 0.42013897252428334\n",
      "Episode 273: Lost\n",
      "Episode 273, Avg Value Loss: 3.6451126635074615, Avg Policy Loss: 3.100564738114675\n",
      "Episode 273, Reward: -29.0791338205945, Moving Avg Reward: -30.615437991127095, Replay Buffer Size: 10317\n",
      "Current Epsilon: 0.4180382776616619\n",
      "Loss rate over the last 128 episodes: 0.60\n",
      "Episode 273, Reward: -29.0791338205945, Moving Avg Reward: -30.615437991127095, Replay Buffer Size: 10317\n",
      "Current Epsilon: 0.4180382776616619\n",
      "Episode 274: Lost\n",
      "Episode 274, Avg Value Loss: 3.2401439348856607, Avg Policy Loss: 2.934808929761251\n",
      "Episode 274, Reward: -28.96279596728261, Moving Avg Reward: -30.600876118751298, Replay Buffer Size: 10329\n",
      "Current Epsilon: 0.4159480862733536\n",
      "Loss rate over the last 129 episodes: 0.61\n",
      "Episode 274, Reward: -28.96279596728261, Moving Avg Reward: -30.600876118751298, Replay Buffer Size: 10329\n",
      "Current Epsilon: 0.4159480862733536\n",
      "Episode 275: Lost\n",
      "Episode 275, Avg Value Loss: 3.9508525491692126, Avg Policy Loss: 3.049489703029394\n",
      "Episode 275, Reward: -42.61379185176889, Moving Avg Reward: -30.816533356916953, Replay Buffer Size: 10393\n",
      "Current Epsilon: 0.41386834584198684\n",
      "Loss rate over the last 129 episodes: 0.61\n",
      "Episode 275, Reward: -42.61379185176889, Moving Avg Reward: -30.816533356916953, Replay Buffer Size: 10393\n",
      "Current Epsilon: 0.41386834584198684\n",
      "Episode 276: Lost\n",
      "Episode 276, Avg Value Loss: 3.9002899112908738, Avg Policy Loss: 3.104009529818659\n",
      "Episode 276, Reward: -78.00377462173728, Moving Avg Reward: -31.766255654654618, Replay Buffer Size: 10439\n",
      "Current Epsilon: 0.4117990041127769\n",
      "Loss rate over the last 130 episodes: 0.61\n",
      "Episode 276, Reward: -78.00377462173728, Moving Avg Reward: -31.766255654654618, Replay Buffer Size: 10439\n",
      "Current Epsilon: 0.4117990041127769\n",
      "Episode 277: Lost\n",
      "Episode 277, Avg Value Loss: 3.193198334087025, Avg Policy Loss: 3.0725711692463267\n",
      "Episode 277, Reward: -32.60216693165839, Moving Avg Reward: -31.055711935785066, Replay Buffer Size: 10450\n",
      "Current Epsilon: 0.40974000909221303\n",
      "Loss rate over the last 130 episodes: 0.61\n",
      "Episode 277, Reward: -32.60216693165839, Moving Avg Reward: -31.055711935785066, Replay Buffer Size: 10450\n",
      "Current Epsilon: 0.40974000909221303\n",
      "Episode 278: Lost\n",
      "Episode 278, Avg Value Loss: 3.3857866617349477, Avg Policy Loss: 3.306430908349844\n",
      "Episode 278, Reward: -30.617641768545624, Moving Avg Reward: -30.996145351554006, Replay Buffer Size: 10463\n",
      "Current Epsilon: 0.40769130904675194\n",
      "Loss rate over the last 130 episodes: 0.61\n",
      "Episode 278, Reward: -30.617641768545624, Moving Avg Reward: -30.996145351554006, Replay Buffer Size: 10463\n",
      "Current Epsilon: 0.40769130904675194\n",
      "Episode 279: Lost\n",
      "Episode 279, Avg Value Loss: 4.223678933249579, Avg Policy Loss: 2.7302865187327066\n",
      "Episode 279, Reward: -30.139760854729907, Moving Avg Reward: -30.669380436328048, Replay Buffer Size: 10472\n",
      "Current Epsilon: 0.40565285250151817\n",
      "Loss rate over the last 130 episodes: 0.61\n",
      "Episode 279, Reward: -30.139760854729907, Moving Avg Reward: -30.669380436328048, Replay Buffer Size: 10472\n",
      "Current Epsilon: 0.40565285250151817\n",
      "Episode 280: Lost\n",
      "Episode 280, Avg Value Loss: 3.98337721824646, Avg Policy Loss: 3.109052538871765\n",
      "Episode 280, Reward: -28.536078183130748, Moving Avg Reward: -30.51714006874287, Replay Buffer Size: 10484\n",
      "Current Epsilon: 0.4036245882390106\n",
      "Loss rate over the last 131 episodes: 0.62\n",
      "Episode 280, Reward: -28.536078183130748, Moving Avg Reward: -30.51714006874287, Replay Buffer Size: 10484\n",
      "Current Epsilon: 0.4036245882390106\n",
      "Episode 281: Lost\n",
      "Episode 281, Avg Value Loss: 3.584686502262398, Avg Policy Loss: 3.07801620165507\n",
      "Episode 281, Reward: 18.46165718769995, Moving Avg Reward: -30.462899508422556, Replay Buffer Size: 10511\n",
      "Current Epsilon: 0.4016064652978155\n",
      "Loss rate over the last 131 episodes: 0.62\n",
      "Episode 281, Reward: 18.46165718769995, Moving Avg Reward: -30.462899508422556, Replay Buffer Size: 10511\n",
      "Current Epsilon: 0.4016064652978155\n",
      "Episode 282: Won\n",
      "Episode 282, Avg Value Loss: 3.7341920867562295, Avg Policy Loss: 3.106380748748779\n",
      "Episode 282, Reward: -128.88293426862455, Moving Avg Reward: -31.51722532159039, Replay Buffer Size: 10591\n",
      "Current Epsilon: 0.3995984329713264\n",
      "Loss rate over the last 132 episodes: 0.61\n",
      "Episode 282, Reward: -128.88293426862455, Moving Avg Reward: -31.51722532159039, Replay Buffer Size: 10591\n",
      "Current Epsilon: 0.3995984329713264\n",
      "Episode 283: Won\n",
      "Episode 283, Avg Value Loss: 4.262210314090435, Avg Policy Loss: 3.035441490320059\n",
      "Episode 283, Reward: -28.45175577349957, Moving Avg Reward: -31.381653391414538, Replay Buffer Size: 10604\n",
      "Current Epsilon: 0.3976004408064698\n",
      "Loss rate over the last 132 episodes: 0.62\n",
      "Episode 283, Reward: -28.45175577349957, Moving Avg Reward: -31.381653391414538, Replay Buffer Size: 10604\n",
      "Current Epsilon: 0.3976004408064698\n",
      "Episode 284: Won\n",
      "Episode 284, Avg Value Loss: 3.674222681671381, Avg Policy Loss: 3.181254029273987\n",
      "Episode 284, Reward: -42.3305446152127, Moving Avg Reward: -31.55501885798796, Replay Buffer Size: 10684\n",
      "Current Epsilon: 0.39561243860243744\n",
      "Loss rate over the last 133 episodes: 0.61\n",
      "Episode 284, Reward: -42.3305446152127, Moving Avg Reward: -31.55501885798796, Replay Buffer Size: 10684\n",
      "Current Epsilon: 0.39561243860243744\n",
      "Episode 285: Lost\n",
      "Episode 285, Avg Value Loss: 3.6036708414554597, Avg Policy Loss: 3.1033313453197477\n",
      "Episode 285, Reward: 4.50529485260767, Moving Avg Reward: -31.19046160151186, Replay Buffer Size: 10764\n",
      "Current Epsilon: 0.3936343764094253\n",
      "Loss rate over the last 134 episodes: 0.61\n",
      "Episode 285, Reward: 4.50529485260767, Moving Avg Reward: -31.19046160151186, Replay Buffer Size: 10764\n",
      "Current Epsilon: 0.3936343764094253\n",
      "Episode 286: Draw\n",
      "Episode 286, Avg Value Loss: 4.085603574911754, Avg Policy Loss: 2.899367034435272\n",
      "Episode 286, Reward: -30.96213103143713, Moving Avg Reward: -31.048381428654924, Replay Buffer Size: 10776\n",
      "Current Epsilon: 0.39166620452737816\n",
      "Loss rate over the last 134 episodes: 0.62\n",
      "Episode 286, Reward: -30.96213103143713, Moving Avg Reward: -31.048381428654924, Replay Buffer Size: 10776\n",
      "Current Epsilon: 0.39166620452737816\n",
      "Episode 287: Lost\n",
      "Episode 287, Avg Value Loss: 3.6001621223077542, Avg Policy Loss: 3.0888127059471318\n",
      "Episode 287, Reward: -57.763501984060255, Moving Avg Reward: -31.31171111261187, Replay Buffer Size: 10817\n",
      "Current Epsilon: 0.3897078735047413\n",
      "Loss rate over the last 135 episodes: 0.62\n",
      "Episode 287, Reward: -57.763501984060255, Moving Avg Reward: -31.31171111261187, Replay Buffer Size: 10817\n",
      "Current Epsilon: 0.3897078735047413\n",
      "Episode 288: Lost\n",
      "Episode 288, Avg Value Loss: 3.1926670487110433, Avg Policy Loss: 3.158009174542549\n",
      "Episode 288, Reward: -45.66429534469004, Moving Avg Reward: -31.58928880594903, Replay Buffer Size: 10856\n",
      "Current Epsilon: 0.3877593341372176\n",
      "Loss rate over the last 135 episodes: 0.62\n",
      "Episode 288, Reward: -45.66429534469004, Moving Avg Reward: -31.58928880594903, Replay Buffer Size: 10856\n",
      "Current Epsilon: 0.3877593341372176\n",
      "Episode 289: Lost\n",
      "Episode 289, Avg Value Loss: 3.4035912935550394, Avg Policy Loss: 3.2211966514587402\n",
      "Episode 289, Reward: -28.49659215711946, Moving Avg Reward: -31.620146981929743, Replay Buffer Size: 10869\n",
      "Current Epsilon: 0.3858205374665315\n",
      "Loss rate over the last 135 episodes: 0.62\n",
      "Episode 289, Reward: -28.49659215711946, Moving Avg Reward: -31.620146981929743, Replay Buffer Size: 10869\n",
      "Current Epsilon: 0.3858205374665315\n",
      "Episode 290: Lost\n",
      "Episode 290, Avg Value Loss: 3.7072396409021664, Avg Policy Loss: 3.122016028182147\n",
      "Episode 290, Reward: 13.914495213935634, Moving Avg Reward: -31.23015727681057, Replay Buffer Size: 10942\n",
      "Current Epsilon: 0.38389143477919885\n",
      "Loss rate over the last 136 episodes: 0.61\n",
      "Episode 290, Reward: 13.914495213935634, Moving Avg Reward: -31.23015727681057, Replay Buffer Size: 10942\n",
      "Current Epsilon: 0.38389143477919885\n",
      "Episode 291: Won\n",
      "Episode 291, Avg Value Loss: 3.165817052125931, Avg Policy Loss: 2.9390039841334024\n",
      "Episode 291, Reward: -32.51216938066521, Moving Avg Reward: -30.609958988024047, Replay Buffer Size: 10954\n",
      "Current Epsilon: 0.3819719776053028\n",
      "Loss rate over the last 136 episodes: 0.62\n",
      "Episode 291, Reward: -32.51216938066521, Moving Avg Reward: -30.609958988024047, Replay Buffer Size: 10954\n",
      "Current Epsilon: 0.3819719776053028\n",
      "Episode 292: Won\n",
      "Episode 292, Avg Value Loss: 3.6743838951505463, Avg Policy Loss: 3.1302067822423476\n",
      "Episode 292, Reward: 14.108806610661931, Moving Avg Reward: -30.572846037770514, Replay Buffer Size: 11012\n",
      "Current Epsilon: 0.3800621177172763\n",
      "Loss rate over the last 137 episodes: 0.62\n",
      "Episode 292, Reward: 14.108806610661931, Moving Avg Reward: -30.572846037770514, Replay Buffer Size: 11012\n",
      "Current Epsilon: 0.3800621177172763\n",
      "Episode 293: Won\n",
      "Episode 293, Avg Value Loss: 3.9152554181905894, Avg Policy Loss: 3.0811423521775465\n",
      "Episode 293, Reward: -32.20425008651928, Moving Avg Reward: -30.60585326211728, Replay Buffer Size: 11025\n",
      "Current Epsilon: 0.37816180712868996\n",
      "Loss rate over the last 137 episodes: 0.62\n",
      "Episode 293, Reward: -32.20425008651928, Moving Avg Reward: -30.60585326211728, Replay Buffer Size: 11025\n",
      "Current Epsilon: 0.37816180712868996\n",
      "Episode 294: Won\n",
      "Episode 294, Avg Value Loss: 3.7562409400939942, Avg Policy Loss: 3.1426952600479128\n",
      "Episode 294, Reward: -25.596414474453447, Moving Avg Reward: -31.014071683205998, Replay Buffer Size: 11035\n",
      "Current Epsilon: 0.37627099809304654\n",
      "Loss rate over the last 137 episodes: 0.62\n",
      "Episode 294, Reward: -25.596414474453447, Moving Avg Reward: -31.014071683205998, Replay Buffer Size: 11035\n",
      "Current Epsilon: 0.37627099809304654\n",
      "Episode 295: Won\n",
      "Episode 295, Avg Value Loss: 3.6659383073449137, Avg Policy Loss: 3.2136805415153504\n",
      "Episode 295, Reward: -11.398727058028248, Moving Avg Reward: -30.768952461824203, Replay Buffer Size: 11115\n",
      "Current Epsilon: 0.3743896431025813\n",
      "Loss rate over the last 138 episodes: 0.62\n",
      "Episode 295, Reward: -11.398727058028248, Moving Avg Reward: -30.768952461824203, Replay Buffer Size: 11115\n",
      "Current Epsilon: 0.3743896431025813\n",
      "Episode 296: Won\n",
      "Episode 296, Avg Value Loss: 3.7362997315824034, Avg Policy Loss: 3.0917396903038026\n",
      "Episode 296, Reward: -29.23664007755417, Moving Avg Reward: -30.817699303792082, Replay Buffer Size: 11195\n",
      "Current Epsilon: 0.37251769488706843\n",
      "Loss rate over the last 139 episodes: 0.61\n",
      "Episode 296, Reward: -29.23664007755417, Moving Avg Reward: -30.817699303792082, Replay Buffer Size: 11195\n",
      "Current Epsilon: 0.37251769488706843\n",
      "Episode 297: Draw\n",
      "Episode 297, Avg Value Loss: 3.517571695148945, Avg Policy Loss: 3.2123742789030074\n",
      "Episode 297, Reward: -30.324383091124446, Moving Avg Reward: -30.845628615077676, Replay Buffer Size: 11275\n",
      "Current Epsilon: 0.3706551064126331\n",
      "Loss rate over the last 140 episodes: 0.61\n",
      "Episode 297, Reward: -30.324383091124446, Moving Avg Reward: -30.845628615077676, Replay Buffer Size: 11275\n",
      "Current Epsilon: 0.3706551064126331\n",
      "Episode 298: Draw\n",
      "Episode 298, Avg Value Loss: 3.6592005491256714, Avg Policy Loss: 3.3005507209084253\n",
      "Episode 298, Reward: -30.840939325180898, Moving Avg Reward: -30.892212370186247, Replay Buffer Size: 11286\n",
      "Current Epsilon: 0.36880183088056995\n",
      "Loss rate over the last 141 episodes: 0.62\n",
      "Episode 298, Reward: -30.840939325180898, Moving Avg Reward: -30.892212370186247, Replay Buffer Size: 11286\n",
      "Current Epsilon: 0.36880183088056995\n",
      "Episode 299: Lost\n",
      "Episode 299, Avg Value Loss: 3.7055234644148083, Avg Policy Loss: 2.7657774554358587\n",
      "Episode 299, Reward: 9.91, Moving Avg Reward: -30.554135655500644, Replay Buffer Size: 11295\n",
      "Current Epsilon: 0.3669578217261671\n",
      "Loss rate over the last 141 episodes: 0.62\n",
      "Episode 299, Reward: 9.91, Moving Avg Reward: -30.554135655500644, Replay Buffer Size: 11295\n",
      "Current Epsilon: 0.3669578217261671\n",
      "Episode 300: Won\n",
      "Episode 300, Avg Value Loss: 3.36729274392128, Avg Policy Loss: 3.193846282362938\n",
      "Episode 300, Reward: -36.38088477405034, Moving Avg Reward: -30.371114833873147, Replay Buffer Size: 11375\n",
      "Current Epsilon: 0.36512303261753626\n",
      "Loss rate over the last 142 episodes: 0.61\n",
      "Episode 300, Reward: -36.38088477405034, Moving Avg Reward: -30.371114833873147, Replay Buffer Size: 11375\n",
      "Current Epsilon: 0.36512303261753626\n",
      "Episode 301: Won\n",
      "Episode 301, Avg Value Loss: 3.4229488149285316, Avg Policy Loss: 3.2114332169294357\n",
      "Episode 301, Reward: -62.19573788675434, Moving Avg Reward: -31.101015642349356, Replay Buffer Size: 11455\n",
      "Current Epsilon: 0.3632974174544486\n",
      "Loss rate over the last 143 episodes: 0.61\n",
      "Episode 301, Reward: -62.19573788675434, Moving Avg Reward: -31.101015642349356, Replay Buffer Size: 11455\n",
      "Current Epsilon: 0.3632974174544486\n",
      "Episode 302: Draw\n",
      "Episode 302, Avg Value Loss: 3.069607126407134, Avg Policy Loss: 3.1691262294084597\n",
      "Episode 302, Reward: 16.39188924826017, Moving Avg Reward: -30.535114062261563, Replay Buffer Size: 11494\n",
      "Current Epsilon: 0.3614809303671764\n",
      "Loss rate over the last 143 episodes: 0.61\n",
      "Episode 302, Reward: 16.39188924826017, Moving Avg Reward: -30.535114062261563, Replay Buffer Size: 11494\n",
      "Current Epsilon: 0.3614809303671764\n",
      "Episode 303: Won\n",
      "Episode 303, Avg Value Loss: 3.9605407153858856, Avg Policy Loss: 3.11926596772437\n",
      "Episode 303, Reward: 6.93822184695202, Moving Avg Reward: -30.184749923944846, Replay Buffer Size: 11545\n",
      "Current Epsilon: 0.3596735257153405\n",
      "Loss rate over the last 144 episodes: 0.60\n",
      "Episode 303, Reward: 6.93822184695202, Moving Avg Reward: -30.184749923944846, Replay Buffer Size: 11545\n",
      "Current Epsilon: 0.3596735257153405\n",
      "Episode 304: Won\n",
      "Episode 304, Avg Value Loss: 3.087969238941486, Avg Policy Loss: 3.058528716747577\n",
      "Episode 304, Reward: -24.756841525759206, Moving Avg Reward: -30.502422114131296, Replay Buffer Size: 11558\n",
      "Current Epsilon: 0.3578751580867638\n",
      "Loss rate over the last 144 episodes: 0.61\n",
      "Episode 304, Reward: -24.756841525759206, Moving Avg Reward: -30.502422114131296, Replay Buffer Size: 11558\n",
      "Current Epsilon: 0.3578751580867638\n",
      "Episode 305: Won\n",
      "Episode 305, Avg Value Loss: 3.877200126647949, Avg Policy Loss: 3.2695470406458926\n",
      "Episode 305, Reward: -34.678153604965246, Moving Avg Reward: -30.60571485252779, Replay Buffer Size: 11571\n",
      "Current Epsilon: 0.35608578229633\n",
      "Loss rate over the last 144 episodes: 0.61\n",
      "Episode 305, Reward: -34.678153604965246, Moving Avg Reward: -30.60571485252779, Replay Buffer Size: 11571\n",
      "Current Epsilon: 0.35608578229633\n",
      "Episode 306: Won\n",
      "Episode 306, Avg Value Loss: 3.8083068251609804, Avg Policy Loss: 3.1718854188919066\n",
      "Episode 306, Reward: -23.781501719954406, Moving Avg Reward: -30.448278176109856, Replay Buffer Size: 11581\n",
      "Current Epsilon: 0.3543053533848483\n",
      "Loss rate over the last 144 episodes: 0.61\n",
      "Episode 306, Reward: -23.781501719954406, Moving Avg Reward: -30.448278176109856, Replay Buffer Size: 11581\n",
      "Current Epsilon: 0.3543053533848483\n",
      "Episode 307: Won\n",
      "Episode 307, Avg Value Loss: 3.31932070851326, Avg Policy Loss: 3.442823847134908\n",
      "Episode 307, Reward: -26.369530859599152, Moving Avg Reward: -30.426810530598303, Replay Buffer Size: 11593\n",
      "Current Epsilon: 0.35253382661792404\n",
      "Loss rate over the last 144 episodes: 0.61\n",
      "Episode 307, Reward: -26.369530859599152, Moving Avg Reward: -30.426810530598303, Replay Buffer Size: 11593\n",
      "Current Epsilon: 0.35253382661792404\n",
      "Episode 308: Won\n",
      "Episode 308, Avg Value Loss: 3.6258143186569214, Avg Policy Loss: 3.1466093182563784\n",
      "Episode 308, Reward: -31.910645073127707, Moving Avg Reward: -30.411177807260675, Replay Buffer Size: 11673\n",
      "Current Epsilon: 0.3507711574848344\n",
      "Loss rate over the last 145 episodes: 0.60\n",
      "Episode 308, Reward: -31.910645073127707, Moving Avg Reward: -30.411177807260675, Replay Buffer Size: 11673\n",
      "Current Epsilon: 0.3507711574848344\n",
      "Episode 309: Won\n",
      "Episode 309, Avg Value Loss: 3.620587331056595, Avg Policy Loss: 3.221559301018715\n",
      "Episode 309, Reward: -93.77475689180937, Moving Avg Reward: -31.513142224419706, Replay Buffer Size: 11753\n",
      "Current Epsilon: 0.34901730169741024\n",
      "Loss rate over the last 146 episodes: 0.60\n",
      "Episode 309, Reward: -93.77475689180937, Moving Avg Reward: -31.513142224419706, Replay Buffer Size: 11753\n",
      "Current Epsilon: 0.34901730169741024\n",
      "Episode 310: Draw\n",
      "Episode 310, Avg Value Loss: 3.615475806593895, Avg Policy Loss: 3.1770662754774093\n",
      "Episode 310, Reward: 3.44138756436322, Moving Avg Reward: -31.278035968988277, Replay Buffer Size: 11833\n",
      "Current Epsilon: 0.3472722151889232\n",
      "Loss rate over the last 147 episodes: 0.59\n",
      "Episode 310, Reward: 3.44138756436322, Moving Avg Reward: -31.278035968988277, Replay Buffer Size: 11833\n",
      "Current Epsilon: 0.3472722151889232\n",
      "Episode 311: Draw\n",
      "Episode 311, Avg Value Loss: 3.216082453727722, Avg Policy Loss: 3.2439989513821073\n",
      "Episode 311, Reward: -25.534235243979268, Moving Avg Reward: -30.60799547039687, Replay Buffer Size: 11842\n",
      "Current Epsilon: 0.3455358541129786\n",
      "Loss rate over the last 148 episodes: 0.60\n",
      "Episode 311, Reward: -25.534235243979268, Moving Avg Reward: -30.60799547039687, Replay Buffer Size: 11842\n",
      "Current Epsilon: 0.3455358541129786\n",
      "Episode 312: Lost\n",
      "Episode 312, Avg Value Loss: 3.6270137916911733, Avg Policy Loss: 3.3072228648445825\n",
      "Episode 312, Reward: -31.846181446057173, Moving Avg Reward: -30.6966652780642, Replay Buffer Size: 11853\n",
      "Current Epsilon: 0.3438081748424137\n",
      "Loss rate over the last 148 episodes: 0.60\n",
      "Episode 312, Reward: -31.846181446057173, Moving Avg Reward: -30.6966652780642, Replay Buffer Size: 11853\n",
      "Current Epsilon: 0.3438081748424137\n",
      "Episode 313: Lost\n",
      "Episode 313, Avg Value Loss: 3.805978608131409, Avg Policy Loss: 3.2320403099060058\n",
      "Episode 313, Reward: -28.100028851321596, Moving Avg Reward: -30.56461248659091, Replay Buffer Size: 11863\n",
      "Current Epsilon: 0.3420891339682016\n",
      "Loss rate over the last 148 episodes: 0.60\n",
      "Episode 313, Reward: -28.100028851321596, Moving Avg Reward: -30.56461248659091, Replay Buffer Size: 11863\n",
      "Current Epsilon: 0.3420891339682016\n",
      "Episode 314: Lost\n",
      "Episode 314, Avg Value Loss: 3.672724829779731, Avg Policy Loss: 3.250555091434055\n",
      "Episode 314, Reward: -23.097583980701614, Moving Avg Reward: -30.156437651638488, Replay Buffer Size: 11872\n",
      "Current Epsilon: 0.3403786882983606\n",
      "Loss rate over the last 148 episodes: 0.60\n",
      "Episode 314, Reward: -23.097583980701614, Moving Avg Reward: -30.156437651638488, Replay Buffer Size: 11872\n",
      "Current Epsilon: 0.3403786882983606\n",
      "Episode 315: Lost\n",
      "Episode 315, Avg Value Loss: 3.7515261292457582, Avg Policy Loss: 3.135043001174927\n",
      "Episode 315, Reward: -23.754418965393107, Moving Avg Reward: -30.034300609503667, Replay Buffer Size: 11882\n",
      "Current Epsilon: 0.3386767948568688\n",
      "Loss rate over the last 148 episodes: 0.60\n",
      "Episode 315, Reward: -23.754418965393107, Moving Avg Reward: -30.034300609503667, Replay Buffer Size: 11882\n",
      "Current Epsilon: 0.3386767948568688\n",
      "Episode 316: Lost\n",
      "Episode 316, Avg Value Loss: 3.831813017527262, Avg Policy Loss: 3.0044674343532987\n",
      "Episode 316, Reward: -28.92801251121506, Moving Avg Reward: -30.049210922850897, Replay Buffer Size: 11891\n",
      "Current Epsilon: 0.33698341088258443\n",
      "Loss rate over the last 148 episodes: 0.60\n",
      "Episode 316, Reward: -28.92801251121506, Moving Avg Reward: -30.049210922850897, Replay Buffer Size: 11891\n",
      "Current Epsilon: 0.33698341088258443\n",
      "Episode 317: Lost\n",
      "Episode 317, Avg Value Loss: 3.688360333442688, Avg Policy Loss: 3.067220376088069\n",
      "Episode 317, Reward: -28.726209090691917, Moving Avg Reward: -30.07982220779677, Replay Buffer Size: 11904\n",
      "Current Epsilon: 0.3352984938281715\n",
      "Loss rate over the last 148 episodes: 0.60\n",
      "Episode 317, Reward: -28.726209090691917, Moving Avg Reward: -30.07982220779677, Replay Buffer Size: 11904\n",
      "Current Epsilon: 0.3352984938281715\n",
      "Episode 318: Lost\n",
      "Episode 318, Avg Value Loss: 4.2167000869909925, Avg Policy Loss: 3.3295486172040305\n",
      "Episode 318, Reward: -28.413581172771664, Moving Avg Reward: -30.075743191769096, Replay Buffer Size: 11916\n",
      "Current Epsilon: 0.33362200135903064\n",
      "Loss rate over the last 148 episodes: 0.60\n",
      "Episode 318, Reward: -28.413581172771664, Moving Avg Reward: -30.075743191769096, Replay Buffer Size: 11916\n",
      "Current Epsilon: 0.33362200135903064\n",
      "Episode 319: Lost\n",
      "Episode 319, Avg Value Loss: 4.2248289585113525, Avg Policy Loss: 3.3115905012403215\n",
      "Episode 319, Reward: -20.307881623468035, Moving Avg Reward: -30.054380917092562, Replay Buffer Size: 11923\n",
      "Current Epsilon: 0.33195389135223546\n",
      "Loss rate over the last 149 episodes: 0.60\n",
      "Episode 319, Reward: -20.307881623468035, Moving Avg Reward: -30.054380917092562, Replay Buffer Size: 11923\n",
      "Current Epsilon: 0.33195389135223546\n",
      "Episode 320: Lost\n",
      "Episode 320, Avg Value Loss: 4.145956667986783, Avg Policy Loss: 3.238266186280684\n",
      "Episode 320, Reward: -28.335966157718588, Moving Avg Reward: -30.529431272194493, Replay Buffer Size: 11934\n",
      "Current Epsilon: 0.3302941218954743\n",
      "Loss rate over the last 149 episodes: 0.60\n",
      "Episode 320, Reward: -28.335966157718588, Moving Avg Reward: -30.529431272194493, Replay Buffer Size: 11934\n",
      "Current Epsilon: 0.3302941218954743\n",
      "Episode 321: Lost\n",
      "Episode 321, Avg Value Loss: 3.425483524799347, Avg Policy Loss: 3.4243319630622864\n",
      "Episode 321, Reward: -26.929793772646036, Moving Avg Reward: -29.82334697688019, Replay Buffer Size: 11946\n",
      "Current Epsilon: 0.32864265128599696\n",
      "Loss rate over the last 149 episodes: 0.60\n",
      "Episode 321, Reward: -26.929793772646036, Moving Avg Reward: -29.82334697688019, Replay Buffer Size: 11946\n",
      "Current Epsilon: 0.32864265128599696\n",
      "Episode 322: Lost\n",
      "Episode 322, Avg Value Loss: 4.9955949783325195, Avg Policy Loss: 3.1889614037105014\n",
      "Episode 322, Reward: -20.873821773282387, Moving Avg Reward: -29.68626800564325, Replay Buffer Size: 11953\n",
      "Current Epsilon: 0.326999438029567\n",
      "Loss rate over the last 149 episodes: 0.60\n",
      "Episode 322, Reward: -20.873821773282387, Moving Avg Reward: -29.68626800564325, Replay Buffer Size: 11953\n",
      "Current Epsilon: 0.326999438029567\n",
      "Episode 323: Lost\n",
      "Episode 323, Avg Value Loss: 3.672218307852745, Avg Policy Loss: 3.244910368323326\n",
      "Episode 323, Reward: -73.55790875198963, Moving Avg Reward: -30.165786443739854, Replay Buffer Size: 12033\n",
      "Current Epsilon: 0.3253644408394192\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 323, Reward: -73.55790875198963, Moving Avg Reward: -30.165786443739854, Replay Buffer Size: 12033\n",
      "Current Epsilon: 0.3253644408394192\n",
      "Episode 324: Lost\n",
      "Episode 324, Avg Value Loss: 3.617250108056598, Avg Policy Loss: 3.3106422622998557\n",
      "Episode 324, Reward: -34.486748403037275, Moving Avg Reward: -29.713438617367032, Replay Buffer Size: 12069\n",
      "Current Epsilon: 0.3237376186352221\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 324, Reward: -34.486748403037275, Moving Avg Reward: -29.713438617367032, Replay Buffer Size: 12069\n",
      "Current Epsilon: 0.3237376186352221\n",
      "Episode 325: Lost\n",
      "Episode 325, Avg Value Loss: 3.3100364930927753, Avg Policy Loss: 3.3135704308748246\n",
      "Episode 325, Reward: -84.24772411347556, Moving Avg Reward: -30.19574953016697, Replay Buffer Size: 12149\n",
      "Current Epsilon: 0.322118930542046\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 325, Reward: -84.24772411347556, Moving Avg Reward: -30.19574953016697, Replay Buffer Size: 12149\n",
      "Current Epsilon: 0.322118930542046\n",
      "Episode 326: Lost\n",
      "Episode 326, Avg Value Loss: 3.8740646705031394, Avg Policy Loss: 3.25225211083889\n",
      "Episode 326, Reward: -23.525611426855846, Moving Avg Reward: -30.141272883992727, Replay Buffer Size: 12229\n",
      "Current Epsilon: 0.32050833588933575\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 326, Reward: -23.525611426855846, Moving Avg Reward: -30.141272883992727, Replay Buffer Size: 12229\n",
      "Current Epsilon: 0.32050833588933575\n",
      "Episode 327: Draw\n",
      "Episode 327, Avg Value Loss: 3.352140086037772, Avg Policy Loss: 3.084707566670009\n",
      "Episode 327, Reward: -23.986329696246194, Moving Avg Reward: -30.21786752351714, Replay Buffer Size: 12236\n",
      "Current Epsilon: 0.31890579420988907\n",
      "Loss rate over the last 150 episodes: 0.60\n",
      "Episode 327, Reward: -23.986329696246194, Moving Avg Reward: -30.21786752351714, Replay Buffer Size: 12236\n",
      "Current Epsilon: 0.31890579420988907\n",
      "Episode 328: Lost\n",
      "Episode 328, Avg Value Loss: 3.655648572742939, Avg Policy Loss: 3.2579108655452726\n",
      "Episode 328, Reward: -0.01707565074950068, Moving Avg Reward: -29.74115069362579, Replay Buffer Size: 12316\n",
      "Current Epsilon: 0.3173112652388396\n",
      "Loss rate over the last 150 episodes: 0.60\n",
      "Episode 328, Reward: -0.01707565074950068, Moving Avg Reward: -29.74115069362579, Replay Buffer Size: 12316\n",
      "Current Epsilon: 0.3173112652388396\n",
      "Episode 329: Lost\n",
      "Episode 329, Avg Value Loss: 3.697514766738528, Avg Policy Loss: 3.3741624639147805\n",
      "Episode 329, Reward: -37.27649983687266, Moving Avg Reward: -29.898052437152547, Replay Buffer Size: 12358\n",
      "Current Epsilon: 0.3157247089126454\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 329, Reward: -37.27649983687266, Moving Avg Reward: -29.898052437152547, Replay Buffer Size: 12358\n",
      "Current Epsilon: 0.3157247089126454\n",
      "Episode 330: Lost\n",
      "Episode 330, Avg Value Loss: 3.7427563534842596, Avg Policy Loss: 3.2943563991122775\n",
      "Episode 330, Reward: -23.144876283065074, Moving Avg Reward: -29.708058571337148, Replay Buffer Size: 12367\n",
      "Current Epsilon: 0.3141460853680822\n",
      "Loss rate over the last 150 episodes: 0.60\n",
      "Episode 330, Reward: -23.144876283065074, Moving Avg Reward: -29.708058571337148, Replay Buffer Size: 12367\n",
      "Current Epsilon: 0.3141460853680822\n",
      "Episode 331: Lost\n",
      "Episode 331, Avg Value Loss: 3.403242680761549, Avg Policy Loss: 3.3646697203318277\n",
      "Episode 331, Reward: -20.94497538803041, Moving Avg Reward: -29.590482493830823, Replay Buffer Size: 12376\n",
      "Current Epsilon: 0.3125753549412418\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 331, Reward: -20.94497538803041, Moving Avg Reward: -29.590482493830823, Replay Buffer Size: 12376\n",
      "Current Epsilon: 0.3125753549412418\n",
      "Episode 332: Lost\n",
      "Episode 332, Avg Value Loss: 4.827481230099996, Avg Policy Loss: 3.618107636769613\n",
      "Episode 332, Reward: -19.983963861336164, Moving Avg Reward: -29.454665728836208, Replay Buffer Size: 12382\n",
      "Current Epsilon: 0.31101247816653554\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 332, Reward: -19.983963861336164, Moving Avg Reward: -29.454665728836208, Replay Buffer Size: 12382\n",
      "Current Epsilon: 0.31101247816653554\n",
      "Episode 333: Lost\n",
      "Episode 333, Avg Value Loss: 3.5790089139571557, Avg Policy Loss: 3.265508074026841\n",
      "Episode 333, Reward: 19.493534769982503, Moving Avg Reward: -29.431700616324164, Replay Buffer Size: 12408\n",
      "Current Epsilon: 0.30945741577570285\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 333, Reward: 19.493534769982503, Moving Avg Reward: -29.431700616324164, Replay Buffer Size: 12408\n",
      "Current Epsilon: 0.30945741577570285\n",
      "Episode 334: Won\n",
      "Episode 334, Avg Value Loss: 3.196490389960153, Avg Policy Loss: 3.2253953388759067\n",
      "Episode 334, Reward: -43.50092722755777, Moving Avg Reward: -29.593714586187744, Replay Buffer Size: 12422\n",
      "Current Epsilon: 0.3079101286968243\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 334, Reward: -43.50092722755777, Moving Avg Reward: -29.593714586187744, Replay Buffer Size: 12422\n",
      "Current Epsilon: 0.3079101286968243\n",
      "Episode 335: Won\n",
      "Episode 335, Avg Value Loss: 4.070766188881614, Avg Policy Loss: 3.24626088142395\n",
      "Episode 335, Reward: -26.696686814569098, Moving Avg Reward: -30.025405303484707, Replay Buffer Size: 12433\n",
      "Current Epsilon: 0.3063705780533402\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 335, Reward: -26.696686814569098, Moving Avg Reward: -30.025405303484707, Replay Buffer Size: 12433\n",
      "Current Epsilon: 0.3063705780533402\n",
      "Episode 336: Won\n",
      "Episode 336, Avg Value Loss: 2.515543262163798, Avg Policy Loss: 3.2241805924309626\n",
      "Episode 336, Reward: -25.97142847182932, Moving Avg Reward: -30.442891700297718, Replay Buffer Size: 12442\n",
      "Current Epsilon: 0.30483872516307353\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 336, Reward: -25.97142847182932, Moving Avg Reward: -30.442891700297718, Replay Buffer Size: 12442\n",
      "Current Epsilon: 0.30483872516307353\n",
      "Episode 337: Won\n",
      "Episode 337, Avg Value Loss: 3.6880995273590087, Avg Policy Loss: 3.5081711292266844\n",
      "Episode 337, Reward: -24.851112015955565, Moving Avg Reward: -30.419881987617313, Replay Buffer Size: 12452\n",
      "Current Epsilon: 0.3033145315372582\n",
      "Loss rate over the last 150 episodes: 0.60\n",
      "Episode 337, Reward: -24.851112015955565, Moving Avg Reward: -30.419881987617313, Replay Buffer Size: 12452\n",
      "Current Epsilon: 0.3033145315372582\n",
      "Episode 338: Won\n",
      "Episode 338, Avg Value Loss: 3.561572044342756, Avg Policy Loss: 3.3695489794015883\n",
      "Episode 338, Reward: 6.147156903338035, Moving Avg Reward: -29.95442788370453, Replay Buffer Size: 12532\n",
      "Current Epsilon: 0.3017979588795719\n",
      "Loss rate over the last 150 episodes: 0.60\n",
      "Episode 338, Reward: 6.147156903338035, Moving Avg Reward: -29.95442788370453, Replay Buffer Size: 12532\n",
      "Current Epsilon: 0.3017979588795719\n",
      "Episode 339: Won\n",
      "Episode 339, Avg Value Loss: 3.8265647128224374, Avg Policy Loss: 3.2254531264305113\n",
      "Episode 339, Reward: 2.7776718319820892, Moving Avg Reward: -29.700561278239125, Replay Buffer Size: 12612\n",
      "Current Epsilon: 0.30028896908517405\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 339, Reward: 2.7776718319820892, Moving Avg Reward: -29.700561278239125, Replay Buffer Size: 12612\n",
      "Current Epsilon: 0.30028896908517405\n",
      "Episode 340: Draw\n",
      "Episode 340, Avg Value Loss: 3.627728409237332, Avg Policy Loss: 3.411016093360053\n",
      "Episode 340, Reward: -27.91286498184598, Moving Avg Reward: -29.740675228972595, Replay Buffer Size: 12621\n",
      "Current Epsilon: 0.2987875242397482\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 340, Reward: -27.91286498184598, Moving Avg Reward: -29.740675228972595, Replay Buffer Size: 12621\n",
      "Current Epsilon: 0.2987875242397482\n",
      "Episode 341: Lost\n",
      "Episode 341, Avg Value Loss: 3.2959088946952195, Avg Policy Loss: 3.290659490178843\n",
      "Episode 341, Reward: -43.077500371294455, Moving Avg Reward: -29.90936411661845, Replay Buffer Size: 12682\n",
      "Current Epsilon: 0.29729358661854943\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 341, Reward: -43.077500371294455, Moving Avg Reward: -29.90936411661845, Replay Buffer Size: 12682\n",
      "Current Epsilon: 0.29729358661854943\n",
      "Episode 342: Lost\n",
      "Episode 342, Avg Value Loss: 3.794241968975511, Avg Policy Loss: 3.3073130152946293\n",
      "Episode 342, Reward: -32.842638793423596, Moving Avg Reward: -30.004399880138248, Replay Buffer Size: 12725\n",
      "Current Epsilon: 0.29580711868545667\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 342, Reward: -32.842638793423596, Moving Avg Reward: -30.004399880138248, Replay Buffer Size: 12725\n",
      "Current Epsilon: 0.29580711868545667\n",
      "Episode 343: Lost\n",
      "Episode 343, Avg Value Loss: 4.072264220979479, Avg Policy Loss: 3.184125714831882\n",
      "Episode 343, Reward: -22.12047174594806, Moving Avg Reward: -29.9826084487556, Replay Buffer Size: 12734\n",
      "Current Epsilon: 0.2943280830920294\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 343, Reward: -22.12047174594806, Moving Avg Reward: -29.9826084487556, Replay Buffer Size: 12734\n",
      "Current Epsilon: 0.2943280830920294\n",
      "Episode 344: Lost\n",
      "Episode 344, Avg Value Loss: 3.426552926748991, Avg Policy Loss: 3.3120748311281205\n",
      "Episode 344, Reward: -26.27805572537238, Moving Avg Reward: -29.798629084447775, Replay Buffer Size: 12814\n",
      "Current Epsilon: 0.29285644267656924\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 344, Reward: -26.27805572537238, Moving Avg Reward: -29.798629084447775, Replay Buffer Size: 12814\n",
      "Current Epsilon: 0.29285644267656924\n",
      "Episode 345: Lost\n",
      "Episode 345, Avg Value Loss: 3.719570001959801, Avg Policy Loss: 3.3093242317438127\n",
      "Episode 345, Reward: -33.50287267041159, Moving Avg Reward: -29.466029983459485, Replay Buffer Size: 12894\n",
      "Current Epsilon: 0.2913921604631864\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 345, Reward: -33.50287267041159, Moving Avg Reward: -29.466029983459485, Replay Buffer Size: 12894\n",
      "Current Epsilon: 0.2913921604631864\n",
      "Episode 346: Draw\n",
      "Episode 346, Avg Value Loss: 3.741412026541574, Avg Policy Loss: 3.3402625152042935\n",
      "Episode 346, Reward: 18.38714829491945, Moving Avg Reward: -29.462865821901843, Replay Buffer Size: 12915\n",
      "Current Epsilon: 0.28993519966087045\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 346, Reward: 18.38714829491945, Moving Avg Reward: -29.462865821901843, Replay Buffer Size: 12915\n",
      "Current Epsilon: 0.28993519966087045\n",
      "Episode 347: Won\n",
      "Episode 347, Avg Value Loss: 3.3294396102428436, Avg Policy Loss: 3.378164830803871\n",
      "Episode 347, Reward: -109.25662637305795, Moving Avg Reward: -30.31499088228207, Replay Buffer Size: 12995\n",
      "Current Epsilon: 0.2884855236625661\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 347, Reward: -109.25662637305795, Moving Avg Reward: -30.31499088228207, Replay Buffer Size: 12995\n",
      "Current Epsilon: 0.2884855236625661\n",
      "Episode 348: Won\n",
      "Episode 348, Avg Value Loss: 3.9772984172616686, Avg Policy Loss: 3.415034238781248\n",
      "Episode 348, Reward: 5.228818922884745, Moving Avg Reward: -29.98214799646339, Replay Buffer Size: 13051\n",
      "Current Epsilon: 0.28704309604425327\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 348, Reward: 5.228818922884745, Moving Avg Reward: -29.98214799646339, Replay Buffer Size: 13051\n",
      "Current Epsilon: 0.28704309604425327\n",
      "Episode 349: Won\n",
      "Episode 349, Avg Value Loss: 3.1523981988430023, Avg Policy Loss: 3.0885724226633706\n",
      "Episode 349, Reward: -31.67244390232777, Moving Avg Reward: -30.028593156908205, Replay Buffer Size: 13063\n",
      "Current Epsilon: 0.285607880564032\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 349, Reward: -31.67244390232777, Moving Avg Reward: -30.028593156908205, Replay Buffer Size: 13063\n",
      "Current Epsilon: 0.285607880564032\n",
      "Episode 350: Won\n",
      "Episode 350, Avg Value Loss: 3.1607221961021423, Avg Policy Loss: 3.2700217962265015\n",
      "Episode 350, Reward: 9.9, Moving Avg Reward: -29.63968120373076, Replay Buffer Size: 13073\n",
      "Current Epsilon: 0.28417984116121187\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 350, Reward: 9.9, Moving Avg Reward: -29.63968120373076, Replay Buffer Size: 13073\n",
      "Current Epsilon: 0.28417984116121187\n",
      "Episode 351: Won\n",
      "Episode 351, Avg Value Loss: 3.263134740293026, Avg Policy Loss: 3.3103090107440947\n",
      "Episode 351, Reward: -1.6270695018468766, Moving Avg Reward: -29.116188291138005, Replay Buffer Size: 13153\n",
      "Current Epsilon: 0.2827589419554058\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 351, Reward: -1.6270695018468766, Moving Avg Reward: -29.116188291138005, Replay Buffer Size: 13153\n",
      "Current Epsilon: 0.2827589419554058\n",
      "Episode 352: Won\n",
      "Episode 352, Avg Value Loss: 3.1241219573550754, Avg Policy Loss: 3.2463559574551053\n",
      "Episode 352, Reward: 9.91, Moving Avg Reward: -28.75975065062447, Replay Buffer Size: 13162\n",
      "Current Epsilon: 0.28134514724562876\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 352, Reward: 9.91, Moving Avg Reward: -28.75975065062447, Replay Buffer Size: 13162\n",
      "Current Epsilon: 0.28134514724562876\n",
      "Episode 353: Won\n",
      "Episode 353, Avg Value Loss: 3.7676866829395292, Avg Policy Loss: 3.2729128152132034\n",
      "Episode 353, Reward: -20.452324645234725, Moving Avg Reward: -29.146877073026292, Replay Buffer Size: 13242\n",
      "Current Epsilon: 0.2799384215094006\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 353, Reward: -20.452324645234725, Moving Avg Reward: -29.146877073026292, Replay Buffer Size: 13242\n",
      "Current Epsilon: 0.2799384215094006\n",
      "Episode 354: Won\n",
      "Episode 354, Avg Value Loss: 3.6537503615021705, Avg Policy Loss: 3.3619733542203902\n",
      "Episode 354, Reward: -92.09815030611978, Moving Avg Reward: -29.85778068211792, Replay Buffer Size: 13322\n",
      "Current Epsilon: 0.27853872940185365\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 354, Reward: -92.09815030611978, Moving Avg Reward: -29.85778068211792, Replay Buffer Size: 13322\n",
      "Current Epsilon: 0.27853872940185365\n",
      "Episode 355: Draw\n",
      "Episode 355, Avg Value Loss: 3.447904203619276, Avg Policy Loss: 3.4322941303253174\n",
      "Episode 355, Reward: 14.002731198097216, Moving Avg Reward: -29.816953370136943, Replay Buffer Size: 13350\n",
      "Current Epsilon: 0.27714603575484437\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 355, Reward: 14.002731198097216, Moving Avg Reward: -29.816953370136943, Replay Buffer Size: 13350\n",
      "Current Epsilon: 0.27714603575484437\n",
      "Episode 356: Won\n",
      "Episode 356, Avg Value Loss: 3.353769397735596, Avg Policy Loss: 3.120548057556152\n",
      "Episode 356, Reward: -24.366251467237632, Moving Avg Reward: -29.27711075436879, Replay Buffer Size: 13360\n",
      "Current Epsilon: 0.2757603055760701\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 356, Reward: -24.366251467237632, Moving Avg Reward: -29.27711075436879, Replay Buffer Size: 13360\n",
      "Current Epsilon: 0.2757603055760701\n",
      "Episode 357: Won\n",
      "Episode 357, Avg Value Loss: 3.77005712389946, Avg Policy Loss: 3.295513904094696\n",
      "Episode 357, Reward: -63.541047662619576, Moving Avg Reward: -30.078020013351214, Replay Buffer Size: 13440\n",
      "Current Epsilon: 0.2743815040481898\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 357, Reward: -63.541047662619576, Moving Avg Reward: -30.078020013351214, Replay Buffer Size: 13440\n",
      "Current Epsilon: 0.2743815040481898\n",
      "Episode 358: Won\n",
      "Episode 358, Avg Value Loss: 3.2048284794603075, Avg Policy Loss: 3.3806663155555725\n",
      "Episode 358, Reward: 15.166871535737597, Moving Avg Reward: -29.104413212482665, Replay Buffer Size: 13468\n",
      "Current Epsilon: 0.2730095965279488\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 358, Reward: 15.166871535737597, Moving Avg Reward: -29.104413212482665, Replay Buffer Size: 13468\n",
      "Current Epsilon: 0.2730095965279488\n",
      "Episode 359: Won\n",
      "Episode 359, Avg Value Loss: 3.4565063847435846, Avg Policy Loss: 3.3741778002844915\n",
      "Episode 359, Reward: -20.810382786846112, Moving Avg Reward: -29.015292690039683, Replay Buffer Size: 13477\n",
      "Current Epsilon: 0.27164454854530906\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 359, Reward: -20.810382786846112, Moving Avg Reward: -29.015292690039683, Replay Buffer Size: 13477\n",
      "Current Epsilon: 0.27164454854530906\n",
      "Episode 360: Won\n",
      "Episode 360, Avg Value Loss: 3.639836037158966, Avg Policy Loss: 3.284415304660797\n",
      "Episode 360, Reward: -18.597337390160963, Moving Avg Reward: -28.839507410835896, Replay Buffer Size: 13557\n",
      "Current Epsilon: 0.2702863258025825\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 360, Reward: -18.597337390160963, Moving Avg Reward: -28.839507410835896, Replay Buffer Size: 13557\n",
      "Current Epsilon: 0.2702863258025825\n",
      "Episode 361: Won\n",
      "Episode 361, Avg Value Loss: 3.690157243183681, Avg Policy Loss: 3.4001500265938893\n",
      "Episode 361, Reward: -24.090093364908817, Moving Avg Reward: -28.665164905624238, Replay Buffer Size: 13564\n",
      "Current Epsilon: 0.2689348941735696\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 361, Reward: -24.090093364908817, Moving Avg Reward: -28.665164905624238, Replay Buffer Size: 13564\n",
      "Current Epsilon: 0.2689348941735696\n",
      "Episode 362: Won\n",
      "Episode 362, Avg Value Loss: 3.3495629305963392, Avg Policy Loss: 3.275289838964289\n",
      "Episode 362, Reward: -51.49695515017928, Moving Avg Reward: -28.907214527367035, Replay Buffer Size: 13641\n",
      "Current Epsilon: 0.26759021970270175\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 362, Reward: -51.49695515017928, Moving Avg Reward: -28.907214527367035, Replay Buffer Size: 13641\n",
      "Current Epsilon: 0.26759021970270175\n",
      "Episode 363: Lost\n",
      "Episode 363, Avg Value Loss: 3.498136568069458, Avg Policy Loss: 3.472677993774414\n",
      "Episode 363, Reward: -29.896415671133646, Moving Avg Reward: -28.90231222645428, Replay Buffer Size: 13651\n",
      "Current Epsilon: 0.2662522686041882\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 363, Reward: -29.896415671133646, Moving Avg Reward: -28.90231222645428, Replay Buffer Size: 13651\n",
      "Current Epsilon: 0.2662522686041882\n",
      "Episode 364: Lost\n",
      "Episode 364, Avg Value Loss: 3.0264600813388824, Avg Policy Loss: 3.4274585247039795\n",
      "Episode 364, Reward: -24.025260198644517, Moving Avg Reward: -28.847658140346375, Replay Buffer Size: 13659\n",
      "Current Epsilon: 0.2649210072611673\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 364, Reward: -24.025260198644517, Moving Avg Reward: -28.847658140346375, Replay Buffer Size: 13659\n",
      "Current Epsilon: 0.2649210072611673\n",
      "Episode 365: Lost\n",
      "Episode 365, Avg Value Loss: 4.302797930581229, Avg Policy Loss: 3.353151866367885\n",
      "Episode 365, Reward: -21.358084006279626, Moving Avg Reward: -28.741093951260492, Replay Buffer Size: 13666\n",
      "Current Epsilon: 0.26359640222486147\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 365, Reward: -21.358084006279626, Moving Avg Reward: -28.741093951260492, Replay Buffer Size: 13666\n",
      "Current Epsilon: 0.26359640222486147\n",
      "Episode 366: Lost\n",
      "Episode 366, Avg Value Loss: 4.200944721698761, Avg Policy Loss: 3.1012908816337585\n",
      "Episode 366, Reward: -24.177126878272517, Moving Avg Reward: -28.710317939087318, Replay Buffer Size: 13674\n",
      "Current Epsilon: 0.26227842021373715\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 366, Reward: -24.177126878272517, Moving Avg Reward: -28.710317939087318, Replay Buffer Size: 13674\n",
      "Current Epsilon: 0.26227842021373715\n",
      "Episode 367: Lost\n",
      "Episode 367, Avg Value Loss: 3.29902982711792, Avg Policy Loss: 3.578293280168013\n",
      "Episode 367, Reward: -20.868853584901622, Moving Avg Reward: -28.59920971633328, Replay Buffer Size: 13685\n",
      "Current Epsilon: 0.2609670281126685\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 367, Reward: -20.868853584901622, Moving Avg Reward: -28.59920971633328, Replay Buffer Size: 13685\n",
      "Current Epsilon: 0.2609670281126685\n",
      "Episode 368: Lost\n",
      "Episode 368, Avg Value Loss: 3.643321827799082, Avg Policy Loss: 3.3635600447654723\n",
      "Episode 368, Reward: -39.56774736954771, Moving Avg Reward: -28.241905942217453, Replay Buffer Size: 13765\n",
      "Current Epsilon: 0.25966219297210513\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 368, Reward: -39.56774736954771, Moving Avg Reward: -28.241905942217453, Replay Buffer Size: 13765\n",
      "Current Epsilon: 0.25966219297210513\n",
      "Episode 369: Lost\n",
      "Episode 369, Avg Value Loss: 2.896771192550659, Avg Policy Loss: 3.6675992012023926\n",
      "Episode 369, Reward: -27.0860442763598, Moving Avg Reward: -27.059081024791826, Replay Buffer Size: 13774\n",
      "Current Epsilon: 0.2583638820072446\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 369, Reward: -27.0860442763598, Moving Avg Reward: -27.059081024791826, Replay Buffer Size: 13774\n",
      "Current Epsilon: 0.2583638820072446\n",
      "Episode 370: Lost\n",
      "Episode 370, Avg Value Loss: 3.823393009957813, Avg Policy Loss: 3.243818510146368\n",
      "Episode 370, Reward: 18.626408113617558, Moving Avg Reward: -26.520527369153648, Replay Buffer Size: 13795\n",
      "Current Epsilon: 0.2570720625972084\n",
      "Loss rate over the last 150 episodes: 0.60\n",
      "Episode 370, Reward: 18.626408113617558, Moving Avg Reward: -26.520527369153648, Replay Buffer Size: 13795\n",
      "Current Epsilon: 0.2570720625972084\n",
      "Episode 371: Won\n",
      "Episode 371, Avg Value Loss: 3.512431718054272, Avg Policy Loss: 3.2828195095062256\n",
      "Episode 371, Reward: 16.3078950050187, Moving Avg Reward: -25.99741966095356, Replay Buffer Size: 13816\n",
      "Current Epsilon: 0.25578670228422234\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 371, Reward: 16.3078950050187, Moving Avg Reward: -25.99741966095356, Replay Buffer Size: 13816\n",
      "Current Epsilon: 0.25578670228422234\n",
      "Episode 372: Won\n",
      "Episode 372, Avg Value Loss: 3.4107590675354005, Avg Policy Loss: 3.3830962657928465\n",
      "Episode 372, Reward: -54.23423794496788, Moving Avg Reward: -26.27444925203338, Replay Buffer Size: 13896\n",
      "Current Epsilon: 0.25450776877280124\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 372, Reward: -54.23423794496788, Moving Avg Reward: -26.27444925203338, Replay Buffer Size: 13896\n",
      "Current Epsilon: 0.25450776877280124\n",
      "Episode 373: Won\n",
      "Episode 373, Avg Value Loss: 3.2630663447909884, Avg Policy Loss: 3.297605858908759\n",
      "Episode 373, Reward: -27.221179450859584, Moving Avg Reward: -26.255869708336032, Replay Buffer Size: 13905\n",
      "Current Epsilon: 0.2532352299289372\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 373, Reward: -27.221179450859584, Moving Avg Reward: -26.255869708336032, Replay Buffer Size: 13905\n",
      "Current Epsilon: 0.2532352299289372\n",
      "Episode 374: Won\n",
      "Episode 374, Avg Value Loss: 3.478798265640552, Avg Policy Loss: 3.3460274231739535\n",
      "Episode 374, Reward: -156.4706678999515, Moving Avg Reward: -27.530948427662715, Replay Buffer Size: 13983\n",
      "Current Epsilon: 0.2519690537792925\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 374, Reward: -156.4706678999515, Moving Avg Reward: -27.530948427662715, Replay Buffer Size: 13983\n",
      "Current Epsilon: 0.2519690537792925\n",
      "Episode 375: Lost\n",
      "Episode 375, Avg Value Loss: 4.05501537322998, Avg Policy Loss: 3.2970079660415648\n",
      "Episode 375, Reward: -25.0174647050462, Moving Avg Reward: -27.35498515619549, Replay Buffer Size: 13993\n",
      "Current Epsilon: 0.2507092085103961\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 375, Reward: -25.0174647050462, Moving Avg Reward: -27.35498515619549, Replay Buffer Size: 13993\n",
      "Current Epsilon: 0.2507092085103961\n",
      "Episode 376: Lost\n",
      "Episode 376, Avg Value Loss: 3.532216739654541, Avg Policy Loss: 3.3937756448984144\n",
      "Episode 376, Reward: -8.950986435018574, Moving Avg Reward: -26.664457274328303, Replay Buffer Size: 14073\n",
      "Current Epsilon: 0.2494556624678441\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 376, Reward: -8.950986435018574, Moving Avg Reward: -26.664457274328303, Replay Buffer Size: 14073\n",
      "Current Epsilon: 0.2494556624678441\n",
      "Episode 377: Lost\n",
      "Episode 377, Avg Value Loss: 3.9007488489151, Avg Policy Loss: 3.4690701961517334\n",
      "Episode 377, Reward: -21.780251894821454, Moving Avg Reward: -26.55623812395994, Replay Buffer Size: 14081\n",
      "Current Epsilon: 0.24820838415550486\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 377, Reward: -21.780251894821454, Moving Avg Reward: -26.55623812395994, Replay Buffer Size: 14081\n",
      "Current Epsilon: 0.24820838415550486\n",
      "Episode 378: Lost\n",
      "Episode 378, Avg Value Loss: 3.6282041370868683, Avg Policy Loss: 3.127005934715271\n",
      "Episode 378, Reward: -28.306494510937384, Moving Avg Reward: -26.53312665138385, Replay Buffer Size: 14089\n",
      "Current Epsilon: 0.24696734223472733\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 378, Reward: -28.306494510937384, Moving Avg Reward: -26.53312665138385, Replay Buffer Size: 14089\n",
      "Current Epsilon: 0.24696734223472733\n",
      "Episode 379: Lost\n",
      "Episode 379, Avg Value Loss: 3.473444972719465, Avg Policy Loss: 3.5011473383222307\n",
      "Episode 379, Reward: 17.24932394890284, Moving Avg Reward: -26.05923580334753, Replay Buffer Size: 14110\n",
      "Current Epsilon: 0.2457325055235537\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 379, Reward: 17.24932394890284, Moving Avg Reward: -26.05923580334753, Replay Buffer Size: 14110\n",
      "Current Epsilon: 0.2457325055235537\n",
      "Episode 380: Won\n",
      "Episode 380, Avg Value Loss: 3.9694497043436225, Avg Policy Loss: 2.945691563866355\n",
      "Episode 380, Reward: -27.347189351252133, Moving Avg Reward: -26.047346915028744, Replay Buffer Size: 14121\n",
      "Current Epsilon: 0.24450384299593592\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 380, Reward: -27.347189351252133, Moving Avg Reward: -26.047346915028744, Replay Buffer Size: 14121\n",
      "Current Epsilon: 0.24450384299593592\n",
      "Episode 381: Won\n",
      "Episode 381, Avg Value Loss: 3.2059422081167046, Avg Policy Loss: 3.4986568797718394\n",
      "Episode 381, Reward: -28.359340628285125, Moving Avg Reward: -26.515556893188595, Replay Buffer Size: 14132\n",
      "Current Epsilon: 0.24328132378095624\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 381, Reward: -28.359340628285125, Moving Avg Reward: -26.515556893188595, Replay Buffer Size: 14132\n",
      "Current Epsilon: 0.24328132378095624\n",
      "Episode 382: Won\n",
      "Episode 382, Avg Value Loss: 3.806186090816151, Avg Policy Loss: 3.4174993471665815\n",
      "Episode 382, Reward: -22.982877487534527, Moving Avg Reward: -25.45655632537769, Replay Buffer Size: 14143\n",
      "Current Epsilon: 0.24206491716205145\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 382, Reward: -22.982877487534527, Moving Avg Reward: -25.45655632537769, Replay Buffer Size: 14143\n",
      "Current Epsilon: 0.24206491716205145\n",
      "Episode 383: Won\n",
      "Episode 383, Avg Value Loss: 3.762656486951388, Avg Policy Loss: 3.622736233931321\n",
      "Episode 383, Reward: -29.332116154165895, Moving Avg Reward: -25.465359929184356, Replay Buffer Size: 14156\n",
      "Current Epsilon: 0.2408545925762412\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 383, Reward: -29.332116154165895, Moving Avg Reward: -25.465359929184356, Replay Buffer Size: 14156\n",
      "Current Epsilon: 0.2408545925762412\n",
      "Episode 384: Won\n",
      "Episode 384, Avg Value Loss: 3.1853161841630935, Avg Policy Loss: 3.426134315133095\n",
      "Episode 384, Reward: 0.8537985920533342, Moving Avg Reward: -25.033516497111695, Replay Buffer Size: 14236\n",
      "Current Epsilon: 0.23965031961336\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 384, Reward: 0.8537985920533342, Moving Avg Reward: -25.033516497111695, Replay Buffer Size: 14236\n",
      "Current Epsilon: 0.23965031961336\n",
      "Episode 385: Won\n",
      "Episode 385, Avg Value Loss: 3.66283590644598, Avg Policy Loss: 3.3974797248840334\n",
      "Episode 385, Reward: 5.9962943723497295, Moving Avg Reward: -25.018606501914274, Replay Buffer Size: 14316\n",
      "Current Epsilon: 0.2384520680152932\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 385, Reward: 5.9962943723497295, Moving Avg Reward: -25.018606501914274, Replay Buffer Size: 14316\n",
      "Current Epsilon: 0.2384520680152932\n",
      "Episode 386: Draw\n",
      "Episode 386, Avg Value Loss: 3.849476993083954, Avg Policy Loss: 3.302921086549759\n",
      "Episode 386, Reward: -17.1957161903473, Moving Avg Reward: -24.880942353503375, Replay Buffer Size: 14396\n",
      "Current Epsilon: 0.23725980767521673\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 386, Reward: -17.1957161903473, Moving Avg Reward: -24.880942353503375, Replay Buffer Size: 14396\n",
      "Current Epsilon: 0.23725980767521673\n",
      "Episode 387: Draw\n",
      "Episode 387, Avg Value Loss: 3.9369833836188683, Avg Policy Loss: 3.3934347262749305\n",
      "Episode 387, Reward: -33.59300071582347, Moving Avg Reward: -24.639237340821005, Replay Buffer Size: 14409\n",
      "Current Epsilon: 0.23607350863684065\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 387, Reward: -33.59300071582347, Moving Avg Reward: -24.639237340821005, Replay Buffer Size: 14409\n",
      "Current Epsilon: 0.23607350863684065\n",
      "Episode 388: Lost\n",
      "Episode 388, Avg Value Loss: 3.079079359769821, Avg Policy Loss: 3.4484910368919373\n",
      "Episode 388, Reward: -25.02064780324105, Moving Avg Reward: -24.43280086540652, Replay Buffer Size: 14417\n",
      "Current Epsilon: 0.23489314109365644\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 388, Reward: -25.02064780324105, Moving Avg Reward: -24.43280086540652, Replay Buffer Size: 14417\n",
      "Current Epsilon: 0.23489314109365644\n",
      "Episode 389: Lost\n",
      "Episode 389, Avg Value Loss: 3.431705844221693, Avg Policy Loss: 3.379314733274055\n",
      "Episode 389, Reward: 2.677024781949294, Moving Avg Reward: -24.12106469601583, Replay Buffer Size: 14483\n",
      "Current Epsilon: 0.23371867538818816\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 389, Reward: 2.677024781949294, Moving Avg Reward: -24.12106469601583, Replay Buffer Size: 14483\n",
      "Current Epsilon: 0.23371867538818816\n",
      "Episode 390: Won\n",
      "Episode 390, Avg Value Loss: 4.225057303905487, Avg Policy Loss: 3.281406283378601\n",
      "Episode 390, Reward: -25.983746709801366, Moving Avg Reward: -24.520047115253202, Replay Buffer Size: 14491\n",
      "Current Epsilon: 0.23255008201124722\n",
      "Loss rate over the last 150 episodes: 0.66\n",
      "Episode 390, Reward: -25.983746709801366, Moving Avg Reward: -24.520047115253202, Replay Buffer Size: 14491\n",
      "Current Epsilon: 0.23255008201124722\n",
      "Episode 391: Won\n",
      "Episode 391, Avg Value Loss: 3.5721020417080984, Avg Policy Loss: 3.359503169854482\n",
      "Episode 391, Reward: -120.41722204031984, Moving Avg Reward: -25.399097641849743, Replay Buffer Size: 14563\n",
      "Current Epsilon: 0.231387331601191\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 391, Reward: -120.41722204031984, Moving Avg Reward: -25.399097641849743, Replay Buffer Size: 14563\n",
      "Current Epsilon: 0.231387331601191\n",
      "Episode 392: Won\n",
      "Episode 392, Avg Value Loss: 3.115296026070913, Avg Policy Loss: 3.4425119956334433\n",
      "Episode 392, Reward: -23.443580514190213, Moving Avg Reward: -25.774621513098264, Replay Buffer Size: 14575\n",
      "Current Epsilon: 0.23023039494318503\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 392, Reward: -23.443580514190213, Moving Avg Reward: -25.774621513098264, Replay Buffer Size: 14575\n",
      "Current Epsilon: 0.23023039494318503\n",
      "Episode 393: Won\n",
      "Episode 393, Avg Value Loss: 3.430710806697607, Avg Policy Loss: 3.391177016496658\n",
      "Episode 393, Reward: -41.163118333904386, Moving Avg Reward: -25.864210195572117, Replay Buffer Size: 14655\n",
      "Current Epsilon: 0.2290792429684691\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 393, Reward: -41.163118333904386, Moving Avg Reward: -25.864210195572117, Replay Buffer Size: 14655\n",
      "Current Epsilon: 0.2290792429684691\n",
      "Episode 394: Lost\n",
      "Episode 394, Avg Value Loss: 3.454418008977717, Avg Policy Loss: 3.526009364561601\n",
      "Episode 394, Reward: -22.680650709036556, Moving Avg Reward: -25.835052557917948, Replay Buffer Size: 14666\n",
      "Current Epsilon: 0.22793384675362674\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 394, Reward: -22.680650709036556, Moving Avg Reward: -25.835052557917948, Replay Buffer Size: 14666\n",
      "Current Epsilon: 0.22793384675362674\n",
      "Episode 395: Lost\n",
      "Episode 395, Avg Value Loss: 3.2326729893684387, Avg Policy Loss: 3.3731818914413454\n",
      "Episode 395, Reward: -27.33805371335503, Moving Avg Reward: -25.994445824471217, Replay Buffer Size: 14676\n",
      "Current Epsilon: 0.22679417751985861\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 395, Reward: -27.33805371335503, Moving Avg Reward: -25.994445824471217, Replay Buffer Size: 14676\n",
      "Current Epsilon: 0.22679417751985861\n",
      "Episode 396: Lost\n",
      "Episode 396, Avg Value Loss: 3.347589996125963, Avg Policy Loss: 3.129793643951416\n",
      "Episode 396, Reward: -27.03522314742829, Moving Avg Reward: -25.972431655169956, Replay Buffer Size: 14685\n",
      "Current Epsilon: 0.22566020663225933\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 396, Reward: -27.03522314742829, Moving Avg Reward: -25.972431655169956, Replay Buffer Size: 14685\n",
      "Current Epsilon: 0.22566020663225933\n",
      "Episode 397: Lost\n",
      "Episode 397, Avg Value Loss: 3.2248394224378796, Avg Policy Loss: 3.358949581782023\n",
      "Episode 397, Reward: -25.231089324511863, Moving Avg Reward: -25.921498717503834, Replay Buffer Size: 14694\n",
      "Current Epsilon: 0.22453190559909803\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 397, Reward: -25.231089324511863, Moving Avg Reward: -25.921498717503834, Replay Buffer Size: 14694\n",
      "Current Epsilon: 0.22453190559909803\n",
      "Episode 398: Lost\n",
      "Episode 398, Avg Value Loss: 3.600216329097748, Avg Policy Loss: 3.3904176516966387\n",
      "Episode 398, Reward: -46.5300325547087, Moving Avg Reward: -26.078389649799107, Replay Buffer Size: 14738\n",
      "Current Epsilon: 0.22340924607110255\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 398, Reward: -46.5300325547087, Moving Avg Reward: -26.078389649799107, Replay Buffer Size: 14738\n",
      "Current Epsilon: 0.22340924607110255\n",
      "Episode 399: Lost\n",
      "Episode 399, Avg Value Loss: 3.6261367047274553, Avg Policy Loss: 3.466845905339276\n",
      "Episode 399, Reward: -44.15971191953416, Moving Avg Reward: -26.619086768994453, Replay Buffer Size: 14792\n",
      "Current Epsilon: 0.22229219984074702\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 399, Reward: -44.15971191953416, Moving Avg Reward: -26.619086768994453, Replay Buffer Size: 14792\n",
      "Current Epsilon: 0.22229219984074702\n",
      "Episode 400: Won\n",
      "Episode 400, Avg Value Loss: 3.5159647479653358, Avg Policy Loss: 3.456593894958496\n",
      "Episode 400, Reward: -72.31688850681054, Moving Avg Reward: -26.978446806322054, Replay Buffer Size: 14872\n",
      "Current Epsilon: 0.2211807388415433\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 400, Reward: -72.31688850681054, Moving Avg Reward: -26.978446806322054, Replay Buffer Size: 14872\n",
      "Current Epsilon: 0.2211807388415433\n",
      "Episode 401: Won\n",
      "Episode 401, Avg Value Loss: 3.653408605040926, Avg Policy Loss: 3.358401551391139\n",
      "Episode 401, Reward: 14.501679724026307, Moving Avg Reward: -26.21147263021425, Replay Buffer Size: 14905\n",
      "Current Epsilon: 0.22007483514733558\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 401, Reward: 14.501679724026307, Moving Avg Reward: -26.21147263021425, Replay Buffer Size: 14905\n",
      "Current Epsilon: 0.22007483514733558\n",
      "Episode 402: Won\n",
      "Episode 402, Avg Value Loss: 3.469860625267029, Avg Policy Loss: 3.4499529778957365\n",
      "Episode 402, Reward: -13.746704604332928, Moving Avg Reward: -26.51285856874018, Replay Buffer Size: 14985\n",
      "Current Epsilon: 0.2189744609715989\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 402, Reward: -13.746704604332928, Moving Avg Reward: -26.51285856874018, Replay Buffer Size: 14985\n",
      "Current Epsilon: 0.2189744609715989\n",
      "Episode 403: Won\n",
      "Episode 403, Avg Value Loss: 3.6385768830776213, Avg Policy Loss: 3.4231499791145326\n",
      "Episode 403, Reward: -6.76321368564212, Moving Avg Reward: -26.649872924066127, Replay Buffer Size: 15065\n",
      "Current Epsilon: 0.2178795886667409\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 403, Reward: -6.76321368564212, Moving Avg Reward: -26.649872924066127, Replay Buffer Size: 15065\n",
      "Current Epsilon: 0.2178795886667409\n",
      "Episode 404: Draw\n",
      "Episode 404, Avg Value Loss: 3.3333394744179468, Avg Policy Loss: 3.1320226842706855\n",
      "Episode 404, Reward: -29.108518675542086, Moving Avg Reward: -26.69338969556395, Replay Buffer Size: 15076\n",
      "Current Epsilon: 0.2167901907234072\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 404, Reward: -29.108518675542086, Moving Avg Reward: -26.69338969556395, Replay Buffer Size: 15076\n",
      "Current Epsilon: 0.2167901907234072\n",
      "Episode 405: Lost\n",
      "Episode 405, Avg Value Loss: 3.5329370697339377, Avg Policy Loss: 3.509738306204478\n",
      "Episode 405, Reward: -29.000136426191837, Moving Avg Reward: -26.63660952377622, Replay Buffer Size: 15088\n",
      "Current Epsilon: 0.21570623976979014\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 405, Reward: -29.000136426191837, Moving Avg Reward: -26.63660952377622, Replay Buffer Size: 15088\n",
      "Current Epsilon: 0.21570623976979014\n",
      "Episode 406: Lost\n",
      "Episode 406, Avg Value Loss: 3.398547315597534, Avg Policy Loss: 3.230984854698181\n",
      "Episode 406, Reward: -23.606709267205474, Moving Avg Reward: -26.63486159924873, Replay Buffer Size: 15098\n",
      "Current Epsilon: 0.21462770857094118\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 406, Reward: -23.606709267205474, Moving Avg Reward: -26.63486159924873, Replay Buffer Size: 15098\n",
      "Current Epsilon: 0.21462770857094118\n",
      "Episode 407: Lost\n",
      "Episode 407, Avg Value Loss: 3.21047138115939, Avg Policy Loss: 3.4409977127523983\n",
      "Episode 407, Reward: 19.197099862752406, Moving Avg Reward: -26.179195292025216, Replay Buffer Size: 15132\n",
      "Current Epsilon: 0.21355457002808648\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 407, Reward: 19.197099862752406, Moving Avg Reward: -26.179195292025216, Replay Buffer Size: 15132\n",
      "Current Epsilon: 0.21355457002808648\n",
      "Episode 408: Won\n",
      "Episode 408, Avg Value Loss: 3.53858745098114, Avg Policy Loss: 3.3862316012382507\n",
      "Episode 408, Reward: -25.358241656675407, Moving Avg Reward: -26.11367125786069, Replay Buffer Size: 15144\n",
      "Current Epsilon: 0.21248679717794605\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 408, Reward: -25.358241656675407, Moving Avg Reward: -26.11367125786069, Replay Buffer Size: 15144\n",
      "Current Epsilon: 0.21248679717794605\n",
      "Episode 409: Won\n",
      "Episode 409, Avg Value Loss: 3.087803681691488, Avg Policy Loss: 3.41833594110277\n",
      "Episode 409, Reward: -21.690757591854144, Moving Avg Reward: -25.392831264861133, Replay Buffer Size: 15153\n",
      "Current Epsilon: 0.21142436319205632\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 409, Reward: -21.690757591854144, Moving Avg Reward: -25.392831264861133, Replay Buffer Size: 15153\n",
      "Current Epsilon: 0.21142436319205632\n",
      "Episode 410: Won\n",
      "Episode 410, Avg Value Loss: 3.605979293971867, Avg Policy Loss: 3.399268137944209\n",
      "Episode 410, Reward: -55.05538409730726, Moving Avg Reward: -25.977798981477836, Replay Buffer Size: 15230\n",
      "Current Epsilon: 0.21036724137609603\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 410, Reward: -55.05538409730726, Moving Avg Reward: -25.977798981477836, Replay Buffer Size: 15230\n",
      "Current Epsilon: 0.21036724137609603\n",
      "Episode 411: Won\n",
      "Episode 411, Avg Value Loss: 3.5077475935220717, Avg Policy Loss: 3.3768057614564895\n",
      "Episode 411, Reward: 0.39296061611184396, Moving Avg Reward: -25.718527022876927, Replay Buffer Size: 15310\n",
      "Current Epsilon: 0.20931540516921554\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 411, Reward: 0.39296061611184396, Moving Avg Reward: -25.718527022876927, Replay Buffer Size: 15310\n",
      "Current Epsilon: 0.20931540516921554\n",
      "Episode 412: Won\n",
      "Episode 412, Avg Value Loss: 3.500766317049662, Avg Policy Loss: 3.381126594543457\n",
      "Episode 412, Reward: 9.85, Moving Avg Reward: -25.301565208416356, Replay Buffer Size: 15325\n",
      "Current Epsilon: 0.20826882814336947\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 412, Reward: 9.85, Moving Avg Reward: -25.301565208416356, Replay Buffer Size: 15325\n",
      "Current Epsilon: 0.20826882814336947\n",
      "Episode 413: Won\n",
      "Episode 413, Avg Value Loss: 3.375942835065185, Avg Policy Loss: 3.453157096612649\n",
      "Episode 413, Reward: 5.49449971868336, Moving Avg Reward: -24.965619922716304, Replay Buffer Size: 15386\n",
      "Current Epsilon: 0.20722748400265262\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 413, Reward: 5.49449971868336, Moving Avg Reward: -24.965619922716304, Replay Buffer Size: 15386\n",
      "Current Epsilon: 0.20722748400265262\n",
      "Episode 414: Won\n",
      "Episode 414, Avg Value Loss: 3.033923322504217, Avg Policy Loss: 3.3412367647344414\n",
      "Episode 414, Reward: -30.637602963962806, Moving Avg Reward: -25.041020112548917, Replay Buffer Size: 15397\n",
      "Current Epsilon: 0.20619134658263935\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 414, Reward: -30.637602963962806, Moving Avg Reward: -25.041020112548917, Replay Buffer Size: 15397\n",
      "Current Epsilon: 0.20619134658263935\n",
      "Episode 415: Won\n",
      "Episode 415, Avg Value Loss: 3.4216123938560488, Avg Policy Loss: 3.4015610218048096\n",
      "Episode 415, Reward: -30.089190732968845, Moving Avg Reward: -25.104367830224678, Replay Buffer Size: 15407\n",
      "Current Epsilon: 0.20516038984972615\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 415, Reward: -30.089190732968845, Moving Avg Reward: -25.104367830224678, Replay Buffer Size: 15407\n",
      "Current Epsilon: 0.20516038984972615\n",
      "Episode 416: Won\n",
      "Episode 416, Avg Value Loss: 3.2477116187413535, Avg Policy Loss: 3.4234078327814736\n",
      "Episode 416, Reward: 17.558112076659057, Moving Avg Reward: -24.639506584345927, Replay Buffer Size: 15425\n",
      "Current Epsilon: 0.2041345879004775\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 416, Reward: 17.558112076659057, Moving Avg Reward: -24.639506584345927, Replay Buffer Size: 15425\n",
      "Current Epsilon: 0.2041345879004775\n",
      "Episode 417: Won\n",
      "Episode 417, Avg Value Loss: 3.639423783123493, Avg Policy Loss: 3.3857057064771654\n",
      "Episode 417, Reward: -50.99872884359352, Moving Avg Reward: -24.862231781874947, Replay Buffer Size: 15505\n",
      "Current Epsilon: 0.2031139149609751\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 417, Reward: -50.99872884359352, Moving Avg Reward: -24.862231781874947, Replay Buffer Size: 15505\n",
      "Current Epsilon: 0.2031139149609751\n",
      "Episode 418: Won\n",
      "Episode 418, Avg Value Loss: 3.439861238002777, Avg Policy Loss: 3.3765356838703156\n",
      "Episode 418, Reward: -63.913885939118266, Moving Avg Reward: -25.217234829538416, Replay Buffer Size: 15585\n",
      "Current Epsilon: 0.20209834538617025\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 418, Reward: -63.913885939118266, Moving Avg Reward: -25.217234829538416, Replay Buffer Size: 15585\n",
      "Current Epsilon: 0.20209834538617025\n",
      "Episode 419: Draw\n",
      "Episode 419, Avg Value Loss: 3.763249397277832, Avg Policy Loss: 3.2332780252803457\n",
      "Episode 419, Reward: -28.20447511576954, Moving Avg Reward: -25.29620076446143, Replay Buffer Size: 15607\n",
      "Current Epsilon: 0.2010878536592394\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 419, Reward: -28.20447511576954, Moving Avg Reward: -25.29620076446143, Replay Buffer Size: 15607\n",
      "Current Epsilon: 0.2010878536592394\n",
      "Episode 420: Lost\n",
      "Episode 420, Avg Value Loss: 3.4618665320532664, Avg Policy Loss: 3.6106597696031844\n",
      "Episode 420, Reward: -21.5061403542257, Moving Avg Reward: -25.227902506426503, Replay Buffer Size: 15614\n",
      "Current Epsilon: 0.2000824143909432\n",
      "Loss rate over the last 150 episodes: 0.66\n",
      "Episode 420, Reward: -21.5061403542257, Moving Avg Reward: -25.227902506426503, Replay Buffer Size: 15614\n",
      "Current Epsilon: 0.2000824143909432\n",
      "Episode 421: Lost\n",
      "Episode 421, Avg Value Loss: 3.443913827339808, Avg Policy Loss: 3.347795764605204\n",
      "Episode 421, Reward: -29.752196424754807, Moving Avg Reward: -25.256126532947597, Replay Buffer Size: 15626\n",
      "Current Epsilon: 0.19908200231898848\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 421, Reward: -29.752196424754807, Moving Avg Reward: -25.256126532947597, Replay Buffer Size: 15626\n",
      "Current Epsilon: 0.19908200231898848\n",
      "Episode 422: Lost\n",
      "Episode 422, Avg Value Loss: 3.3329869642853738, Avg Policy Loss: 3.452665534615517\n",
      "Episode 422, Reward: -59.743984499170715, Moving Avg Reward: -25.64482816020648, Replay Buffer Size: 15706\n",
      "Current Epsilon: 0.19808659230739353\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 422, Reward: -59.743984499170715, Moving Avg Reward: -25.64482816020648, Replay Buffer Size: 15706\n",
      "Current Epsilon: 0.19808659230739353\n",
      "Episode 423: Lost\n",
      "Episode 423, Avg Value Loss: 3.6651899081010084, Avg Policy Loss: 3.3343373261965237\n",
      "Episode 423, Reward: -29.46123504596622, Moving Avg Reward: -25.20386142314624, Replay Buffer Size: 15719\n",
      "Current Epsilon: 0.19709615934585656\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 423, Reward: -29.46123504596622, Moving Avg Reward: -25.20386142314624, Replay Buffer Size: 15719\n",
      "Current Epsilon: 0.19709615934585656\n",
      "Episode 424: Lost\n",
      "Episode 424, Avg Value Loss: 3.2662824392318726, Avg Policy Loss: 3.5750818451245627\n",
      "Episode 424, Reward: -31.416646697470505, Moving Avg Reward: -25.173160406090574, Replay Buffer Size: 15731\n",
      "Current Epsilon: 0.19611067854912728\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 424, Reward: -31.416646697470505, Moving Avg Reward: -25.173160406090574, Replay Buffer Size: 15731\n",
      "Current Epsilon: 0.19611067854912728\n",
      "Episode 425: Lost\n",
      "Episode 425, Avg Value Loss: 3.511535310745239, Avg Policy Loss: 3.40969540476799\n",
      "Episode 425, Reward: -51.32393866393226, Moving Avg Reward: -24.843922551595142, Replay Buffer Size: 15811\n",
      "Current Epsilon: 0.19513012515638165\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 425, Reward: -51.32393866393226, Moving Avg Reward: -24.843922551595142, Replay Buffer Size: 15811\n",
      "Current Epsilon: 0.19513012515638165\n",
      "Episode 426: Lost\n",
      "Episode 426, Avg Value Loss: 3.187750908044668, Avg Policy Loss: 3.44097094810926\n",
      "Episode 426, Reward: -44.00971531331743, Moving Avg Reward: -25.048763590459753, Replay Buffer Size: 15863\n",
      "Current Epsilon: 0.19415447453059972\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 426, Reward: -44.00971531331743, Moving Avg Reward: -25.048763590459753, Replay Buffer Size: 15863\n",
      "Current Epsilon: 0.19415447453059972\n",
      "Episode 427: Lost\n",
      "Episode 427, Avg Value Loss: 3.4254611849784853, Avg Policy Loss: 3.392040768265724\n",
      "Episode 427, Reward: 4.479225294934901, Moving Avg Reward: -24.764108040547946, Replay Buffer Size: 15943\n",
      "Current Epsilon: 0.19318370215794672\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 427, Reward: 4.479225294934901, Moving Avg Reward: -24.764108040547946, Replay Buffer Size: 15943\n",
      "Current Epsilon: 0.19318370215794672\n",
      "Episode 428: Lost\n",
      "Episode 428, Avg Value Loss: 3.361624604022061, Avg Policy Loss: 3.3906536323052867\n",
      "Episode 428, Reward: 8.635204310066808, Moving Avg Reward: -24.677585240939774, Replay Buffer Size: 15997\n",
      "Current Epsilon: 0.192217783647157\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 428, Reward: 8.635204310066808, Moving Avg Reward: -24.677585240939774, Replay Buffer Size: 15997\n",
      "Current Epsilon: 0.192217783647157\n",
      "Episode 429: Won\n",
      "Episode 429, Avg Value Loss: 3.2176120281219482, Avg Policy Loss: 3.4963203191757204\n",
      "Episode 429, Reward: -28.641875095091933, Moving Avg Reward: -24.59123899352197, Replay Buffer Size: 16007\n",
      "Current Epsilon: 0.1912566947289212\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 429, Reward: -28.641875095091933, Moving Avg Reward: -24.59123899352197, Replay Buffer Size: 16007\n",
      "Current Epsilon: 0.1912566947289212\n",
      "Episode 430: Won\n",
      "Episode 430, Avg Value Loss: 2.885937523841858, Avg Policy Loss: 3.307596945762634\n",
      "Episode 430, Reward: -26.205805285962178, Moving Avg Reward: -24.621848283550943, Replay Buffer Size: 16017\n",
      "Current Epsilon: 0.1903004112552766\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 430, Reward: -26.205805285962178, Moving Avg Reward: -24.621848283550943, Replay Buffer Size: 16017\n",
      "Current Epsilon: 0.1903004112552766\n",
      "Episode 431: Won\n",
      "Episode 431, Avg Value Loss: 3.429443749514493, Avg Policy Loss: 3.408305059779774\n",
      "Episode 431, Reward: -26.29563129591483, Moving Avg Reward: -24.67535484262979, Replay Buffer Size: 16028\n",
      "Current Epsilon: 0.18934890919900021\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 431, Reward: -26.29563129591483, Moving Avg Reward: -24.67535484262979, Replay Buffer Size: 16028\n",
      "Current Epsilon: 0.18934890919900021\n",
      "Episode 432: Won\n",
      "Episode 432, Avg Value Loss: 3.958720097175011, Avg Policy Loss: 3.3127474051255446\n",
      "Episode 432, Reward: -32.83018215668168, Moving Avg Reward: -24.803817025583243, Replay Buffer Size: 16041\n",
      "Current Epsilon: 0.18840216465300522\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 432, Reward: -32.83018215668168, Moving Avg Reward: -24.803817025583243, Replay Buffer Size: 16041\n",
      "Current Epsilon: 0.18840216465300522\n",
      "Episode 433: Won\n",
      "Episode 433, Avg Value Loss: 3.469353112578392, Avg Policy Loss: 3.4560677617788316\n",
      "Episode 433, Reward: -84.9807241574406, Moving Avg Reward: -25.84855961485748, Replay Buffer Size: 16121\n",
      "Current Epsilon: 0.18746015382974018\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 433, Reward: -84.9807241574406, Moving Avg Reward: -25.84855961485748, Replay Buffer Size: 16121\n",
      "Current Epsilon: 0.18746015382974018\n",
      "Episode 434: Won\n",
      "Episode 434, Avg Value Loss: 3.77024178703626, Avg Policy Loss: 3.353180746237437\n",
      "Episode 434, Reward: -26.101288689981466, Moving Avg Reward: -25.674563229481713, Replay Buffer Size: 16133\n",
      "Current Epsilon: 0.1865228530605915\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 434, Reward: -26.101288689981466, Moving Avg Reward: -25.674563229481713, Replay Buffer Size: 16133\n",
      "Current Epsilon: 0.1865228530605915\n",
      "Episode 435: Won\n",
      "Episode 435, Avg Value Loss: 3.5174815813700357, Avg Policy Loss: 3.5001780562930636\n",
      "Episode 435, Reward: -47.22531238028532, Moving Avg Reward: -25.879849485138877, Replay Buffer Size: 16178\n",
      "Current Epsilon: 0.18559023879528855\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 435, Reward: -47.22531238028532, Moving Avg Reward: -25.879849485138877, Replay Buffer Size: 16178\n",
      "Current Epsilon: 0.18559023879528855\n",
      "Episode 436: Lost\n",
      "Episode 436, Avg Value Loss: 3.3235634729266166, Avg Policy Loss: 3.3591690331697466\n",
      "Episode 436, Reward: -18.54049061046789, Moving Avg Reward: -25.805540106525264, Replay Buffer Size: 16258\n",
      "Current Epsilon: 0.1846622876013121\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 436, Reward: -18.54049061046789, Moving Avg Reward: -25.805540106525264, Replay Buffer Size: 16258\n",
      "Current Epsilon: 0.1846622876013121\n",
      "Episode 437: Lost\n",
      "Episode 437, Avg Value Loss: 3.3797477143151418, Avg Policy Loss: 3.3500797294435047\n",
      "Episode 437, Reward: 8.427399265696602, Moving Avg Reward: -25.47275499370874, Replay Buffer Size: 16279\n",
      "Current Epsilon: 0.18373897616330553\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 437, Reward: 8.427399265696602, Moving Avg Reward: -25.47275499370874, Replay Buffer Size: 16279\n",
      "Current Epsilon: 0.18373897616330553\n",
      "Episode 438: Won\n",
      "Episode 438, Avg Value Loss: 3.3457065373659134, Avg Policy Loss: 3.6568134228388467\n",
      "Episode 438, Reward: -23.837851379275577, Moving Avg Reward: -25.77260507653487, Replay Buffer Size: 16291\n",
      "Current Epsilon: 0.182820281282489\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 438, Reward: -23.837851379275577, Moving Avg Reward: -25.77260507653487, Replay Buffer Size: 16291\n",
      "Current Epsilon: 0.182820281282489\n",
      "Episode 439: Won\n",
      "Episode 439, Avg Value Loss: 3.5690983022962297, Avg Policy Loss: 3.3232271671295166\n",
      "Episode 439, Reward: -20.520822357936346, Moving Avg Reward: -26.005590018434056, Replay Buffer Size: 16298\n",
      "Current Epsilon: 0.18190617987607657\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 439, Reward: -20.520822357936346, Moving Avg Reward: -26.005590018434056, Replay Buffer Size: 16298\n",
      "Current Epsilon: 0.18190617987607657\n",
      "Episode 440: Won\n",
      "Episode 440, Avg Value Loss: 3.3525193996727465, Avg Policy Loss: 3.411614552140236\n",
      "Episode 440, Reward: -81.37706130369223, Moving Avg Reward: -26.540231981652518, Replay Buffer Size: 16378\n",
      "Current Epsilon: 0.18099664897669618\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 440, Reward: -81.37706130369223, Moving Avg Reward: -26.540231981652518, Replay Buffer Size: 16378\n",
      "Current Epsilon: 0.18099664897669618\n",
      "Episode 441: Won\n",
      "Episode 441, Avg Value Loss: 3.7000315740704535, Avg Policy Loss: 3.4261293411254883\n",
      "Episode 441, Reward: -3.7710115580368613, Moving Avg Reward: -26.14716709351994, Replay Buffer Size: 16458\n",
      "Current Epsilon: 0.1800916657318127\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 441, Reward: -3.7710115580368613, Moving Avg Reward: -26.14716709351994, Replay Buffer Size: 16458\n",
      "Current Epsilon: 0.1800916657318127\n",
      "Episode 442: Draw\n",
      "Episode 442, Avg Value Loss: 3.4978613659739493, Avg Policy Loss: 3.417348650097847\n",
      "Episode 442, Reward: -7.086738899319613, Moving Avg Reward: -25.889608094578907, Replay Buffer Size: 16538\n",
      "Current Epsilon: 0.17919120740315364\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 442, Reward: -7.086738899319613, Moving Avg Reward: -25.889608094578907, Replay Buffer Size: 16538\n",
      "Current Epsilon: 0.17919120740315364\n",
      "Episode 443: Draw\n",
      "Episode 443, Avg Value Loss: 3.354049502313137, Avg Policy Loss: 3.4248964339494705\n",
      "Episode 443, Reward: 3.4273344074600107, Moving Avg Reward: -25.634130033044826, Replay Buffer Size: 16618\n",
      "Current Epsilon: 0.17829525136613786\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 443, Reward: 3.4273344074600107, Moving Avg Reward: -25.634130033044826, Replay Buffer Size: 16618\n",
      "Current Epsilon: 0.17829525136613786\n",
      "Episode 444: Draw\n",
      "Episode 444, Avg Value Loss: 3.2424447536468506, Avg Policy Loss: 4.602878570556641\n",
      "Episode 444, Reward: 9.99, Moving Avg Reward: -25.271449475791105, Replay Buffer Size: 16619\n",
      "Current Epsilon: 0.17740377510930716\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 444, Reward: 9.99, Moving Avg Reward: -25.271449475791105, Replay Buffer Size: 16619\n",
      "Current Epsilon: 0.17740377510930716\n",
      "Episode 445: Won\n",
      "Episode 445, Avg Value Loss: 3.764504073560238, Avg Policy Loss: 3.3989497631788255\n",
      "Episode 445, Reward: -58.37566514802726, Moving Avg Reward: -25.52017740056726, Replay Buffer Size: 16699\n",
      "Current Epsilon: 0.17651675623376062\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 445, Reward: -58.37566514802726, Moving Avg Reward: -25.52017740056726, Replay Buffer Size: 16699\n",
      "Current Epsilon: 0.17651675623376062\n",
      "Episode 446: Won\n",
      "Episode 446, Avg Value Loss: 3.5563638299703597, Avg Policy Loss: 3.424071365594864\n",
      "Episode 446, Reward: -1.627806959401665, Moving Avg Reward: -25.720326953110472, Replay Buffer Size: 16779\n",
      "Current Epsilon: 0.1756341724525918\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 446, Reward: -1.627806959401665, Moving Avg Reward: -25.720326953110472, Replay Buffer Size: 16779\n",
      "Current Epsilon: 0.1756341724525918\n",
      "Episode 447: Draw\n",
      "Episode 447, Avg Value Loss: 3.2367726614077887, Avg Policy Loss: 3.464789718389511\n",
      "Episode 447, Reward: -44.400420880060075, Moving Avg Reward: -25.071764898180493, Replay Buffer Size: 16827\n",
      "Current Epsilon: 0.17475600159032884\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 447, Reward: -44.400420880060075, Moving Avg Reward: -25.071764898180493, Replay Buffer Size: 16827\n",
      "Current Epsilon: 0.17475600159032884\n",
      "Episode 448: Lost\n",
      "Episode 448, Avg Value Loss: 3.5067525467147, Avg Policy Loss: 3.41681414583455\n",
      "Episode 448, Reward: 17.217605759337825, Moving Avg Reward: -24.951877029815964, Replay Buffer Size: 16873\n",
      "Current Epsilon: 0.17388222158237718\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 448, Reward: 17.217605759337825, Moving Avg Reward: -24.951877029815964, Replay Buffer Size: 16873\n",
      "Current Epsilon: 0.17388222158237718\n",
      "Episode 449: Won\n",
      "Episode 449, Avg Value Loss: 3.6968386836349962, Avg Policy Loss: 3.33894858956337\n",
      "Episode 449, Reward: -105.14460642383294, Moving Avg Reward: -25.686598655031013, Replay Buffer Size: 16953\n",
      "Current Epsilon: 0.1730128104744653\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 449, Reward: -105.14460642383294, Moving Avg Reward: -25.686598655031013, Replay Buffer Size: 16953\n",
      "Current Epsilon: 0.1730128104744653\n",
      "Episode 450: Won\n",
      "Episode 450, Avg Value Loss: 3.46195148229599, Avg Policy Loss: 3.3421333014965056\n",
      "Episode 450, Reward: -18.51822173143485, Moving Avg Reward: -25.97078087234536, Replay Buffer Size: 17033\n",
      "Current Epsilon: 0.17214774642209296\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 450, Reward: -18.51822173143485, Moving Avg Reward: -25.97078087234536, Replay Buffer Size: 17033\n",
      "Current Epsilon: 0.17214774642209296\n",
      "Episode 451: Draw\n",
      "Episode 451, Avg Value Loss: 3.2877103090286255, Avg Policy Loss: 3.4863873958587646\n",
      "Episode 451, Reward: -26.28545192757734, Moving Avg Reward: -26.217364696602672, Replay Buffer Size: 17043\n",
      "Current Epsilon: 0.1712870076899825\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 451, Reward: -26.28545192757734, Moving Avg Reward: -26.217364696602672, Replay Buffer Size: 17043\n",
      "Current Epsilon: 0.1712870076899825\n",
      "Episode 452: Lost\n",
      "Episode 452, Avg Value Loss: 3.361088367608877, Avg Policy Loss: 3.3284158523266134\n",
      "Episode 452, Reward: -29.059666059787183, Moving Avg Reward: -26.607061357200543, Replay Buffer Size: 17056\n",
      "Current Epsilon: 0.17043057265153258\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 452, Reward: -29.059666059787183, Moving Avg Reward: -26.607061357200543, Replay Buffer Size: 17056\n",
      "Current Epsilon: 0.17043057265153258\n",
      "Episode 453: Lost\n",
      "Episode 453, Avg Value Loss: 3.9326454309316783, Avg Policy Loss: 3.407254640872662\n",
      "Episode 453, Reward: -28.54951396274139, Moving Avg Reward: -26.688033250375607, Replay Buffer Size: 17069\n",
      "Current Epsilon: 0.16957841978827493\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 453, Reward: -28.54951396274139, Moving Avg Reward: -26.688033250375607, Replay Buffer Size: 17069\n",
      "Current Epsilon: 0.16957841978827493\n",
      "Episode 454: Lost\n",
      "Episode 454, Avg Value Loss: 3.3161134704947473, Avg Policy Loss: 3.4497843593358994\n",
      "Episode 454, Reward: -54.12138952019645, Moving Avg Reward: -26.30826564251637, Replay Buffer Size: 17149\n",
      "Current Epsilon: 0.16873052768933355\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 454, Reward: -54.12138952019645, Moving Avg Reward: -26.30826564251637, Replay Buffer Size: 17149\n",
      "Current Epsilon: 0.16873052768933355\n",
      "Episode 455: Lost\n",
      "Episode 455, Avg Value Loss: 3.2742804288864136, Avg Policy Loss: 3.4074268164458097\n",
      "Episode 455, Reward: 15.063547135410015, Moving Avg Reward: -26.297657483143244, Replay Buffer Size: 17176\n",
      "Current Epsilon: 0.1678868750508869\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 455, Reward: 15.063547135410015, Moving Avg Reward: -26.297657483143244, Replay Buffer Size: 17176\n",
      "Current Epsilon: 0.1678868750508869\n",
      "Episode 456: Won\n",
      "Episode 456, Avg Value Loss: 3.457663251507667, Avg Policy Loss: 3.403278012429514\n",
      "Episode 456, Reward: -59.39015364775079, Moving Avg Reward: -26.64789650494837, Replay Buffer Size: 17207\n",
      "Current Epsilon: 0.16704744067563246\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 456, Reward: -59.39015364775079, Moving Avg Reward: -26.64789650494837, Replay Buffer Size: 17207\n",
      "Current Epsilon: 0.16704744067563246\n",
      "Episode 457: Won\n",
      "Episode 457, Avg Value Loss: 3.506052366711877, Avg Policy Loss: 3.444159697402607\n",
      "Episode 457, Reward: 8.737464283526805, Moving Avg Reward: -25.92511138548691, Replay Buffer Size: 17251\n",
      "Current Epsilon: 0.1662122034722543\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 457, Reward: 8.737464283526805, Moving Avg Reward: -25.92511138548691, Replay Buffer Size: 17251\n",
      "Current Epsilon: 0.1662122034722543\n",
      "Episode 458: Won\n",
      "Episode 458, Avg Value Loss: 3.3522609869639077, Avg Policy Loss: 3.311647097269694\n",
      "Episode 458, Reward: -27.14658793998983, Moving Avg Reward: -26.348245980244187, Replay Buffer Size: 17260\n",
      "Current Epsilon: 0.16538114245489302\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 458, Reward: -27.14658793998983, Moving Avg Reward: -26.348245980244187, Replay Buffer Size: 17260\n",
      "Current Epsilon: 0.16538114245489302\n",
      "Episode 459: Won\n",
      "Episode 459, Avg Value Loss: 3.431665962934494, Avg Policy Loss: 3.3992567241191862\n",
      "Episode 459, Reward: -23.097872814831934, Moving Avg Reward: -26.371120880524046, Replay Buffer Size: 17340\n",
      "Current Epsilon: 0.16455423674261854\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 459, Reward: -23.097872814831934, Moving Avg Reward: -26.371120880524046, Replay Buffer Size: 17340\n",
      "Current Epsilon: 0.16455423674261854\n",
      "Episode 460: Won\n",
      "Episode 460, Avg Value Loss: 3.429564118385315, Avg Policy Loss: 3.5018055737018585\n",
      "Episode 460, Reward: -24.10582856369465, Moving Avg Reward: -26.426205792259385, Replay Buffer Size: 17348\n",
      "Current Epsilon: 0.16373146555890544\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 460, Reward: -24.10582856369465, Moving Avg Reward: -26.426205792259385, Replay Buffer Size: 17348\n",
      "Current Epsilon: 0.16373146555890544\n",
      "Episode 461: Won\n",
      "Episode 461, Avg Value Loss: 3.638709013278668, Avg Policy Loss: 3.403069257736206\n",
      "Episode 461, Reward: -28.859800902316508, Moving Avg Reward: -26.47390286763346, Replay Buffer Size: 17361\n",
      "Current Epsilon: 0.16291280823111093\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 461, Reward: -28.859800902316508, Moving Avg Reward: -26.47390286763346, Replay Buffer Size: 17361\n",
      "Current Epsilon: 0.16291280823111093\n",
      "Episode 462: Won\n",
      "Episode 462, Avg Value Loss: 2.816459834575653, Avg Policy Loss: 3.284096747636795\n",
      "Episode 462, Reward: -25.71216751611085, Moving Avg Reward: -26.216054991292772, Replay Buffer Size: 17369\n",
      "Current Epsilon: 0.16209824418995536\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 462, Reward: -25.71216751611085, Moving Avg Reward: -26.216054991292772, Replay Buffer Size: 17369\n",
      "Current Epsilon: 0.16209824418995536\n",
      "Episode 463: Won\n",
      "Episode 463, Avg Value Loss: 3.4765925854444504, Avg Policy Loss: 3.553529977798462\n",
      "Episode 463, Reward: -24.333692140881453, Moving Avg Reward: -26.16042775599025, Replay Buffer Size: 17377\n",
      "Current Epsilon: 0.16128775296900558\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 463, Reward: -24.333692140881453, Moving Avg Reward: -26.16042775599025, Replay Buffer Size: 17377\n",
      "Current Epsilon: 0.16128775296900558\n",
      "Episode 464: Won\n",
      "Episode 464, Avg Value Loss: 3.4178052678704263, Avg Policy Loss: 3.4068596392869948\n",
      "Episode 464, Reward: -27.6072223769858, Moving Avg Reward: -26.196247377773663, Replay Buffer Size: 17457\n",
      "Current Epsilon: 0.16048131420416054\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 464, Reward: -27.6072223769858, Moving Avg Reward: -26.196247377773663, Replay Buffer Size: 17457\n",
      "Current Epsilon: 0.16048131420416054\n",
      "Episode 465: Lost\n",
      "Episode 465, Avg Value Loss: 3.352626875335095, Avg Policy Loss: 3.342708999035405\n",
      "Episode 465, Reward: -39.52953493378078, Moving Avg Reward: -26.377961887048674, Replay Buffer Size: 17508\n",
      "Current Epsilon: 0.15967890763313974\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 465, Reward: -39.52953493378078, Moving Avg Reward: -26.377961887048674, Replay Buffer Size: 17508\n",
      "Current Epsilon: 0.15967890763313974\n",
      "Episode 466: Lost\n",
      "Episode 466, Avg Value Loss: 3.5333764016628266, Avg Policy Loss: 3.430908793210983\n",
      "Episode 466, Reward: 4.940306506338471, Moving Avg Reward: -26.086787553202562, Replay Buffer Size: 17588\n",
      "Current Epsilon: 0.15888051309497406\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 466, Reward: 4.940306506338471, Moving Avg Reward: -26.086787553202562, Replay Buffer Size: 17588\n",
      "Current Epsilon: 0.15888051309497406\n",
      "Episode 467: Lost\n",
      "Episode 467, Avg Value Loss: 3.302461066842079, Avg Policy Loss: 3.3544166296720506\n",
      "Episode 467, Reward: -70.12429765849238, Moving Avg Reward: -26.57934199393847, Replay Buffer Size: 17668\n",
      "Current Epsilon: 0.1580861105294992\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 467, Reward: -70.12429765849238, Moving Avg Reward: -26.57934199393847, Replay Buffer Size: 17668\n",
      "Current Epsilon: 0.1580861105294992\n",
      "Episode 468: Draw\n",
      "Episode 468, Avg Value Loss: 3.675331540107727, Avg Policy Loss: 3.3266364097595216\n",
      "Episode 468, Reward: -41.81108608910085, Moving Avg Reward: -26.601775381134004, Replay Buffer Size: 17718\n",
      "Current Epsilon: 0.1572956799768517\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 468, Reward: -41.81108608910085, Moving Avg Reward: -26.601775381134004, Replay Buffer Size: 17718\n",
      "Current Epsilon: 0.1572956799768517\n",
      "Episode 469: Lost\n",
      "Episode 469, Avg Value Loss: 3.4586342871189117, Avg Policy Loss: 3.3253618478775024\n",
      "Episode 469, Reward: -18.276711204967214, Moving Avg Reward: -26.513682050420076, Replay Buffer Size: 17754\n",
      "Current Epsilon: 0.15650920157696743\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 469, Reward: -18.276711204967214, Moving Avg Reward: -26.513682050420076, Replay Buffer Size: 17754\n",
      "Current Epsilon: 0.15650920157696743\n",
      "Episode 470: Lost\n",
      "Episode 470, Avg Value Loss: 3.900944301060268, Avg Policy Loss: 3.3995134830474854\n",
      "Episode 470, Reward: 19.18035446738171, Moving Avg Reward: -26.508142586882432, Replay Buffer Size: 17775\n",
      "Current Epsilon: 0.1557266555690826\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 470, Reward: 19.18035446738171, Moving Avg Reward: -26.508142586882432, Replay Buffer Size: 17775\n",
      "Current Epsilon: 0.1557266555690826\n",
      "Episode 471: Won\n",
      "Episode 471, Avg Value Loss: 3.079539524184333, Avg Policy Loss: 3.4126400152842202\n",
      "Episode 471, Reward: -24.378796303518754, Moving Avg Reward: -26.91500949996781, Replay Buffer Size: 17784\n",
      "Current Epsilon: 0.1549480222912372\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 471, Reward: -24.378796303518754, Moving Avg Reward: -26.91500949996781, Replay Buffer Size: 17784\n",
      "Current Epsilon: 0.1549480222912372\n",
      "Episode 472: Won\n",
      "Episode 472, Avg Value Loss: 4.072198731558664, Avg Policy Loss: 3.703167029789516\n",
      "Episode 472, Reward: -23.73313202646964, Moving Avg Reward: -26.60999844078283, Replay Buffer Size: 17791\n",
      "Current Epsilon: 0.15417328217978102\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 472, Reward: -23.73313202646964, Moving Avg Reward: -26.60999844078283, Replay Buffer Size: 17791\n",
      "Current Epsilon: 0.15417328217978102\n",
      "Episode 473: Won\n",
      "Episode 473, Avg Value Loss: 3.2481581514531914, Avg Policy Loss: 3.367171439257535\n",
      "Episode 473, Reward: -28.29260482627449, Moving Avg Reward: -26.620712694536984, Replay Buffer Size: 17802\n",
      "Current Epsilon: 0.1534024157688821\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 473, Reward: -28.29260482627449, Moving Avg Reward: -26.620712694536984, Replay Buffer Size: 17802\n",
      "Current Epsilon: 0.1534024157688821\n",
      "Episode 474: Won\n",
      "Episode 474, Avg Value Loss: 3.3888389666875205, Avg Policy Loss: 3.462644020716349\n",
      "Episode 474, Reward: -24.676294330348647, Moving Avg Reward: -25.302768958840954, Replay Buffer Size: 17814\n",
      "Current Epsilon: 0.1526354036900377\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 474, Reward: -24.676294330348647, Moving Avg Reward: -25.302768958840954, Replay Buffer Size: 17814\n",
      "Current Epsilon: 0.1526354036900377\n",
      "Episode 475: Won\n",
      "Episode 475, Avg Value Loss: 3.4936484903097154, Avg Policy Loss: 3.4009486556053163\n",
      "Episode 475, Reward: -3.3296267832023667, Moving Avg Reward: -25.085890579622514, Replay Buffer Size: 17894\n",
      "Current Epsilon: 0.1518722266715875\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 475, Reward: -3.3296267832023667, Moving Avg Reward: -25.085890579622514, Replay Buffer Size: 17894\n",
      "Current Epsilon: 0.1518722266715875\n",
      "Episode 476: Won\n",
      "Episode 476, Avg Value Loss: 3.2600046396255493, Avg Policy Loss: 3.3841576308012007\n",
      "Episode 476, Reward: -43.34882598588168, Moving Avg Reward: -25.429868975131143, Replay Buffer Size: 17974\n",
      "Current Epsilon: 0.15111286553822956\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 476, Reward: -43.34882598588168, Moving Avg Reward: -25.429868975131143, Replay Buffer Size: 17974\n",
      "Current Epsilon: 0.15111286553822956\n",
      "Episode 477: Draw\n",
      "Episode 477, Avg Value Loss: 3.2578475730759755, Avg Policy Loss: 3.620393991470337\n",
      "Episode 477, Reward: -29.63811763005952, Moving Avg Reward: -25.50844763248353, Replay Buffer Size: 17988\n",
      "Current Epsilon: 0.15035730121053842\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 477, Reward: -29.63811763005952, Moving Avg Reward: -25.50844763248353, Replay Buffer Size: 17988\n",
      "Current Epsilon: 0.15035730121053842\n",
      "Episode 478: Lost\n",
      "Episode 478, Avg Value Loss: 3.5681260988116263, Avg Policy Loss: 3.4027232587337495\n",
      "Episode 478, Reward: -85.18922252184949, Moving Avg Reward: -26.077274912592646, Replay Buffer Size: 18068\n",
      "Current Epsilon: 0.14960551470448571\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 478, Reward: -85.18922252184949, Moving Avg Reward: -26.077274912592646, Replay Buffer Size: 18068\n",
      "Current Epsilon: 0.14960551470448571\n",
      "Episode 479: Lost\n",
      "Episode 479, Avg Value Loss: 3.342448280751705, Avg Policy Loss: 3.4511724770069123\n",
      "Episode 479, Reward: -52.72664348398528, Moving Avg Reward: -26.777034586921527, Replay Buffer Size: 18148\n",
      "Current Epsilon: 0.14885748713096328\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 479, Reward: -52.72664348398528, Moving Avg Reward: -26.777034586921527, Replay Buffer Size: 18148\n",
      "Current Epsilon: 0.14885748713096328\n",
      "Episode 480: Draw\n",
      "Episode 480, Avg Value Loss: 3.0907972388797336, Avg Policy Loss: 3.2145683500501843\n",
      "Episode 480, Reward: -27.474176386858655, Moving Avg Reward: -26.778304457277592, Replay Buffer Size: 18157\n",
      "Current Epsilon: 0.14811319969530845\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 480, Reward: -27.474176386858655, Moving Avg Reward: -26.778304457277592, Replay Buffer Size: 18157\n",
      "Current Epsilon: 0.14811319969530845\n",
      "Episode 481: Lost\n",
      "Episode 481, Avg Value Loss: 2.9106896817684174, Avg Policy Loss: 3.5434619188308716\n",
      "Episode 481, Reward: -26.679498279596423, Moving Avg Reward: -26.761506033790702, Replay Buffer Size: 18169\n",
      "Current Epsilon: 0.1473726336968319\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 481, Reward: -26.679498279596423, Moving Avg Reward: -26.761506033790702, Replay Buffer Size: 18169\n",
      "Current Epsilon: 0.1473726336968319\n",
      "Episode 482: Lost\n",
      "Episode 482, Avg Value Loss: 3.642351821064949, Avg Policy Loss: 3.5164663910865785\n",
      "Episode 482, Reward: -28.5252895214326, Moving Avg Reward: -26.816930154129686, Replay Buffer Size: 18249\n",
      "Current Epsilon: 0.14663577052834775\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 482, Reward: -28.5252895214326, Moving Avg Reward: -26.816930154129686, Replay Buffer Size: 18249\n",
      "Current Epsilon: 0.14663577052834775\n",
      "Episode 483: Lost\n",
      "Episode 483, Avg Value Loss: 3.9768943190574646, Avg Policy Loss: 3.3831256151199343\n",
      "Episode 483, Reward: -30.702528885839907, Moving Avg Reward: -26.83063428144642, Replay Buffer Size: 18259\n",
      "Current Epsilon: 0.14590259167570602\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 483, Reward: -30.702528885839907, Moving Avg Reward: -26.83063428144642, Replay Buffer Size: 18259\n",
      "Current Epsilon: 0.14590259167570602\n",
      "Episode 484: Lost\n",
      "Episode 484, Avg Value Loss: 3.584539743570181, Avg Policy Loss: 3.3565462735983043\n",
      "Episode 484, Reward: -27.763111697772523, Moving Avg Reward: -27.116803384344685, Replay Buffer Size: 18272\n",
      "Current Epsilon: 0.1451730787173275\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 484, Reward: -27.763111697772523, Moving Avg Reward: -27.116803384344685, Replay Buffer Size: 18272\n",
      "Current Epsilon: 0.1451730787173275\n",
      "Episode 485: Lost\n",
      "Episode 485, Avg Value Loss: 3.3408536302282457, Avg Policy Loss: 3.3967232551980526\n",
      "Episode 485, Reward: 18.2755614057631, Moving Avg Reward: -26.99401071401055, Replay Buffer Size: 18319\n",
      "Current Epsilon: 0.14444721332374086\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 485, Reward: 18.2755614057631, Moving Avg Reward: -26.99401071401055, Replay Buffer Size: 18319\n",
      "Current Epsilon: 0.14444721332374086\n",
      "Episode 486: Won\n",
      "Episode 486, Avg Value Loss: 3.5816908933222296, Avg Policy Loss: 3.3638668775558473\n",
      "Episode 486, Reward: -2.809693478964726, Moving Avg Reward: -26.850150486896723, Replay Buffer Size: 18399\n",
      "Current Epsilon: 0.14372497725712216\n",
      "Loss rate over the last 150 episodes: 0.60\n",
      "Episode 486, Reward: -2.809693478964726, Moving Avg Reward: -26.850150486896723, Replay Buffer Size: 18399\n",
      "Current Epsilon: 0.14372497725712216\n",
      "Episode 487: Won\n",
      "Episode 487, Avg Value Loss: 3.162285193800926, Avg Policy Loss: 3.475793331861496\n",
      "Episode 487, Reward: -21.65063061667238, Moving Avg Reward: -26.730726785905212, Replay Buffer Size: 18407\n",
      "Current Epsilon: 0.14300635237083656\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 487, Reward: -21.65063061667238, Moving Avg Reward: -26.730726785905212, Replay Buffer Size: 18407\n",
      "Current Epsilon: 0.14300635237083656\n",
      "Episode 488: Won\n",
      "Episode 488, Avg Value Loss: 3.174294942954801, Avg Policy Loss: 3.4263542328240737\n",
      "Episode 488, Reward: -11.408130664719087, Moving Avg Reward: -26.59460161451999, Replay Buffer Size: 18460\n",
      "Current Epsilon: 0.14229132060898236\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 488, Reward: -11.408130664719087, Moving Avg Reward: -26.59460161451999, Replay Buffer Size: 18460\n",
      "Current Epsilon: 0.14229132060898236\n",
      "Episode 489: Won\n",
      "Episode 489, Avg Value Loss: 3.4815825260046758, Avg Policy Loss: 3.478183796911529\n",
      "Episode 489, Reward: -82.13483009103948, Moving Avg Reward: -27.442720163249877, Replay Buffer Size: 18526\n",
      "Current Epsilon: 0.14157986400593744\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 489, Reward: -82.13483009103948, Moving Avg Reward: -27.442720163249877, Replay Buffer Size: 18526\n",
      "Current Epsilon: 0.14157986400593744\n",
      "Episode 490: Won\n",
      "Episode 490, Avg Value Loss: 3.5629188613966107, Avg Policy Loss: 3.409491464495659\n",
      "Episode 490, Reward: 7.6319263457276785, Moving Avg Reward: -27.106563432694593, Replay Buffer Size: 18590\n",
      "Current Epsilon: 0.14087196468590776\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 490, Reward: 7.6319263457276785, Moving Avg Reward: -27.106563432694593, Replay Buffer Size: 18590\n",
      "Current Epsilon: 0.14087196468590776\n",
      "Episode 491: Won\n",
      "Episode 491, Avg Value Loss: 3.8894363183241625, Avg Policy Loss: 3.446757720066951\n",
      "Episode 491, Reward: -33.096705036612775, Moving Avg Reward: -26.23335826265752, Replay Buffer Size: 18603\n",
      "Current Epsilon: 0.14016760486247823\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 491, Reward: -33.096705036612775, Moving Avg Reward: -26.23335826265752, Replay Buffer Size: 18603\n",
      "Current Epsilon: 0.14016760486247823\n",
      "Episode 492: Won\n",
      "Episode 492, Avg Value Loss: 3.1583170130848885, Avg Policy Loss: 3.503526446223259\n",
      "Episode 492, Reward: -43.962819582712, Moving Avg Reward: -26.43855065334274, Replay Buffer Size: 18683\n",
      "Current Epsilon: 0.13946676683816583\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 492, Reward: -43.962819582712, Moving Avg Reward: -26.43855065334274, Replay Buffer Size: 18683\n",
      "Current Epsilon: 0.13946676683816583\n",
      "Episode 493: Won\n",
      "Episode 493, Avg Value Loss: 3.156865322589874, Avg Policy Loss: 3.305491399765015\n",
      "Episode 493, Reward: -27.624065549199024, Moving Avg Reward: -26.303160125495687, Replay Buffer Size: 18693\n",
      "Current Epsilon: 0.138769433003975\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 493, Reward: -27.624065549199024, Moving Avg Reward: -26.303160125495687, Replay Buffer Size: 18693\n",
      "Current Epsilon: 0.138769433003975\n",
      "Episode 494: Won\n",
      "Episode 494, Avg Value Loss: 3.3898628801107407, Avg Policy Loss: 3.3373245298862457\n",
      "Episode 494, Reward: -24.82843144729107, Moving Avg Reward: -26.324637932878233, Replay Buffer Size: 18701\n",
      "Current Epsilon: 0.13807558583895513\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 494, Reward: -24.82843144729107, Moving Avg Reward: -26.324637932878233, Replay Buffer Size: 18701\n",
      "Current Epsilon: 0.13807558583895513\n",
      "Episode 495: Won\n",
      "Episode 495, Avg Value Loss: 3.610804537932078, Avg Policy Loss: 3.393653154373169\n",
      "Episode 495, Reward: -26.555165368036636, Moving Avg Reward: -26.316809049425046, Replay Buffer Size: 18713\n",
      "Current Epsilon: 0.13738520790976036\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 495, Reward: -26.555165368036636, Moving Avg Reward: -26.316809049425046, Replay Buffer Size: 18713\n",
      "Current Epsilon: 0.13738520790976036\n",
      "Episode 496: Won\n",
      "Episode 496, Avg Value Loss: 3.4394428499042986, Avg Policy Loss: 3.3688888400793076\n",
      "Episode 496, Reward: -102.63569744581231, Moving Avg Reward: -27.072813792408883, Replay Buffer Size: 18793\n",
      "Current Epsilon: 0.13669828187021155\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 496, Reward: -102.63569744581231, Moving Avg Reward: -27.072813792408883, Replay Buffer Size: 18793\n",
      "Current Epsilon: 0.13669828187021155\n",
      "Episode 497: Lost\n",
      "Episode 497, Avg Value Loss: 3.384974093735218, Avg Policy Loss: 3.378677576780319\n",
      "Episode 497, Reward: -14.228924865166539, Moving Avg Reward: -26.96279214781543, Replay Buffer Size: 18873\n",
      "Current Epsilon: 0.13601479046086049\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 497, Reward: -14.228924865166539, Moving Avg Reward: -26.96279214781543, Replay Buffer Size: 18873\n",
      "Current Epsilon: 0.13601479046086049\n",
      "Episode 498: Draw\n",
      "Episode 498, Avg Value Loss: 3.664023082703352, Avg Policy Loss: 3.304149642586708\n",
      "Episode 498, Reward: -32.82153999371716, Moving Avg Reward: -26.825707222205516, Replay Buffer Size: 18889\n",
      "Current Epsilon: 0.1353347165085562\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 498, Reward: -32.82153999371716, Moving Avg Reward: -26.825707222205516, Replay Buffer Size: 18889\n",
      "Current Epsilon: 0.1353347165085562\n",
      "Episode 499: Lost\n",
      "Episode 499, Avg Value Loss: 3.0229439368614783, Avg Policy Loss: 3.4950434244596043\n",
      "Episode 499, Reward: -34.46050401670281, Moving Avg Reward: -26.7287151431772, Replay Buffer Size: 18902\n",
      "Current Epsilon: 0.1346580429260134\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 499, Reward: -34.46050401670281, Moving Avg Reward: -26.7287151431772, Replay Buffer Size: 18902\n",
      "Current Epsilon: 0.1346580429260134\n",
      "Episode 500: Lost\n",
      "Episode 500, Avg Value Loss: 3.580353157464848, Avg Policy Loss: 3.4466036530428155\n",
      "Episode 500, Reward: -26.496321530479563, Moving Avg Reward: -26.270509473413895, Replay Buffer Size: 18945\n",
      "Current Epsilon: 0.13398475271138335\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 500, Reward: -26.496321530479563, Moving Avg Reward: -26.270509473413895, Replay Buffer Size: 18945\n",
      "Current Epsilon: 0.13398475271138335\n",
      "Episode 501: Lost\n",
      "Episode 501, Avg Value Loss: 3.728809069097042, Avg Policy Loss: 3.2715054035186766\n",
      "Episode 501, Reward: -56.71463987360817, Moving Avg Reward: -26.98267266939024, Replay Buffer Size: 19025\n",
      "Current Epsilon: 0.13331482894782642\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 501, Reward: -56.71463987360817, Moving Avg Reward: -26.98267266939024, Replay Buffer Size: 19025\n",
      "Current Epsilon: 0.13331482894782642\n",
      "Episode 502: Lost\n",
      "Episode 502, Avg Value Loss: 3.1886203207075594, Avg Policy Loss: 3.3893753349781037\n",
      "Episode 502, Reward: 1.785569090682586, Moving Avg Reward: -26.82734993244009, Replay Buffer Size: 19105\n",
      "Current Epsilon: 0.13264825480308728\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 502, Reward: 1.785569090682586, Moving Avg Reward: -26.82734993244009, Replay Buffer Size: 19105\n",
      "Current Epsilon: 0.13264825480308728\n",
      "Episode 503: Draw\n",
      "Episode 503, Avg Value Loss: 3.2175684911864146, Avg Policy Loss: 3.458858166422163\n",
      "Episode 503, Reward: -31.9031691946606, Moving Avg Reward: -27.078749487530267, Replay Buffer Size: 19119\n",
      "Current Epsilon: 0.13198501352907185\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 503, Reward: -31.9031691946606, Moving Avg Reward: -27.078749487530267, Replay Buffer Size: 19119\n",
      "Current Epsilon: 0.13198501352907185\n",
      "Episode 504: Lost\n",
      "Episode 504, Avg Value Loss: 3.4000738208944146, Avg Policy Loss: 3.508703491904519\n",
      "Episode 504, Reward: -33.096651198879655, Moving Avg Reward: -27.118630812763644, Replay Buffer Size: 19130\n",
      "Current Epsilon: 0.1313250884614265\n",
      "Loss rate over the last 150 episodes: 0.66\n",
      "Episode 504, Reward: -33.096651198879655, Moving Avg Reward: -27.118630812763644, Replay Buffer Size: 19130\n",
      "Current Epsilon: 0.1313250884614265\n",
      "Episode 505: Lost\n",
      "Episode 505, Avg Value Loss: 3.420952469110489, Avg Policy Loss: 3.4571519523859022\n",
      "Episode 505, Reward: -86.51494880339388, Moving Avg Reward: -27.693778936535665, Replay Buffer Size: 19210\n",
      "Current Epsilon: 0.13066846301911936\n",
      "Loss rate over the last 150 episodes: 0.66\n",
      "Episode 505, Reward: -86.51494880339388, Moving Avg Reward: -27.693778936535665, Replay Buffer Size: 19210\n",
      "Current Epsilon: 0.13066846301911936\n",
      "Episode 506: Lost\n",
      "Episode 506, Avg Value Loss: 3.7264852957292036, Avg Policy Loss: 3.4571316892450508\n",
      "Episode 506, Reward: -23.54824202303459, Moving Avg Reward: -27.693194264093954, Replay Buffer Size: 19221\n",
      "Current Epsilon: 0.13001512070402377\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 506, Reward: -23.54824202303459, Moving Avg Reward: -27.693194264093954, Replay Buffer Size: 19221\n",
      "Current Epsilon: 0.13001512070402377\n",
      "Episode 507: Lost\n",
      "Episode 507, Avg Value Loss: 3.5212837934494017, Avg Policy Loss: 3.3897837162017823\n",
      "Episode 507, Reward: -23.679285020835053, Moving Avg Reward: -28.12195811292983, Replay Buffer Size: 19231\n",
      "Current Epsilon: 0.12936504510050365\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 507, Reward: -23.679285020835053, Moving Avg Reward: -28.12195811292983, Replay Buffer Size: 19231\n",
      "Current Epsilon: 0.12936504510050365\n",
      "Episode 508: Lost\n",
      "Episode 508, Avg Value Loss: 3.798151284456253, Avg Policy Loss: 3.440755844116211\n",
      "Episode 508, Reward: -34.8378185518867, Moving Avg Reward: -28.216753881881942, Replay Buffer Size: 19243\n",
      "Current Epsilon: 0.12871821987500112\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 508, Reward: -34.8378185518867, Moving Avg Reward: -28.216753881881942, Replay Buffer Size: 19243\n",
      "Current Epsilon: 0.12871821987500112\n",
      "Episode 509: Lost\n",
      "Episode 509, Avg Value Loss: 3.3441391449708204, Avg Policy Loss: 3.5018337139716516\n",
      "Episode 509, Reward: -29.962708959182855, Moving Avg Reward: -28.29947339555523, Replay Buffer Size: 19256\n",
      "Current Epsilon: 0.12807462877562611\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 509, Reward: -29.962708959182855, Moving Avg Reward: -28.29947339555523, Replay Buffer Size: 19256\n",
      "Current Epsilon: 0.12807462877562611\n",
      "Episode 510: Lost\n",
      "Episode 510, Avg Value Loss: 3.4421413938204446, Avg Policy Loss: 3.3346656958262124\n",
      "Episode 510, Reward: -28.79765043672327, Moving Avg Reward: -28.036896058949388, Replay Buffer Size: 19265\n",
      "Current Epsilon: 0.12743425563174798\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 510, Reward: -28.79765043672327, Moving Avg Reward: -28.036896058949388, Replay Buffer Size: 19265\n",
      "Current Epsilon: 0.12743425563174798\n",
      "Episode 511: Lost\n",
      "Episode 511, Avg Value Loss: 3.2224806688725947, Avg Policy Loss: 3.4686019271612167\n",
      "Episode 511, Reward: -115.2938579824167, Moving Avg Reward: -29.193764244934677, Replay Buffer Size: 19345\n",
      "Current Epsilon: 0.12679708435358925\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 511, Reward: -115.2938579824167, Moving Avg Reward: -29.193764244934677, Replay Buffer Size: 19345\n",
      "Current Epsilon: 0.12679708435358925\n",
      "Episode 512: Lost\n",
      "Episode 512, Avg Value Loss: 3.2928161710500716, Avg Policy Loss: 3.440730205178261\n",
      "Episode 512, Reward: 5.523913368713777, Moving Avg Reward: -29.237025111247537, Replay Buffer Size: 19425\n",
      "Current Epsilon: 0.1261630989318213\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 512, Reward: 5.523913368713777, Moving Avg Reward: -29.237025111247537, Replay Buffer Size: 19425\n",
      "Current Epsilon: 0.1261630989318213\n",
      "Episode 513: Draw\n",
      "Episode 513, Avg Value Loss: 3.917851686477661, Avg Policy Loss: 3.4216445485750833\n",
      "Episode 513, Reward: -32.19437987147519, Moving Avg Reward: -29.613913907149126, Replay Buffer Size: 19437\n",
      "Current Epsilon: 0.1255322834371622\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 513, Reward: -32.19437987147519, Moving Avg Reward: -29.613913907149126, Replay Buffer Size: 19437\n",
      "Current Epsilon: 0.1255322834371622\n",
      "Episode 514: Lost\n",
      "Episode 514, Avg Value Loss: 3.63949064463377, Avg Policy Loss: 3.464039146900177\n",
      "Episode 514, Reward: -108.48936683487197, Moving Avg Reward: -30.392431545858212, Replay Buffer Size: 19517\n",
      "Current Epsilon: 0.12490462201997637\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 514, Reward: -108.48936683487197, Moving Avg Reward: -30.392431545858212, Replay Buffer Size: 19517\n",
      "Current Epsilon: 0.12490462201997637\n",
      "Episode 515: Lost\n",
      "Episode 515, Avg Value Loss: 3.3351477895464217, Avg Policy Loss: 3.4187258992876326\n",
      "Episode 515, Reward: 18.940546434198325, Moving Avg Reward: -29.90213417418654, Replay Buffer Size: 19538\n",
      "Current Epsilon: 0.1242800989098765\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 515, Reward: 18.940546434198325, Moving Avg Reward: -29.90213417418654, Replay Buffer Size: 19538\n",
      "Current Epsilon: 0.1242800989098765\n",
      "Episode 516: Won\n",
      "Episode 516, Avg Value Loss: 3.1919575184583664, Avg Policy Loss: 3.8368895053863525\n",
      "Episode 516, Reward: -26.40774677682212, Moving Avg Reward: -30.341792762721347, Replay Buffer Size: 19546\n",
      "Current Epsilon: 0.12365869841532712\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 516, Reward: -26.40774677682212, Moving Avg Reward: -30.341792762721347, Replay Buffer Size: 19546\n",
      "Current Epsilon: 0.12365869841532712\n",
      "Episode 517: Won\n",
      "Episode 517, Avg Value Loss: 3.8038553432984785, Avg Policy Loss: 3.3094849586486816\n",
      "Episode 517, Reward: -32.71641719563851, Moving Avg Reward: -30.1589696462418, Replay Buffer Size: 19557\n",
      "Current Epsilon: 0.12304040492325048\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 517, Reward: -32.71641719563851, Moving Avg Reward: -30.1589696462418, Replay Buffer Size: 19557\n",
      "Current Epsilon: 0.12304040492325048\n",
      "Episode 518: Won\n",
      "Episode 518, Avg Value Loss: 3.2460721269249917, Avg Policy Loss: 3.423951095342636\n",
      "Episode 518, Reward: -34.09913649168558, Moving Avg Reward: -29.86082215176747, Replay Buffer Size: 19637\n",
      "Current Epsilon: 0.12242520289863423\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 518, Reward: -34.09913649168558, Moving Avg Reward: -29.86082215176747, Replay Buffer Size: 19637\n",
      "Current Epsilon: 0.12242520289863423\n",
      "Episode 519: Won\n",
      "Episode 519, Avg Value Loss: 3.912926024860806, Avg Policy Loss: 3.3936991691589355\n",
      "Episode 519, Reward: -20.70488653365009, Moving Avg Reward: -29.78582626594628, Replay Buffer Size: 19646\n",
      "Current Epsilon: 0.12181307688414106\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 519, Reward: -20.70488653365009, Moving Avg Reward: -29.78582626594628, Replay Buffer Size: 19646\n",
      "Current Epsilon: 0.12181307688414106\n",
      "Episode 520: Won\n",
      "Episode 520, Avg Value Loss: 3.6785627007484436, Avg Policy Loss: 3.5473912954330444\n",
      "Episode 520, Reward: -8.921921803534891, Moving Avg Reward: -29.659984080439372, Replay Buffer Size: 19654\n",
      "Current Epsilon: 0.12120401149972035\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 520, Reward: -8.921921803534891, Moving Avg Reward: -29.659984080439372, Replay Buffer Size: 19654\n",
      "Current Epsilon: 0.12120401149972035\n",
      "Episode 521: Won\n",
      "Episode 521, Avg Value Loss: 3.59945450109594, Avg Policy Loss: 3.546856908237233\n",
      "Episode 521, Reward: 9.147986270532277, Moving Avg Reward: -29.270982253486505, Replay Buffer Size: 19671\n",
      "Current Epsilon: 0.12059799144222175\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 521, Reward: 9.147986270532277, Moving Avg Reward: -29.270982253486505, Replay Buffer Size: 19671\n",
      "Current Epsilon: 0.12059799144222175\n",
      "Episode 522: Won\n",
      "Episode 522, Avg Value Loss: 2.8358359228480947, Avg Policy Loss: 3.6315544952045786\n",
      "Episode 522, Reward: -25.519692857791036, Moving Avg Reward: -28.928739337072713, Replay Buffer Size: 19682\n",
      "Current Epsilon: 0.11999500148501063\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 522, Reward: -25.519692857791036, Moving Avg Reward: -28.928739337072713, Replay Buffer Size: 19682\n",
      "Current Epsilon: 0.11999500148501063\n",
      "Episode 523: Won\n",
      "Episode 523, Avg Value Loss: 3.1320632696151733, Avg Policy Loss: 3.3820008260232433\n",
      "Episode 523, Reward: 15.455499579291299, Moving Avg Reward: -28.479571990820133, Replay Buffer Size: 19709\n",
      "Current Epsilon: 0.11939502647758558\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 523, Reward: 15.455499579291299, Moving Avg Reward: -28.479571990820133, Replay Buffer Size: 19709\n",
      "Current Epsilon: 0.11939502647758558\n",
      "Episode 524: Won\n",
      "Episode 524, Avg Value Loss: 3.3740978807210924, Avg Policy Loss: 3.4106374025344848\n",
      "Episode 524, Reward: -0.8000000000000005, Moving Avg Reward: -28.17340552384543, Replay Buffer Size: 19789\n",
      "Current Epsilon: 0.11879805134519765\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 524, Reward: -0.8000000000000005, Moving Avg Reward: -28.17340552384543, Replay Buffer Size: 19789\n",
      "Current Epsilon: 0.11879805134519765\n",
      "Episode 525: Won\n",
      "Episode 525, Avg Value Loss: 3.311729534715414, Avg Policy Loss: 3.4227086812257768\n",
      "Episode 525, Reward: -31.56262570586701, Moving Avg Reward: -27.975792394264776, Replay Buffer Size: 19869\n",
      "Current Epsilon: 0.11820406108847166\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 525, Reward: -31.56262570586701, Moving Avg Reward: -27.975792394264776, Replay Buffer Size: 19869\n",
      "Current Epsilon: 0.11820406108847166\n",
      "Episode 526: Draw\n",
      "Episode 526, Avg Value Loss: 3.4185196862501255, Avg Policy Loss: 3.5097405489753273\n",
      "Episode 526, Reward: 10.035034105155258, Moving Avg Reward: -27.43534490008005, Replay Buffer Size: 19937\n",
      "Current Epsilon: 0.1176130407830293\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 526, Reward: 10.035034105155258, Moving Avg Reward: -27.43534490008005, Replay Buffer Size: 19937\n",
      "Current Epsilon: 0.1176130407830293\n",
      "Episode 527: Won\n",
      "Episode 527, Avg Value Loss: 3.5193855497572155, Avg Policy Loss: 3.1986725330352783\n",
      "Episode 527, Reward: -33.99733667336813, Moving Avg Reward: -27.820110519763077, Replay Buffer Size: 19946\n",
      "Current Epsilon: 0.11702497557911415\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 527, Reward: -33.99733667336813, Moving Avg Reward: -27.820110519763077, Replay Buffer Size: 19946\n",
      "Current Epsilon: 0.11702497557911415\n",
      "Episode 528: Won\n",
      "Episode 528, Avg Value Loss: 3.226306886996253, Avg Policy Loss: 3.5142736030837236\n",
      "Episode 528, Reward: -22.83481546310702, Moving Avg Reward: -28.134810717494815, Replay Buffer Size: 20005\n",
      "Current Epsilon: 0.11643985070121858\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 528, Reward: -22.83481546310702, Moving Avg Reward: -28.134810717494815, Replay Buffer Size: 20005\n",
      "Current Epsilon: 0.11643985070121858\n",
      "Episode 529: Won\n",
      "Episode 529, Avg Value Loss: 3.8279086450735726, Avg Policy Loss: 3.616246223449707\n",
      "Episode 529, Reward: -35.383609857658044, Moving Avg Reward: -28.202228065120476, Replay Buffer Size: 20017\n",
      "Current Epsilon: 0.11585765144771248\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 529, Reward: -35.383609857658044, Moving Avg Reward: -28.202228065120476, Replay Buffer Size: 20017\n",
      "Current Epsilon: 0.11585765144771248\n",
      "Episode 530: Won\n",
      "Episode 530, Avg Value Loss: 3.231128752231598, Avg Policy Loss: 3.5628206928571067\n",
      "Episode 530, Reward: -28.127687159271836, Moving Avg Reward: -28.221446883853574, Replay Buffer Size: 20029\n",
      "Current Epsilon: 0.11527836319047392\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 530, Reward: -28.127687159271836, Moving Avg Reward: -28.221446883853574, Replay Buffer Size: 20029\n",
      "Current Epsilon: 0.11527836319047392\n",
      "Episode 531: Won\n",
      "Episode 531, Avg Value Loss: 3.3217474102973936, Avg Policy Loss: 3.3801304578781126\n",
      "Episode 531, Reward: 19.136935422498933, Moving Avg Reward: -27.767121216669437, Replay Buffer Size: 20049\n",
      "Current Epsilon: 0.11470197137452155\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 531, Reward: 19.136935422498933, Moving Avg Reward: -27.767121216669437, Replay Buffer Size: 20049\n",
      "Current Epsilon: 0.11470197137452155\n",
      "Episode 532: Won\n",
      "Episode 532, Avg Value Loss: 4.2663781472614835, Avg Policy Loss: 3.1895525796072826\n",
      "Episode 532, Reward: -32.398680167318886, Moving Avg Reward: -27.76280619677581, Replay Buffer Size: 20063\n",
      "Current Epsilon: 0.11412846151764894\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 532, Reward: -32.398680167318886, Moving Avg Reward: -27.76280619677581, Replay Buffer Size: 20063\n",
      "Current Epsilon: 0.11412846151764894\n",
      "Episode 533: Won\n",
      "Episode 533, Avg Value Loss: 3.59379518224347, Avg Policy Loss: 3.4662620828997706\n",
      "Episode 533, Reward: -48.82523032708116, Moving Avg Reward: -27.40125125847221, Replay Buffer Size: 20125\n",
      "Current Epsilon: 0.1135578192100607\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 533, Reward: -48.82523032708116, Moving Avg Reward: -27.40125125847221, Replay Buffer Size: 20125\n",
      "Current Epsilon: 0.1135578192100607\n",
      "Episode 534: Won\n",
      "Episode 534, Avg Value Loss: 3.467306435108185, Avg Policy Loss: 3.4527889251708985\n",
      "Episode 534, Reward: -45.394600625164045, Moving Avg Reward: -27.59418437782404, Replay Buffer Size: 20205\n",
      "Current Epsilon: 0.11299003011401039\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 534, Reward: -45.394600625164045, Moving Avg Reward: -27.59418437782404, Replay Buffer Size: 20205\n",
      "Current Epsilon: 0.11299003011401039\n",
      "Episode 535: Lost\n",
      "Episode 535, Avg Value Loss: 3.614091145992279, Avg Policy Loss: 3.399128884077072\n",
      "Episode 535, Reward: -34.38856283298759, Moving Avg Reward: -27.465816882351064, Replay Buffer Size: 20285\n",
      "Current Epsilon: 0.11242507996344034\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 535, Reward: -34.38856283298759, Moving Avg Reward: -27.465816882351064, Replay Buffer Size: 20285\n",
      "Current Epsilon: 0.11242507996344034\n",
      "Episode 536: Draw\n",
      "Episode 536, Avg Value Loss: 3.7295611792919683, Avg Policy Loss: 3.473693810257257\n",
      "Episode 536, Reward: 10.994768555563871, Moving Avg Reward: -27.17046429069074, Replay Buffer Size: 20336\n",
      "Current Epsilon: 0.11186295456362313\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 536, Reward: 10.994768555563871, Moving Avg Reward: -27.17046429069074, Replay Buffer Size: 20336\n",
      "Current Epsilon: 0.11186295456362313\n",
      "Episode 537: Won\n",
      "Episode 537, Avg Value Loss: 2.792857418457667, Avg Policy Loss: 3.5854172507921853\n",
      "Episode 537, Reward: -22.947621186504293, Moving Avg Reward: -27.484214495212754, Replay Buffer Size: 20348\n",
      "Current Epsilon: 0.11130363979080501\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 537, Reward: -22.947621186504293, Moving Avg Reward: -27.484214495212754, Replay Buffer Size: 20348\n",
      "Current Epsilon: 0.11130363979080501\n",
      "Episode 538: Won\n",
      "Episode 538, Avg Value Loss: 3.230657775523299, Avg Policy Loss: 3.3152874364691267\n",
      "Episode 538, Reward: -32.83606536161252, Moving Avg Reward: -27.574196635036124, Replay Buffer Size: 20407\n",
      "Current Epsilon: 0.11074712159185099\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 538, Reward: -32.83606536161252, Moving Avg Reward: -27.574196635036124, Replay Buffer Size: 20407\n",
      "Current Epsilon: 0.11074712159185099\n",
      "Episode 539: Won\n",
      "Episode 539, Avg Value Loss: 3.1443125188350676, Avg Policy Loss: 3.4834383845329286\n",
      "Episode 539, Reward: 0.17324978589737605, Moving Avg Reward: -27.367255913597788, Replay Buffer Size: 20487\n",
      "Current Epsilon: 0.11019338598389174\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 539, Reward: 0.17324978589737605, Moving Avg Reward: -27.367255913597788, Replay Buffer Size: 20487\n",
      "Current Epsilon: 0.11019338598389174\n",
      "Episode 540: Lost\n",
      "Episode 540, Avg Value Loss: 3.1531095821410418, Avg Policy Loss: 3.400551550090313\n",
      "Episode 540, Reward: -9.135818414188234, Moving Avg Reward: -26.644843484702744, Replay Buffer Size: 20519\n",
      "Current Epsilon: 0.10964241905397228\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 540, Reward: -9.135818414188234, Moving Avg Reward: -26.644843484702744, Replay Buffer Size: 20519\n",
      "Current Epsilon: 0.10964241905397228\n",
      "Episode 541: Won\n",
      "Episode 541, Avg Value Loss: 3.8899647822746863, Avg Policy Loss: 3.435554155936608\n",
      "Episode 541, Reward: -25.872759226300644, Moving Avg Reward: -26.865860961385383, Replay Buffer Size: 20532\n",
      "Current Epsilon: 0.10909420695870241\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 541, Reward: -25.872759226300644, Moving Avg Reward: -26.865860961385383, Replay Buffer Size: 20532\n",
      "Current Epsilon: 0.10909420695870241\n",
      "Episode 542: Won\n",
      "Episode 542, Avg Value Loss: 3.17106650092385, Avg Policy Loss: 3.682215929031372\n",
      "Episode 542, Reward: -25.247585469985495, Moving Avg Reward: -27.047469427092047, Replay Buffer Size: 20543\n",
      "Current Epsilon: 0.1085487359239089\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 542, Reward: -25.247585469985495, Moving Avg Reward: -27.047469427092047, Replay Buffer Size: 20543\n",
      "Current Epsilon: 0.1085487359239089\n",
      "Episode 543: Won\n",
      "Episode 543, Avg Value Loss: 3.2288539019914775, Avg Policy Loss: 3.3758710072590756\n",
      "Episode 543, Reward: 13.355188296537744, Moving Avg Reward: -26.94819088820127, Replay Buffer Size: 20569\n",
      "Current Epsilon: 0.10800599224428936\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 543, Reward: 13.355188296537744, Moving Avg Reward: -26.94819088820127, Replay Buffer Size: 20569\n",
      "Current Epsilon: 0.10800599224428936\n",
      "Episode 544: Won\n",
      "Episode 544, Avg Value Loss: 3.5476621985435486, Avg Policy Loss: 3.4457921028137206\n",
      "Episode 544, Reward: -28.300371961425554, Moving Avg Reward: -27.331094607815526, Replay Buffer Size: 20579\n",
      "Current Epsilon: 0.10746596228306791\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 544, Reward: -28.300371961425554, Moving Avg Reward: -27.331094607815526, Replay Buffer Size: 20579\n",
      "Current Epsilon: 0.10746596228306791\n",
      "Episode 545: Won\n",
      "Episode 545, Avg Value Loss: 3.521462915837765, Avg Policy Loss: 3.4519900500774385\n",
      "Episode 545, Reward: 2.0890465894987464, Moving Avg Reward: -26.72644749044027, Replay Buffer Size: 20659\n",
      "Current Epsilon: 0.10692863247165257\n",
      "Loss rate over the last 150 episodes: 0.60\n",
      "Episode 545, Reward: 2.0890465894987464, Moving Avg Reward: -26.72644749044027, Replay Buffer Size: 20659\n",
      "Current Epsilon: 0.10692863247165257\n",
      "Episode 546: Won\n",
      "Episode 546, Avg Value Loss: 3.3301067361464867, Avg Policy Loss: 3.445980057349572\n",
      "Episode 546, Reward: -43.8664427622233, Moving Avg Reward: -27.148833848468485, Replay Buffer Size: 20724\n",
      "Current Epsilon: 0.1063939893092943\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 546, Reward: -43.8664427622233, Moving Avg Reward: -27.148833848468485, Replay Buffer Size: 20724\n",
      "Current Epsilon: 0.1063939893092943\n",
      "Episode 547: Lost\n",
      "Episode 547, Avg Value Loss: 3.7354226793561662, Avg Policy Loss: 3.175157036100115\n",
      "Episode 547, Reward: -21.759753335143373, Moving Avg Reward: -26.922427173019315, Replay Buffer Size: 20731\n",
      "Current Epsilon: 0.10586201936274783\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 547, Reward: -21.759753335143373, Moving Avg Reward: -26.922427173019315, Replay Buffer Size: 20731\n",
      "Current Epsilon: 0.10586201936274783\n",
      "Episode 548: Lost\n",
      "Episode 548, Avg Value Loss: 3.744040122628212, Avg Policy Loss: 3.504448938369751\n",
      "Episode 548, Reward: 4.828290560538932, Moving Avg Reward: -27.0463203250073, Replay Buffer Size: 20811\n",
      "Current Epsilon: 0.10533270926593409\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 548, Reward: 4.828290560538932, Moving Avg Reward: -27.0463203250073, Replay Buffer Size: 20811\n",
      "Current Epsilon: 0.10533270926593409\n",
      "Episode 549: Lost\n",
      "Episode 549, Avg Value Loss: 3.2310245358026943, Avg Policy Loss: 3.378610821870657\n",
      "Episode 549, Reward: 18.270826079240607, Moving Avg Reward: -25.812165999976564, Replay Buffer Size: 20837\n",
      "Current Epsilon: 0.10480604571960442\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 549, Reward: 18.270826079240607, Moving Avg Reward: -25.812165999976564, Replay Buffer Size: 20837\n",
      "Current Epsilon: 0.10480604571960442\n",
      "Episode 550: Won\n",
      "Episode 550, Avg Value Loss: 3.402303810119629, Avg Policy Loss: 3.455041561126709\n",
      "Episode 550, Reward: -40.6712708598999, Moving Avg Reward: -26.033696491261207, Replay Buffer Size: 20862\n",
      "Current Epsilon: 0.1042820154910064\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 550, Reward: -40.6712708598999, Moving Avg Reward: -26.033696491261207, Replay Buffer Size: 20862\n",
      "Current Epsilon: 0.1042820154910064\n",
      "Episode 551: Won\n",
      "Episode 551, Avg Value Loss: 3.455862532962452, Avg Policy Loss: 3.614579764279452\n",
      "Episode 551, Reward: -31.803139051749568, Moving Avg Reward: -26.088873362502937, Replay Buffer Size: 20873\n",
      "Current Epsilon: 0.10376060541355137\n",
      "Loss rate over the last 150 episodes: 0.60\n",
      "Episode 551, Reward: -31.803139051749568, Moving Avg Reward: -26.088873362502937, Replay Buffer Size: 20873\n",
      "Current Epsilon: 0.10376060541355137\n",
      "Episode 552: Won\n",
      "Episode 552, Avg Value Loss: 3.4597216195919933, Avg Policy Loss: 3.4018340952256145\n",
      "Episode 552, Reward: -37.801302008484875, Moving Avg Reward: -26.17628972198991, Replay Buffer Size: 20941\n",
      "Current Epsilon: 0.1032418023864836\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 552, Reward: -37.801302008484875, Moving Avg Reward: -26.17628972198991, Replay Buffer Size: 20941\n",
      "Current Epsilon: 0.1032418023864836\n",
      "Episode 553: Won\n",
      "Episode 553, Avg Value Loss: 3.402234123647213, Avg Policy Loss: 3.5242664724588395\n",
      "Episode 553, Reward: 4.534178440146772, Moving Avg Reward: -25.84545279796103, Replay Buffer Size: 21021\n",
      "Current Epsilon: 0.10272559337455119\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 553, Reward: 4.534178440146772, Moving Avg Reward: -25.84545279796103, Replay Buffer Size: 21021\n",
      "Current Epsilon: 0.10272559337455119\n",
      "Episode 554: Won\n",
      "Episode 554, Avg Value Loss: 3.287287402153015, Avg Policy Loss: 3.5327375888824464\n",
      "Episode 554, Reward: -7.30391881665256, Moving Avg Reward: -25.377278090925593, Replay Buffer Size: 21101\n",
      "Current Epsilon: 0.10221196540767843\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 554, Reward: -7.30391881665256, Moving Avg Reward: -25.377278090925593, Replay Buffer Size: 21101\n",
      "Current Epsilon: 0.10221196540767843\n",
      "Episode 555: Draw\n",
      "Episode 555, Avg Value Loss: 3.3099880635738375, Avg Policy Loss: 3.4186176538467405\n",
      "Episode 555, Reward: 17.179095500953476, Moving Avg Reward: -25.356122607270162, Replay Buffer Size: 21121\n",
      "Current Epsilon: 0.10170090558064004\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 555, Reward: 17.179095500953476, Moving Avg Reward: -25.356122607270162, Replay Buffer Size: 21121\n",
      "Current Epsilon: 0.10170090558064004\n",
      "Episode 556: Won\n",
      "Episode 556, Avg Value Loss: 3.3484430611133575, Avg Policy Loss: 3.6272870898246765\n",
      "Episode 556, Reward: -29.31493005711168, Moving Avg Reward: -25.055370371363768, Replay Buffer Size: 21129\n",
      "Current Epsilon: 0.10119240105273684\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 556, Reward: -29.31493005711168, Moving Avg Reward: -25.055370371363768, Replay Buffer Size: 21129\n",
      "Current Epsilon: 0.10119240105273684\n",
      "Episode 557: Won\n",
      "Episode 557, Avg Value Loss: 3.6155133828332153, Avg Policy Loss: 3.459694421744045\n",
      "Episode 557, Reward: -7.8880896280203014, Moving Avg Reward: -25.221625910479244, Replay Buffer Size: 21208\n",
      "Current Epsilon: 0.10068643904747315\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 557, Reward: -7.8880896280203014, Moving Avg Reward: -25.221625910479244, Replay Buffer Size: 21208\n",
      "Current Epsilon: 0.10068643904747315\n",
      "Episode 558: Won\n",
      "Episode 558, Avg Value Loss: 3.685946762561798, Avg Policy Loss: 3.2804924488067626\n",
      "Episode 558, Reward: -32.162916823164075, Moving Avg Reward: -25.271789199310973, Replay Buffer Size: 21218\n",
      "Current Epsilon: 0.10018300685223579\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 558, Reward: -32.162916823164075, Moving Avg Reward: -25.271789199310973, Replay Buffer Size: 21218\n",
      "Current Epsilon: 0.10018300685223579\n",
      "Episode 559: Won\n",
      "Episode 559, Avg Value Loss: 3.687575951218605, Avg Policy Loss: 3.3197562038898467\n",
      "Episode 559, Reward: -31.905665736745608, Moving Avg Reward: -25.359867128530112, Replay Buffer Size: 21298\n",
      "Current Epsilon: 0.0996820918179746\n",
      "Loss rate over the last 150 episodes: 0.58\n",
      "Episode 559, Reward: -31.905665736745608, Moving Avg Reward: -25.359867128530112, Replay Buffer Size: 21298\n",
      "Current Epsilon: 0.0996820918179746\n",
      "Episode 560: Won\n",
      "Episode 560, Avg Value Loss: 4.351508259773254, Avg Policy Loss: 3.401031017303467\n",
      "Episode 560, Reward: -27.492676691701234, Moving Avg Reward: -25.393735609810175, Replay Buffer Size: 21308\n",
      "Current Epsilon: 0.09918368135888474\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 560, Reward: -27.492676691701234, Moving Avg Reward: -25.393735609810175, Replay Buffer Size: 21308\n",
      "Current Epsilon: 0.09918368135888474\n",
      "Episode 561: Won\n",
      "Episode 561, Avg Value Loss: 3.018276084553112, Avg Policy Loss: 3.601849534294822\n",
      "Episode 561, Reward: -31.613876469574684, Moving Avg Reward: -25.421276365482758, Replay Buffer Size: 21319\n",
      "Current Epsilon: 0.09868776295209031\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 561, Reward: -31.613876469574684, Moving Avg Reward: -25.421276365482758, Replay Buffer Size: 21319\n",
      "Current Epsilon: 0.09868776295209031\n",
      "Episode 562: Won\n",
      "Episode 562, Avg Value Loss: 3.8029419316185846, Avg Policy Loss: 3.4435689979129367\n",
      "Episode 562, Reward: -21.193113398299765, Moving Avg Reward: -25.37608582430465, Replay Buffer Size: 21328\n",
      "Current Epsilon: 0.09819432413732986\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 562, Reward: -21.193113398299765, Moving Avg Reward: -25.37608582430465, Replay Buffer Size: 21328\n",
      "Current Epsilon: 0.09819432413732986\n",
      "Episode 563: Won\n",
      "Episode 563, Avg Value Loss: 2.7851460456848143, Avg Policy Loss: 3.5370556831359865\n",
      "Episode 563, Reward: -28.27148034663452, Moving Avg Reward: -25.415463706362186, Replay Buffer Size: 21338\n",
      "Current Epsilon: 0.09770335251664321\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 563, Reward: -28.27148034663452, Moving Avg Reward: -25.415463706362186, Replay Buffer Size: 21338\n",
      "Current Epsilon: 0.09770335251664321\n",
      "Episode 564: Won\n",
      "Episode 564, Avg Value Loss: 3.4404221095144747, Avg Policy Loss: 3.3651825189590454\n",
      "Episode 564, Reward: -82.18702298220852, Moving Avg Reward: -25.961261712414412, Replay Buffer Size: 21418\n",
      "Current Epsilon: 0.09721483575406\n",
      "Loss rate over the last 150 episodes: 0.60\n",
      "Episode 564, Reward: -82.18702298220852, Moving Avg Reward: -25.961261712414412, Replay Buffer Size: 21418\n",
      "Current Epsilon: 0.09721483575406\n",
      "Episode 565: Lost\n",
      "Episode 565, Avg Value Loss: 3.3248956203460693, Avg Policy Loss: 3.513401678630284\n",
      "Episode 565, Reward: -22.240792303655347, Moving Avg Reward: -25.788374286113157, Replay Buffer Size: 21425\n",
      "Current Epsilon: 0.09672876157528969\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 565, Reward: -22.240792303655347, Moving Avg Reward: -25.788374286113157, Replay Buffer Size: 21425\n",
      "Current Epsilon: 0.09672876157528969\n",
      "Episode 566: Lost\n",
      "Episode 566, Avg Value Loss: 3.351224641005198, Avg Policy Loss: 3.388481746117274\n",
      "Episode 566, Reward: 17.8174876173334, Moving Avg Reward: -25.659602475003208, Replay Buffer Size: 21449\n",
      "Current Epsilon: 0.09624511776741324\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 566, Reward: 17.8174876173334, Moving Avg Reward: -25.659602475003208, Replay Buffer Size: 21449\n",
      "Current Epsilon: 0.09624511776741324\n",
      "Episode 567: Won\n",
      "Episode 567, Avg Value Loss: 3.86467673097338, Avg Policy Loss: 3.478132281984602\n",
      "Episode 567, Reward: -31.327813746814112, Moving Avg Reward: -25.27163763588642, Replay Buffer Size: 21463\n",
      "Current Epsilon: 0.09576389217857617\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 567, Reward: -31.327813746814112, Moving Avg Reward: -25.27163763588642, Replay Buffer Size: 21463\n",
      "Current Epsilon: 0.09576389217857617\n",
      "Episode 568: Won\n",
      "Episode 568, Avg Value Loss: 3.280618179928173, Avg Policy Loss: 3.538438927043568\n",
      "Episode 568, Reward: -26.12292952984422, Moving Avg Reward: -25.11475607029386, Replay Buffer Size: 21474\n",
      "Current Epsilon: 0.09528507271768329\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 568, Reward: -26.12292952984422, Moving Avg Reward: -25.11475607029386, Replay Buffer Size: 21474\n",
      "Current Epsilon: 0.09528507271768329\n",
      "Episode 569: Won\n",
      "Episode 569, Avg Value Loss: 3.2581199586391447, Avg Policy Loss: 3.468733364343643\n",
      "Episode 569, Reward: 1.2140566803326132, Moving Avg Reward: -24.919848391440865, Replay Buffer Size: 21554\n",
      "Current Epsilon: 0.09480864735409487\n",
      "Loss rate over the last 150 episodes: 0.58\n",
      "Episode 569, Reward: 1.2140566803326132, Moving Avg Reward: -24.919848391440865, Replay Buffer Size: 21554\n",
      "Current Epsilon: 0.09480864735409487\n",
      "Episode 570: Won\n",
      "Episode 570, Avg Value Loss: 3.1696508288383485, Avg Policy Loss: 3.40030797123909\n",
      "Episode 570, Reward: -21.707122641932404, Moving Avg Reward: -25.328723162534, Replay Buffer Size: 21634\n",
      "Current Epsilon: 0.0943346041173244\n",
      "Loss rate over the last 150 episodes: 0.57\n",
      "Episode 570, Reward: -21.707122641932404, Moving Avg Reward: -25.328723162534, Replay Buffer Size: 21634\n",
      "Current Epsilon: 0.0943346041173244\n",
      "Episode 571: Draw\n",
      "Episode 571, Avg Value Loss: 3.332068943977356, Avg Policy Loss: 3.4329548507928846\n",
      "Episode 571, Reward: -1.103989028625651, Moving Avg Reward: -25.095975089785075, Replay Buffer Size: 21714\n",
      "Current Epsilon: 0.09386293109673778\n",
      "Loss rate over the last 150 episodes: 0.57\n",
      "Episode 571, Reward: -1.103989028625651, Moving Avg Reward: -25.095975089785075, Replay Buffer Size: 21714\n",
      "Current Epsilon: 0.09386293109673778\n",
      "Episode 572: Draw\n",
      "Episode 572, Avg Value Loss: 3.230758422613144, Avg Policy Loss: 3.4263178139925\n",
      "Episode 572, Reward: -3.972887237344181, Moving Avg Reward: -24.898372641893815, Replay Buffer Size: 21794\n",
      "Current Epsilon: 0.09339361644125409\n",
      "Loss rate over the last 150 episodes: 0.57\n",
      "Episode 572, Reward: -3.972887237344181, Moving Avg Reward: -24.898372641893815, Replay Buffer Size: 21794\n",
      "Current Epsilon: 0.09339361644125409\n",
      "Episode 573: Draw\n",
      "Episode 573, Avg Value Loss: 2.60479474067688, Avg Policy Loss: 3.2726032733917236\n",
      "Episode 573, Reward: 9.99, Moving Avg Reward: -24.515546593631075, Replay Buffer Size: 21795\n",
      "Current Epsilon: 0.09292664835904782\n",
      "Loss rate over the last 150 episodes: 0.57\n",
      "Episode 573, Reward: 9.99, Moving Avg Reward: -24.515546593631075, Replay Buffer Size: 21795\n",
      "Current Epsilon: 0.09292664835904782\n",
      "Episode 574: Won\n",
      "Episode 574, Avg Value Loss: 3.041060502712543, Avg Policy Loss: 3.4897336776439962\n",
      "Episode 574, Reward: -30.9821224495236, Moving Avg Reward: -24.578604874822823, Replay Buffer Size: 21834\n",
      "Current Epsilon: 0.09246201511725258\n",
      "Loss rate over the last 150 episodes: 0.58\n",
      "Episode 574, Reward: -30.9821224495236, Moving Avg Reward: -24.578604874822823, Replay Buffer Size: 21834\n",
      "Current Epsilon: 0.09246201511725258\n",
      "Episode 575: Won\n",
      "Episode 575, Avg Value Loss: 3.6085628673434256, Avg Policy Loss: 3.4494076788425447\n",
      "Episode 575, Reward: 0.5430496021692639, Moving Avg Reward: -24.539878110969113, Replay Buffer Size: 21914\n",
      "Current Epsilon: 0.09199970504166631\n",
      "Loss rate over the last 150 episodes: 0.57\n",
      "Episode 575, Reward: 0.5430496021692639, Moving Avg Reward: -24.539878110969113, Replay Buffer Size: 21914\n",
      "Current Epsilon: 0.09199970504166631\n",
      "Episode 576: Won\n",
      "Episode 576, Avg Value Loss: 3.407555610686541, Avg Policy Loss: 3.3842573285102846\n",
      "Episode 576, Reward: 3.330465451338796, Moving Avg Reward: -24.073085196596903, Replay Buffer Size: 21994\n",
      "Current Epsilon: 0.09153970651645797\n",
      "Loss rate over the last 150 episodes: 0.57\n",
      "Episode 576, Reward: 3.330465451338796, Moving Avg Reward: -24.073085196596903, Replay Buffer Size: 21994\n",
      "Current Epsilon: 0.09153970651645797\n",
      "Episode 577: Draw\n",
      "Episode 577, Avg Value Loss: 3.7975364923477173, Avg Policy Loss: 3.544260025024414\n",
      "Episode 577, Reward: -30.760886147904507, Moving Avg Reward: -24.084312881775354, Replay Buffer Size: 22004\n",
      "Current Epsilon: 0.09108200798387568\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 577, Reward: -30.760886147904507, Moving Avg Reward: -24.084312881775354, Replay Buffer Size: 22004\n",
      "Current Epsilon: 0.09108200798387568\n",
      "Episode 578: Lost\n",
      "Episode 578, Avg Value Loss: 3.5875310464338823, Avg Policy Loss: 3.515587871724909\n",
      "Episode 578, Reward: -28.4161099490102, Moving Avg Reward: -23.516581756046957, Replay Buffer Size: 22015\n",
      "Current Epsilon: 0.0906265979439563\n",
      "Loss rate over the last 150 episodes: 0.57\n",
      "Episode 578, Reward: -28.4161099490102, Moving Avg Reward: -23.516581756046957, Replay Buffer Size: 22015\n",
      "Current Epsilon: 0.0906265979439563\n",
      "Episode 579: Lost\n",
      "Episode 579, Avg Value Loss: 3.416838471706097, Avg Policy Loss: 3.4665478559640737\n",
      "Episode 579, Reward: -30.575739904540804, Moving Avg Reward: -23.29507272025251, Replay Buffer Size: 22028\n",
      "Current Epsilon: 0.09017346495423652\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 579, Reward: -30.575739904540804, Moving Avg Reward: -23.29507272025251, Replay Buffer Size: 22028\n",
      "Current Epsilon: 0.09017346495423652\n",
      "Episode 580: Lost\n",
      "Episode 580, Avg Value Loss: 3.5203371282134737, Avg Policy Loss: 3.4975539531026567\n",
      "Episode 580, Reward: 12.678681201138096, Moving Avg Reward: -22.89354414437254, Replay Buffer Size: 22056\n",
      "Current Epsilon: 0.08972259762946533\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 580, Reward: 12.678681201138096, Moving Avg Reward: -22.89354414437254, Replay Buffer Size: 22056\n",
      "Current Epsilon: 0.08972259762946533\n",
      "Episode 581: Won\n",
      "Episode 581, Avg Value Loss: 3.7744150705959485, Avg Policy Loss: 3.494839631992838\n",
      "Episode 581, Reward: -47.13049342304491, Moving Avg Reward: -23.09805409580703, Replay Buffer Size: 22102\n",
      "Current Epsilon: 0.089273984641318\n",
      "Loss rate over the last 150 episodes: 0.56\n",
      "Episode 581, Reward: -47.13049342304491, Moving Avg Reward: -23.09805409580703, Replay Buffer Size: 22102\n",
      "Current Epsilon: 0.089273984641318\n",
      "Episode 582: Won\n",
      "Episode 582, Avg Value Loss: 3.6953395292162896, Avg Policy Loss: 3.41939817070961\n",
      "Episode 582, Reward: 4.872926510312906, Moving Avg Reward: -22.764071935489575, Replay Buffer Size: 22182\n",
      "Current Epsilon: 0.0888276147181114\n",
      "Loss rate over the last 150 episodes: 0.55\n",
      "Episode 582, Reward: 4.872926510312906, Moving Avg Reward: -22.764071935489575, Replay Buffer Size: 22182\n",
      "Current Epsilon: 0.0888276147181114\n",
      "Episode 583: Won\n",
      "Episode 583, Avg Value Loss: 2.9975723326206207, Avg Policy Loss: 3.464420884847641\n",
      "Episode 583, Reward: -24.459079067735672, Moving Avg Reward: -22.70163743730853, Replay Buffer Size: 22190\n",
      "Current Epsilon: 0.08838347664452084\n",
      "Loss rate over the last 150 episodes: 0.58\n",
      "Episode 583, Reward: -24.459079067735672, Moving Avg Reward: -22.70163743730853, Replay Buffer Size: 22190\n",
      "Current Epsilon: 0.08838347664452084\n",
      "Episode 584: Won\n",
      "Episode 584, Avg Value Loss: 3.3519066134277655, Avg Policy Loss: 3.3743995938982283\n",
      "Episode 584, Reward: 11.60258049660408, Moving Avg Reward: -22.307980515364772, Replay Buffer Size: 22239\n",
      "Current Epsilon: 0.08794155926129824\n",
      "Loss rate over the last 150 episodes: 0.57\n",
      "Episode 584, Reward: 11.60258049660408, Moving Avg Reward: -22.307980515364772, Replay Buffer Size: 22239\n",
      "Current Epsilon: 0.08794155926129824\n",
      "Episode 585: Won\n",
      "Episode 585, Avg Value Loss: 3.5089230354015646, Avg Policy Loss: 3.436152091393104\n",
      "Episode 585, Reward: -30.98813490336741, Moving Avg Reward: -22.800617478456076, Replay Buffer Size: 22252\n",
      "Current Epsilon: 0.08750185146499175\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 585, Reward: -30.98813490336741, Moving Avg Reward: -22.800617478456076, Replay Buffer Size: 22252\n",
      "Current Epsilon: 0.08750185146499175\n",
      "Episode 586: Won\n",
      "Episode 586, Avg Value Loss: 3.193792608049181, Avg Policy Loss: 3.365137073728773\n",
      "Episode 586, Reward: -23.614777232301172, Moving Avg Reward: -23.008668315989443, Replay Buffer Size: 22261\n",
      "Current Epsilon: 0.08706434220766679\n",
      "Loss rate over the last 150 episodes: 0.57\n",
      "Episode 586, Reward: -23.614777232301172, Moving Avg Reward: -23.008668315989443, Replay Buffer Size: 22261\n",
      "Current Epsilon: 0.08706434220766679\n",
      "Episode 587: Won\n",
      "Episode 587, Avg Value Loss: 3.377888973166303, Avg Policy Loss: 3.403996188466142\n",
      "Episode 587, Reward: -33.55394808281172, Moving Avg Reward: -23.12770149065083, Replay Buffer Size: 22302\n",
      "Current Epsilon: 0.08662902049662846\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 587, Reward: -33.55394808281172, Moving Avg Reward: -23.12770149065083, Replay Buffer Size: 22302\n",
      "Current Epsilon: 0.08662902049662846\n",
      "Episode 588: Won\n",
      "Episode 588, Avg Value Loss: 3.2336520207555672, Avg Policy Loss: 3.44513107600965\n",
      "Episode 588, Reward: -46.133538467125064, Moving Avg Reward: -23.47495556867489, Replay Buffer Size: 22359\n",
      "Current Epsilon: 0.08619587539414532\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 588, Reward: -46.133538467125064, Moving Avg Reward: -23.47495556867489, Replay Buffer Size: 22359\n",
      "Current Epsilon: 0.08619587539414532\n",
      "Episode 589: Won\n",
      "Episode 589, Avg Value Loss: 3.6062821581959725, Avg Policy Loss: 3.444260784983635\n",
      "Episode 589, Reward: -43.25795394542654, Moving Avg Reward: -23.086186807218755, Replay Buffer Size: 22439\n",
      "Current Epsilon: 0.08576489601717459\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 589, Reward: -43.25795394542654, Moving Avg Reward: -23.086186807218755, Replay Buffer Size: 22439\n",
      "Current Epsilon: 0.08576489601717459\n",
      "Episode 590: Lost\n",
      "Episode 590, Avg Value Loss: 3.2016046174029085, Avg Policy Loss: 3.406503936077686\n",
      "Episode 590, Reward: -62.60413220473075, Moving Avg Reward: -23.788547392723345, Replay Buffer Size: 22486\n",
      "Current Epsilon: 0.08533607153708872\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 590, Reward: -62.60413220473075, Moving Avg Reward: -23.788547392723345, Replay Buffer Size: 22486\n",
      "Current Epsilon: 0.08533607153708872\n",
      "Episode 591: Lost\n",
      "Episode 591, Avg Value Loss: 4.1966035691174595, Avg Policy Loss: 3.1366785006089644\n",
      "Episode 591, Reward: -25.547942059868937, Moving Avg Reward: -23.713059762955908, Replay Buffer Size: 22497\n",
      "Current Epsilon: 0.08490939117940327\n",
      "Loss rate over the last 150 episodes: 0.57\n",
      "Episode 591, Reward: -25.547942059868937, Moving Avg Reward: -23.713059762955908, Replay Buffer Size: 22497\n",
      "Current Epsilon: 0.08490939117940327\n",
      "Episode 592: Lost\n",
      "Episode 592, Avg Value Loss: 3.365324231711301, Avg Policy Loss: 3.4425932494076816\n",
      "Episode 592, Reward: 15.325155053699733, Moving Avg Reward: -23.12018001659179, Replay Buffer Size: 22519\n",
      "Current Epsilon: 0.08448484422350626\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 592, Reward: 15.325155053699733, Moving Avg Reward: -23.12018001659179, Replay Buffer Size: 22519\n",
      "Current Epsilon: 0.08448484422350626\n",
      "Episode 593: Won\n",
      "Episode 593, Avg Value Loss: 3.253049914325987, Avg Policy Loss: 3.400329896381923\n",
      "Episode 593, Reward: -67.83266298256262, Moving Avg Reward: -23.522265990925426, Replay Buffer Size: 22575\n",
      "Current Epsilon: 0.08406242000238873\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 593, Reward: -67.83266298256262, Moving Avg Reward: -23.522265990925426, Replay Buffer Size: 22575\n",
      "Current Epsilon: 0.08406242000238873\n",
      "Episode 594: Won\n",
      "Episode 594, Avg Value Loss: 3.2364376141474795, Avg Policy Loss: 3.465476843026968\n",
      "Episode 594, Reward: -34.00844828597874, Moving Avg Reward: -23.6140661593123, Replay Buffer Size: 22588\n",
      "Current Epsilon: 0.08364210790237678\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 594, Reward: -34.00844828597874, Moving Avg Reward: -23.6140661593123, Replay Buffer Size: 22588\n",
      "Current Epsilon: 0.08364210790237678\n",
      "Episode 595: Won\n",
      "Episode 595, Avg Value Loss: 3.2017556970769707, Avg Policy Loss: 3.4619723450053823\n",
      "Episode 595, Reward: -35.468370765331706, Moving Avg Reward: -23.70319821328525, Replay Buffer Size: 22599\n",
      "Current Epsilon: 0.0832238973628649\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 595, Reward: -35.468370765331706, Moving Avg Reward: -23.70319821328525, Replay Buffer Size: 22599\n",
      "Current Epsilon: 0.0832238973628649\n",
      "Episode 596: Won\n",
      "Episode 596, Avg Value Loss: 3.050773411989212, Avg Policy Loss: 3.5903948664665224\n",
      "Episode 596, Reward: -36.14509318888822, Moving Avg Reward: -23.03829217071601, Replay Buffer Size: 22679\n",
      "Current Epsilon: 0.08280777787605056\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 596, Reward: -36.14509318888822, Moving Avg Reward: -23.03829217071601, Replay Buffer Size: 22679\n",
      "Current Epsilon: 0.08280777787605056\n",
      "Episode 597: Lost\n",
      "Episode 597, Avg Value Loss: 3.7453240614670973, Avg Policy Loss: 3.4123626672304592\n",
      "Episode 597, Reward: -34.20801163727676, Moving Avg Reward: -23.23808303843711, Replay Buffer Size: 22692\n",
      "Current Epsilon: 0.08239373898667031\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 597, Reward: -34.20801163727676, Moving Avg Reward: -23.23808303843711, Replay Buffer Size: 22692\n",
      "Current Epsilon: 0.08239373898667031\n",
      "Episode 598: Lost\n",
      "Episode 598, Avg Value Loss: 3.5199420668862085, Avg Policy Loss: 3.158731850710782\n",
      "Episode 598, Reward: -27.644898673731433, Moving Avg Reward: -23.186316625237254, Replay Buffer Size: 22703\n",
      "Current Epsilon: 0.08198177029173696\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 598, Reward: -27.644898673731433, Moving Avg Reward: -23.186316625237254, Replay Buffer Size: 22703\n",
      "Current Epsilon: 0.08198177029173696\n",
      "Episode 599: Lost\n",
      "Episode 599, Avg Value Loss: 3.7491968870162964, Avg Policy Loss: 3.765891989072164\n",
      "Episode 599, Reward: -32.382470077829595, Moving Avg Reward: -23.16553628584852, Replay Buffer Size: 22715\n",
      "Current Epsilon: 0.08157186144027828\n",
      "Loss rate over the last 150 episodes: 0.57\n",
      "Episode 599, Reward: -32.382470077829595, Moving Avg Reward: -23.16553628584852, Replay Buffer Size: 22715\n",
      "Current Epsilon: 0.08157186144027828\n",
      "Episode 600: Lost\n",
      "Episode 600, Avg Value Loss: 3.344919437711889, Avg Policy Loss: 3.407861868540446\n",
      "Episode 600, Reward: -16.021648635502103, Moving Avg Reward: -23.060789556898747, Replay Buffer Size: 22781\n",
      "Current Epsilon: 0.0811640021330769\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 600, Reward: -16.021648635502103, Moving Avg Reward: -23.060789556898747, Replay Buffer Size: 22781\n",
      "Current Epsilon: 0.0811640021330769\n",
      "Episode 601: Won\n",
      "Episode 601, Avg Value Loss: 2.609702909986178, Avg Policy Loss: 3.3520171443621316\n",
      "Episode 601, Reward: -35.733092863481964, Moving Avg Reward: -22.850974086797486, Replay Buffer Size: 22793\n",
      "Current Epsilon: 0.08075818212241151\n",
      "Loss rate over the last 150 episodes: 0.57\n",
      "Episode 601, Reward: -35.733092863481964, Moving Avg Reward: -22.850974086797486, Replay Buffer Size: 22793\n",
      "Current Epsilon: 0.08075818212241151\n",
      "Episode 602: Won\n",
      "Episode 602, Avg Value Loss: 3.512327818547265, Avg Policy Loss: 3.489237195354397\n",
      "Episode 602, Reward: -15.6550381658859, Moving Avg Reward: -23.02538015936317, Replay Buffer Size: 22852\n",
      "Current Epsilon: 0.08035439121179945\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 602, Reward: -15.6550381658859, Moving Avg Reward: -23.02538015936317, Replay Buffer Size: 22852\n",
      "Current Epsilon: 0.08035439121179945\n",
      "Episode 603: Won\n",
      "Episode 603, Avg Value Loss: 3.0379965121929464, Avg Policy Loss: 3.440529071367704\n",
      "Episode 603, Reward: -29.156115577955823, Moving Avg Reward: -22.997909623196122, Replay Buffer Size: 22865\n",
      "Current Epsilon: 0.07995261925574046\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 603, Reward: -29.156115577955823, Moving Avg Reward: -22.997909623196122, Replay Buffer Size: 22865\n",
      "Current Epsilon: 0.07995261925574046\n",
      "Episode 604: Won\n",
      "Episode 604, Avg Value Loss: 3.098093019081996, Avg Policy Loss: 3.470067574427678\n",
      "Episode 604, Reward: -27.479473957902076, Moving Avg Reward: -22.94173785078635, Replay Buffer Size: 22878\n",
      "Current Epsilon: 0.07955285615946175\n",
      "Loss rate over the last 150 episodes: 0.57\n",
      "Episode 604, Reward: -27.479473957902076, Moving Avg Reward: -22.94173785078635, Replay Buffer Size: 22878\n",
      "Current Epsilon: 0.07955285615946175\n",
      "Episode 605: Won\n",
      "Episode 605, Avg Value Loss: 3.4713550090789793, Avg Policy Loss: 3.72322154045105\n",
      "Episode 605, Reward: -32.27787098485901, Moving Avg Reward: -22.399367072601, Replay Buffer Size: 22888\n",
      "Current Epsilon: 0.07915509187866444\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 605, Reward: -32.27787098485901, Moving Avg Reward: -22.399367072601, Replay Buffer Size: 22888\n",
      "Current Epsilon: 0.07915509187866444\n",
      "Episode 606: Won\n",
      "Episode 606, Avg Value Loss: 3.625615464316474, Avg Policy Loss: 3.4730703830718994\n",
      "Episode 606, Reward: -26.587973144454836, Moving Avg Reward: -22.4297643838152, Replay Buffer Size: 22897\n",
      "Current Epsilon: 0.07875931641927113\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 606, Reward: -26.587973144454836, Moving Avg Reward: -22.4297643838152, Replay Buffer Size: 22897\n",
      "Current Epsilon: 0.07875931641927113\n",
      "Episode 607: Won\n",
      "Episode 607, Avg Value Loss: 3.6123307650206518, Avg Policy Loss: 3.4384943891744144\n",
      "Episode 607, Reward: -98.01002320427118, Moving Avg Reward: -23.17307176564956, Replay Buffer Size: 22958\n",
      "Current Epsilon: 0.07836551983717477\n",
      "Loss rate over the last 150 episodes: 0.57\n",
      "Episode 607, Reward: -98.01002320427118, Moving Avg Reward: -23.17307176564956, Replay Buffer Size: 22958\n",
      "Current Epsilon: 0.07836551983717477\n",
      "Episode 608: Won\n",
      "Episode 608, Avg Value Loss: 3.4246008715459277, Avg Policy Loss: 3.4733480513095856\n",
      "Episode 608, Reward: -12.503809152270632, Moving Avg Reward: -22.9497316716534, Replay Buffer Size: 23014\n",
      "Current Epsilon: 0.07797369223798889\n",
      "Loss rate over the last 150 episodes: 0.60\n",
      "Episode 608, Reward: -12.503809152270632, Moving Avg Reward: -22.9497316716534, Replay Buffer Size: 23014\n",
      "Current Epsilon: 0.07797369223798889\n",
      "Episode 609: Won\n",
      "Episode 609, Avg Value Loss: 3.0839947121483937, Avg Policy Loss: 3.5573110580444336\n",
      "Episode 609, Reward: -21.38812545625595, Moving Avg Reward: -22.863985836624128, Replay Buffer Size: 23021\n",
      "Current Epsilon: 0.07758382377679894\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 609, Reward: -21.38812545625595, Moving Avg Reward: -22.863985836624128, Replay Buffer Size: 23021\n",
      "Current Epsilon: 0.07758382377679894\n",
      "Episode 610: Won\n",
      "Episode 610, Avg Value Loss: 3.4215257856207835, Avg Policy Loss: 3.493320364347646\n",
      "Episode 610, Reward: -83.09163481754457, Moving Avg Reward: -23.406925680432344, Replay Buffer Size: 23092\n",
      "Current Epsilon: 0.07719590465791494\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 610, Reward: -83.09163481754457, Moving Avg Reward: -23.406925680432344, Replay Buffer Size: 23092\n",
      "Current Epsilon: 0.07719590465791494\n",
      "Episode 611: Won\n",
      "Episode 611, Avg Value Loss: 3.029457320769628, Avg Policy Loss: 3.4374040563901267\n",
      "Episode 611, Reward: -29.514117891575324, Moving Avg Reward: -22.54912827952393, Replay Buffer Size: 23104\n",
      "Current Epsilon: 0.07680992513462537\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 611, Reward: -29.514117891575324, Moving Avg Reward: -22.54912827952393, Replay Buffer Size: 23104\n",
      "Current Epsilon: 0.07680992513462537\n",
      "Episode 612: Won\n",
      "Episode 612, Avg Value Loss: 3.239731403440237, Avg Policy Loss: 3.5403298109769823\n",
      "Episode 612, Reward: -79.4498745013736, Moving Avg Reward: -23.39886615822481, Replay Buffer Size: 23184\n",
      "Current Epsilon: 0.07642587550895225\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 612, Reward: -79.4498745013736, Moving Avg Reward: -23.39886615822481, Replay Buffer Size: 23184\n",
      "Current Epsilon: 0.07642587550895225\n",
      "Episode 613: Lost\n",
      "Episode 613, Avg Value Loss: 2.9233989583121405, Avg Policy Loss: 3.6296655072106256\n",
      "Episode 613, Reward: -12.590112254160005, Moving Avg Reward: -23.202823482051652, Replay Buffer Size: 23193\n",
      "Current Epsilon: 0.07604374613140748\n",
      "Loss rate over the last 150 episodes: 0.58\n",
      "Episode 613, Reward: -12.590112254160005, Moving Avg Reward: -23.202823482051652, Replay Buffer Size: 23193\n",
      "Current Epsilon: 0.07604374613140748\n",
      "Episode 614: Lost\n",
      "Episode 614, Avg Value Loss: 3.6074244275689127, Avg Policy Loss: 3.443342849612236\n",
      "Episode 614, Reward: -26.135426837231392, Moving Avg Reward: -22.379284082075245, Replay Buffer Size: 23273\n",
      "Current Epsilon: 0.07566352740075044\n",
      "Loss rate over the last 150 episodes: 0.58\n",
      "Episode 614, Reward: -26.135426837231392, Moving Avg Reward: -22.379284082075245, Replay Buffer Size: 23273\n",
      "Current Epsilon: 0.07566352740075044\n",
      "Episode 615: Lost\n",
      "Episode 615, Avg Value Loss: 3.1535961031913757, Avg Policy Loss: 3.603686511516571\n",
      "Episode 615, Reward: -26.225742512027765, Moving Avg Reward: -22.830946971537504, Replay Buffer Size: 23281\n",
      "Current Epsilon: 0.07528520976374668\n",
      "Loss rate over the last 150 episodes: 0.60\n",
      "Episode 615, Reward: -26.225742512027765, Moving Avg Reward: -22.830946971537504, Replay Buffer Size: 23281\n",
      "Current Epsilon: 0.07528520976374668\n",
      "Episode 616: Lost\n",
      "Episode 616, Avg Value Loss: 3.192483365535736, Avg Policy Loss: 3.7977207601070404\n",
      "Episode 616, Reward: -23.460191068204377, Moving Avg Reward: -22.801471414451335, Replay Buffer Size: 23289\n",
      "Current Epsilon: 0.07490878371492794\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 616, Reward: -23.460191068204377, Moving Avg Reward: -22.801471414451335, Replay Buffer Size: 23289\n",
      "Current Epsilon: 0.07490878371492794\n",
      "Episode 617: Lost\n",
      "Episode 617, Avg Value Loss: 3.4007458209991457, Avg Policy Loss: 3.464993715286255\n",
      "Episode 617, Reward: -28.295375529252244, Moving Avg Reward: -22.75726099778747, Replay Buffer Size: 23299\n",
      "Current Epsilon: 0.0745342397963533\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 617, Reward: -28.295375529252244, Moving Avg Reward: -22.75726099778747, Replay Buffer Size: 23299\n",
      "Current Epsilon: 0.0745342397963533\n",
      "Episode 618: Lost\n",
      "Episode 618, Avg Value Loss: 3.543282389640808, Avg Policy Loss: 3.6197683612505593\n",
      "Episode 618, Reward: -34.70671002688455, Moving Avg Reward: -22.763336733139464, Replay Buffer Size: 23311\n",
      "Current Epsilon: 0.07416156859737154\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 618, Reward: -34.70671002688455, Moving Avg Reward: -22.763336733139464, Replay Buffer Size: 23311\n",
      "Current Epsilon: 0.07416156859737154\n",
      "Episode 619: Lost\n",
      "Episode 619, Avg Value Loss: 4.110130453109742, Avg Policy Loss: 3.2150699138641357\n",
      "Episode 619, Reward: -23.436670609746525, Moving Avg Reward: -22.79065457390043, Replay Buffer Size: 23321\n",
      "Current Epsilon: 0.07379076075438468\n",
      "Loss rate over the last 150 episodes: 0.60\n",
      "Episode 619, Reward: -23.436670609746525, Moving Avg Reward: -22.79065457390043, Replay Buffer Size: 23321\n",
      "Current Epsilon: 0.07379076075438468\n",
      "Episode 620: Lost\n",
      "Episode 620, Avg Value Loss: 3.2297971513536243, Avg Policy Loss: 3.5271392398410373\n",
      "Episode 620, Reward: -23.159076710645174, Moving Avg Reward: -22.933026122971523, Replay Buffer Size: 23330\n",
      "Current Epsilon: 0.07342180695061275\n",
      "Loss rate over the last 150 episodes: 0.60\n",
      "Episode 620, Reward: -23.159076710645174, Moving Avg Reward: -22.933026122971523, Replay Buffer Size: 23330\n",
      "Current Epsilon: 0.07342180695061275\n",
      "Episode 621: Lost\n",
      "Episode 621, Avg Value Loss: 3.351412014263432, Avg Policy Loss: 3.518286646866217\n",
      "Episode 621, Reward: 18.192053067350333, Moving Avg Reward: -22.842585455003345, Replay Buffer Size: 23371\n",
      "Current Epsilon: 0.07305469791585968\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 621, Reward: 18.192053067350333, Moving Avg Reward: -22.842585455003345, Replay Buffer Size: 23371\n",
      "Current Epsilon: 0.07305469791585968\n",
      "Episode 622: Won\n",
      "Episode 622, Avg Value Loss: 3.325948053598404, Avg Policy Loss: 3.470030093193054\n",
      "Episode 622, Reward: -8.857931983000407, Moving Avg Reward: -22.675967846255443, Replay Buffer Size: 23451\n",
      "Current Epsilon: 0.07268942442628039\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 622, Reward: -8.857931983000407, Moving Avg Reward: -22.675967846255443, Replay Buffer Size: 23451\n",
      "Current Epsilon: 0.07268942442628039\n",
      "Episode 623: Won\n",
      "Episode 623, Avg Value Loss: 3.630493233601252, Avg Policy Loss: 3.6575771967569985\n",
      "Episode 623, Reward: -28.206115406842088, Moving Avg Reward: -23.112583996116772, Replay Buffer Size: 23463\n",
      "Current Epsilon: 0.07232597730414898\n",
      "Loss rate over the last 150 episodes: 0.60\n",
      "Episode 623, Reward: -28.206115406842088, Moving Avg Reward: -23.112583996116772, Replay Buffer Size: 23463\n",
      "Current Epsilon: 0.07232597730414898\n",
      "Episode 624: Won\n",
      "Episode 624, Avg Value Loss: 3.466393190271714, Avg Policy Loss: 3.444330192079731\n",
      "Episode 624, Reward: -15.705766842130382, Moving Avg Reward: -23.261641664538075, Replay Buffer Size: 23514\n",
      "Current Epsilon: 0.07196434741762824\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 624, Reward: -15.705766842130382, Moving Avg Reward: -23.261641664538075, Replay Buffer Size: 23514\n",
      "Current Epsilon: 0.07196434741762824\n",
      "Episode 625: Won\n",
      "Episode 625, Avg Value Loss: 3.324647231400013, Avg Policy Loss: 3.472084939479828\n",
      "Episode 625, Reward: -47.646118329568715, Moving Avg Reward: -23.42247659077509, Replay Buffer Size: 23594\n",
      "Current Epsilon: 0.0716045256805401\n",
      "Loss rate over the last 150 episodes: 0.58\n",
      "Episode 625, Reward: -47.646118329568715, Moving Avg Reward: -23.42247659077509, Replay Buffer Size: 23594\n",
      "Current Epsilon: 0.0716045256805401\n",
      "Episode 626: Won\n",
      "Episode 626, Avg Value Loss: 3.724114274978638, Avg Policy Loss: 3.249478316307068\n",
      "Episode 626, Reward: -31.302149429321624, Moving Avg Reward: -23.83584842611986, Replay Buffer Size: 23604\n",
      "Current Epsilon: 0.0712465030521374\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 626, Reward: -31.302149429321624, Moving Avg Reward: -23.83584842611986, Replay Buffer Size: 23604\n",
      "Current Epsilon: 0.0712465030521374\n",
      "Episode 627: Won\n",
      "Episode 627, Avg Value Loss: 3.5599670626900415, Avg Policy Loss: 3.400134043260054\n",
      "Episode 627, Reward: -36.44105367066123, Moving Avg Reward: -23.860285596092794, Replay Buffer Size: 23615\n",
      "Current Epsilon: 0.0708902705368767\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 627, Reward: -36.44105367066123, Moving Avg Reward: -23.860285596092794, Replay Buffer Size: 23615\n",
      "Current Epsilon: 0.0708902705368767\n",
      "Episode 628: Won\n",
      "Episode 628, Avg Value Loss: 3.17974493238661, Avg Policy Loss: 3.458839681413439\n",
      "Episode 628, Reward: -7.202684842398018, Moving Avg Reward: -23.703964289885707, Replay Buffer Size: 23624\n",
      "Current Epsilon: 0.07053581918419231\n",
      "Loss rate over the last 150 episodes: 0.60\n",
      "Episode 628, Reward: -7.202684842398018, Moving Avg Reward: -23.703964289885707, Replay Buffer Size: 23624\n",
      "Current Epsilon: 0.07053581918419231\n",
      "Episode 629: Won\n",
      "Episode 629, Avg Value Loss: 3.3918100222945213, Avg Policy Loss: 3.6047015309333803\n",
      "Episode 629, Reward: -19.846799126157393, Moving Avg Reward: -23.548596182570694, Replay Buffer Size: 23664\n",
      "Current Epsilon: 0.07018314008827135\n",
      "Loss rate over the last 150 episodes: 0.60\n",
      "Episode 629, Reward: -19.846799126157393, Moving Avg Reward: -23.548596182570694, Replay Buffer Size: 23664\n",
      "Current Epsilon: 0.07018314008827135\n",
      "Episode 630: Lost\n",
      "Episode 630, Avg Value Loss: 3.268091265971844, Avg Policy Loss: 3.5770713549393873\n",
      "Episode 630, Reward: -30.358944125661782, Moving Avg Reward: -23.570908752234594, Replay Buffer Size: 23677\n",
      "Current Epsilon: 0.06983222438783\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 630, Reward: -30.358944125661782, Moving Avg Reward: -23.570908752234594, Replay Buffer Size: 23677\n",
      "Current Epsilon: 0.06983222438783\n",
      "Episode 631: Lost\n",
      "Episode 631, Avg Value Loss: 3.251514029502869, Avg Policy Loss: 3.4477376103401185\n",
      "Episode 631, Reward: 4.954686407407571, Moving Avg Reward: -23.71273124238551, Replay Buffer Size: 23757\n",
      "Current Epsilon: 0.06948306326589085\n",
      "Loss rate over the last 150 episodes: 0.59\n",
      "Episode 631, Reward: 4.954686407407571, Moving Avg Reward: -23.71273124238551, Replay Buffer Size: 23757\n",
      "Current Epsilon: 0.06948306326589085\n",
      "Episode 632: Lost\n",
      "Episode 632, Avg Value Loss: 3.702063739299774, Avg Policy Loss: 3.3029428243637087\n",
      "Episode 632, Reward: -31.811667340055738, Moving Avg Reward: -23.706861114112876, Replay Buffer Size: 23777\n",
      "Current Epsilon: 0.0691356479495614\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 632, Reward: -31.811667340055738, Moving Avg Reward: -23.706861114112876, Replay Buffer Size: 23777\n",
      "Current Epsilon: 0.0691356479495614\n",
      "Episode 633: Lost\n",
      "Episode 633, Avg Value Loss: 3.276759734004736, Avg Policy Loss: 3.4756603807210924\n",
      "Episode 633, Reward: -10.25007477568631, Moving Avg Reward: -23.321109558598934, Replay Buffer Size: 23857\n",
      "Current Epsilon: 0.06878996970981359\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 633, Reward: -10.25007477568631, Moving Avg Reward: -23.321109558598934, Replay Buffer Size: 23857\n",
      "Current Epsilon: 0.06878996970981359\n",
      "Episode 634: Lost\n",
      "Episode 634, Avg Value Loss: 3.9911744594573975, Avg Policy Loss: 3.5299060344696045\n",
      "Episode 634, Reward: 9.99, Moving Avg Reward: -22.76726355234729, Replay Buffer Size: 23858\n",
      "Current Epsilon: 0.06844601986126451\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 634, Reward: 9.99, Moving Avg Reward: -22.76726355234729, Replay Buffer Size: 23858\n",
      "Current Epsilon: 0.06844601986126451\n",
      "Episode 635: Won\n",
      "Episode 635, Avg Value Loss: 3.5036310818460255, Avg Policy Loss: 3.3612325853771634\n",
      "Episode 635, Reward: -29.665865453035572, Moving Avg Reward: -22.720036578547774, Replay Buffer Size: 23912\n",
      "Current Epsilon: 0.06810378976195819\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 635, Reward: -29.665865453035572, Moving Avg Reward: -22.720036578547774, Replay Buffer Size: 23912\n",
      "Current Epsilon: 0.06810378976195819\n",
      "Episode 636: Won\n",
      "Episode 636, Avg Value Loss: 3.4018771648406982, Avg Policy Loss: 3.502430420655471\n",
      "Episode 636, Reward: -36.385927846677895, Moving Avg Reward: -23.19384354257019, Replay Buffer Size: 23925\n",
      "Current Epsilon: 0.0677632708131484\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 636, Reward: -36.385927846677895, Moving Avg Reward: -23.19384354257019, Replay Buffer Size: 23925\n",
      "Current Epsilon: 0.0677632708131484\n",
      "Episode 637: Won\n",
      "Episode 637, Avg Value Loss: 3.3914451281229656, Avg Policy Loss: 3.501420407825046\n",
      "Episode 637, Reward: -4.613913366645134, Moving Avg Reward: -23.010506464371602, Replay Buffer Size: 23970\n",
      "Current Epsilon: 0.06742445445908266\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 637, Reward: -4.613913366645134, Moving Avg Reward: -23.010506464371602, Replay Buffer Size: 23970\n",
      "Current Epsilon: 0.06742445445908266\n",
      "Episode 638: Won\n",
      "Episode 638, Avg Value Loss: 3.396838358470372, Avg Policy Loss: 3.7637729985373363\n",
      "Episode 638, Reward: -22.29379176181503, Moving Avg Reward: -22.90508372837362, Replay Buffer Size: 23977\n",
      "Current Epsilon: 0.06708733218678724\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 638, Reward: -22.29379176181503, Moving Avg Reward: -22.90508372837362, Replay Buffer Size: 23977\n",
      "Current Epsilon: 0.06708733218678724\n",
      "Episode 639: Won\n",
      "Episode 639, Avg Value Loss: 3.4022805988788605, Avg Policy Loss: 3.389618420600891\n",
      "Episode 639, Reward: -25.57514208547751, Moving Avg Reward: -23.162567647087375, Replay Buffer Size: 23987\n",
      "Current Epsilon: 0.0667518955258533\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 639, Reward: -25.57514208547751, Moving Avg Reward: -23.162567647087375, Replay Buffer Size: 23987\n",
      "Current Epsilon: 0.0667518955258533\n",
      "Episode 640: Won\n",
      "Episode 640, Avg Value Loss: 3.2964236550033092, Avg Policy Loss: 3.546410655975342\n",
      "Episode 640, Reward: -11.158522208334096, Moving Avg Reward: -23.18279468502883, Replay Buffer Size: 24067\n",
      "Current Epsilon: 0.06641813604822402\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 640, Reward: -11.158522208334096, Moving Avg Reward: -23.18279468502883, Replay Buffer Size: 24067\n",
      "Current Epsilon: 0.06641813604822402\n",
      "Episode 641: Won\n",
      "Episode 641, Avg Value Loss: 3.2274026530129567, Avg Policy Loss: 3.2297844886779785\n",
      "Episode 641, Reward: -25.54152159732443, Moving Avg Reward: -23.179482308739065, Replay Buffer Size: 24074\n",
      "Current Epsilon: 0.0660860453679829\n",
      "Loss rate over the last 150 episodes: 0.60\n",
      "Episode 641, Reward: -25.54152159732443, Moving Avg Reward: -23.179482308739065, Replay Buffer Size: 24074\n",
      "Current Epsilon: 0.0660860453679829\n",
      "Episode 642: Won\n",
      "Episode 642, Avg Value Loss: 4.362734821107653, Avg Policy Loss: 3.6797897020975747\n",
      "Episode 642, Reward: -32.68288825508604, Moving Avg Reward: -23.253835336590072, Replay Buffer Size: 24083\n",
      "Current Epsilon: 0.06575561514114299\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 642, Reward: -32.68288825508604, Moving Avg Reward: -23.253835336590072, Replay Buffer Size: 24083\n",
      "Current Epsilon: 0.06575561514114299\n",
      "Episode 643: Won\n",
      "Episode 643, Avg Value Loss: 3.7212297171354294, Avg Policy Loss: 3.4933987855911255\n",
      "Episode 643, Reward: -23.817223564394908, Moving Avg Reward: -23.6255594551994, Replay Buffer Size: 24091\n",
      "Current Epsilon: 0.06542683706543727\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 643, Reward: -23.817223564394908, Moving Avg Reward: -23.6255594551994, Replay Buffer Size: 24091\n",
      "Current Epsilon: 0.06542683706543727\n",
      "Episode 644: Won\n",
      "Episode 644, Avg Value Loss: 3.1935709692537784, Avg Policy Loss: 3.529231321811676\n",
      "Episode 644, Reward: -51.55484838848367, Moving Avg Reward: -23.858104219469983, Replay Buffer Size: 24171\n",
      "Current Epsilon: 0.06509970288011008\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 644, Reward: -51.55484838848367, Moving Avg Reward: -23.858104219469983, Replay Buffer Size: 24171\n",
      "Current Epsilon: 0.06509970288011008\n",
      "Episode 645: Lost\n",
      "Episode 645, Avg Value Loss: 3.5083085619486294, Avg Policy Loss: 3.4592390152124257\n",
      "Episode 645, Reward: 15.387090801700438, Moving Avg Reward: -23.725123777347967, Replay Buffer Size: 24197\n",
      "Current Epsilon: 0.06477420436570952\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 645, Reward: 15.387090801700438, Moving Avg Reward: -23.725123777347967, Replay Buffer Size: 24197\n",
      "Current Epsilon: 0.06477420436570952\n",
      "Episode 646: Won\n",
      "Episode 646, Avg Value Loss: 2.9316857010126114, Avg Policy Loss: 3.3580311238765717\n",
      "Episode 646, Reward: -25.22838826280958, Moving Avg Reward: -23.538743232353816, Replay Buffer Size: 24205\n",
      "Current Epsilon: 0.06445033334388098\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 646, Reward: -25.22838826280958, Moving Avg Reward: -23.538743232353816, Replay Buffer Size: 24205\n",
      "Current Epsilon: 0.06445033334388098\n",
      "Episode 647: Won\n",
      "Episode 647, Avg Value Loss: 3.4522531140934336, Avg Policy Loss: 3.4065020084381104\n",
      "Episode 647, Reward: -24.53470916225347, Moving Avg Reward: -23.56649279062492, Replay Buffer Size: 24216\n",
      "Current Epsilon: 0.06412808167716157\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 647, Reward: -24.53470916225347, Moving Avg Reward: -23.56649279062492, Replay Buffer Size: 24216\n",
      "Current Epsilon: 0.06412808167716157\n",
      "Episode 648: Won\n",
      "Episode 648, Avg Value Loss: 3.331059607592496, Avg Policy Loss: 3.7023244987834585\n",
      "Episode 648, Reward: -28.280577900181385, Moving Avg Reward: -23.897581475232126, Replay Buffer Size: 24227\n",
      "Current Epsilon: 0.06380744126877576\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 648, Reward: -28.280577900181385, Moving Avg Reward: -23.897581475232126, Replay Buffer Size: 24227\n",
      "Current Epsilon: 0.06380744126877576\n",
      "Episode 649: Won\n",
      "Episode 649, Avg Value Loss: 3.3733198983328685, Avg Policy Loss: 3.5238375663757324\n",
      "Episode 649, Reward: -20.782097122393303, Moving Avg Reward: -24.288110707248464, Replay Buffer Size: 24234\n",
      "Current Epsilon: 0.06348840406243188\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 649, Reward: -20.782097122393303, Moving Avg Reward: -24.288110707248464, Replay Buffer Size: 24234\n",
      "Current Epsilon: 0.06348840406243188\n",
      "Episode 650: Won\n",
      "Episode 650, Avg Value Loss: 3.2505093589425087, Avg Policy Loss: 3.490884003043175\n",
      "Episode 650, Reward: -4.0772655282653885, Moving Avg Reward: -23.92217065393212, Replay Buffer Size: 24314\n",
      "Current Epsilon: 0.06317096204211972\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 650, Reward: -4.0772655282653885, Moving Avg Reward: -23.92217065393212, Replay Buffer Size: 24314\n",
      "Current Epsilon: 0.06317096204211972\n",
      "Episode 651: Won\n",
      "Episode 651, Avg Value Loss: 3.1503990814089775, Avg Policy Loss: 3.5884973883628843\n",
      "Episode 651, Reward: -7.433274992374996, Moving Avg Reward: -23.67847201333838, Replay Buffer Size: 24394\n",
      "Current Epsilon: 0.06285510723190912\n",
      "Loss rate over the last 150 episodes: 0.60\n",
      "Episode 651, Reward: -7.433274992374996, Moving Avg Reward: -23.67847201333838, Replay Buffer Size: 24394\n",
      "Current Epsilon: 0.06285510723190912\n",
      "Episode 652: Draw\n",
      "Episode 652, Avg Value Loss: 3.536716914176941, Avg Policy Loss: 3.812650227546692\n",
      "Episode 652, Reward: -26.5395765835082, Moving Avg Reward: -23.565854759088612, Replay Buffer Size: 24404\n",
      "Current Epsilon: 0.06254083169574957\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 652, Reward: -26.5395765835082, Moving Avg Reward: -23.565854759088612, Replay Buffer Size: 24404\n",
      "Current Epsilon: 0.06254083169574957\n",
      "Episode 653: Lost\n",
      "Episode 653, Avg Value Loss: 3.5919829115271567, Avg Policy Loss: 3.501459449529648\n",
      "Episode 653, Reward: -18.854415163178707, Moving Avg Reward: -23.799740695121862, Replay Buffer Size: 24484\n",
      "Current Epsilon: 0.062228127537270826\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 653, Reward: -18.854415163178707, Moving Avg Reward: -23.799740695121862, Replay Buffer Size: 24484\n",
      "Current Epsilon: 0.062228127537270826\n",
      "Episode 654: Lost\n",
      "Episode 654, Avg Value Loss: 3.3113828644156458, Avg Policy Loss: 3.501467561721802\n",
      "Episode 654, Reward: -64.90178942290223, Moving Avg Reward: -24.375719401184366, Replay Buffer Size: 24564\n",
      "Current Epsilon: 0.06191698689958447\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 654, Reward: -64.90178942290223, Moving Avg Reward: -24.375719401184366, Replay Buffer Size: 24564\n",
      "Current Epsilon: 0.06191698689958447\n",
      "Episode 655: Draw\n",
      "Episode 655, Avg Value Loss: 3.3085394009947775, Avg Policy Loss: 3.479287472367287\n",
      "Episode 655, Reward: -78.21058944715702, Moving Avg Reward: -25.329616250665467, Replay Buffer Size: 24644\n",
      "Current Epsilon: 0.061607401965086545\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 655, Reward: -78.21058944715702, Moving Avg Reward: -25.329616250665467, Replay Buffer Size: 24644\n",
      "Current Epsilon: 0.061607401965086545\n",
      "Episode 656: Draw\n",
      "Episode 656, Avg Value Loss: 2.76981583237648, Avg Policy Loss: 3.4602045714855194\n",
      "Episode 656, Reward: -26.467657393043666, Moving Avg Reward: -25.301143524024788, Replay Buffer Size: 24652\n",
      "Current Epsilon: 0.06129936495526111\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 656, Reward: -26.467657393043666, Moving Avg Reward: -25.301143524024788, Replay Buffer Size: 24652\n",
      "Current Epsilon: 0.06129936495526111\n",
      "Episode 657: Lost\n",
      "Episode 657, Avg Value Loss: 3.404585912823677, Avg Policy Loss: 3.4394905388355257\n",
      "Episode 657, Reward: 4.161078371353493, Moving Avg Reward: -25.18065184403105, Replay Buffer Size: 24732\n",
      "Current Epsilon: 0.0609928681304848\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 657, Reward: 4.161078371353493, Moving Avg Reward: -25.18065184403105, Replay Buffer Size: 24732\n",
      "Current Epsilon: 0.0609928681304848\n",
      "Episode 658: Lost\n",
      "Episode 658, Avg Value Loss: 2.9246403127908707, Avg Policy Loss: 3.4380980134010315\n",
      "Episode 658, Reward: -25.836446650088323, Moving Avg Reward: -25.117387142300295, Replay Buffer Size: 24744\n",
      "Current Epsilon: 0.060687903789832374\n",
      "Loss rate over the last 150 episodes: 0.60\n",
      "Episode 658, Reward: -25.836446650088323, Moving Avg Reward: -25.117387142300295, Replay Buffer Size: 24744\n",
      "Current Epsilon: 0.060687903789832374\n",
      "Episode 659: Lost\n",
      "Episode 659, Avg Value Loss: 3.648776822619968, Avg Policy Loss: 3.5523043738471136\n",
      "Episode 659, Reward: -27.36621359251644, Moving Avg Reward: -25.071992620858, Replay Buffer Size: 24753\n",
      "Current Epsilon: 0.06038446427088321\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 659, Reward: -27.36621359251644, Moving Avg Reward: -25.071992620858, Replay Buffer Size: 24753\n",
      "Current Epsilon: 0.06038446427088321\n",
      "Episode 660: Lost\n",
      "Episode 660, Avg Value Loss: 3.49149389564991, Avg Policy Loss: 3.341210901737213\n",
      "Episode 660, Reward: -25.578546888039256, Moving Avg Reward: -25.052851322821382, Replay Buffer Size: 24761\n",
      "Current Epsilon: 0.06008254194952879\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 660, Reward: -25.578546888039256, Moving Avg Reward: -25.052851322821382, Replay Buffer Size: 24761\n",
      "Current Epsilon: 0.06008254194952879\n",
      "Episode 661: Lost\n",
      "Episode 661, Avg Value Loss: 3.4617839824585688, Avg Policy Loss: 3.4526898179735457\n",
      "Episode 661, Reward: 15.064772803012136, Moving Avg Reward: -24.586064830095516, Replay Buffer Size: 24803\n",
      "Current Epsilon: 0.05978212923978115\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 661, Reward: 15.064772803012136, Moving Avg Reward: -24.586064830095516, Replay Buffer Size: 24803\n",
      "Current Epsilon: 0.05978212923978115\n",
      "Episode 662: Won\n",
      "Episode 662, Avg Value Loss: 2.946861171722412, Avg Policy Loss: 3.6042841911315917\n",
      "Episode 662, Reward: -26.59394355424697, Moving Avg Reward: -24.640073131654987, Replay Buffer Size: 24813\n",
      "Current Epsilon: 0.05948321859358224\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 662, Reward: -26.59394355424697, Moving Avg Reward: -24.640073131654987, Replay Buffer Size: 24813\n",
      "Current Epsilon: 0.05948321859358224\n",
      "Episode 663: Won\n",
      "Episode 663, Avg Value Loss: 2.747859458128611, Avg Policy Loss: 3.6486936807632446\n",
      "Episode 663, Reward: -27.87335777537058, Moving Avg Reward: -24.636091905942344, Replay Buffer Size: 24825\n",
      "Current Epsilon: 0.05918580250061433\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 663, Reward: -27.87335777537058, Moving Avg Reward: -24.636091905942344, Replay Buffer Size: 24825\n",
      "Current Epsilon: 0.05918580250061433\n",
      "Episode 664: Won\n",
      "Episode 664, Avg Value Loss: 3.580574930930624, Avg Policy Loss: 3.4548725741250172\n",
      "Episode 664, Reward: -17.643912504884284, Moving Avg Reward: -23.990660801169103, Replay Buffer Size: 24874\n",
      "Current Epsilon: 0.058889873488111255\n",
      "Loss rate over the last 150 episodes: 0.60\n",
      "Episode 664, Reward: -17.643912504884284, Moving Avg Reward: -23.990660801169103, Replay Buffer Size: 24874\n",
      "Current Epsilon: 0.058889873488111255\n",
      "Episode 665: Won\n",
      "Episode 665, Avg Value Loss: 3.278906409442425, Avg Policy Loss: 3.484283369779587\n",
      "Episode 665, Reward: 4.08421861794875, Moving Avg Reward: -23.72741069195306, Replay Buffer Size: 24954\n",
      "Current Epsilon: 0.058595424120670696\n",
      "Loss rate over the last 150 episodes: 0.60\n",
      "Episode 665, Reward: 4.08421861794875, Moving Avg Reward: -23.72741069195306, Replay Buffer Size: 24954\n",
      "Current Epsilon: 0.058595424120670696\n",
      "Episode 666: Lost\n",
      "Episode 666, Avg Value Loss: 2.970713973045349, Avg Policy Loss: 3.4674065663264346\n",
      "Episode 666, Reward: -33.35961591818094, Moving Avg Reward: -24.2391817273082, Replay Buffer Size: 24967\n",
      "Current Epsilon: 0.05830244700006734\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 666, Reward: -33.35961591818094, Moving Avg Reward: -24.2391817273082, Replay Buffer Size: 24967\n",
      "Current Epsilon: 0.05830244700006734\n",
      "Episode 667: Lost\n",
      "Episode 667, Avg Value Loss: 3.3336850079623135, Avg Policy Loss: 3.6876766031438653\n",
      "Episode 667, Reward: -23.956090710867045, Moving Avg Reward: -24.165464496948733, Replay Buffer Size: 24978\n",
      "Current Epsilon: 0.058010934765067\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 667, Reward: -23.956090710867045, Moving Avg Reward: -24.165464496948733, Replay Buffer Size: 24978\n",
      "Current Epsilon: 0.058010934765067\n",
      "Episode 668: Lost\n",
      "Episode 668, Avg Value Loss: 2.9283817755548576, Avg Policy Loss: 3.4270522029776322\n",
      "Episode 668, Reward: 17.604836694049524, Moving Avg Reward: -23.7281868347098, Replay Buffer Size: 25016\n",
      "Current Epsilon: 0.05772088009124167\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 668, Reward: 17.604836694049524, Moving Avg Reward: -23.7281868347098, Replay Buffer Size: 25016\n",
      "Current Epsilon: 0.05772088009124167\n",
      "Episode 669: Won\n",
      "Episode 669, Avg Value Loss: 3.72759006023407, Avg Policy Loss: 3.5120720148086546\n",
      "Episode 669, Reward: 15.982981753194768, Moving Avg Reward: -23.580497583981177, Replay Buffer Size: 25036\n",
      "Current Epsilon: 0.05743227569078546\n",
      "Loss rate over the last 150 episodes: 0.60\n",
      "Episode 669, Reward: 15.982981753194768, Moving Avg Reward: -23.580497583981177, Replay Buffer Size: 25036\n",
      "Current Epsilon: 0.05743227569078546\n",
      "Episode 670: Won\n",
      "Episode 670, Avg Value Loss: 3.1107186476389566, Avg Policy Loss: 3.687301834424337\n",
      "Episode 670, Reward: -18.359084692217294, Moving Avg Reward: -23.54701720448402, Replay Buffer Size: 25042\n",
      "Current Epsilon: 0.05714511431233153\n",
      "Loss rate over the last 150 episodes: 0.61\n",
      "Episode 670, Reward: -18.359084692217294, Moving Avg Reward: -23.54701720448402, Replay Buffer Size: 25042\n",
      "Current Epsilon: 0.05714511431233153\n",
      "Episode 671: Won\n",
      "Episode 671, Avg Value Loss: 3.5741973140022973, Avg Policy Loss: 3.565713882446289\n",
      "Episode 671, Reward: -30.648014967383148, Moving Avg Reward: -23.8424574638716, Replay Buffer Size: 25053\n",
      "Current Epsilon: 0.05685938874076987\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 671, Reward: -30.648014967383148, Moving Avg Reward: -23.8424574638716, Replay Buffer Size: 25053\n",
      "Current Epsilon: 0.05685938874076987\n",
      "Episode 672: Won\n",
      "Episode 672, Avg Value Loss: 3.4055127018973943, Avg Policy Loss: 3.4434993380591985\n",
      "Episode 672, Reward: 16.91464186696024, Moving Avg Reward: -23.633582172828554, Replay Buffer Size: 25074\n",
      "Current Epsilon: 0.056575091797066025\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 672, Reward: 16.91464186696024, Moving Avg Reward: -23.633582172828554, Replay Buffer Size: 25074\n",
      "Current Epsilon: 0.056575091797066025\n",
      "Episode 673: Won\n",
      "Episode 673, Avg Value Loss: 2.647220194339752, Avg Policy Loss: 3.438675880432129\n",
      "Episode 673, Reward: -26.678300743468476, Moving Avg Reward: -24.00026518026324, Replay Buffer Size: 25082\n",
      "Current Epsilon: 0.056292216338080694\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 673, Reward: -26.678300743468476, Moving Avg Reward: -24.00026518026324, Replay Buffer Size: 25082\n",
      "Current Epsilon: 0.056292216338080694\n",
      "Episode 674: Won\n",
      "Episode 674, Avg Value Loss: 3.4395298287272453, Avg Policy Loss: 3.5301243126392365\n",
      "Episode 674, Reward: -4.770569750449976, Moving Avg Reward: -23.738149653272497, Replay Buffer Size: 25162\n",
      "Current Epsilon: 0.05601075525639029\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 674, Reward: -4.770569750449976, Moving Avg Reward: -23.738149653272497, Replay Buffer Size: 25162\n",
      "Current Epsilon: 0.05601075525639029\n",
      "Episode 675: Won\n",
      "Episode 675, Avg Value Loss: 4.106234959193638, Avg Policy Loss: 3.6024577958243236\n",
      "Episode 675, Reward: -22.44771886188344, Moving Avg Reward: -23.968057337913027, Replay Buffer Size: 25169\n",
      "Current Epsilon: 0.05573070148010834\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 675, Reward: -22.44771886188344, Moving Avg Reward: -23.968057337913027, Replay Buffer Size: 25169\n",
      "Current Epsilon: 0.05573070148010834\n",
      "Episode 676: Won\n",
      "Episode 676, Avg Value Loss: 3.117518365383148, Avg Policy Loss: 3.529189403240497\n",
      "Episode 676, Reward: -31.262980855244443, Moving Avg Reward: -24.313991800978854, Replay Buffer Size: 25182\n",
      "Current Epsilon: 0.0554520479727078\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 676, Reward: -31.262980855244443, Moving Avg Reward: -24.313991800978854, Replay Buffer Size: 25182\n",
      "Current Epsilon: 0.0554520479727078\n",
      "Episode 677: Won\n",
      "Episode 677, Avg Value Loss: 3.388207256793976, Avg Policy Loss: 3.503428014990402\n",
      "Episode 677, Reward: -47.7209893676655, Moving Avg Reward: -24.483592833176466, Replay Buffer Size: 25255\n",
      "Current Epsilon: 0.05517478773284426\n",
      "Loss rate over the last 150 episodes: 0.66\n",
      "Episode 677, Reward: -47.7209893676655, Moving Avg Reward: -24.483592833176466, Replay Buffer Size: 25255\n",
      "Current Epsilon: 0.05517478773284426\n",
      "Episode 678: Lost\n",
      "Episode 678, Avg Value Loss: 3.314549560844898, Avg Policy Loss: 3.535596823692322\n",
      "Episode 678, Reward: -18.392110195330332, Moving Avg Reward: -24.38335283563967, Replay Buffer Size: 25335\n",
      "Current Epsilon: 0.05489891379418004\n",
      "Loss rate over the last 150 episodes: 0.66\n",
      "Episode 678, Reward: -18.392110195330332, Moving Avg Reward: -24.38335283563967, Replay Buffer Size: 25335\n",
      "Current Epsilon: 0.05489891379418004\n",
      "Episode 679: Lost\n",
      "Episode 679, Avg Value Loss: 3.3499675137656078, Avg Policy Loss: 3.4952037164143155\n",
      "Episode 679, Reward: -32.09268906243333, Moving Avg Reward: -24.398522327218593, Replay Buffer Size: 25349\n",
      "Current Epsilon: 0.05462441922520914\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 679, Reward: -32.09268906243333, Moving Avg Reward: -24.398522327218593, Replay Buffer Size: 25349\n",
      "Current Epsilon: 0.05462441922520914\n",
      "Episode 680: Lost\n",
      "Episode 680, Avg Value Loss: 3.070316392928362, Avg Policy Loss: 3.527978368103504\n",
      "Episode 680, Reward: 12.106651684926367, Moving Avg Reward: -24.40424262238071, Replay Buffer Size: 25381\n",
      "Current Epsilon: 0.0543512971290831\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 680, Reward: 12.106651684926367, Moving Avg Reward: -24.40424262238071, Replay Buffer Size: 25381\n",
      "Current Epsilon: 0.0543512971290831\n",
      "Episode 681: Won\n",
      "Episode 681, Avg Value Loss: 3.6705246722256697, Avg Policy Loss: 3.4635828336079917\n",
      "Episode 681, Reward: -41.024549025624346, Moving Avg Reward: -24.343183178406505, Replay Buffer Size: 25408\n",
      "Current Epsilon: 0.05407954064343768\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 681, Reward: -41.024549025624346, Moving Avg Reward: -24.343183178406505, Replay Buffer Size: 25408\n",
      "Current Epsilon: 0.05407954064343768\n",
      "Episode 682: Won\n",
      "Episode 682, Avg Value Loss: 3.431539836384001, Avg Policy Loss: 3.6066640672229586\n",
      "Episode 682, Reward: -21.876531367392182, Moving Avg Reward: -24.610677757183556, Replay Buffer Size: 25450\n",
      "Current Epsilon: 0.05380914294022049\n",
      "Loss rate over the last 150 episodes: 0.68\n",
      "Episode 682, Reward: -21.876531367392182, Moving Avg Reward: -24.610677757183556, Replay Buffer Size: 25450\n",
      "Current Epsilon: 0.05380914294022049\n",
      "Episode 683: Won\n",
      "Episode 683, Avg Value Loss: 3.1430524984995523, Avg Policy Loss: 3.609433650970459\n",
      "Episode 683, Reward: -21.57121984487991, Moving Avg Reward: -24.581799164955, Replay Buffer Size: 25459\n",
      "Current Epsilon: 0.05354009722551939\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 683, Reward: -21.57121984487991, Moving Avg Reward: -24.581799164955, Replay Buffer Size: 25459\n",
      "Current Epsilon: 0.05354009722551939\n",
      "Episode 684: Won\n",
      "Episode 684, Avg Value Loss: 3.6581113555214624, Avg Policy Loss: 3.488431453704834\n",
      "Episode 684, Reward: 9.89, Moving Avg Reward: -24.598924969921043, Replay Buffer Size: 25470\n",
      "Current Epsilon: 0.05327239673939179\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 684, Reward: 9.89, Moving Avg Reward: -24.598924969921043, Replay Buffer Size: 25470\n",
      "Current Epsilon: 0.05327239673939179\n",
      "Episode 685: Won\n",
      "Episode 685, Avg Value Loss: 3.3392999932169913, Avg Policy Loss: 3.572318562865257\n",
      "Episode 685, Reward: -18.078881102082093, Moving Avg Reward: -24.469832431908188, Replay Buffer Size: 25550\n",
      "Current Epsilon: 0.053006034755694834\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 685, Reward: -18.078881102082093, Moving Avg Reward: -24.469832431908188, Replay Buffer Size: 25550\n",
      "Current Epsilon: 0.053006034755694834\n",
      "Episode 686: Won\n",
      "Episode 686, Avg Value Loss: 3.4975587037893443, Avg Policy Loss: 3.4180269424731913\n",
      "Episode 686, Reward: -31.217001445397663, Moving Avg Reward: -24.545854674039155, Replay Buffer Size: 25563\n",
      "Current Epsilon: 0.052741004581916356\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 686, Reward: -31.217001445397663, Moving Avg Reward: -24.545854674039155, Replay Buffer Size: 25563\n",
      "Current Epsilon: 0.052741004581916356\n",
      "Episode 687: Won\n",
      "Episode 687, Avg Value Loss: 3.530219808513043, Avg Policy Loss: 3.521962039610919\n",
      "Episode 687, Reward: -26.310881799929835, Moving Avg Reward: -24.47342401121034, Replay Buffer Size: 25614\n",
      "Current Epsilon: 0.052477299559006776\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 687, Reward: -26.310881799929835, Moving Avg Reward: -24.47342401121034, Replay Buffer Size: 25614\n",
      "Current Epsilon: 0.052477299559006776\n",
      "Episode 688: Won\n",
      "Episode 688, Avg Value Loss: 3.2594629372319868, Avg Policy Loss: 3.5506109691435292\n",
      "Episode 688, Reward: 13.068822166722914, Moving Avg Reward: -23.88140040487186, Replay Buffer Size: 25676\n",
      "Current Epsilon: 0.052214913061211746\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 688, Reward: 13.068822166722914, Moving Avg Reward: -23.88140040487186, Replay Buffer Size: 25676\n",
      "Current Epsilon: 0.052214913061211746\n",
      "Episode 689: Won\n",
      "Episode 689, Avg Value Loss: 3.0667665243148803, Avg Policy Loss: 3.599592661857605\n",
      "Episode 689, Reward: -29.98346220923431, Moving Avg Reward: -23.74865548750994, Replay Buffer Size: 25686\n",
      "Current Epsilon: 0.05195383849590569\n",
      "Loss rate over the last 150 episodes: 0.66\n",
      "Episode 689, Reward: -29.98346220923431, Moving Avg Reward: -23.74865548750994, Replay Buffer Size: 25686\n",
      "Current Epsilon: 0.05195383849590569\n",
      "Episode 690: Won\n",
      "Episode 690, Avg Value Loss: 2.9494587421417235, Avg Policy Loss: 3.497781682014465\n",
      "Episode 690, Reward: -26.639166485279183, Moving Avg Reward: -23.389005830315423, Replay Buffer Size: 25696\n",
      "Current Epsilon: 0.05169406930342616\n",
      "Loss rate over the last 150 episodes: 0.66\n",
      "Episode 690, Reward: -26.639166485279183, Moving Avg Reward: -23.389005830315423, Replay Buffer Size: 25696\n",
      "Current Epsilon: 0.05169406930342616\n",
      "Episode 691: Won\n",
      "Episode 691, Avg Value Loss: 3.381828556458155, Avg Policy Loss: 3.5417574644088745\n",
      "Episode 691, Reward: -31.266400267420366, Moving Avg Reward: -23.446190412390937, Replay Buffer Size: 25750\n",
      "Current Epsilon: 0.05143559895690903\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 691, Reward: -31.266400267420366, Moving Avg Reward: -23.446190412390937, Replay Buffer Size: 25750\n",
      "Current Epsilon: 0.05143559895690903\n",
      "Episode 692: Won\n",
      "Episode 692, Avg Value Loss: 3.568708211183548, Avg Policy Loss: 3.3521463572978973\n",
      "Episode 692, Reward: -27.633440144016785, Moving Avg Reward: -23.875776364368097, Replay Buffer Size: 25758\n",
      "Current Epsilon: 0.051178420962124486\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 692, Reward: -27.633440144016785, Moving Avg Reward: -23.875776364368097, Replay Buffer Size: 25758\n",
      "Current Epsilon: 0.051178420962124486\n",
      "Episode 693: Won\n",
      "Episode 693, Avg Value Loss: 3.625962885943326, Avg Policy Loss: 3.582927617159757\n",
      "Episode 693, Reward: -27.90354346943058, Moving Avg Reward: -23.476485169236774, Replay Buffer Size: 25769\n",
      "Current Epsilon: 0.05092252885731386\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 693, Reward: -27.90354346943058, Moving Avg Reward: -23.476485169236774, Replay Buffer Size: 25769\n",
      "Current Epsilon: 0.05092252885731386\n",
      "Episode 694: Won\n",
      "Episode 694, Avg Value Loss: 3.6164632943960338, Avg Policy Loss: 3.4438160749582143\n",
      "Episode 694, Reward: -33.04977974196983, Moving Avg Reward: -23.46689848379669, Replay Buffer Size: 25782\n",
      "Current Epsilon: 0.05066791621302729\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 694, Reward: -33.04977974196983, Moving Avg Reward: -23.46689848379669, Replay Buffer Size: 25782\n",
      "Current Epsilon: 0.05066791621302729\n",
      "Episode 695: Won\n",
      "Episode 695, Avg Value Loss: 3.2445212710987437, Avg Policy Loss: 3.3312491286884653\n",
      "Episode 695, Reward: -27.515937010513117, Moving Avg Reward: -23.387374146248504, Replay Buffer Size: 25793\n",
      "Current Epsilon: 0.05041457663196215\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 695, Reward: -27.515937010513117, Moving Avg Reward: -23.387374146248504, Replay Buffer Size: 25793\n",
      "Current Epsilon: 0.05041457663196215\n",
      "Episode 696: Won\n",
      "Episode 696, Avg Value Loss: 3.767028496815608, Avg Policy Loss: 3.539296260246864\n",
      "Episode 696, Reward: -31.736718763753174, Moving Avg Reward: -23.34329040199715, Replay Buffer Size: 25806\n",
      "Current Epsilon: 0.050162503748802344\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 696, Reward: -31.736718763753174, Moving Avg Reward: -23.34329040199715, Replay Buffer Size: 25806\n",
      "Current Epsilon: 0.050162503748802344\n",
      "Episode 697: Won\n",
      "Episode 697, Avg Value Loss: 4.139348957273695, Avg Policy Loss: 3.330746465259128\n",
      "Episode 697, Reward: -24.99988203900407, Moving Avg Reward: -23.251209106014425, Replay Buffer Size: 25815\n",
      "Current Epsilon: 0.049911691230058335\n",
      "Loss rate over the last 150 episodes: 0.66\n",
      "Episode 697, Reward: -24.99988203900407, Moving Avg Reward: -23.251209106014425, Replay Buffer Size: 25815\n",
      "Current Epsilon: 0.049911691230058335\n",
      "Episode 698: Won\n",
      "Episode 698, Avg Value Loss: 3.3464377323786416, Avg Policy Loss: 3.494746526082357\n",
      "Episode 698, Reward: -24.449751898195174, Moving Avg Reward: -23.219257638259066, Replay Buffer Size: 25824\n",
      "Current Epsilon: 0.04966213277390804\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 698, Reward: -24.449751898195174, Moving Avg Reward: -23.219257638259066, Replay Buffer Size: 25824\n",
      "Current Epsilon: 0.04966213277390804\n",
      "Episode 699: Won\n",
      "Episode 699, Avg Value Loss: 3.3523354932665823, Avg Policy Loss: 3.595592510700226\n",
      "Episode 699, Reward: -32.47049264352479, Moving Avg Reward: -23.22013786391602, Replay Buffer Size: 25904\n",
      "Current Epsilon: 0.0494138221100385\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 699, Reward: -32.47049264352479, Moving Avg Reward: -23.22013786391602, Replay Buffer Size: 25904\n",
      "Current Epsilon: 0.0494138221100385\n",
      "Episode 700: Lost\n",
      "Episode 700, Avg Value Loss: 3.329255485534668, Avg Policy Loss: 3.30767560005188\n",
      "Episode 700, Reward: -25.35044584063812, Moving Avg Reward: -23.31342583596738, Replay Buffer Size: 25914\n",
      "Current Epsilon: 0.04916675299948831\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 700, Reward: -25.35044584063812, Moving Avg Reward: -23.31342583596738, Replay Buffer Size: 25914\n",
      "Current Epsilon: 0.04916675299948831\n",
      "Episode 701: Lost\n",
      "Episode 701, Avg Value Loss: 3.3363560140132904, Avg Policy Loss: 3.6707793209287853\n",
      "Episode 701, Reward: -15.897774321909909, Moving Avg Reward: -23.115072650551657, Replay Buffer Size: 25986\n",
      "Current Epsilon: 0.04892091923449087\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 701, Reward: -15.897774321909909, Moving Avg Reward: -23.115072650551657, Replay Buffer Size: 25986\n",
      "Current Epsilon: 0.04892091923449087\n",
      "Episode 702: Won\n",
      "Episode 702, Avg Value Loss: 2.92185748120149, Avg Policy Loss: 3.512895872195562\n",
      "Episode 702, Reward: 16.713994274031943, Moving Avg Reward: -22.79138232615248, Replay Buffer Size: 26010\n",
      "Current Epsilon: 0.04867631463831842\n",
      "Loss rate over the last 150 episodes: 0.68\n",
      "Episode 702, Reward: 16.713994274031943, Moving Avg Reward: -22.79138232615248, Replay Buffer Size: 26010\n",
      "Current Epsilon: 0.04867631463831842\n",
      "Episode 703: Won\n",
      "Episode 703, Avg Value Loss: 3.452725112438202, Avg Policy Loss: 3.3957907259464264\n",
      "Episode 703, Reward: -25.7962921187992, Moving Avg Reward: -22.75778409156091, Replay Buffer Size: 26018\n",
      "Current Epsilon: 0.048432933065126825\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 703, Reward: -25.7962921187992, Moving Avg Reward: -22.75778409156091, Replay Buffer Size: 26018\n",
      "Current Epsilon: 0.048432933065126825\n",
      "Episode 704: Won\n",
      "Episode 704, Avg Value Loss: 3.4908195406198503, Avg Policy Loss: 3.4838299989700316\n",
      "Episode 704, Reward: -65.05294544813752, Moving Avg Reward: -23.133518806463268, Replay Buffer Size: 26058\n",
      "Current Epsilon: 0.048190768399801194\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 704, Reward: -65.05294544813752, Moving Avg Reward: -23.133518806463268, Replay Buffer Size: 26058\n",
      "Current Epsilon: 0.048190768399801194\n",
      "Episode 705: Won\n",
      "Episode 705, Avg Value Loss: 3.107849983870983, Avg Policy Loss: 3.5843000173568726\n",
      "Episode 705, Reward: -20.749928705177467, Moving Avg Reward: -23.01823938366645, Replay Buffer Size: 26138\n",
      "Current Epsilon: 0.04794981455780219\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 705, Reward: -20.749928705177467, Moving Avg Reward: -23.01823938366645, Replay Buffer Size: 26138\n",
      "Current Epsilon: 0.04794981455780219\n",
      "Episode 706: Won\n",
      "Episode 706, Avg Value Loss: 3.4663347005844116, Avg Policy Loss: 3.5423015475273134\n",
      "Episode 706, Reward: -44.806044320877746, Moving Avg Reward: -23.200420095430676, Replay Buffer Size: 26218\n",
      "Current Epsilon: 0.04771006548501318\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 706, Reward: -44.806044320877746, Moving Avg Reward: -23.200420095430676, Replay Buffer Size: 26218\n",
      "Current Epsilon: 0.04771006548501318\n",
      "Episode 707: Draw\n",
      "Episode 707, Avg Value Loss: 2.987177453257821, Avg Policy Loss: 3.4939563924616035\n",
      "Episode 707, Reward: -10.86268965096067, Moving Avg Reward: -22.328946759897573, Replay Buffer Size: 26229\n",
      "Current Epsilon: 0.047471515157588115\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 707, Reward: -10.86268965096067, Moving Avg Reward: -22.328946759897573, Replay Buffer Size: 26229\n",
      "Current Epsilon: 0.047471515157588115\n",
      "Episode 708: Lost\n",
      "Episode 708, Avg Value Loss: 3.6264234893023968, Avg Policy Loss: 3.571266296505928\n",
      "Episode 708, Reward: 6.263840577941566, Moving Avg Reward: -22.141270262595455, Replay Buffer Size: 26309\n",
      "Current Epsilon: 0.047234157581800176\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 708, Reward: 6.263840577941566, Moving Avg Reward: -22.141270262595455, Replay Buffer Size: 26309\n",
      "Current Epsilon: 0.047234157581800176\n",
      "Episode 709: Lost\n",
      "Episode 709, Avg Value Loss: 2.98647651773818, Avg Policy Loss: 3.6138122994849025\n",
      "Episode 709, Reward: -32.712840652176865, Moving Avg Reward: -22.254517414554662, Replay Buffer Size: 26356\n",
      "Current Epsilon: 0.046997986793891174\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 709, Reward: -32.712840652176865, Moving Avg Reward: -22.254517414554662, Replay Buffer Size: 26356\n",
      "Current Epsilon: 0.046997986793891174\n",
      "Episode 710: Lost\n",
      "Episode 710, Avg Value Loss: 3.5494289308786393, Avg Policy Loss: 3.546223646402359\n",
      "Episode 710, Reward: -153.56753044233972, Moving Avg Reward: -22.959276370802613, Replay Buffer Size: 26436\n",
      "Current Epsilon: 0.04676299685992172\n",
      "Loss rate over the last 150 episodes: 0.66\n",
      "Episode 710, Reward: -153.56753044233972, Moving Avg Reward: -22.959276370802613, Replay Buffer Size: 26436\n",
      "Current Epsilon: 0.04676299685992172\n",
      "Episode 711: Lost\n",
      "Episode 711, Avg Value Loss: 3.512225242761465, Avg Policy Loss: 3.658077826866737\n",
      "Episode 711, Reward: -28.930827830668477, Moving Avg Reward: -22.95344347019354, Replay Buffer Size: 26449\n",
      "Current Epsilon: 0.04652918187562211\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 711, Reward: -28.930827830668477, Moving Avg Reward: -22.95344347019354, Replay Buffer Size: 26449\n",
      "Current Epsilon: 0.04652918187562211\n",
      "Episode 712: Lost\n",
      "Episode 712, Avg Value Loss: 2.900702476501465, Avg Policy Loss: 3.653075236540574\n",
      "Episode 712, Reward: -33.68305141510517, Moving Avg Reward: -22.49577523933086, Replay Buffer Size: 26462\n",
      "Current Epsilon: 0.046296535966244\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 712, Reward: -33.68305141510517, Moving Avg Reward: -22.49577523933086, Replay Buffer Size: 26462\n",
      "Current Epsilon: 0.046296535966244\n",
      "Episode 713: Lost\n",
      "Episode 713, Avg Value Loss: 3.6322207884355024, Avg Policy Loss: 3.7121297012675893\n",
      "Episode 713, Reward: -24.25269847466147, Moving Avg Reward: -22.61240110153587, Replay Buffer Size: 26473\n",
      "Current Epsilon: 0.046065053286412784\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 713, Reward: -24.25269847466147, Moving Avg Reward: -22.61240110153587, Replay Buffer Size: 26473\n",
      "Current Epsilon: 0.046065053286412784\n",
      "Episode 714: Lost\n",
      "Episode 714, Avg Value Loss: 3.232106614112854, Avg Policy Loss: 3.7633461236953734\n",
      "Episode 714, Reward: -28.578506691785925, Moving Avg Reward: -22.636831900081415, Replay Buffer Size: 26483\n",
      "Current Epsilon: 0.04583472801998072\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 714, Reward: -28.578506691785925, Moving Avg Reward: -22.636831900081415, Replay Buffer Size: 26483\n",
      "Current Epsilon: 0.04583472801998072\n",
      "Episode 715: Lost\n",
      "Episode 715, Avg Value Loss: 3.356389535797967, Avg Policy Loss: 3.447485738330417\n",
      "Episode 715, Reward: -26.21536568633597, Moving Avg Reward: -22.6367281318245, Replay Buffer Size: 26492\n",
      "Current Epsilon: 0.045605554379880814\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 715, Reward: -26.21536568633597, Moving Avg Reward: -22.6367281318245, Replay Buffer Size: 26492\n",
      "Current Epsilon: 0.045605554379880814\n",
      "Episode 716: Lost\n",
      "Episode 716, Avg Value Loss: 3.3491924539208413, Avg Policy Loss: 3.5221461445093154\n",
      "Episode 716, Reward: -56.11641450542602, Moving Avg Reward: -22.96329036619672, Replay Buffer Size: 26572\n",
      "Current Epsilon: 0.04537752660798141\n",
      "Loss rate over the last 150 episodes: 0.66\n",
      "Episode 716, Reward: -56.11641450542602, Moving Avg Reward: -22.96329036619672, Replay Buffer Size: 26572\n",
      "Current Epsilon: 0.04537752660798141\n",
      "Episode 717: Lost\n",
      "Episode 717, Avg Value Loss: 3.395785629749298, Avg Policy Loss: 3.5163983949800817\n",
      "Episode 717, Reward: -4.450542401670839, Moving Avg Reward: -22.724842034920904, Replay Buffer Size: 26613\n",
      "Current Epsilon: 0.0451506389749415\n",
      "Loss rate over the last 150 episodes: 0.68\n",
      "Episode 717, Reward: -4.450542401670839, Moving Avg Reward: -22.724842034920904, Replay Buffer Size: 26613\n",
      "Current Epsilon: 0.0451506389749415\n",
      "Episode 718: Won\n",
      "Episode 718, Avg Value Loss: 2.9717889964580535, Avg Policy Loss: 3.5173274755477903\n",
      "Episode 718, Reward: 19.396097425473798, Moving Avg Reward: -22.183813960397316, Replay Buffer Size: 26633\n",
      "Current Epsilon: 0.044924885780066794\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 718, Reward: 19.396097425473798, Moving Avg Reward: -22.183813960397316, Replay Buffer Size: 26633\n",
      "Current Epsilon: 0.044924885780066794\n",
      "Episode 719: Won\n",
      "Episode 719, Avg Value Loss: 3.6482726443897593, Avg Policy Loss: 3.488606409593062\n",
      "Episode 719, Reward: -25.72079792051752, Moving Avg Reward: -22.206655233505032, Replay Buffer Size: 26644\n",
      "Current Epsilon: 0.04470026135116646\n",
      "Loss rate over the last 150 episodes: 0.66\n",
      "Episode 719, Reward: -25.72079792051752, Moving Avg Reward: -22.206655233505032, Replay Buffer Size: 26644\n",
      "Current Epsilon: 0.04470026135116646\n",
      "Episode 720: Won\n",
      "Episode 720, Avg Value Loss: 3.120609508620368, Avg Policy Loss: 3.7027924590640597\n",
      "Episode 720, Reward: -32.52052797543504, Moving Avg Reward: -22.300269746152924, Replay Buffer Size: 26653\n",
      "Current Epsilon: 0.04447676004441063\n",
      "Loss rate over the last 150 episodes: 0.66\n",
      "Episode 720, Reward: -32.52052797543504, Moving Avg Reward: -22.300269746152924, Replay Buffer Size: 26653\n",
      "Current Epsilon: 0.04447676004441063\n",
      "Episode 721: Won\n",
      "Episode 721, Avg Value Loss: 3.4836281768977644, Avg Policy Loss: 3.5570311427116392\n",
      "Episode 721, Reward: -85.33241867460288, Moving Avg Reward: -23.335514463572462, Replay Buffer Size: 26733\n",
      "Current Epsilon: 0.04425437624418858\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 721, Reward: -85.33241867460288, Moving Avg Reward: -23.335514463572462, Replay Buffer Size: 26733\n",
      "Current Epsilon: 0.04425437624418858\n",
      "Episode 722: Won\n",
      "Episode 722, Avg Value Loss: 3.557840837372674, Avg Policy Loss: 3.4085475073920355\n",
      "Episode 722, Reward: -24.843070311177108, Moving Avg Reward: -23.495365846854227, Replay Buffer Size: 26742\n",
      "Current Epsilon: 0.04403310436296763\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 722, Reward: -24.843070311177108, Moving Avg Reward: -23.495365846854227, Replay Buffer Size: 26742\n",
      "Current Epsilon: 0.04403310436296763\n",
      "Episode 723: Won\n",
      "Episode 723, Avg Value Loss: 3.9976540406545005, Avg Policy Loss: 3.6037156846788196\n",
      "Episode 723, Reward: -25.478484722905872, Moving Avg Reward: -23.468089540014862, Replay Buffer Size: 26751\n",
      "Current Epsilon: 0.043812938841152796\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 723, Reward: -25.478484722905872, Moving Avg Reward: -23.468089540014862, Replay Buffer Size: 26751\n",
      "Current Epsilon: 0.043812938841152796\n",
      "Episode 724: Won\n",
      "Episode 724, Avg Value Loss: 3.8538229167461395, Avg Policy Loss: 3.174289256334305\n",
      "Episode 724, Reward: -24.84000843747772, Moving Avg Reward: -23.559431955968336, Replay Buffer Size: 26759\n",
      "Current Epsilon: 0.04359387414694703\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 724, Reward: -24.84000843747772, Moving Avg Reward: -23.559431955968336, Replay Buffer Size: 26759\n",
      "Current Epsilon: 0.04359387414694703\n",
      "Episode 725: Won\n",
      "Episode 725, Avg Value Loss: 3.394282911717892, Avg Policy Loss: 3.5499036580324175\n",
      "Episode 725, Reward: -31.694808218698963, Moving Avg Reward: -23.39991885485964, Replay Buffer Size: 26839\n",
      "Current Epsilon: 0.043375904776212296\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 725, Reward: -31.694808218698963, Moving Avg Reward: -23.39991885485964, Replay Buffer Size: 26839\n",
      "Current Epsilon: 0.043375904776212296\n",
      "Episode 726: Lost\n",
      "Episode 726, Avg Value Loss: 3.7705303728580475, Avg Policy Loss: 3.430409699678421\n",
      "Episode 726, Reward: 15.906569843754342, Moving Avg Reward: -22.927831662128877, Replay Buffer Size: 26863\n",
      "Current Epsilon: 0.043159025252331236\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 726, Reward: 15.906569843754342, Moving Avg Reward: -22.927831662128877, Replay Buffer Size: 26863\n",
      "Current Epsilon: 0.043159025252331236\n",
      "Episode 727: Won\n",
      "Episode 727, Avg Value Loss: 2.883645488665654, Avg Policy Loss: 3.702924508314866\n",
      "Episode 727, Reward: -31.170035969808886, Moving Avg Reward: -22.875121485120356, Replay Buffer Size: 26876\n",
      "Current Epsilon: 0.04294323012606958\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 727, Reward: -31.170035969808886, Moving Avg Reward: -22.875121485120356, Replay Buffer Size: 26876\n",
      "Current Epsilon: 0.04294323012606958\n",
      "Episode 728: Won\n",
      "Episode 728, Avg Value Loss: 3.3735132176300575, Avg Policy Loss: 3.5052271546988654\n",
      "Episode 728, Reward: 16.598879407376387, Moving Avg Reward: -22.637105842622613, Replay Buffer Size: 26905\n",
      "Current Epsilon: 0.04272851397543923\n",
      "Loss rate over the last 150 episodes: 0.66\n",
      "Episode 728, Reward: 16.598879407376387, Moving Avg Reward: -22.637105842622613, Replay Buffer Size: 26905\n",
      "Current Epsilon: 0.04272851397543923\n",
      "Episode 729: Won\n",
      "Episode 729, Avg Value Loss: 3.532107928395271, Avg Policy Loss: 3.465206519762675\n",
      "Episode 729, Reward: -38.340502887285, Moving Avg Reward: -22.822042880233884, Replay Buffer Size: 26965\n",
      "Current Epsilon: 0.04251487140556204\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 729, Reward: -38.340502887285, Moving Avg Reward: -22.822042880233884, Replay Buffer Size: 26965\n",
      "Current Epsilon: 0.04251487140556204\n",
      "Episode 730: Won\n",
      "Episode 730, Avg Value Loss: 3.3832098340988157, Avg Policy Loss: 3.5430909824371337\n",
      "Episode 730, Reward: 11.120750896794718, Moving Avg Reward: -22.40724593000932, Replay Buffer Size: 27015\n",
      "Current Epsilon: 0.04230229704853423\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 730, Reward: 11.120750896794718, Moving Avg Reward: -22.40724593000932, Replay Buffer Size: 27015\n",
      "Current Epsilon: 0.04230229704853423\n",
      "Episode 731: Won\n",
      "Episode 731, Avg Value Loss: 4.134377135170831, Avg Policy Loss: 3.4305397669474282\n",
      "Episode 731, Reward: -27.122837730767145, Moving Avg Reward: -22.728021171391074, Replay Buffer Size: 27024\n",
      "Current Epsilon: 0.04209078556329156\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 731, Reward: -27.122837730767145, Moving Avg Reward: -22.728021171391074, Replay Buffer Size: 27024\n",
      "Current Epsilon: 0.04209078556329156\n",
      "Episode 732: Won\n",
      "Episode 732, Avg Value Loss: 3.457640502601862, Avg Policy Loss: 3.5884163439273835\n",
      "Episode 732, Reward: -62.32720092501128, Moving Avg Reward: -23.03317650724063, Replay Buffer Size: 27104\n",
      "Current Epsilon: 0.0418803316354751\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 732, Reward: -62.32720092501128, Moving Avg Reward: -23.03317650724063, Replay Buffer Size: 27104\n",
      "Current Epsilon: 0.0418803316354751\n",
      "Episode 733: Won\n",
      "Episode 733, Avg Value Loss: 3.365276448428631, Avg Policy Loss: 3.517268347740173\n",
      "Episode 733, Reward: -73.29883607799026, Moving Avg Reward: -23.66366412026367, Replay Buffer Size: 27184\n",
      "Current Epsilon: 0.041670929977297724\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 733, Reward: -73.29883607799026, Moving Avg Reward: -23.66366412026367, Replay Buffer Size: 27184\n",
      "Current Epsilon: 0.041670929977297724\n",
      "Episode 734: Draw\n",
      "Episode 734, Avg Value Loss: 3.39708012342453, Avg Policy Loss: 3.65681055188179\n",
      "Episode 734, Reward: -23.734742550209965, Moving Avg Reward: -24.000911545765767, Replay Buffer Size: 27192\n",
      "Current Epsilon: 0.04146257532741124\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 734, Reward: -23.734742550209965, Moving Avg Reward: -24.000911545765767, Replay Buffer Size: 27192\n",
      "Current Epsilon: 0.04146257532741124\n",
      "Episode 735: Lost\n",
      "Episode 735, Avg Value Loss: 3.5236223022143047, Avg Policy Loss: 3.507775338490804\n",
      "Episode 735, Reward: 13.780446445476676, Moving Avg Reward: -23.566448426780642, Replay Buffer Size: 27222\n",
      "Current Epsilon: 0.04125526245077418\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 735, Reward: 13.780446445476676, Moving Avg Reward: -23.566448426780642, Replay Buffer Size: 27222\n",
      "Current Epsilon: 0.04125526245077418\n",
      "Episode 736: Won\n",
      "Episode 736, Avg Value Loss: 3.544658607624947, Avg Policy Loss: 3.4647137104196752\n",
      "Episode 736, Reward: -25.619375963772058, Moving Avg Reward: -23.458782907951587, Replay Buffer Size: 27269\n",
      "Current Epsilon: 0.04104898613852031\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 736, Reward: -25.619375963772058, Moving Avg Reward: -23.458782907951587, Replay Buffer Size: 27269\n",
      "Current Epsilon: 0.04104898613852031\n",
      "Episode 737: Won\n",
      "Episode 737, Avg Value Loss: 3.406534345448017, Avg Policy Loss: 3.56293685734272\n",
      "Episode 737, Reward: -7.477793931205175, Moving Avg Reward: -23.487421713597186, Replay Buffer Size: 27349\n",
      "Current Epsilon: 0.04084374120782771\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 737, Reward: -7.477793931205175, Moving Avg Reward: -23.487421713597186, Replay Buffer Size: 27349\n",
      "Current Epsilon: 0.04084374120782771\n",
      "Episode 738: Won\n",
      "Episode 738, Avg Value Loss: 3.2825161695480345, Avg Policy Loss: 3.579435461759567\n",
      "Episode 738, Reward: -31.562770559564957, Moving Avg Reward: -23.580111501574688, Replay Buffer Size: 27429\n",
      "Current Epsilon: 0.04063952250178857\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 738, Reward: -31.562770559564957, Moving Avg Reward: -23.580111501574688, Replay Buffer Size: 27429\n",
      "Current Epsilon: 0.04063952250178857\n",
      "Episode 739: Won\n",
      "Episode 739, Avg Value Loss: 3.5473935648798944, Avg Policy Loss: 3.5975082993507383\n",
      "Episode 739, Reward: -15.710120782548817, Moving Avg Reward: -23.4814612885454, Replay Buffer Size: 27509\n",
      "Current Epsilon: 0.04043632488927963\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 739, Reward: -15.710120782548817, Moving Avg Reward: -23.4814612885454, Replay Buffer Size: 27509\n",
      "Current Epsilon: 0.04043632488927963\n",
      "Episode 740: Draw\n",
      "Episode 740, Avg Value Loss: 3.0987553291022776, Avg Policy Loss: 3.612582856416702\n",
      "Episode 740, Reward: -274.3297539836759, Moving Avg Reward: -26.113173606298815, Replay Buffer Size: 27589\n",
      "Current Epsilon: 0.04023414326483323\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 740, Reward: -274.3297539836759, Moving Avg Reward: -26.113173606298815, Replay Buffer Size: 27589\n",
      "Current Epsilon: 0.04023414326483323\n",
      "Episode 741: Draw\n",
      "Episode 741, Avg Value Loss: 3.50109723508358, Avg Policy Loss: 3.5821454405784605\n",
      "Episode 741, Reward: -0.6299051877738127, Moving Avg Reward: -25.864057442203308, Replay Buffer Size: 27669\n",
      "Current Epsilon: 0.040032972548509065\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 741, Reward: -0.6299051877738127, Moving Avg Reward: -25.864057442203308, Replay Buffer Size: 27669\n",
      "Current Epsilon: 0.040032972548509065\n",
      "Episode 742: Draw\n",
      "Episode 742, Avg Value Loss: 3.247232359647751, Avg Policy Loss: 3.582589646180471\n",
      "Episode 742, Reward: -54.63675863019883, Moving Avg Reward: -26.083596145954438, Replay Buffer Size: 27729\n",
      "Current Epsilon: 0.03983280768576652\n",
      "Loss rate over the last 150 episodes: 0.66\n",
      "Episode 742, Reward: -54.63675863019883, Moving Avg Reward: -26.083596145954438, Replay Buffer Size: 27729\n",
      "Current Epsilon: 0.03983280768576652\n",
      "Episode 743: Lost\n",
      "Episode 743, Avg Value Loss: 3.6951050996780395, Avg Policy Loss: 3.468603491783142\n",
      "Episode 743, Reward: -27.034767361204246, Moving Avg Reward: -26.115771583922527, Replay Buffer Size: 27739\n",
      "Current Epsilon: 0.03963364364733769\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 743, Reward: -27.034767361204246, Moving Avg Reward: -26.115771583922527, Replay Buffer Size: 27739\n",
      "Current Epsilon: 0.03963364364733769\n",
      "Episode 744: Lost\n",
      "Episode 744, Avg Value Loss: 3.1430606096982956, Avg Policy Loss: 3.6661400198936462\n",
      "Episode 744, Reward: -21.572195171157844, Moving Avg Reward: -25.81594505174927, Replay Buffer Size: 27747\n",
      "Current Epsilon: 0.039435475429100995\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 744, Reward: -21.572195171157844, Moving Avg Reward: -25.81594505174927, Replay Buffer Size: 27747\n",
      "Current Epsilon: 0.039435475429100995\n",
      "Episode 745: Lost\n",
      "Episode 745, Avg Value Loss: 3.6209965414471097, Avg Policy Loss: 3.4831064807044134\n",
      "Episode 745, Reward: -23.207811113567708, Moving Avg Reward: -26.201894070901957, Replay Buffer Size: 27756\n",
      "Current Epsilon: 0.03923829805195549\n",
      "Loss rate over the last 150 episodes: 0.62\n",
      "Episode 745, Reward: -23.207811113567708, Moving Avg Reward: -26.201894070901957, Replay Buffer Size: 27756\n",
      "Current Epsilon: 0.03923829805195549\n",
      "Episode 746: Lost\n",
      "Episode 746, Avg Value Loss: 3.1732892231507734, Avg Policy Loss: 3.627391880208796\n",
      "Episode 746, Reward: -30.40405337039082, Moving Avg Reward: -26.25365072197777, Replay Buffer Size: 27767\n",
      "Current Epsilon: 0.03904210656169572\n",
      "Loss rate over the last 150 episodes: 0.68\n",
      "Episode 746, Reward: -30.40405337039082, Moving Avg Reward: -26.25365072197777, Replay Buffer Size: 27767\n",
      "Current Epsilon: 0.03904210656169572\n",
      "Episode 747: Lost\n",
      "Episode 747, Avg Value Loss: 3.5374405659162083, Avg Policy Loss: 3.730805653792161\n",
      "Episode 747, Reward: -28.725830866930558, Moving Avg Reward: -26.29556193902454, Replay Buffer Size: 27780\n",
      "Current Epsilon: 0.03884689602888724\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 747, Reward: -28.725830866930558, Moving Avg Reward: -26.29556193902454, Replay Buffer Size: 27780\n",
      "Current Epsilon: 0.03884689602888724\n",
      "Episode 748: Lost\n",
      "Episode 748, Avg Value Loss: 3.172615611553192, Avg Policy Loss: 3.665180039405823\n",
      "Episode 748, Reward: -29.834639015638366, Moving Avg Reward: -26.31110255017911, Replay Buffer Size: 27790\n",
      "Current Epsilon: 0.0386526615487428\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 748, Reward: -29.834639015638366, Moving Avg Reward: -26.31110255017911, Replay Buffer Size: 27790\n",
      "Current Epsilon: 0.0386526615487428\n",
      "Episode 749: Lost\n",
      "Episode 749, Avg Value Loss: 3.76761718229814, Avg Policy Loss: 3.819431348280473\n",
      "Episode 749, Reward: -28.39293862846604, Moving Avg Reward: -26.387210965239838, Replay Buffer Size: 27801\n",
      "Current Epsilon: 0.03845939824099909\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 749, Reward: -28.39293862846604, Moving Avg Reward: -26.387210965239838, Replay Buffer Size: 27801\n",
      "Current Epsilon: 0.03845939824099909\n",
      "Episode 750: Lost\n",
      "Episode 750, Avg Value Loss: 3.540834175215827, Avg Policy Loss: 3.4907937314775257\n",
      "Episode 750, Reward: -22.072306161498773, Moving Avg Reward: -26.567161371572173, Replay Buffer Size: 27810\n",
      "Current Epsilon: 0.03826710124979409\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 750, Reward: -22.072306161498773, Moving Avg Reward: -26.567161371572173, Replay Buffer Size: 27810\n",
      "Current Epsilon: 0.03826710124979409\n",
      "Episode 751: Lost\n",
      "Episode 751, Avg Value Loss: 3.2803157459605825, Avg Policy Loss: 3.7179806449196557\n",
      "Episode 751, Reward: -26.50745261590073, Moving Avg Reward: -26.75790314780743, Replay Buffer Size: 27821\n",
      "Current Epsilon: 0.038075765743545126\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 751, Reward: -26.50745261590073, Moving Avg Reward: -26.75790314780743, Replay Buffer Size: 27821\n",
      "Current Epsilon: 0.038075765743545126\n",
      "Episode 752: Lost\n",
      "Episode 752, Avg Value Loss: 3.46503122150898, Avg Policy Loss: 3.834939807653427\n",
      "Episode 752, Reward: -23.50419492997671, Moving Avg Reward: -26.72754933127211, Replay Buffer Size: 27829\n",
      "Current Epsilon: 0.0378853869148274\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 752, Reward: -23.50419492997671, Moving Avg Reward: -26.72754933127211, Replay Buffer Size: 27829\n",
      "Current Epsilon: 0.0378853869148274\n",
      "Episode 753: Lost\n",
      "Episode 753, Avg Value Loss: 4.271281003952026, Avg Policy Loss: 3.448466976483663\n",
      "Episode 753, Reward: -33.96718974414989, Moving Avg Reward: -26.878677077081825, Replay Buffer Size: 27841\n",
      "Current Epsilon: 0.03769595998025326\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 753, Reward: -33.96718974414989, Moving Avg Reward: -26.878677077081825, Replay Buffer Size: 27841\n",
      "Current Epsilon: 0.03769595998025326\n",
      "Episode 754: Lost\n",
      "Episode 754, Avg Value Loss: 3.275498929619789, Avg Policy Loss: 3.575934794545174\n",
      "Episode 754, Reward: -28.524355000151953, Moving Avg Reward: -26.514902732854317, Replay Buffer Size: 27921\n",
      "Current Epsilon: 0.03750748018035199\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 754, Reward: -28.524355000151953, Moving Avg Reward: -26.514902732854317, Replay Buffer Size: 27921\n",
      "Current Epsilon: 0.03750748018035199\n",
      "Episode 755: Lost\n",
      "Episode 755, Avg Value Loss: 3.3356393666371056, Avg Policy Loss: 3.604387640953064\n",
      "Episode 755, Reward: 7.084809222784352, Moving Avg Reward: -25.66194874615491, Replay Buffer Size: 27967\n",
      "Current Epsilon: 0.037319942779450235\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 755, Reward: 7.084809222784352, Moving Avg Reward: -25.66194874615491, Replay Buffer Size: 27967\n",
      "Current Epsilon: 0.037319942779450235\n",
      "Episode 756: Won\n",
      "Episode 756, Avg Value Loss: 3.1465430359045663, Avg Policy Loss: 3.7465680837631226\n",
      "Episode 756, Reward: -29.12851355140807, Moving Avg Reward: -25.688557307738552, Replay Buffer Size: 27979\n",
      "Current Epsilon: 0.037133343065552986\n",
      "Loss rate over the last 150 episodes: 0.66\n",
      "Episode 756, Reward: -29.12851355140807, Moving Avg Reward: -25.688557307738552, Replay Buffer Size: 27979\n",
      "Current Epsilon: 0.037133343065552986\n",
      "Episode 757: Won\n",
      "Episode 757, Avg Value Loss: 3.2759560445944467, Avg Policy Loss: 3.4593847592671714\n",
      "Episode 757, Reward: -30.635846883282625, Moving Avg Reward: -26.036526560284916, Replay Buffer Size: 27991\n",
      "Current Epsilon: 0.03694767635022522\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 757, Reward: -30.635846883282625, Moving Avg Reward: -26.036526560284916, Replay Buffer Size: 27991\n",
      "Current Epsilon: 0.03694767635022522\n",
      "Episode 758: Won\n",
      "Episode 758, Avg Value Loss: 3.484670674800873, Avg Policy Loss: 3.641757535934448\n",
      "Episode 758, Reward: -19.68242211400987, Moving Avg Reward: -25.97498631492413, Replay Buffer Size: 28071\n",
      "Current Epsilon: 0.036762937968474095\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 758, Reward: -19.68242211400987, Moving Avg Reward: -25.97498631492413, Replay Buffer Size: 28071\n",
      "Current Epsilon: 0.036762937968474095\n",
      "Episode 759: Won\n",
      "Episode 759, Avg Value Loss: 3.7180576258235507, Avg Policy Loss: 3.31699538230896\n",
      "Episode 759, Reward: -27.926364318619264, Moving Avg Reward: -25.980587822185157, Replay Buffer Size: 28080\n",
      "Current Epsilon: 0.03657912327863173\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 759, Reward: -27.926364318619264, Moving Avg Reward: -25.980587822185157, Replay Buffer Size: 28080\n",
      "Current Epsilon: 0.03657912327863173\n",
      "Episode 760: Won\n",
      "Episode 760, Avg Value Loss: 3.1035147721950826, Avg Policy Loss: 3.9250358801621656\n",
      "Episode 760, Reward: -33.26669020317295, Moving Avg Reward: -26.057469255336493, Replay Buffer Size: 28093\n",
      "Current Epsilon: 0.036396227662238566\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 760, Reward: -33.26669020317295, Moving Avg Reward: -26.057469255336493, Replay Buffer Size: 28093\n",
      "Current Epsilon: 0.036396227662238566\n",
      "Episode 761: Won\n",
      "Episode 761, Avg Value Loss: 3.483624068173495, Avg Policy Loss: 3.6522427688945425\n",
      "Episode 761, Reward: -29.71790989785314, Moving Avg Reward: -26.505296082345144, Replay Buffer Size: 28104\n",
      "Current Epsilon: 0.03621424652392737\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 761, Reward: -29.71790989785314, Moving Avg Reward: -26.505296082345144, Replay Buffer Size: 28104\n",
      "Current Epsilon: 0.03621424652392737\n",
      "Episode 762: Won\n",
      "Episode 762, Avg Value Loss: 3.101990424669706, Avg Policy Loss: 3.679960635992197\n",
      "Episode 762, Reward: -34.38799338223514, Moving Avg Reward: -26.58323658062502, Replay Buffer Size: 28117\n",
      "Current Epsilon: 0.036033175291307735\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 762, Reward: -34.38799338223514, Moving Avg Reward: -26.58323658062502, Replay Buffer Size: 28117\n",
      "Current Epsilon: 0.036033175291307735\n",
      "Episode 763: Lost\n",
      "Episode 763, Avg Value Loss: 3.1349285382490892, Avg Policy Loss: 3.745169547887949\n",
      "Episode 763, Reward: -32.376136861163765, Moving Avg Reward: -26.628264371482953, Replay Buffer Size: 28130\n",
      "Current Epsilon: 0.03585300941485119\n",
      "Loss rate over the last 150 episodes: 0.66\n",
      "Episode 763, Reward: -32.376136861163765, Moving Avg Reward: -26.628264371482953, Replay Buffer Size: 28130\n",
      "Current Epsilon: 0.03585300941485119\n",
      "Episode 764: Lost\n",
      "Episode 764, Avg Value Loss: 3.0836421251296997, Avg Policy Loss: 3.729937663445106\n",
      "Episode 764, Reward: -29.283943089164097, Moving Avg Reward: -26.74466467732575, Replay Buffer Size: 28143\n",
      "Current Epsilon: 0.035673744367776934\n",
      "Loss rate over the last 150 episodes: 0.66\n",
      "Episode 764, Reward: -29.283943089164097, Moving Avg Reward: -26.74466467732575, Replay Buffer Size: 28143\n",
      "Current Epsilon: 0.035673744367776934\n",
      "Episode 765: Lost\n",
      "Episode 765, Avg Value Loss: 3.7468226714567705, Avg Policy Loss: 3.9146179285916416\n",
      "Episode 765, Reward: -29.402232013642625, Moving Avg Reward: -27.079529183641668, Replay Buffer Size: 28154\n",
      "Current Epsilon: 0.03549537564593805\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 765, Reward: -29.402232013642625, Moving Avg Reward: -27.079529183641668, Replay Buffer Size: 28154\n",
      "Current Epsilon: 0.03549537564593805\n",
      "Episode 766: Lost\n",
      "Episode 766, Avg Value Loss: 3.286392192840576, Avg Policy Loss: 3.6202769088745117\n",
      "Episode 766, Reward: 17.014966658973567, Moving Avg Reward: -26.575783357870122, Replay Buffer Size: 28179\n",
      "Current Epsilon: 0.035317898767708356\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 766, Reward: 17.014966658973567, Moving Avg Reward: -26.575783357870122, Replay Buffer Size: 28179\n",
      "Current Epsilon: 0.035317898767708356\n",
      "Episode 767: Won\n",
      "Episode 767, Avg Value Loss: 2.9855740070343018, Avg Policy Loss: 3.8123993078867593\n",
      "Episode 767, Reward: -19.699360063982084, Moving Avg Reward: -26.533216051401272, Replay Buffer Size: 28185\n",
      "Current Epsilon: 0.03514130927386981\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 767, Reward: -19.699360063982084, Moving Avg Reward: -26.533216051401272, Replay Buffer Size: 28185\n",
      "Current Epsilon: 0.03514130927386981\n",
      "Episode 768: Won\n",
      "Episode 768, Avg Value Loss: 3.810200322758068, Avg Policy Loss: 3.2875698046250776\n",
      "Episode 768, Reward: -31.43514290859735, Moving Avg Reward: -27.023615847427745, Replay Buffer Size: 28196\n",
      "Current Epsilon: 0.03496560272750046\n",
      "Loss rate over the last 150 episodes: 0.66\n",
      "Episode 768, Reward: -31.43514290859735, Moving Avg Reward: -27.023615847427745, Replay Buffer Size: 28196\n",
      "Current Epsilon: 0.03496560272750046\n",
      "Episode 769: Won\n",
      "Episode 769, Avg Value Loss: 3.2184573962138248, Avg Policy Loss: 3.7277172528780422\n",
      "Episode 769, Reward: -29.074067795792686, Moving Avg Reward: -27.47418634291762, Replay Buffer Size: 28209\n",
      "Current Epsilon: 0.03479077471386296\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 769, Reward: -29.074067795792686, Moving Avg Reward: -27.47418634291762, Replay Buffer Size: 28209\n",
      "Current Epsilon: 0.03479077471386296\n",
      "Episode 770: Won\n",
      "Episode 770, Avg Value Loss: 3.1985929770903154, Avg Policy Loss: 3.5488359277898613\n",
      "Episode 770, Reward: -31.304102568276107, Moving Avg Reward: -27.603636521678204, Replay Buffer Size: 28220\n",
      "Current Epsilon: 0.03461682084029365\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 770, Reward: -31.304102568276107, Moving Avg Reward: -27.603636521678204, Replay Buffer Size: 28220\n",
      "Current Epsilon: 0.03461682084029365\n",
      "Episode 771: Won\n",
      "Episode 771, Avg Value Loss: 3.4365798234939575, Avg Policy Loss: 3.816881921556261\n",
      "Episode 771, Reward: -25.705239461804823, Moving Avg Reward: -27.554208766622423, Replay Buffer Size: 28229\n",
      "Current Epsilon: 0.034443736736092176\n",
      "Loss rate over the last 150 episodes: 0.64\n",
      "Episode 771, Reward: -25.705239461804823, Moving Avg Reward: -27.554208766622423, Replay Buffer Size: 28229\n",
      "Current Epsilon: 0.034443736736092176\n",
      "Episode 772: Won\n",
      "Episode 772, Avg Value Loss: 3.9569647770661573, Avg Policy Loss: 3.4441490906935472\n",
      "Episode 772, Reward: -31.31215632700347, Moving Avg Reward: -28.036476748562063, Replay Buffer Size: 28242\n",
      "Current Epsilon: 0.034271518052411715\n",
      "Loss rate over the last 150 episodes: 0.63\n",
      "Episode 772, Reward: -31.31215632700347, Moving Avg Reward: -28.036476748562063, Replay Buffer Size: 28242\n",
      "Current Epsilon: 0.034271518052411715\n",
      "Episode 773: Won\n",
      "Episode 773, Avg Value Loss: 3.472496951619784, Avg Policy Loss: 3.535264770189921\n",
      "Episode 773, Reward: -31.30581935724254, Moving Avg Reward: -28.0827519346998, Replay Buffer Size: 28254\n",
      "Current Epsilon: 0.034100160462149656\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 773, Reward: -31.30581935724254, Moving Avg Reward: -28.0827519346998, Replay Buffer Size: 28254\n",
      "Current Epsilon: 0.034100160462149656\n",
      "Episode 774: Won\n",
      "Episode 774, Avg Value Loss: 3.435368075966835, Avg Policy Loss: 3.9163636565208435\n",
      "Episode 774, Reward: -26.247907268138164, Moving Avg Reward: -28.297525309876683, Replay Buffer Size: 28262\n",
      "Current Epsilon: 0.03392965965983891\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 774, Reward: -26.247907268138164, Moving Avg Reward: -28.297525309876683, Replay Buffer Size: 28262\n",
      "Current Epsilon: 0.03392965965983891\n",
      "Episode 775: Won\n",
      "Episode 775, Avg Value Loss: 3.553160674870014, Avg Policy Loss: 3.6759105436503887\n",
      "Episode 775, Reward: -53.63407251668741, Moving Avg Reward: -28.60938884642472, Replay Buffer Size: 28326\n",
      "Current Epsilon: 0.033760011361539714\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 775, Reward: -53.63407251668741, Moving Avg Reward: -28.60938884642472, Replay Buffer Size: 28326\n",
      "Current Epsilon: 0.033760011361539714\n",
      "Episode 776: Won\n",
      "Episode 776, Avg Value Loss: 3.1733194142580032, Avg Policy Loss: 3.659186840057373\n",
      "Episode 776, Reward: -30.760753491362117, Moving Avg Reward: -28.6043665727859, Replay Buffer Size: 28366\n",
      "Current Epsilon: 0.03359121130473201\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 776, Reward: -30.760753491362117, Moving Avg Reward: -28.6043665727859, Replay Buffer Size: 28366\n",
      "Current Epsilon: 0.03359121130473201\n",
      "Episode 777: Lost\n",
      "Episode 777, Avg Value Loss: 3.368520114570856, Avg Policy Loss: 3.657140961289406\n",
      "Episode 777, Reward: 6.475701576534274, Moving Avg Reward: -28.062399663343903, Replay Buffer Size: 28446\n",
      "Current Epsilon: 0.033423255248208356\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 777, Reward: 6.475701576534274, Moving Avg Reward: -28.062399663343903, Replay Buffer Size: 28446\n",
      "Current Epsilon: 0.033423255248208356\n",
      "Episode 778: Lost\n",
      "Episode 778, Avg Value Loss: 2.260096481868199, Avg Policy Loss: 3.7434355531420027\n",
      "Episode 778, Reward: -19.705326843764606, Moving Avg Reward: -28.075531829828243, Replay Buffer Size: 28453\n",
      "Current Epsilon: 0.03325613897196732\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 778, Reward: -19.705326843764606, Moving Avg Reward: -28.075531829828243, Replay Buffer Size: 28453\n",
      "Current Epsilon: 0.03325613897196732\n",
      "Episode 779: Lost\n",
      "Episode 779, Avg Value Loss: 3.0031799994982205, Avg Policy Loss: 3.547446929491483\n",
      "Episode 779, Reward: -30.001136009171418, Moving Avg Reward: -28.05461629929562, Replay Buffer Size: 28466\n",
      "Current Epsilon: 0.03308985827710748\n",
      "Loss rate over the last 150 episodes: 0.65\n",
      "Episode 779, Reward: -30.001136009171418, Moving Avg Reward: -28.05461629929562, Replay Buffer Size: 28466\n",
      "Current Epsilon: 0.03308985827710748\n",
      "Episode 780: Lost\n",
      "Episode 780, Avg Value Loss: 3.311735298898485, Avg Policy Loss: 3.745011188365795\n",
      "Episode 780, Reward: -38.99542098866835, Moving Avg Reward: -28.565637026031563, Replay Buffer Size: 28493\n",
      "Current Epsilon: 0.032924408985721944\n",
      "Loss rate over the last 150 episodes: 0.66\n",
      "Episode 780, Reward: -38.99542098866835, Moving Avg Reward: -28.565637026031563, Replay Buffer Size: 28493\n",
      "Current Epsilon: 0.032924408985721944\n",
      "Episode 781: Lost\n",
      "Episode 781, Avg Value Loss: 3.3953947358661227, Avg Policy Loss: 3.5613982041676837\n",
      "Episode 781, Reward: -51.08256058233456, Moving Avg Reward: -28.66621714159867, Replay Buffer Size: 28538\n",
      "Current Epsilon: 0.03275978694079333\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 781, Reward: -51.08256058233456, Moving Avg Reward: -28.66621714159867, Replay Buffer Size: 28538\n",
      "Current Epsilon: 0.03275978694079333\n",
      "Episode 782: Lost\n",
      "Episode 782, Avg Value Loss: 3.81196910684759, Avg Policy Loss: 3.718795906413685\n",
      "Episode 782, Reward: -29.60623811531032, Moving Avg Reward: -28.74351420907785, Replay Buffer Size: 28549\n",
      "Current Epsilon: 0.032595988006089364\n",
      "Loss rate over the last 150 episodes: 0.66\n",
      "Episode 782, Reward: -29.60623811531032, Moving Avg Reward: -28.74351420907785, Replay Buffer Size: 28549\n",
      "Current Epsilon: 0.032595988006089364\n",
      "Episode 783: Lost\n",
      "Episode 783, Avg Value Loss: 3.405839601288671, Avg Policy Loss: 3.7268463735995083\n",
      "Episode 783, Reward: 18.161277581983924, Moving Avg Reward: -28.346189234809213, Replay Buffer Size: 28572\n",
      "Current Epsilon: 0.032433008066058915\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 783, Reward: 18.161277581983924, Moving Avg Reward: -28.346189234809213, Replay Buffer Size: 28572\n",
      "Current Epsilon: 0.032433008066058915\n",
      "Episode 784: Won\n",
      "Episode 784, Avg Value Loss: 3.545049869097196, Avg Policy Loss: 3.6333833107581506\n",
      "Episode 784, Reward: -35.41453877754342, Moving Avg Reward: -28.79923462258465, Replay Buffer Size: 28585\n",
      "Current Epsilon: 0.03227084302572862\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 784, Reward: -35.41453877754342, Moving Avg Reward: -28.79923462258465, Replay Buffer Size: 28585\n",
      "Current Epsilon: 0.03227084302572862\n",
      "Episode 785: Won\n",
      "Episode 785, Avg Value Loss: 3.0922642052173615, Avg Policy Loss: 3.737946927547455\n",
      "Episode 785, Reward: -24.892445075375235, Moving Avg Reward: -28.867370262317582, Replay Buffer Size: 28593\n",
      "Current Epsilon: 0.032109488810599975\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 785, Reward: -24.892445075375235, Moving Avg Reward: -28.867370262317582, Replay Buffer Size: 28593\n",
      "Current Epsilon: 0.032109488810599975\n",
      "Episode 786: Won\n",
      "Episode 786, Avg Value Loss: 3.5701216326819525, Avg Policy Loss: 3.569471491707696\n",
      "Episode 786, Reward: -25.51560796713763, Moving Avg Reward: -28.810356327534986, Replay Buffer Size: 28602\n",
      "Current Epsilon: 0.031948941366546975\n",
      "Loss rate over the last 150 episodes: 0.66\n",
      "Episode 786, Reward: -25.51560796713763, Moving Avg Reward: -28.810356327534986, Replay Buffer Size: 28602\n",
      "Current Epsilon: 0.031948941366546975\n",
      "Episode 787: Won\n",
      "Episode 787, Avg Value Loss: 3.377553084622259, Avg Policy Loss: 3.609593743863313\n",
      "Episode 787, Reward: 18.789059409716245, Moving Avg Reward: -28.359356915438525, Replay Buffer Size: 28625\n",
      "Current Epsilon: 0.03178919665971424\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 787, Reward: 18.789059409716245, Moving Avg Reward: -28.359356915438525, Replay Buffer Size: 28625\n",
      "Current Epsilon: 0.03178919665971424\n",
      "Episode 788: Won\n",
      "Episode 788, Avg Value Loss: 3.1396475583314896, Avg Policy Loss: 3.9479399919509888\n",
      "Episode 788, Reward: -25.368512110759895, Moving Avg Reward: -28.743730258213358, Replay Buffer Size: 28633\n",
      "Current Epsilon: 0.03163025067641567\n",
      "Loss rate over the last 150 episodes: 0.66\n",
      "Episode 788, Reward: -25.368512110759895, Moving Avg Reward: -28.743730258213358, Replay Buffer Size: 28633\n",
      "Current Epsilon: 0.03163025067641567\n",
      "Episode 789: Won\n",
      "Episode 789, Avg Value Loss: 3.0630161265532174, Avg Policy Loss: 3.5722783406575522\n",
      "Episode 789, Reward: -35.24855757741845, Moving Avg Reward: -28.7963812118952, Replay Buffer Size: 28645\n",
      "Current Epsilon: 0.03147209942303359\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 789, Reward: -35.24855757741845, Moving Avg Reward: -28.7963812118952, Replay Buffer Size: 28645\n",
      "Current Epsilon: 0.03147209942303359\n",
      "Episode 790: Won\n",
      "Episode 790, Avg Value Loss: 4.035336997773912, Avg Policy Loss: 3.7341363694932728\n",
      "Episode 790, Reward: -27.096526419375394, Moving Avg Reward: -28.800954811236156, Replay Buffer Size: 28654\n",
      "Current Epsilon: 0.03131473892591842\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 790, Reward: -27.096526419375394, Moving Avg Reward: -28.800954811236156, Replay Buffer Size: 28654\n",
      "Current Epsilon: 0.03131473892591842\n",
      "Episode 791: Won\n",
      "Episode 791, Avg Value Loss: 3.4581065967679026, Avg Policy Loss: 3.6506768345832823\n",
      "Episode 791, Reward: -75.86254853048375, Moving Avg Reward: -29.246916293866793, Replay Buffer Size: 28734\n",
      "Current Epsilon: 0.031158165231288826\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 791, Reward: -75.86254853048375, Moving Avg Reward: -29.246916293866793, Replay Buffer Size: 28734\n",
      "Current Epsilon: 0.031158165231288826\n",
      "Episode 792: Won\n",
      "Episode 792, Avg Value Loss: 2.8112241864204406, Avg Policy Loss: 3.734340214729309\n",
      "Episode 792, Reward: -25.881902251462996, Moving Avg Reward: -29.22940091494125, Replay Buffer Size: 28744\n",
      "Current Epsilon: 0.03100237440513238\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 792, Reward: -25.881902251462996, Moving Avg Reward: -29.22940091494125, Replay Buffer Size: 28744\n",
      "Current Epsilon: 0.03100237440513238\n",
      "Episode 793: Won\n",
      "Episode 793, Avg Value Loss: 3.9047945618629454, Avg Policy Loss: 3.6610092639923097\n",
      "Episode 793, Reward: -29.30143425105546, Moving Avg Reward: -29.2433798227575, Replay Buffer Size: 28754\n",
      "Current Epsilon: 0.030847362533106718\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 793, Reward: -29.30143425105546, Moving Avg Reward: -29.2433798227575, Replay Buffer Size: 28754\n",
      "Current Epsilon: 0.030847362533106718\n",
      "Episode 794: Won\n",
      "Episode 794, Avg Value Loss: 3.3881103450601753, Avg Policy Loss: 3.44777605750344\n",
      "Episode 794, Reward: -25.944040258292006, Moving Avg Reward: -29.172322427920722, Replay Buffer Size: 28765\n",
      "Current Epsilon: 0.030693125720441184\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 794, Reward: -25.944040258292006, Moving Avg Reward: -29.172322427920722, Replay Buffer Size: 28765\n",
      "Current Epsilon: 0.030693125720441184\n",
      "Episode 795: Won\n",
      "Episode 795, Avg Value Loss: 3.733088967204094, Avg Policy Loss: 3.6150430768728254\n",
      "Episode 795, Reward: -36.0636118389898, Moving Avg Reward: -29.25779917620549, Replay Buffer Size: 28845\n",
      "Current Epsilon: 0.030539660091838977\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 795, Reward: -36.0636118389898, Moving Avg Reward: -29.25779917620549, Replay Buffer Size: 28845\n",
      "Current Epsilon: 0.030539660091838977\n",
      "Episode 796: Lost\n",
      "Episode 796, Avg Value Loss: 3.5034076564013956, Avg Policy Loss: 3.6831059247255324\n",
      "Episode 796, Reward: -93.28245935722045, Moving Avg Reward: -29.87325658214016, Replay Buffer Size: 28925\n",
      "Current Epsilon: 0.03038696179137978\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 796, Reward: -93.28245935722045, Moving Avg Reward: -29.87325658214016, Replay Buffer Size: 28925\n",
      "Current Epsilon: 0.03038696179137978\n",
      "Episode 797: Draw\n",
      "Episode 797, Avg Value Loss: 3.2499452255666257, Avg Policy Loss: 3.7026938796043396\n",
      "Episode 797, Reward: -48.75545253254769, Moving Avg Reward: -30.110812287075596, Replay Buffer Size: 29005\n",
      "Current Epsilon: 0.030235026982422884\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 797, Reward: -48.75545253254769, Moving Avg Reward: -30.110812287075596, Replay Buffer Size: 29005\n",
      "Current Epsilon: 0.030235026982422884\n",
      "Episode 798: Draw\n",
      "Episode 798, Avg Value Loss: 3.7680715152195523, Avg Policy Loss: 3.7769151415143694\n",
      "Episode 798, Reward: -23.409661594945113, Moving Avg Reward: -30.1004113840431, Replay Buffer Size: 29012\n",
      "Current Epsilon: 0.030083851847510768\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 798, Reward: -23.409661594945113, Moving Avg Reward: -30.1004113840431, Replay Buffer Size: 29012\n",
      "Current Epsilon: 0.030083851847510768\n",
      "Episode 799: Lost\n",
      "Episode 799, Avg Value Loss: 2.9274386316537857, Avg Policy Loss: 3.8034844398498535\n",
      "Episode 799, Reward: -25.450663842070306, Moving Avg Reward: -30.030213096028557, Replay Buffer Size: 29020\n",
      "Current Epsilon: 0.029933432588273214\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 799, Reward: -25.450663842070306, Moving Avg Reward: -30.030213096028557, Replay Buffer Size: 29020\n",
      "Current Epsilon: 0.029933432588273214\n",
      "Episode 800: Lost\n",
      "Episode 800, Avg Value Loss: 3.1226749528538096, Avg Policy Loss: 3.608606533570723\n",
      "Episode 800, Reward: -29.574822210750128, Moving Avg Reward: -30.072456859729673, Replay Buffer Size: 29031\n",
      "Current Epsilon: 0.029783765425331846\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 800, Reward: -29.574822210750128, Moving Avg Reward: -30.072456859729673, Replay Buffer Size: 29031\n",
      "Current Epsilon: 0.029783765425331846\n",
      "Episode 801: Lost\n",
      "Episode 801, Avg Value Loss: 4.035275936126709, Avg Policy Loss: 3.469144344329834\n",
      "Episode 801, Reward: -23.80201868662123, Moving Avg Reward: -30.151499303376784, Replay Buffer Size: 29039\n",
      "Current Epsilon: 0.029634846598205186\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 801, Reward: -23.80201868662123, Moving Avg Reward: -30.151499303376784, Replay Buffer Size: 29039\n",
      "Current Epsilon: 0.029634846598205186\n",
      "Episode 802: Lost\n",
      "Episode 802, Avg Value Loss: 3.2995853315700185, Avg Policy Loss: 3.8038171638141978\n",
      "Episode 802, Reward: -30.931965007398485, Moving Avg Reward: -30.627958896191085, Replay Buffer Size: 29050\n",
      "Current Epsilon: 0.02948667236521416\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 802, Reward: -30.931965007398485, Moving Avg Reward: -30.627958896191085, Replay Buffer Size: 29050\n",
      "Current Epsilon: 0.02948667236521416\n",
      "Episode 803: Lost\n",
      "Episode 803, Avg Value Loss: 3.2498714526494346, Avg Policy Loss: 3.6424279646439985\n",
      "Episode 803, Reward: 12.795786843282631, Moving Avg Reward: -30.242038106570266, Replay Buffer Size: 29083\n",
      "Current Epsilon: 0.029339239003388088\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 803, Reward: 12.795786843282631, Moving Avg Reward: -30.242038106570266, Replay Buffer Size: 29083\n",
      "Current Epsilon: 0.029339239003388088\n",
      "Episode 804: Won\n",
      "Episode 804, Avg Value Loss: 3.6337020176428334, Avg Policy Loss: 3.6811686798378274\n",
      "Episode 804, Reward: 3.5715204866117567, Moving Avg Reward: -29.555793447222776, Replay Buffer Size: 29137\n",
      "Current Epsilon: 0.029192542808371146\n",
      "Loss rate over the last 150 episodes: 0.66\n",
      "Episode 804, Reward: 3.5715204866117567, Moving Avg Reward: -29.555793447222776, Replay Buffer Size: 29137\n",
      "Current Epsilon: 0.029192542808371146\n",
      "Episode 805: Won\n",
      "Episode 805, Avg Value Loss: 2.9804102314843073, Avg Policy Loss: 3.646204153696696\n",
      "Episode 805, Reward: -30.052938875215162, Moving Avg Reward: -29.648823548923154, Replay Buffer Size: 29146\n",
      "Current Epsilon: 0.02904658009432929\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 805, Reward: -30.052938875215162, Moving Avg Reward: -29.648823548923154, Replay Buffer Size: 29146\n",
      "Current Epsilon: 0.02904658009432929\n",
      "Episode 806: Won\n",
      "Episode 806, Avg Value Loss: 3.4092453166842462, Avg Policy Loss: 3.6585822999477386\n",
      "Episode 806, Reward: -35.35049059790534, Moving Avg Reward: -29.554268011693424, Replay Buffer Size: 29226\n",
      "Current Epsilon: 0.028901347193857643\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 806, Reward: -35.35049059790534, Moving Avg Reward: -29.554268011693424, Replay Buffer Size: 29226\n",
      "Current Epsilon: 0.028901347193857643\n",
      "Episode 807: Won\n",
      "Episode 807, Avg Value Loss: 3.1902050177256265, Avg Policy Loss: 3.5552609264850616\n",
      "Episode 807, Reward: 18.85028201893202, Moving Avg Reward: -29.2571382949945, Replay Buffer Size: 29250\n",
      "Current Epsilon: 0.028756840457888354\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 807, Reward: 18.85028201893202, Moving Avg Reward: -29.2571382949945, Replay Buffer Size: 29250\n",
      "Current Epsilon: 0.028756840457888354\n",
      "Episode 808: Won\n",
      "Episode 808, Avg Value Loss: 3.0342566115515575, Avg Policy Loss: 3.735421453203474\n",
      "Episode 808, Reward: -22.08631737854924, Moving Avg Reward: -29.54063987455941, Replay Buffer Size: 29257\n",
      "Current Epsilon: 0.02861305625559891\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 808, Reward: -22.08631737854924, Moving Avg Reward: -29.54063987455941, Replay Buffer Size: 29257\n",
      "Current Epsilon: 0.02861305625559891\n",
      "Episode 809: Won\n",
      "Episode 809, Avg Value Loss: 3.3299441154186544, Avg Policy Loss: 3.6909425992232103\n",
      "Episode 809, Reward: -26.72173234436408, Moving Avg Reward: -29.480728791481287, Replay Buffer Size: 29270\n",
      "Current Epsilon: 0.028469990974320916\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 809, Reward: -26.72173234436408, Moving Avg Reward: -29.480728791481287, Replay Buffer Size: 29270\n",
      "Current Epsilon: 0.028469990974320916\n",
      "Episode 810: Won\n",
      "Episode 810, Avg Value Loss: 3.3893953554332255, Avg Policy Loss: 3.6463705748319626\n",
      "Episode 810, Reward: -2.6113452661837973, Moving Avg Reward: -27.971166939719723, Replay Buffer Size: 29350\n",
      "Current Epsilon: 0.02832764101944931\n",
      "Loss rate over the last 150 episodes: 0.68\n",
      "Episode 810, Reward: -2.6113452661837973, Moving Avg Reward: -27.971166939719723, Replay Buffer Size: 29350\n",
      "Current Epsilon: 0.02832764101944931\n",
      "Episode 811: Won\n",
      "Episode 811, Avg Value Loss: 3.4760568633675577, Avg Policy Loss: 3.7228880286216737\n",
      "Episode 811, Reward: -56.19461130865549, Moving Avg Reward: -28.243804774499594, Replay Buffer Size: 29430\n",
      "Current Epsilon: 0.028186002814352063\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 811, Reward: -56.19461130865549, Moving Avg Reward: -28.243804774499594, Replay Buffer Size: 29430\n",
      "Current Epsilon: 0.028186002814352063\n",
      "Episode 812: Draw\n",
      "Episode 812, Avg Value Loss: 3.6794766068458555, Avg Policy Loss: 3.73646342754364\n",
      "Episode 812, Reward: -29.249681039102946, Moving Avg Reward: -28.199471070739573, Replay Buffer Size: 29440\n",
      "Current Epsilon: 0.0280450728002803\n",
      "Loss rate over the last 150 episodes: 0.68\n",
      "Episode 812, Reward: -29.249681039102946, Moving Avg Reward: -28.199471070739573, Replay Buffer Size: 29440\n",
      "Current Epsilon: 0.0280450728002803\n",
      "Episode 813: Lost\n",
      "Episode 813, Avg Value Loss: 3.3932734727859497, Avg Policy Loss: 3.5704692602157593\n",
      "Episode 813, Reward: -27.36657861614712, Moving Avg Reward: -28.23060987215443, Replay Buffer Size: 29450\n",
      "Current Epsilon: 0.0279048474362789\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 813, Reward: -27.36657861614712, Moving Avg Reward: -28.23060987215443, Replay Buffer Size: 29450\n",
      "Current Epsilon: 0.0279048474362789\n",
      "Episode 814: Lost\n",
      "Episode 814, Avg Value Loss: 3.2639584973454476, Avg Policy Loss: 3.7123249977827073\n",
      "Episode 814, Reward: -0.3868192966492008, Moving Avg Reward: -27.948692998203065, Replay Buffer Size: 29530\n",
      "Current Epsilon: 0.027765323199097504\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 814, Reward: -0.3868192966492008, Moving Avg Reward: -27.948692998203065, Replay Buffer Size: 29530\n",
      "Current Epsilon: 0.027765323199097504\n",
      "Episode 815: Lost\n",
      "Episode 815, Avg Value Loss: 3.6547323653572485, Avg Policy Loss: 3.613040008042988\n",
      "Episode 815, Reward: 15.491109942431903, Moving Avg Reward: -27.53162824191538, Replay Buffer Size: 29549\n",
      "Current Epsilon: 0.027626496583102015\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 815, Reward: 15.491109942431903, Moving Avg Reward: -27.53162824191538, Replay Buffer Size: 29549\n",
      "Current Epsilon: 0.027626496583102015\n",
      "Episode 816: Won\n",
      "Episode 816, Avg Value Loss: 3.1664345926708646, Avg Policy Loss: 3.872443517049154\n",
      "Episode 816, Reward: -28.238080809315317, Moving Avg Reward: -27.252844904954276, Replay Buffer Size: 29558\n",
      "Current Epsilon: 0.027488364100186506\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 816, Reward: -28.238080809315317, Moving Avg Reward: -27.252844904954276, Replay Buffer Size: 29558\n",
      "Current Epsilon: 0.027488364100186506\n",
      "Episode 817: Won\n",
      "Episode 817, Avg Value Loss: 3.2438725729783378, Avg Policy Loss: 3.5533944169680276\n",
      "Episode 817, Reward: -32.67355291023645, Moving Avg Reward: -27.535075010039932, Replay Buffer Size: 29570\n",
      "Current Epsilon: 0.027350922279685573\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 817, Reward: -32.67355291023645, Moving Avg Reward: -27.535075010039932, Replay Buffer Size: 29570\n",
      "Current Epsilon: 0.027350922279685573\n",
      "Episode 818: Won\n",
      "Episode 818, Avg Value Loss: 3.640610992908478, Avg Policy Loss: 3.8988174915313722\n",
      "Episode 818, Reward: -26.674292217930628, Moving Avg Reward: -27.99577890647398, Replay Buffer Size: 29580\n",
      "Current Epsilon: 0.027214167668287145\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 818, Reward: -26.674292217930628, Moving Avg Reward: -27.99577890647398, Replay Buffer Size: 29580\n",
      "Current Epsilon: 0.027214167668287145\n",
      "Episode 819: Won\n",
      "Episode 819, Avg Value Loss: 3.383001308692129, Avg Policy Loss: 3.6544444749229834\n",
      "Episode 819, Reward: -46.38115630084517, Moving Avg Reward: -28.202382490277255, Replay Buffer Size: 29618\n",
      "Current Epsilon: 0.02707809682994571\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 819, Reward: -46.38115630084517, Moving Avg Reward: -28.202382490277255, Replay Buffer Size: 29618\n",
      "Current Epsilon: 0.02707809682994571\n",
      "Episode 820: Won\n",
      "Episode 820, Avg Value Loss: 2.6022662043571474, Avg Policy Loss: 3.993606424331665\n",
      "Episode 820, Reward: -26.4115174502017, Moving Avg Reward: -28.141292385024926, Replay Buffer Size: 29628\n",
      "Current Epsilon: 0.02694270634579598\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 820, Reward: -26.4115174502017, Moving Avg Reward: -28.141292385024926, Replay Buffer Size: 29628\n",
      "Current Epsilon: 0.02694270634579598\n",
      "Episode 821: Won\n",
      "Episode 821, Avg Value Loss: 3.6782741943995156, Avg Policy Loss: 3.5988665421803794\n",
      "Episode 821, Reward: -24.399475618073026, Moving Avg Reward: -27.531962954459622, Replay Buffer Size: 29637\n",
      "Current Epsilon: 0.026807992814067\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 821, Reward: -24.399475618073026, Moving Avg Reward: -27.531962954459622, Replay Buffer Size: 29637\n",
      "Current Epsilon: 0.026807992814067\n",
      "Episode 822: Won\n",
      "Episode 822, Avg Value Loss: 3.08396108597517, Avg Policy Loss: 3.7229698270559313\n",
      "Episode 822, Reward: -28.071162896286747, Moving Avg Reward: -27.564243880310723, Replay Buffer Size: 29717\n",
      "Current Epsilon: 0.026673952849996664\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 822, Reward: -28.071162896286747, Moving Avg Reward: -27.564243880310723, Replay Buffer Size: 29717\n",
      "Current Epsilon: 0.026673952849996664\n",
      "Episode 823: Lost\n",
      "Episode 823, Avg Value Loss: 3.0352297295694766, Avg Policy Loss: 3.8390047550201416\n",
      "Episode 823, Reward: -51.556294824949866, Moving Avg Reward: -27.82502198133116, Replay Buffer Size: 29740\n",
      "Current Epsilon: 0.02654058308574668\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 823, Reward: -51.556294824949866, Moving Avg Reward: -27.82502198133116, Replay Buffer Size: 29740\n",
      "Current Epsilon: 0.02654058308574668\n",
      "Episode 824: Lost\n",
      "Episode 824, Avg Value Loss: 3.660083844111516, Avg Policy Loss: 3.657962468954233\n",
      "Episode 824, Reward: -35.69431811355479, Moving Avg Reward: -27.933565078091934, Replay Buffer Size: 29753\n",
      "Current Epsilon: 0.026407880170317945\n",
      "Loss rate over the last 150 episodes: 0.68\n",
      "Episode 824, Reward: -35.69431811355479, Moving Avg Reward: -27.933565078091934, Replay Buffer Size: 29753\n",
      "Current Epsilon: 0.026407880170317945\n",
      "Episode 825: Lost\n",
      "Episode 825, Avg Value Loss: 3.1300566700788646, Avg Policy Loss: 3.749657007364126\n",
      "Episode 825, Reward: -24.663856642408422, Moving Avg Reward: -27.863255562329023, Replay Buffer Size: 29779\n",
      "Current Epsilon: 0.026275840769466357\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 825, Reward: -24.663856642408422, Moving Avg Reward: -27.863255562329023, Replay Buffer Size: 29779\n",
      "Current Epsilon: 0.026275840769466357\n",
      "Episode 826: Lost\n",
      "Episode 826, Avg Value Loss: 3.5759707364169033, Avg Policy Loss: 3.533226555044001\n",
      "Episode 826, Reward: -41.282306356933674, Moving Avg Reward: -28.4351443243359, Replay Buffer Size: 29801\n",
      "Current Epsilon: 0.026144461565619025\n",
      "Loss rate over the last 150 episodes: 0.68\n",
      "Episode 826, Reward: -41.282306356933674, Moving Avg Reward: -28.4351443243359, Replay Buffer Size: 29801\n",
      "Current Epsilon: 0.026144461565619025\n",
      "Episode 827: Lost\n",
      "Episode 827, Avg Value Loss: 3.0477964878082275, Avg Policy Loss: 3.531563848257065\n",
      "Episode 827, Reward: -7.979563752298646, Moving Avg Reward: -28.203239602160806, Replay Buffer Size: 29809\n",
      "Current Epsilon: 0.02601373925779093\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 827, Reward: -7.979563752298646, Moving Avg Reward: -28.203239602160806, Replay Buffer Size: 29809\n",
      "Current Epsilon: 0.02601373925779093\n",
      "Episode 828: Lost\n",
      "Episode 828, Avg Value Loss: 3.3413273518284163, Avg Policy Loss: 3.6679728478193283\n",
      "Episode 828, Reward: -53.19617377138322, Moving Avg Reward: -28.901190133948397, Replay Buffer Size: 29857\n",
      "Current Epsilon: 0.025883670561501974\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 828, Reward: -53.19617377138322, Moving Avg Reward: -28.901190133948397, Replay Buffer Size: 29857\n",
      "Current Epsilon: 0.025883670561501974\n",
      "Episode 829: Lost\n",
      "Episode 829, Avg Value Loss: 3.230257487574289, Avg Policy Loss: 3.747248960095783\n",
      "Episode 829, Reward: -45.55607734910432, Moving Avg Reward: -28.973345878566594, Replay Buffer Size: 29900\n",
      "Current Epsilon: 0.025754252208694463\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 829, Reward: -45.55607734910432, Moving Avg Reward: -28.973345878566594, Replay Buffer Size: 29900\n",
      "Current Epsilon: 0.025754252208694463\n",
      "Episode 830: Lost\n",
      "Episode 830, Avg Value Loss: 3.3947782883277307, Avg Policy Loss: 3.7882090348463793\n",
      "Episode 830, Reward: -29.99066698033235, Moving Avg Reward: -29.38446005733786, Replay Buffer Size: 29913\n",
      "Current Epsilon: 0.02562548094765099\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 830, Reward: -29.99066698033235, Moving Avg Reward: -29.38446005733786, Replay Buffer Size: 29913\n",
      "Current Epsilon: 0.02562548094765099\n",
      "Episode 831: Lost\n",
      "Episode 831, Avg Value Loss: 3.5595015486081443, Avg Policy Loss: 3.7442031105359397\n",
      "Episode 831, Reward: -25.68735201274879, Moving Avg Reward: -29.370105200157678, Replay Buffer Size: 29925\n",
      "Current Epsilon: 0.025497353542912736\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 831, Reward: -25.68735201274879, Moving Avg Reward: -29.370105200157678, Replay Buffer Size: 29925\n",
      "Current Epsilon: 0.025497353542912736\n",
      "Episode 832: Lost\n",
      "Episode 832, Avg Value Loss: 3.073962860637241, Avg Policy Loss: 3.6803407933976917\n",
      "Episode 832, Reward: -22.535970037960148, Moving Avg Reward: -28.972192891287158, Replay Buffer Size: 29934\n",
      "Current Epsilon: 0.02536986677519817\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 832, Reward: -22.535970037960148, Moving Avg Reward: -28.972192891287158, Replay Buffer Size: 29934\n",
      "Current Epsilon: 0.02536986677519817\n",
      "Episode 833: Lost\n",
      "Episode 833, Avg Value Loss: 3.6133905107324775, Avg Policy Loss: 3.811280091603597\n",
      "Episode 833, Reward: 17.857706111281637, Moving Avg Reward: -28.060627469394444, Replay Buffer Size: 29967\n",
      "Current Epsilon: 0.02524301744132218\n",
      "Loss rate over the last 150 episodes: 0.68\n",
      "Episode 833, Reward: 17.857706111281637, Moving Avg Reward: -28.060627469394444, Replay Buffer Size: 29967\n",
      "Current Epsilon: 0.02524301744132218\n",
      "Episode 834: Won\n",
      "Episode 834, Avg Value Loss: 3.255705108245214, Avg Policy Loss: 3.608636418978373\n",
      "Episode 834, Reward: -33.51027154989028, Moving Avg Reward: -28.158382759391248, Replay Buffer Size: 29979\n",
      "Current Epsilon: 0.025116802354115567\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 834, Reward: -33.51027154989028, Moving Avg Reward: -28.158382759391248, Replay Buffer Size: 29979\n",
      "Current Epsilon: 0.025116802354115567\n",
      "Episode 835: Won\n",
      "Episode 835, Avg Value Loss: 3.549947662787004, Avg Policy Loss: 3.6348779634995894\n",
      "Episode 835, Reward: 9.89, Moving Avg Reward: -28.197287223846015, Replay Buffer Size: 29990\n",
      "Current Epsilon: 0.024991218342344988\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 835, Reward: 9.89, Moving Avg Reward: -28.197287223846015, Replay Buffer Size: 29990\n",
      "Current Epsilon: 0.024991218342344988\n",
      "Episode 836: Won\n",
      "Episode 836, Avg Value Loss: 3.3722329042851924, Avg Policy Loss: 3.6852880358695983\n",
      "Episode 836, Reward: -37.05447188821392, Moving Avg Reward: -28.311638183090437, Replay Buffer Size: 30070\n",
      "Current Epsilon: 0.024866262250633264\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 836, Reward: -37.05447188821392, Moving Avg Reward: -28.311638183090437, Replay Buffer Size: 30070\n",
      "Current Epsilon: 0.024866262250633264\n",
      "Episode 837: Won\n",
      "Episode 837, Avg Value Loss: 3.810220039807833, Avg Policy Loss: 3.337221915905292\n",
      "Episode 837, Reward: -26.599071032462266, Moving Avg Reward: -28.502850954103007, Replay Buffer Size: 30083\n",
      "Current Epsilon: 0.024741930939380097\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 837, Reward: -26.599071032462266, Moving Avg Reward: -28.502850954103007, Replay Buffer Size: 30083\n",
      "Current Epsilon: 0.024741930939380097\n",
      "Episode 838: Won\n",
      "Episode 838, Avg Value Loss: 2.994980665353628, Avg Policy Loss: 3.8670580432965207\n",
      "Episode 838, Reward: -67.62791318098833, Moving Avg Reward: -28.86350238031724, Replay Buffer Size: 30135\n",
      "Current Epsilon: 0.024618221284683196\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 838, Reward: -67.62791318098833, Moving Avg Reward: -28.86350238031724, Replay Buffer Size: 30135\n",
      "Current Epsilon: 0.024618221284683196\n",
      "Episode 839: Won\n",
      "Episode 839, Avg Value Loss: 3.6652710139751434, Avg Policy Loss: 3.6188486417134604\n",
      "Episode 839, Reward: -30.949624955804097, Moving Avg Reward: -29.015897422049797, Replay Buffer Size: 30147\n",
      "Current Epsilon: 0.02449513017825978\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 839, Reward: -30.949624955804097, Moving Avg Reward: -29.015897422049797, Replay Buffer Size: 30147\n",
      "Current Epsilon: 0.02449513017825978\n",
      "Episode 840: Lost\n",
      "Episode 840, Avg Value Loss: 3.326551413536072, Avg Policy Loss: 3.7952010154724123\n",
      "Episode 840, Reward: -26.7558259626297, Moving Avg Reward: -26.540158141839335, Replay Buffer Size: 30157\n",
      "Current Epsilon: 0.02437265452736848\n",
      "Loss rate over the last 150 episodes: 0.68\n",
      "Episode 840, Reward: -26.7558259626297, Moving Avg Reward: -26.540158141839335, Replay Buffer Size: 30157\n",
      "Current Epsilon: 0.02437265452736848\n",
      "Episode 841: Lost\n",
      "Episode 841, Avg Value Loss: 4.203626970450084, Avg Policy Loss: 3.4816210667292276\n",
      "Episode 841, Reward: -28.14928777627083, Moving Avg Reward: -26.815351967724304, Replay Buffer Size: 30169\n",
      "Current Epsilon: 0.024250791254731636\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 841, Reward: -28.14928777627083, Moving Avg Reward: -26.815351967724304, Replay Buffer Size: 30169\n",
      "Current Epsilon: 0.024250791254731636\n",
      "Episode 842: Lost\n",
      "Episode 842, Avg Value Loss: 3.211298179626465, Avg Policy Loss: 4.003703236579895\n",
      "Episode 842, Reward: -26.82174782675751, Moving Avg Reward: -26.53720185968989, Replay Buffer Size: 30179\n",
      "Current Epsilon: 0.024129537298457977\n",
      "Loss rate over the last 150 episodes: 0.68\n",
      "Episode 842, Reward: -26.82174782675751, Moving Avg Reward: -26.53720185968989, Replay Buffer Size: 30179\n",
      "Current Epsilon: 0.024129537298457977\n",
      "Episode 843: Lost\n",
      "Episode 843, Avg Value Loss: 4.104860132390803, Avg Policy Loss: 3.7611704522913154\n",
      "Episode 843, Reward: -37.220749914028545, Moving Avg Reward: -26.639061685218135, Replay Buffer Size: 30190\n",
      "Current Epsilon: 0.024008889611965685\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 843, Reward: -37.220749914028545, Moving Avg Reward: -26.639061685218135, Replay Buffer Size: 30190\n",
      "Current Epsilon: 0.024008889611965685\n",
      "Episode 844: Lost\n",
      "Episode 844, Avg Value Loss: 3.3195324142773948, Avg Policy Loss: 3.780370752016703\n",
      "Episode 844, Reward: -30.4763771328777, Moving Avg Reward: -26.72810350483533, Replay Buffer Size: 30202\n",
      "Current Epsilon: 0.023888845163905856\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 844, Reward: -30.4763771328777, Moving Avg Reward: -26.72810350483533, Replay Buffer Size: 30202\n",
      "Current Epsilon: 0.023888845163905856\n",
      "Episode 845: Lost\n",
      "Episode 845, Avg Value Loss: 3.729162242677477, Avg Policy Loss: 3.4469058513641357\n",
      "Episode 845, Reward: -27.17533142313335, Moving Avg Reward: -26.767778707930987, Replay Buffer Size: 30211\n",
      "Current Epsilon: 0.023769400938086327\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 845, Reward: -27.17533142313335, Moving Avg Reward: -26.767778707930987, Replay Buffer Size: 30211\n",
      "Current Epsilon: 0.023769400938086327\n",
      "Episode 846: Lost\n",
      "Episode 846, Avg Value Loss: 3.6373483085632325, Avg Policy Loss: 3.55908390045166\n",
      "Episode 846, Reward: 18.609544676670268, Moving Avg Reward: -26.277642727460375, Replay Buffer Size: 30236\n",
      "Current Epsilon: 0.023650553933395897\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 846, Reward: 18.609544676670268, Moving Avg Reward: -26.277642727460375, Replay Buffer Size: 30236\n",
      "Current Epsilon: 0.023650553933395897\n",
      "Episode 847: Won\n",
      "Episode 847, Avg Value Loss: 3.2702372170984746, Avg Policy Loss: 3.8488616824150084\n",
      "Episode 847, Reward: -4.876469737604322, Moving Avg Reward: -26.03914911616711, Replay Buffer Size: 30316\n",
      "Current Epsilon: 0.023532301163728918\n",
      "Loss rate over the last 150 episodes: 0.68\n",
      "Episode 847, Reward: -4.876469737604322, Moving Avg Reward: -26.03914911616711, Replay Buffer Size: 30316\n",
      "Current Epsilon: 0.023532301163728918\n",
      "Episode 848: Won\n",
      "Episode 848, Avg Value Loss: 3.5110374782599654, Avg Policy Loss: 3.7200639201145544\n",
      "Episode 848, Reward: -30.7954351438054, Moving Avg Reward: -26.048757077448776, Replay Buffer Size: 30367\n",
      "Current Epsilon: 0.023414639657910272\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 848, Reward: -30.7954351438054, Moving Avg Reward: -26.048757077448776, Replay Buffer Size: 30367\n",
      "Current Epsilon: 0.023414639657910272\n",
      "Episode 849: Won\n",
      "Episode 849, Avg Value Loss: 3.4768977385980113, Avg Policy Loss: 3.720477165999236\n",
      "Episode 849, Reward: -40.464068840938616, Moving Avg Reward: -26.169468379573505, Replay Buffer Size: 30394\n",
      "Current Epsilon: 0.023297566459620722\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 849, Reward: -40.464068840938616, Moving Avg Reward: -26.169468379573505, Replay Buffer Size: 30394\n",
      "Current Epsilon: 0.023297566459620722\n",
      "Episode 850: Won\n",
      "Episode 850, Avg Value Loss: 2.9783535612126193, Avg Policy Loss: 3.766702115535736\n",
      "Episode 850, Reward: -71.56647598002075, Moving Avg Reward: -26.66441007775872, Replay Buffer Size: 30442\n",
      "Current Epsilon: 0.023181078627322618\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 850, Reward: -71.56647598002075, Moving Avg Reward: -26.66441007775872, Replay Buffer Size: 30442\n",
      "Current Epsilon: 0.023181078627322618\n",
      "Episode 851: Won\n",
      "Episode 851, Avg Value Loss: 3.5736576967379627, Avg Policy Loss: 3.720819718697492\n",
      "Episode 851, Reward: -8.89013170662256, Moving Avg Reward: -26.488236868665943, Replay Buffer Size: 30510\n",
      "Current Epsilon: 0.023065173234186005\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 851, Reward: -8.89013170662256, Moving Avg Reward: -26.488236868665943, Replay Buffer Size: 30510\n",
      "Current Epsilon: 0.023065173234186005\n",
      "Episode 852: Won\n",
      "Episode 852, Avg Value Loss: 3.4810320447992393, Avg Policy Loss: 3.705725484424167\n",
      "Episode 852, Reward: 16.408369105127747, Moving Avg Reward: -26.089111228314902, Replay Buffer Size: 30537\n",
      "Current Epsilon: 0.022949847368015076\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 852, Reward: 16.408369105127747, Moving Avg Reward: -26.089111228314902, Replay Buffer Size: 30537\n",
      "Current Epsilon: 0.022949847368015076\n",
      "Episode 853: Won\n",
      "Episode 853, Avg Value Loss: 3.30684170126915, Avg Policy Loss: 3.7464607914288837\n",
      "Episode 853, Reward: -2.4710776593409456, Moving Avg Reward: -25.774150107466813, Replay Buffer Size: 30597\n",
      "Current Epsilon: 0.022835098131175\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 853, Reward: -2.4710776593409456, Moving Avg Reward: -25.774150107466813, Replay Buffer Size: 30597\n",
      "Current Epsilon: 0.022835098131175\n",
      "Episode 854: Won\n",
      "Episode 854, Avg Value Loss: 3.539352317651113, Avg Policy Loss: 3.7618722319602966\n",
      "Episode 854, Reward: -32.102365589732656, Moving Avg Reward: -25.80993021336262, Replay Buffer Size: 30609\n",
      "Current Epsilon: 0.022720922640519125\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 854, Reward: -32.102365589732656, Moving Avg Reward: -25.80993021336262, Replay Buffer Size: 30609\n",
      "Current Epsilon: 0.022720922640519125\n",
      "Episode 855: Won\n",
      "Episode 855, Avg Value Loss: 3.546402004030016, Avg Policy Loss: 3.487996631198459\n",
      "Episode 855, Reward: -22.0402484976959, Moving Avg Reward: -26.101180790567422, Replay Buffer Size: 30618\n",
      "Current Epsilon: 0.02260731802731653\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 855, Reward: -22.0402484976959, Moving Avg Reward: -26.101180790567422, Replay Buffer Size: 30618\n",
      "Current Epsilon: 0.02260731802731653\n",
      "Episode 856: Won\n",
      "Episode 856, Avg Value Loss: 3.2152059450745583, Avg Policy Loss: 3.7977920830249787\n",
      "Episode 856, Reward: -53.12904760588357, Moving Avg Reward: -26.341186131112178, Replay Buffer Size: 30698\n",
      "Current Epsilon: 0.022494281437179946\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 856, Reward: -53.12904760588357, Moving Avg Reward: -26.341186131112178, Replay Buffer Size: 30698\n",
      "Current Epsilon: 0.022494281437179946\n",
      "Episode 857: Won\n",
      "Episode 857, Avg Value Loss: 3.1698687835173174, Avg Policy Loss: 3.7388569224964487\n",
      "Episode 857, Reward: -25.980478460742724, Moving Avg Reward: -26.294632446886773, Replay Buffer Size: 30709\n",
      "Current Epsilon: 0.022381810029994047\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 857, Reward: -25.980478460742724, Moving Avg Reward: -26.294632446886773, Replay Buffer Size: 30709\n",
      "Current Epsilon: 0.022381810029994047\n",
      "Episode 858: Won\n",
      "Episode 858, Avg Value Loss: 3.8502558036284014, Avg Policy Loss: 3.5470690727233887\n",
      "Episode 858, Reward: -26.292373177107898, Moving Avg Reward: -26.360731957517757, Replay Buffer Size: 30720\n",
      "Current Epsilon: 0.022269900979844076\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 858, Reward: -26.292373177107898, Moving Avg Reward: -26.360731957517757, Replay Buffer Size: 30720\n",
      "Current Epsilon: 0.022269900979844076\n",
      "Episode 859: Won\n",
      "Episode 859, Avg Value Loss: 3.2899183332920074, Avg Policy Loss: 3.8596869111061096\n",
      "Episode 859, Reward: -25.54448416128008, Moving Avg Reward: -26.336913155944373, Replay Buffer Size: 30728\n",
      "Current Epsilon: 0.022158551474944856\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 859, Reward: -25.54448416128008, Moving Avg Reward: -26.336913155944373, Replay Buffer Size: 30728\n",
      "Current Epsilon: 0.022158551474944856\n",
      "Episode 860: Won\n",
      "Episode 860, Avg Value Loss: 4.328701901435852, Avg Policy Loss: 3.682548189163208\n",
      "Episode 860, Reward: -32.291542090696176, Moving Avg Reward: -26.327161674819603, Replay Buffer Size: 30738\n",
      "Current Epsilon: 0.022047758717570132\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 860, Reward: -32.291542090696176, Moving Avg Reward: -26.327161674819603, Replay Buffer Size: 30738\n",
      "Current Epsilon: 0.022047758717570132\n",
      "Episode 861: Won\n",
      "Episode 861, Avg Value Loss: 3.6097990151109367, Avg Policy Loss: 3.7581167714349153\n",
      "Episode 861, Reward: -4.741219572583933, Moving Avg Reward: -26.077394771566905, Replay Buffer Size: 30796\n",
      "Current Epsilon: 0.021937519923982282\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 861, Reward: -4.741219572583933, Moving Avg Reward: -26.077394771566905, Replay Buffer Size: 30796\n",
      "Current Epsilon: 0.021937519923982282\n",
      "Episode 862: Won\n",
      "Episode 862, Avg Value Loss: 2.760267186164856, Avg Policy Loss: 3.727327847480774\n",
      "Episode 862, Reward: -30.348035917906106, Moving Avg Reward: -26.036995196923623, Replay Buffer Size: 30806\n",
      "Current Epsilon: 0.021827832324362372\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 862, Reward: -30.348035917906106, Moving Avg Reward: -26.036995196923623, Replay Buffer Size: 30806\n",
      "Current Epsilon: 0.021827832324362372\n",
      "Episode 863: Won\n",
      "Episode 863, Avg Value Loss: 2.6110987530814276, Avg Policy Loss: 3.6847163836161294\n",
      "Episode 863, Reward: -30.801447871651135, Moving Avg Reward: -26.02124830702849, Replay Buffer Size: 30815\n",
      "Current Epsilon: 0.02171869316274056\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 863, Reward: -30.801447871651135, Moving Avg Reward: -26.02124830702849, Replay Buffer Size: 30815\n",
      "Current Epsilon: 0.02171869316274056\n",
      "Episode 864: Won\n",
      "Episode 864, Avg Value Loss: 3.5358567416667936, Avg Policy Loss: 3.7677701473236085\n",
      "Episode 864, Reward: -50.693607905728854, Moving Avg Reward: -26.235344955194133, Replay Buffer Size: 30895\n",
      "Current Epsilon: 0.021610099696926857\n",
      "Loss rate over the last 150 episodes: 0.68\n",
      "Episode 864, Reward: -50.693607905728854, Moving Avg Reward: -26.235344955194133, Replay Buffer Size: 30895\n",
      "Current Epsilon: 0.021610099696926857\n",
      "Episode 865: Won\n",
      "Episode 865, Avg Value Loss: 2.7874634087085726, Avg Policy Loss: 3.834778571128845\n",
      "Episode 865, Reward: -26.448546345873268, Moving Avg Reward: -26.205808098516446, Replay Buffer Size: 30905\n",
      "Current Epsilon: 0.021502049198442223\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 865, Reward: -26.448546345873268, Moving Avg Reward: -26.205808098516446, Replay Buffer Size: 30905\n",
      "Current Epsilon: 0.021502049198442223\n",
      "Episode 866: Won\n",
      "Episode 866, Avg Value Loss: 3.198332593991206, Avg Policy Loss: 3.701274981865516\n",
      "Episode 866, Reward: -28.29274800067583, Moving Avg Reward: -26.658885245112934, Replay Buffer Size: 30918\n",
      "Current Epsilon: 0.021394538952450012\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 866, Reward: -28.29274800067583, Moving Avg Reward: -26.658885245112934, Replay Buffer Size: 30918\n",
      "Current Epsilon: 0.021394538952450012\n",
      "Episode 867: Won\n",
      "Episode 867, Avg Value Loss: 3.243862483431311, Avg Policy Loss: 3.8065665399326996\n",
      "Episode 867, Reward: -19.050182391718728, Moving Avg Reward: -26.652393468390304, Replay Buffer Size: 30986\n",
      "Current Epsilon: 0.02128756625768776\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 867, Reward: -19.050182391718728, Moving Avg Reward: -26.652393468390304, Replay Buffer Size: 30986\n",
      "Current Epsilon: 0.02128756625768776\n",
      "Episode 868: Won\n",
      "Episode 868, Avg Value Loss: 3.367466515302658, Avg Policy Loss: 3.6626957178115847\n",
      "Episode 868, Reward: -28.54354273979039, Moving Avg Reward: -26.623477466702237, Replay Buffer Size: 30996\n",
      "Current Epsilon: 0.021181128426399323\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 868, Reward: -28.54354273979039, Moving Avg Reward: -26.623477466702237, Replay Buffer Size: 30996\n",
      "Current Epsilon: 0.021181128426399323\n",
      "Episode 869: Won\n",
      "Episode 869, Avg Value Loss: 3.468115448951721, Avg Policy Loss: 3.824550191561381\n",
      "Episode 869, Reward: -27.45006286081876, Moving Avg Reward: -26.607237417352497, Replay Buffer Size: 31008\n",
      "Current Epsilon: 0.021075222784267326\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 869, Reward: -27.45006286081876, Moving Avg Reward: -26.607237417352497, Replay Buffer Size: 31008\n",
      "Current Epsilon: 0.021075222784267326\n",
      "Episode 870: Won\n",
      "Episode 870, Avg Value Loss: 3.170404684954676, Avg Policy Loss: 3.723499265210382\n",
      "Episode 870, Reward: 17.342167639785337, Moving Avg Reward: -26.120774715271885, Replay Buffer Size: 31037\n",
      "Current Epsilon: 0.020969846670345987\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 870, Reward: 17.342167639785337, Moving Avg Reward: -26.120774715271885, Replay Buffer Size: 31037\n",
      "Current Epsilon: 0.020969846670345987\n",
      "Episode 871: Won\n",
      "Episode 871, Avg Value Loss: 3.289866492152214, Avg Policy Loss: 3.797468274831772\n",
      "Episode 871, Reward: -40.283256473203124, Moving Avg Reward: -26.26655488538587, Replay Buffer Size: 31117\n",
      "Current Epsilon: 0.020864997436994256\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 871, Reward: -40.283256473203124, Moving Avg Reward: -26.26655488538587, Replay Buffer Size: 31117\n",
      "Current Epsilon: 0.020864997436994256\n",
      "Episode 872: Won\n",
      "Episode 872, Avg Value Loss: 3.116029214859009, Avg Policy Loss: 3.935012602806091\n",
      "Episode 872, Reward: -27.754134872100433, Moving Avg Reward: -26.230974670836837, Replay Buffer Size: 31127\n",
      "Current Epsilon: 0.020760672449809284\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 872, Reward: -27.754134872100433, Moving Avg Reward: -26.230974670836837, Replay Buffer Size: 31127\n",
      "Current Epsilon: 0.020760672449809284\n",
      "Episode 873: Won\n",
      "Episode 873, Avg Value Loss: 3.532097102701664, Avg Policy Loss: 3.723902755975723\n",
      "Episode 873, Reward: -25.150499347289305, Moving Avg Reward: -26.1694214707373, Replay Buffer Size: 31207\n",
      "Current Epsilon: 0.020656869087560238\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 873, Reward: -25.150499347289305, Moving Avg Reward: -26.1694214707373, Replay Buffer Size: 31207\n",
      "Current Epsilon: 0.020656869087560238\n",
      "Episode 874: Lost\n",
      "Episode 874, Avg Value Loss: 3.3492241285064, Avg Policy Loss: 3.8360775492408057\n",
      "Episode 874, Reward: -26.610212245406967, Moving Avg Reward: -26.173044520509993, Replay Buffer Size: 31251\n",
      "Current Epsilon: 0.020553584742122436\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 874, Reward: -26.610212245406967, Moving Avg Reward: -26.173044520509993, Replay Buffer Size: 31251\n",
      "Current Epsilon: 0.020553584742122436\n",
      "Episode 875: Lost\n",
      "Episode 875, Avg Value Loss: 3.8899793922901154, Avg Policy Loss: 3.904800832271576\n",
      "Episode 875, Reward: -24.0574218478084, Moving Avg Reward: -25.8772780138212, Replay Buffer Size: 31259\n",
      "Current Epsilon: 0.020450816818411825\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 875, Reward: -24.0574218478084, Moving Avg Reward: -25.8772780138212, Replay Buffer Size: 31259\n",
      "Current Epsilon: 0.020450816818411825\n",
      "Episode 876: Lost\n",
      "Episode 876, Avg Value Loss: 3.446114331483841, Avg Policy Loss: 3.754569669564565\n",
      "Episode 876, Reward: -32.52384959513103, Moving Avg Reward: -25.89490897485889, Replay Buffer Size: 31271\n",
      "Current Epsilon: 0.020348562734319765\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 876, Reward: -32.52384959513103, Moving Avg Reward: -25.89490897485889, Replay Buffer Size: 31271\n",
      "Current Epsilon: 0.020348562734319765\n",
      "Episode 877: Lost\n",
      "Episode 877, Avg Value Loss: 2.7810906022787094, Avg Policy Loss: 4.0251688957214355\n",
      "Episode 877, Reward: -26.30907042253964, Moving Avg Reward: -26.222756694849625, Replay Buffer Size: 31279\n",
      "Current Epsilon: 0.020246819920648168\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 877, Reward: -26.30907042253964, Moving Avg Reward: -26.222756694849625, Replay Buffer Size: 31279\n",
      "Current Epsilon: 0.020246819920648168\n",
      "Episode 878: Lost\n",
      "Episode 878, Avg Value Loss: 3.383549062907696, Avg Policy Loss: 3.7653894633054734\n",
      "Episode 878, Reward: -7.243635981181619, Moving Avg Reward: -26.0981397862238, Replay Buffer Size: 31359\n",
      "Current Epsilon: 0.020145585821044927\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 878, Reward: -7.243635981181619, Moving Avg Reward: -26.0981397862238, Replay Buffer Size: 31359\n",
      "Current Epsilon: 0.020145585821044927\n",
      "Episode 879: Lost\n",
      "Episode 879, Avg Value Loss: 3.7708613780828624, Avg Policy Loss: 3.8012584906357985\n",
      "Episode 879, Reward: 9.870000000000001, Moving Avg Reward: -25.69942842613209, Replay Buffer Size: 31372\n",
      "Current Epsilon: 0.020044857891939702\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 879, Reward: 9.870000000000001, Moving Avg Reward: -25.69942842613209, Replay Buffer Size: 31372\n",
      "Current Epsilon: 0.020044857891939702\n",
      "Episode 880: Won\n",
      "Episode 880, Avg Value Loss: 3.0737590193748474, Avg Policy Loss: 3.8627481162548065\n",
      "Episode 880, Reward: -23.024408937640167, Moving Avg Reward: -25.539718305621804, Replay Buffer Size: 31380\n",
      "Current Epsilon: 0.019944633602480003\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 880, Reward: -23.024408937640167, Moving Avg Reward: -25.539718305621804, Replay Buffer Size: 31380\n",
      "Current Epsilon: 0.019944633602480003\n",
      "Episode 881: Won\n",
      "Episode 881, Avg Value Loss: 3.2316183393651787, Avg Policy Loss: 3.84987980669195\n",
      "Episode 881, Reward: -26.614535988861707, Moving Avg Reward: -25.29503805968708, Replay Buffer Size: 31391\n",
      "Current Epsilon: 0.019844910434467605\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 881, Reward: -26.614535988861707, Moving Avg Reward: -25.29503805968708, Replay Buffer Size: 31391\n",
      "Current Epsilon: 0.019844910434467605\n",
      "Episode 882: Won\n",
      "Episode 882, Avg Value Loss: 3.3456964500248434, Avg Policy Loss: 3.7510430812835693\n",
      "Episode 882, Reward: -0.008806146490653444, Moving Avg Reward: -24.99906373999888, Replay Buffer Size: 31471\n",
      "Current Epsilon: 0.019745685882295267\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 882, Reward: -0.008806146490653444, Moving Avg Reward: -24.99906373999888, Replay Buffer Size: 31471\n",
      "Current Epsilon: 0.019745685882295267\n",
      "Episode 883: Won\n",
      "Episode 883, Avg Value Loss: 3.440478260700519, Avg Policy Loss: 3.8639608163100023\n",
      "Episode 883, Reward: -30.388894021835227, Moving Avg Reward: -25.484565456037075, Replay Buffer Size: 31484\n",
      "Current Epsilon: 0.01964695745288379\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 883, Reward: -30.388894021835227, Moving Avg Reward: -25.484565456037075, Replay Buffer Size: 31484\n",
      "Current Epsilon: 0.01964695745288379\n",
      "Episode 884: Won\n",
      "Episode 884, Avg Value Loss: 3.3065225407481194, Avg Policy Loss: 3.820546117424965\n",
      "Episode 884, Reward: 3.5847163593832274, Moving Avg Reward: -25.09457290466781, Replay Buffer Size: 31564\n",
      "Current Epsilon: 0.01954872266561937\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 884, Reward: 3.5847163593832274, Moving Avg Reward: -25.09457290466781, Replay Buffer Size: 31564\n",
      "Current Epsilon: 0.01954872266561937\n",
      "Episode 885: Lost\n",
      "Episode 885, Avg Value Loss: 3.725759118795395, Avg Policy Loss: 3.7171164453029633\n",
      "Episode 885, Reward: -8.07043058607374, Moving Avg Reward: -24.926352759774787, Replay Buffer Size: 31572\n",
      "Current Epsilon: 0.019450979052291272\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 885, Reward: -8.07043058607374, Moving Avg Reward: -24.926352759774787, Replay Buffer Size: 31572\n",
      "Current Epsilon: 0.019450979052291272\n",
      "Episode 886: Lost\n",
      "Episode 886, Avg Value Loss: 2.992888816197713, Avg Policy Loss: 3.867003806432088\n",
      "Episode 886, Reward: 9.85, Moving Avg Reward: -24.572696680103412, Replay Buffer Size: 31587\n",
      "Current Epsilon: 0.019353724157029815\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 886, Reward: 9.85, Moving Avg Reward: -24.572696680103412, Replay Buffer Size: 31587\n",
      "Current Epsilon: 0.019353724157029815\n",
      "Episode 887: Won\n",
      "Episode 887, Avg Value Loss: 3.371417276818177, Avg Policy Loss: 3.824094114632442\n",
      "Episode 887, Reward: -6.053633102603966, Moving Avg Reward: -24.821123605226617, Replay Buffer Size: 31645\n",
      "Current Epsilon: 0.019256955536244666\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 887, Reward: -6.053633102603966, Moving Avg Reward: -24.821123605226617, Replay Buffer Size: 31645\n",
      "Current Epsilon: 0.019256955536244666\n",
      "Episode 888: Won\n",
      "Episode 888, Avg Value Loss: 3.4388250559568405, Avg Policy Loss: 3.704140027364095\n",
      "Episode 888, Reward: -28.596274731476157, Moving Avg Reward: -24.85340123143378, Replay Buffer Size: 31693\n",
      "Current Epsilon: 0.019160670758563442\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 888, Reward: -28.596274731476157, Moving Avg Reward: -24.85340123143378, Replay Buffer Size: 31693\n",
      "Current Epsilon: 0.019160670758563442\n",
      "Episode 889: Won\n",
      "Episode 889, Avg Value Loss: 3.335783326107523, Avg Policy Loss: 3.769182941187983\n",
      "Episode 889, Reward: 18.3895698460363, Moving Avg Reward: -24.317019957199232, Replay Buffer Size: 31716\n",
      "Current Epsilon: 0.019064867404770626\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 889, Reward: 18.3895698460363, Moving Avg Reward: -24.317019957199232, Replay Buffer Size: 31716\n",
      "Current Epsilon: 0.019064867404770626\n",
      "Episode 890: Won\n",
      "Episode 890, Avg Value Loss: 3.096684288978577, Avg Policy Loss: 3.8295648097991943\n",
      "Episode 890, Reward: -24.436351031182596, Moving Avg Reward: -24.290418203317305, Replay Buffer Size: 31726\n",
      "Current Epsilon: 0.018969543067746772\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 890, Reward: -24.436351031182596, Moving Avg Reward: -24.290418203317305, Replay Buffer Size: 31726\n",
      "Current Epsilon: 0.018969543067746772\n",
      "Episode 891: Won\n",
      "Episode 891, Avg Value Loss: 3.2400205589476085, Avg Policy Loss: 3.944154523667835\n",
      "Episode 891, Reward: 20.039178249164383, Moving Avg Reward: -23.33140093552082, Replay Buffer Size: 31747\n",
      "Current Epsilon: 0.018874695352408037\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 891, Reward: 20.039178249164383, Moving Avg Reward: -23.33140093552082, Replay Buffer Size: 31747\n",
      "Current Epsilon: 0.018874695352408037\n",
      "Episode 892: Won\n",
      "Episode 892, Avg Value Loss: 3.724000081783388, Avg Policy Loss: 3.76008357071295\n",
      "Episode 892, Reward: -28.183771224867915, Moving Avg Reward: -23.354419625254874, Replay Buffer Size: 31788\n",
      "Current Epsilon: 0.018780321875645996\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 892, Reward: -28.183771224867915, Moving Avg Reward: -23.354419625254874, Replay Buffer Size: 31788\n",
      "Current Epsilon: 0.018780321875645996\n",
      "Episode 893: Won\n",
      "Episode 893, Avg Value Loss: 2.8499873768199575, Avg Policy Loss: 3.887029669501565\n",
      "Episode 893, Reward: -33.04236318832417, Moving Avg Reward: -23.39182891462756, Replay Buffer Size: 31799\n",
      "Current Epsilon: 0.018686420266267767\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 893, Reward: -33.04236318832417, Moving Avg Reward: -23.39182891462756, Replay Buffer Size: 31799\n",
      "Current Epsilon: 0.018686420266267767\n",
      "Episode 894: Won\n",
      "Episode 894, Avg Value Loss: 3.237464514374733, Avg Policy Loss: 3.739136499166489\n",
      "Episode 894, Reward: -36.22061230890977, Moving Avg Reward: -23.49459463513374, Replay Buffer Size: 31839\n",
      "Current Epsilon: 0.018592988164936427\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 894, Reward: -36.22061230890977, Moving Avg Reward: -23.49459463513374, Replay Buffer Size: 31839\n",
      "Current Epsilon: 0.018592988164936427\n",
      "Episode 895: Won\n",
      "Episode 895, Avg Value Loss: 3.5148189663887024, Avg Policy Loss: 3.8287577342987063\n",
      "Episode 895, Reward: 17.531212850559847, Moving Avg Reward: -22.95864638823824, Replay Buffer Size: 31889\n",
      "Current Epsilon: 0.018500023224111744\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 895, Reward: 17.531212850559847, Moving Avg Reward: -22.95864638823824, Replay Buffer Size: 31889\n",
      "Current Epsilon: 0.018500023224111744\n",
      "Episode 896: Won\n",
      "Episode 896, Avg Value Loss: 4.019383788108826, Avg Policy Loss: 3.702623099088669\n",
      "Episode 896, Reward: -25.016874765101214, Moving Avg Reward: -22.27599054231705, Replay Buffer Size: 31897\n",
      "Current Epsilon: 0.018407523107991184\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 896, Reward: -25.016874765101214, Moving Avg Reward: -22.27599054231705, Replay Buffer Size: 31897\n",
      "Current Epsilon: 0.018407523107991184\n",
      "Episode 897: Won\n",
      "Episode 897, Avg Value Loss: 3.979701592372014, Avg Policy Loss: 3.599837908378014\n",
      "Episode 897, Reward: -27.190991635012203, Moving Avg Reward: -22.060345933341697, Replay Buffer Size: 31910\n",
      "Current Epsilon: 0.01831548549245123\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 897, Reward: -27.190991635012203, Moving Avg Reward: -22.060345933341697, Replay Buffer Size: 31910\n",
      "Current Epsilon: 0.01831548549245123\n",
      "Episode 898: Won\n",
      "Episode 898, Avg Value Loss: 3.415377786665252, Avg Policy Loss: 3.863239382252549\n",
      "Episode 898, Reward: 10.316165019906558, Moving Avg Reward: -21.723087667193177, Replay Buffer Size: 31943\n",
      "Current Epsilon: 0.018223908064988973\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 898, Reward: 10.316165019906558, Moving Avg Reward: -21.723087667193177, Replay Buffer Size: 31943\n",
      "Current Epsilon: 0.018223908064988973\n",
      "Episode 899: Won\n",
      "Episode 899, Avg Value Loss: 2.56018503010273, Avg Policy Loss: 3.683966487646103\n",
      "Episode 899, Reward: -27.02409001339336, Moving Avg Reward: -21.738821928906408, Replay Buffer Size: 31951\n",
      "Current Epsilon: 0.018132788524664028\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 899, Reward: -27.02409001339336, Moving Avg Reward: -21.738821928906408, Replay Buffer Size: 31951\n",
      "Current Epsilon: 0.018132788524664028\n",
      "Episode 900: Won\n",
      "Episode 900, Avg Value Loss: 3.4155172511935232, Avg Policy Loss: 3.7128106206655502\n",
      "Episode 900, Reward: -29.61036061638648, Moving Avg Reward: -21.73917731296277, Replay Buffer Size: 32031\n",
      "Current Epsilon: 0.018042124582040707\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 900, Reward: -29.61036061638648, Moving Avg Reward: -21.73917731296277, Replay Buffer Size: 32031\n",
      "Current Epsilon: 0.018042124582040707\n",
      "Episode 901: Won\n",
      "Episode 901, Avg Value Loss: 3.660766427218914, Avg Policy Loss: 3.7783604502677917\n",
      "Episode 901, Reward: -24.028271437483347, Moving Avg Reward: -21.74143984047139, Replay Buffer Size: 32111\n",
      "Current Epsilon: 0.017951913959130504\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 901, Reward: -24.028271437483347, Moving Avg Reward: -21.74143984047139, Replay Buffer Size: 32111\n",
      "Current Epsilon: 0.017951913959130504\n",
      "Episode 902: Draw\n",
      "Episode 902, Avg Value Loss: 3.369487073686388, Avg Policy Loss: 3.677792681588067\n",
      "Episode 902, Reward: -30.707177317055866, Moving Avg Reward: -21.739191963567965, Replay Buffer Size: 32120\n",
      "Current Epsilon: 0.01786215438933485\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 902, Reward: -30.707177317055866, Moving Avg Reward: -21.739191963567965, Replay Buffer Size: 32120\n",
      "Current Epsilon: 0.01786215438933485\n",
      "Episode 903: Lost\n",
      "Episode 903, Avg Value Loss: 3.4640766799449922, Avg Policy Loss: 3.7689743558565776\n",
      "Episode 903, Reward: -65.1171678451151, Moving Avg Reward: -22.518321510451937, Replay Buffer Size: 32180\n",
      "Current Epsilon: 0.017772843617388175\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 903, Reward: -65.1171678451151, Moving Avg Reward: -22.518321510451937, Replay Buffer Size: 32180\n",
      "Current Epsilon: 0.017772843617388175\n",
      "Episode 904: Lost\n",
      "Episode 904, Avg Value Loss: 3.3262362134456636, Avg Policy Loss: 3.7130630159378053\n",
      "Episode 904, Reward: -55.93027310522539, Moving Avg Reward: -23.11333944637031, Replay Buffer Size: 32230\n",
      "Current Epsilon: 0.017683979399301233\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 904, Reward: -55.93027310522539, Moving Avg Reward: -23.11333944637031, Replay Buffer Size: 32230\n",
      "Current Epsilon: 0.017683979399301233\n",
      "Episode 905: Lost\n",
      "Episode 905, Avg Value Loss: 3.085063706744801, Avg Policy Loss: 3.821541417728771\n",
      "Episode 905, Reward: -25.7615181712428, Moving Avg Reward: -23.070425239330586, Replay Buffer Size: 32241\n",
      "Current Epsilon: 0.017595559502304726\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 905, Reward: -25.7615181712428, Moving Avg Reward: -23.070425239330586, Replay Buffer Size: 32241\n",
      "Current Epsilon: 0.017595559502304726\n",
      "Episode 906: Lost\n",
      "Episode 906, Avg Value Loss: 3.423875440657139, Avg Policy Loss: 3.7917404383420945\n",
      "Episode 906, Reward: -9.910529019666798, Moving Avg Reward: -22.816025623548192, Replay Buffer Size: 32321\n",
      "Current Epsilon: 0.0175075817047932\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 906, Reward: -9.910529019666798, Moving Avg Reward: -22.816025623548192, Replay Buffer Size: 32321\n",
      "Current Epsilon: 0.0175075817047932\n",
      "Episode 907: Lost\n",
      "Episode 907, Avg Value Loss: 3.0080085039138793, Avg Policy Loss: 3.810299587249756\n",
      "Episode 907, Reward: -25.567070119057732, Moving Avg Reward: -23.260199144928098, Replay Buffer Size: 32331\n",
      "Current Epsilon: 0.017420043796269234\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 907, Reward: -25.567070119057732, Moving Avg Reward: -23.260199144928098, Replay Buffer Size: 32331\n",
      "Current Epsilon: 0.017420043796269234\n",
      "Episode 908: Lost\n",
      "Episode 908, Avg Value Loss: 3.3691279657185076, Avg Policy Loss: 3.7568107664585115\n",
      "Episode 908, Reward: -69.1275220088451, Moving Avg Reward: -23.730611191231056, Replay Buffer Size: 32411\n",
      "Current Epsilon: 0.017332943577287888\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 908, Reward: -69.1275220088451, Moving Avg Reward: -23.730611191231056, Replay Buffer Size: 32411\n",
      "Current Epsilon: 0.017332943577287888\n",
      "Episode 909: Lost\n",
      "Episode 909, Avg Value Loss: 3.4689014568924903, Avg Policy Loss: 3.809257873892784\n",
      "Episode 909, Reward: -20.98873921944056, Moving Avg Reward: -23.673281259981824, Replay Buffer Size: 32491\n",
      "Current Epsilon: 0.01724627885940145\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 909, Reward: -20.98873921944056, Moving Avg Reward: -23.673281259981824, Replay Buffer Size: 32491\n",
      "Current Epsilon: 0.01724627885940145\n",
      "Episode 910: Draw\n",
      "Episode 910, Avg Value Loss: 3.3699803337454797, Avg Policy Loss: 3.8161412209272383\n",
      "Episode 910, Reward: -33.639604086728305, Moving Avg Reward: -23.98356384818727, Replay Buffer Size: 32571\n",
      "Current Epsilon: 0.017160047465104442\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 910, Reward: -33.639604086728305, Moving Avg Reward: -23.98356384818727, Replay Buffer Size: 32571\n",
      "Current Epsilon: 0.017160047465104442\n",
      "Episode 911: Draw\n",
      "Episode 911, Avg Value Loss: 3.9737398624420166, Avg Policy Loss: 3.9497831968160777\n",
      "Episode 911, Reward: -33.70737047903047, Moving Avg Reward: -23.758691439891017, Replay Buffer Size: 32584\n",
      "Current Epsilon: 0.01707424722777892\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 911, Reward: -33.70737047903047, Moving Avg Reward: -23.758691439891017, Replay Buffer Size: 32584\n",
      "Current Epsilon: 0.01707424722777892\n",
      "Episode 912: Lost\n",
      "Episode 912, Avg Value Loss: 3.5329938031733037, Avg Policy Loss: 3.855782824754715\n",
      "Episode 912, Reward: 3.5683024049374428, Moving Avg Reward: -23.430511605450615, Replay Buffer Size: 32664\n",
      "Current Epsilon: 0.016988875991640028\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 912, Reward: 3.5683024049374428, Moving Avg Reward: -23.430511605450615, Replay Buffer Size: 32664\n",
      "Current Epsilon: 0.016988875991640028\n",
      "Episode 913: Lost\n",
      "Episode 913, Avg Value Loss: 3.1905282898382707, Avg Policy Loss: 3.8571754585612905\n",
      "Episode 913, Reward: -32.80685346269084, Moving Avg Reward: -23.48491435391605, Replay Buffer Size: 32686\n",
      "Current Epsilon: 0.016903931611681827\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 913, Reward: -32.80685346269084, Moving Avg Reward: -23.48491435391605, Replay Buffer Size: 32686\n",
      "Current Epsilon: 0.016903931611681827\n",
      "Episode 914: Lost\n",
      "Episode 914, Avg Value Loss: 2.9897225350141525, Avg Policy Loss: 3.8438152074813843\n",
      "Episode 914, Reward: -24.073299627607113, Moving Avg Reward: -23.72177915722563, Replay Buffer Size: 32694\n",
      "Current Epsilon: 0.01681941195362342\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 914, Reward: -24.073299627607113, Moving Avg Reward: -23.72177915722563, Replay Buffer Size: 32694\n",
      "Current Epsilon: 0.01681941195362342\n",
      "Episode 915: Lost\n",
      "Episode 915, Avg Value Loss: 3.0317578546462522, Avg Policy Loss: 3.7912757550516436\n",
      "Episode 915, Reward: 16.242608785142057, Moving Avg Reward: -23.714264168798525, Replay Buffer Size: 32725\n",
      "Current Epsilon: 0.016735314893855303\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 915, Reward: 16.242608785142057, Moving Avg Reward: -23.714264168798525, Replay Buffer Size: 32725\n",
      "Current Epsilon: 0.016735314893855303\n",
      "Episode 916: Won\n",
      "Episode 916, Avg Value Loss: 3.8415985974398525, Avg Policy Loss: 3.6919064738533716\n",
      "Episode 916, Reward: -9.08958634560507, Moving Avg Reward: -23.52277922416142, Replay Buffer Size: 32736\n",
      "Current Epsilon: 0.016651638319386028\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 916, Reward: -9.08958634560507, Moving Avg Reward: -23.52277922416142, Replay Buffer Size: 32736\n",
      "Current Epsilon: 0.016651638319386028\n",
      "Episode 917: Won\n",
      "Episode 917, Avg Value Loss: 3.1523503916604176, Avg Policy Loss: 3.8989701781954085\n",
      "Episode 917, Reward: 24.870000000000005, Moving Avg Reward: -22.947343695059057, Replay Buffer Size: 32750\n",
      "Current Epsilon: 0.0165683801277891\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 917, Reward: 24.870000000000005, Moving Avg Reward: -22.947343695059057, Replay Buffer Size: 32750\n",
      "Current Epsilon: 0.0165683801277891\n",
      "Episode 918: Won\n",
      "Episode 918, Avg Value Loss: 3.6929952998956046, Avg Policy Loss: 3.8700957894325256\n",
      "Episode 918, Reward: -32.049715471350034, Moving Avg Reward: -23.00109792759325, Replay Buffer Size: 32762\n",
      "Current Epsilon: 0.016485538227150154\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 918, Reward: -32.049715471350034, Moving Avg Reward: -23.00109792759325, Replay Buffer Size: 32762\n",
      "Current Epsilon: 0.016485538227150154\n",
      "Episode 919: Won\n",
      "Episode 919, Avg Value Loss: 3.437583860754967, Avg Policy Loss: 3.857396772503853\n",
      "Episode 919, Reward: -3.230138053271454, Moving Avg Reward: -22.569587745117516, Replay Buffer Size: 32842\n",
      "Current Epsilon: 0.0164031105360144\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 919, Reward: -3.230138053271454, Moving Avg Reward: -22.569587745117516, Replay Buffer Size: 32842\n",
      "Current Epsilon: 0.0164031105360144\n",
      "Episode 920: Won\n",
      "Episode 920, Avg Value Loss: 3.1100609567430286, Avg Policy Loss: 3.675650305218167\n",
      "Episode 920, Reward: -21.55057308286001, Moving Avg Reward: -22.520978301444103, Replay Buffer Size: 32851\n",
      "Current Epsilon: 0.01632109498333433\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 920, Reward: -21.55057308286001, Moving Avg Reward: -22.520978301444103, Replay Buffer Size: 32851\n",
      "Current Epsilon: 0.01632109498333433\n",
      "Episode 921: Won\n",
      "Episode 921, Avg Value Loss: 3.294843160189115, Avg Policy Loss: 3.8342732649583082\n",
      "Episode 921, Reward: -38.836444434557606, Moving Avg Reward: -22.66534798960894, Replay Buffer Size: 32864\n",
      "Current Epsilon: 0.016239489508417658\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 921, Reward: -38.836444434557606, Moving Avg Reward: -22.66534798960894, Replay Buffer Size: 32864\n",
      "Current Epsilon: 0.016239489508417658\n",
      "Episode 922: Won\n",
      "Episode 922, Avg Value Loss: 3.2514691136100073, Avg Policy Loss: 3.8490370403636587\n",
      "Episode 922, Reward: -23.936153708325897, Moving Avg Reward: -22.623997897729332, Replay Buffer Size: 32875\n",
      "Current Epsilon: 0.01615829206087557\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 922, Reward: -23.936153708325897, Moving Avg Reward: -22.623997897729332, Replay Buffer Size: 32875\n",
      "Current Epsilon: 0.01615829206087557\n",
      "Episode 923: Won\n",
      "Episode 923, Avg Value Loss: 3.204272449016571, Avg Policy Loss: 3.7796750985659084\n",
      "Episode 923, Reward: 16.672845627858507, Moving Avg Reward: -21.941706493201256, Replay Buffer Size: 32901\n",
      "Current Epsilon: 0.01607750060057119\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 923, Reward: 16.672845627858507, Moving Avg Reward: -21.941706493201256, Replay Buffer Size: 32901\n",
      "Current Epsilon: 0.01607750060057119\n",
      "Episode 924: Won\n",
      "Episode 924, Avg Value Loss: 3.4069249749183657, Avg Policy Loss: 3.793926340341568\n",
      "Episode 924, Reward: 4.78007666353035, Moving Avg Reward: -21.536962545430402, Replay Buffer Size: 32981\n",
      "Current Epsilon: 0.015997113097568336\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 924, Reward: 4.78007666353035, Moving Avg Reward: -21.536962545430402, Replay Buffer Size: 32981\n",
      "Current Epsilon: 0.015997113097568336\n",
      "Episode 925: Won\n",
      "Episode 925, Avg Value Loss: 3.3950161091983317, Avg Policy Loss: 3.873570057749748\n",
      "Episode 925, Reward: -46.297930827910925, Moving Avg Reward: -21.753303287285426, Replay Buffer Size: 33061\n",
      "Current Epsilon: 0.015917127532080494\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 925, Reward: -46.297930827910925, Moving Avg Reward: -21.753303287285426, Replay Buffer Size: 33061\n",
      "Current Epsilon: 0.015917127532080494\n",
      "Episode 926: Draw\n",
      "Episode 926, Avg Value Loss: 3.514602747456781, Avg Policy Loss: 3.7553189047451676\n",
      "Episode 926, Reward: 18.369265216270048, Moving Avg Reward: -21.156787571553394, Replay Buffer Size: 33090\n",
      "Current Epsilon: 0.01583754189442009\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 926, Reward: 18.369265216270048, Moving Avg Reward: -21.156787571553394, Replay Buffer Size: 33090\n",
      "Current Epsilon: 0.01583754189442009\n",
      "Episode 927: Won\n",
      "Episode 927, Avg Value Loss: 3.5787231430411337, Avg Policy Loss: 3.7906187206506727\n",
      "Episode 927, Reward: -95.73686554307196, Moving Avg Reward: -22.034360589461127, Replay Buffer Size: 33170\n",
      "Current Epsilon: 0.01575835418494799\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 927, Reward: -95.73686554307196, Moving Avg Reward: -22.034360589461127, Replay Buffer Size: 33170\n",
      "Current Epsilon: 0.01575835418494799\n",
      "Episode 928: Won\n",
      "Episode 928, Avg Value Loss: 3.273812885582447, Avg Policy Loss: 3.8025561571121216\n",
      "Episode 928, Reward: -9.490705406092031, Moving Avg Reward: -21.597305905808213, Replay Buffer Size: 33250\n",
      "Current Epsilon: 0.01567956241402325\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 928, Reward: -9.490705406092031, Moving Avg Reward: -21.597305905808213, Replay Buffer Size: 33250\n",
      "Current Epsilon: 0.01567956241402325\n",
      "Episode 929: Draw\n",
      "Episode 929, Avg Value Loss: 3.15760892087763, Avg Policy Loss: 3.7876959930766714\n",
      "Episode 929, Reward: -28.536718734035908, Moving Avg Reward: -21.42711231965752, Replay Buffer Size: 33261\n",
      "Current Epsilon: 0.015601164601953134\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 929, Reward: -28.536718734035908, Moving Avg Reward: -21.42711231965752, Replay Buffer Size: 33261\n",
      "Current Epsilon: 0.015601164601953134\n",
      "Episode 930: Lost\n",
      "Episode 930, Avg Value Loss: 3.802991910414262, Avg Policy Loss: 3.8621943213722925\n",
      "Episode 930, Reward: -28.079376398579186, Moving Avg Reward: -21.40799941384, Replay Buffer Size: 33272\n",
      "Current Epsilon: 0.015523158778943369\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 930, Reward: -28.079376398579186, Moving Avg Reward: -21.40799941384, Replay Buffer Size: 33272\n",
      "Current Epsilon: 0.015523158778943369\n",
      "Episode 931: Lost\n",
      "Episode 931, Avg Value Loss: 3.582534395158291, Avg Policy Loss: 3.7529103815555573\n",
      "Episode 931, Reward: -78.60026718148731, Moving Avg Reward: -21.937128565527377, Replay Buffer Size: 33352\n",
      "Current Epsilon: 0.015445542985048652\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 931, Reward: -78.60026718148731, Moving Avg Reward: -21.937128565527377, Replay Buffer Size: 33352\n",
      "Current Epsilon: 0.015445542985048652\n",
      "Episode 932: Lost\n",
      "Episode 932, Avg Value Loss: 3.0193846906934465, Avg Policy Loss: 3.773916857583182\n",
      "Episode 932, Reward: -21.465192542916306, Moving Avg Reward: -21.926420790576945, Replay Buffer Size: 33359\n",
      "Current Epsilon: 0.015368315270123408\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 932, Reward: -21.465192542916306, Moving Avg Reward: -21.926420790576945, Replay Buffer Size: 33359\n",
      "Current Epsilon: 0.015368315270123408\n",
      "Episode 933: Lost\n",
      "Episode 933, Avg Value Loss: 3.4022011756896973, Avg Policy Loss: 3.8043128384484186\n",
      "Episode 933, Reward: -27.250859182963616, Moving Avg Reward: -22.3775064435194, Replay Buffer Size: 33368\n",
      "Current Epsilon: 0.01529147369377279\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 933, Reward: -27.250859182963616, Moving Avg Reward: -22.3775064435194, Replay Buffer Size: 33368\n",
      "Current Epsilon: 0.01529147369377279\n",
      "Episode 934: Lost\n",
      "Episode 934, Avg Value Loss: 3.304484207238724, Avg Policy Loss: 3.7626457000846294\n",
      "Episode 934, Reward: 15.358655101122396, Moving Avg Reward: -21.88881717700927, Replay Buffer Size: 33435\n",
      "Current Epsilon: 0.015215016325303928\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 934, Reward: 15.358655101122396, Moving Avg Reward: -21.88881717700927, Replay Buffer Size: 33435\n",
      "Current Epsilon: 0.015215016325303928\n",
      "Episode 935: Won\n",
      "Episode 935, Avg Value Loss: 3.4529182059424266, Avg Policy Loss: 3.864597967692784\n",
      "Episode 935, Reward: -21.79038817095882, Moving Avg Reward: -22.205621058718858, Replay Buffer Size: 33442\n",
      "Current Epsilon: 0.015138941243677408\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 935, Reward: -21.79038817095882, Moving Avg Reward: -22.205621058718858, Replay Buffer Size: 33442\n",
      "Current Epsilon: 0.015138941243677408\n",
      "Episode 936: Won\n",
      "Episode 936, Avg Value Loss: 3.6022529204686484, Avg Policy Loss: 3.7661576201950293\n",
      "Episode 936, Reward: -104.5367910275407, Moving Avg Reward: -22.880444250112127, Replay Buffer Size: 33511\n",
      "Current Epsilon: 0.01506324653745902\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 936, Reward: -104.5367910275407, Moving Avg Reward: -22.880444250112127, Replay Buffer Size: 33511\n",
      "Current Epsilon: 0.01506324653745902\n",
      "Episode 937: Won\n",
      "Episode 937, Avg Value Loss: 3.342112504518949, Avg Policy Loss: 3.8724400446965146\n",
      "Episode 937, Reward: -31.226623525683216, Moving Avg Reward: -22.92671977504434, Replay Buffer Size: 33524\n",
      "Current Epsilon: 0.014987930304771725\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 937, Reward: -31.226623525683216, Moving Avg Reward: -22.92671977504434, Replay Buffer Size: 33524\n",
      "Current Epsilon: 0.014987930304771725\n",
      "Episode 938: Won\n",
      "Episode 938, Avg Value Loss: 3.4083059628804526, Avg Policy Loss: 3.6601303153567843\n",
      "Episode 938, Reward: -22.98434157059171, Moving Avg Reward: -22.480284058940374, Replay Buffer Size: 33533\n",
      "Current Epsilon: 0.014912990653247866\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 938, Reward: -22.98434157059171, Moving Avg Reward: -22.480284058940374, Replay Buffer Size: 33533\n",
      "Current Epsilon: 0.014912990653247866\n",
      "Episode 939: Won\n",
      "Episode 939, Avg Value Loss: 3.432024585455656, Avg Policy Loss: 3.8576066732406615\n",
      "Episode 939, Reward: -80.29103246583031, Moving Avg Reward: -22.97369813404063, Replay Buffer Size: 33613\n",
      "Current Epsilon: 0.014838425699981627\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 939, Reward: -80.29103246583031, Moving Avg Reward: -22.97369813404063, Replay Buffer Size: 33613\n",
      "Current Epsilon: 0.014838425699981627\n",
      "Episode 940: Lost\n",
      "Episode 940, Avg Value Loss: 3.3508577988697934, Avg Policy Loss: 3.8815884039952206\n",
      "Episode 940, Reward: 6.257309894259786, Moving Avg Reward: -22.643566775471733, Replay Buffer Size: 33626\n",
      "Current Epsilon: 0.01476423357148172\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 940, Reward: 6.257309894259786, Moving Avg Reward: -22.643566775471733, Replay Buffer Size: 33626\n",
      "Current Epsilon: 0.01476423357148172\n",
      "Episode 941: Won\n",
      "Episode 941, Avg Value Loss: 3.1949404776096344, Avg Policy Loss: 3.870985825856527\n",
      "Episode 941, Reward: -29.82556435023252, Moving Avg Reward: -22.660329541211354, Replay Buffer Size: 33638\n",
      "Current Epsilon: 0.014690412403624311\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 941, Reward: -29.82556435023252, Moving Avg Reward: -22.660329541211354, Replay Buffer Size: 33638\n",
      "Current Epsilon: 0.014690412403624311\n",
      "Episode 942: Won\n",
      "Episode 942, Avg Value Loss: 3.2580327838659286, Avg Policy Loss: 3.850450223684311\n",
      "Episode 942, Reward: -39.29661452431176, Moving Avg Reward: -22.785078208186903, Replay Buffer Size: 33718\n",
      "Current Epsilon: 0.01461696034160619\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 942, Reward: -39.29661452431176, Moving Avg Reward: -22.785078208186903, Replay Buffer Size: 33718\n",
      "Current Epsilon: 0.01461696034160619\n",
      "Episode 943: Won\n",
      "Episode 943, Avg Value Loss: 3.5467228651046754, Avg Policy Loss: 3.834689074754715\n",
      "Episode 943, Reward: -16.625775504135614, Moving Avg Reward: -22.579128464087965, Replay Buffer Size: 33798\n",
      "Current Epsilon: 0.014543875539898159\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 943, Reward: -16.625775504135614, Moving Avg Reward: -22.579128464087965, Replay Buffer Size: 33798\n",
      "Current Epsilon: 0.014543875539898159\n",
      "Episode 944: Draw\n",
      "Episode 944, Avg Value Loss: 3.2698879688978195, Avg Policy Loss: 3.87669657766819\n",
      "Episode 944, Reward: -4.30904975873516, Moving Avg Reward: -22.317455190346536, Replay Buffer Size: 33878\n",
      "Current Epsilon: 0.014471156162198668\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 944, Reward: -4.30904975873516, Moving Avg Reward: -22.317455190346536, Replay Buffer Size: 33878\n",
      "Current Epsilon: 0.014471156162198668\n",
      "Episode 945: Draw\n",
      "Episode 945, Avg Value Loss: 2.844330072402954, Avg Policy Loss: 3.9947377920150755\n",
      "Episode 945, Reward: -27.466587859102017, Moving Avg Reward: -22.32036775470623, Replay Buffer Size: 33888\n",
      "Current Epsilon: 0.014398800381387675\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 945, Reward: -27.466587859102017, Moving Avg Reward: -22.32036775470623, Replay Buffer Size: 33888\n",
      "Current Epsilon: 0.014398800381387675\n",
      "Episode 946: Lost\n",
      "Episode 946, Avg Value Loss: 4.05926420471885, Avg Policy Loss: 3.88642464984547\n",
      "Episode 946, Reward: -29.25993017534114, Moving Avg Reward: -22.799062503226345, Replay Buffer Size: 33899\n",
      "Current Epsilon: 0.014326806379480736\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 946, Reward: -29.25993017534114, Moving Avg Reward: -22.799062503226345, Replay Buffer Size: 33899\n",
      "Current Epsilon: 0.014326806379480736\n",
      "Episode 947: Lost\n",
      "Episode 947, Avg Value Loss: 3.8103798826535544, Avg Policy Loss: 3.8376378615697226\n",
      "Episode 947, Reward: -24.582722137432874, Moving Avg Reward: -22.99612502722463, Replay Buffer Size: 33911\n",
      "Current Epsilon: 0.014255172347583332\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 947, Reward: -24.582722137432874, Moving Avg Reward: -22.99612502722463, Replay Buffer Size: 33911\n",
      "Current Epsilon: 0.014255172347583332\n",
      "Episode 948: Lost\n",
      "Episode 948, Avg Value Loss: 3.497446392444854, Avg Policy Loss: 3.8925729355913528\n",
      "Episode 948, Reward: -47.865384451311186, Moving Avg Reward: -23.166824520299684, Replay Buffer Size: 33958\n",
      "Current Epsilon: 0.014183896485845416\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 948, Reward: -47.865384451311186, Moving Avg Reward: -23.166824520299684, Replay Buffer Size: 33958\n",
      "Current Epsilon: 0.014183896485845416\n",
      "Episode 949: Lost\n",
      "Episode 949, Avg Value Loss: 3.2228789594438343, Avg Policy Loss: 3.7738423215018377\n",
      "Episode 949, Reward: 15.774867353883536, Moving Avg Reward: -22.60443515835146, Replay Buffer Size: 33976\n",
      "Current Epsilon: 0.014112977003416188\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 949, Reward: 15.774867353883536, Moving Avg Reward: -22.60443515835146, Replay Buffer Size: 33976\n",
      "Current Epsilon: 0.014112977003416188\n",
      "Episode 950: Won\n",
      "Episode 950, Avg Value Loss: 2.947917550802231, Avg Policy Loss: 3.8695040345191956\n",
      "Episode 950, Reward: -36.506548595637135, Moving Avg Reward: -22.25383588450762, Replay Buffer Size: 33988\n",
      "Current Epsilon: 0.014042412118399107\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 950, Reward: -36.506548595637135, Moving Avg Reward: -22.25383588450762, Replay Buffer Size: 33988\n",
      "Current Epsilon: 0.014042412118399107\n",
      "Episode 951: Won\n",
      "Episode 951, Avg Value Loss: 4.016664071516558, Avg Policy Loss: 3.7745222828604956\n",
      "Episode 951, Reward: 9.89, Moving Avg Reward: -22.066034567441402, Replay Buffer Size: 33999\n",
      "Current Epsilon: 0.013972200057807112\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 951, Reward: 9.89, Moving Avg Reward: -22.066034567441402, Replay Buffer Size: 33999\n",
      "Current Epsilon: 0.013972200057807112\n",
      "Episode 952: Won\n",
      "Episode 952, Avg Value Loss: 3.4696350529789926, Avg Policy Loss: 3.8283019185066225\n",
      "Episode 952, Reward: -10.450883188474373, Moving Avg Reward: -22.33462709037742, Replay Buffer Size: 34079\n",
      "Current Epsilon: 0.013902339057518077\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 952, Reward: -10.450883188474373, Moving Avg Reward: -22.33462709037742, Replay Buffer Size: 34079\n",
      "Current Epsilon: 0.013902339057518077\n",
      "Episode 953: Won\n",
      "Episode 953, Avg Value Loss: 2.888414726807521, Avg Policy Loss: 3.823875207167405\n",
      "Episode 953, Reward: 18.388520809125293, Moving Avg Reward: -22.126031105692764, Replay Buffer Size: 34105\n",
      "Current Epsilon: 0.013832827362230486\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 953, Reward: 18.388520809125293, Moving Avg Reward: -22.126031105692764, Replay Buffer Size: 34105\n",
      "Current Epsilon: 0.013832827362230486\n",
      "Episode 954: Won\n",
      "Episode 954, Avg Value Loss: 3.1024858401371884, Avg Policy Loss: 3.8323536836183987\n",
      "Episode 954, Reward: -34.98162780021158, Moving Avg Reward: -22.154823727797556, Replay Buffer Size: 34118\n",
      "Current Epsilon: 0.013763663225419333\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 954, Reward: -34.98162780021158, Moving Avg Reward: -22.154823727797556, Replay Buffer Size: 34118\n",
      "Current Epsilon: 0.013763663225419333\n",
      "Episode 955: Won\n",
      "Episode 955, Avg Value Loss: 3.295995730620164, Avg Policy Loss: 3.906486602929922\n",
      "Episode 955, Reward: -37.45962434722591, Moving Avg Reward: -22.309017486292856, Replay Buffer Size: 34131\n",
      "Current Epsilon: 0.013694844909292236\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 955, Reward: -37.45962434722591, Moving Avg Reward: -22.309017486292856, Replay Buffer Size: 34131\n",
      "Current Epsilon: 0.013694844909292236\n",
      "Episode 956: Won\n",
      "Episode 956, Avg Value Loss: 3.335253393650055, Avg Policy Loss: 3.71421799659729\n",
      "Episode 956, Reward: -26.788726296896446, Moving Avg Reward: -22.045614273202986, Replay Buffer Size: 34141\n",
      "Current Epsilon: 0.013626370684745774\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 956, Reward: -26.788726296896446, Moving Avg Reward: -22.045614273202986, Replay Buffer Size: 34141\n",
      "Current Epsilon: 0.013626370684745774\n",
      "Episode 957: Won\n",
      "Episode 957, Avg Value Loss: 3.317964185367931, Avg Policy Loss: 4.00218636339361\n",
      "Episode 957, Reward: -26.60635789642462, Moving Avg Reward: -22.0518730675598, Replay Buffer Size: 34152\n",
      "Current Epsilon: 0.013558238831322046\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 957, Reward: -26.60635789642462, Moving Avg Reward: -22.0518730675598, Replay Buffer Size: 34152\n",
      "Current Epsilon: 0.013558238831322046\n",
      "Episode 958: Won\n",
      "Episode 958, Avg Value Loss: 3.2210874795913695, Avg Policy Loss: 3.851987205852162\n",
      "Episode 958, Reward: -0.15912296669880277, Moving Avg Reward: -21.790540565455707, Replay Buffer Size: 34207\n",
      "Current Epsilon: 0.013490447637165436\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 958, Reward: -0.15912296669880277, Moving Avg Reward: -21.790540565455707, Replay Buffer Size: 34207\n",
      "Current Epsilon: 0.013490447637165436\n",
      "Episode 959: Won\n",
      "Episode 959, Avg Value Loss: 4.202938477198283, Avg Policy Loss: 3.809641573164198\n",
      "Episode 959, Reward: -22.611933298473623, Moving Avg Reward: -21.76121505682764, Replay Buffer Size: 34216\n",
      "Current Epsilon: 0.013422995398979608\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 959, Reward: -22.611933298473623, Moving Avg Reward: -21.76121505682764, Replay Buffer Size: 34216\n",
      "Current Epsilon: 0.013422995398979608\n",
      "Episode 960: Won\n",
      "Episode 960, Avg Value Loss: 2.9238156997240505, Avg Policy Loss: 3.8898774843949537\n",
      "Episode 960, Reward: -27.628246747371605, Moving Avg Reward: -21.714582103394395, Replay Buffer Size: 34229\n",
      "Current Epsilon: 0.01335588042198471\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 960, Reward: -27.628246747371605, Moving Avg Reward: -21.714582103394395, Replay Buffer Size: 34229\n",
      "Current Epsilon: 0.01335588042198471\n",
      "Episode 961: Won\n",
      "Episode 961, Avg Value Loss: 3.8451522418430875, Avg Policy Loss: 3.8031040940965926\n",
      "Episode 961, Reward: -23.018804571725134, Moving Avg Reward: -21.897357953385807, Replay Buffer Size: 34236\n",
      "Current Epsilon: 0.013289101019874787\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 961, Reward: -23.018804571725134, Moving Avg Reward: -21.897357953385807, Replay Buffer Size: 34236\n",
      "Current Epsilon: 0.013289101019874787\n",
      "Episode 962: Won\n",
      "Episode 962, Avg Value Loss: 3.0774042285405674, Avg Policy Loss: 3.7738437285790076\n",
      "Episode 962, Reward: -34.336645916911, Moving Avg Reward: -21.937244053375856, Replay Buffer Size: 34249\n",
      "Current Epsilon: 0.013222655514775413\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 962, Reward: -34.336645916911, Moving Avg Reward: -21.937244053375856, Replay Buffer Size: 34249\n",
      "Current Epsilon: 0.013222655514775413\n",
      "Episode 963: Won\n",
      "Episode 963, Avg Value Loss: 3.783410974911281, Avg Policy Loss: 3.8812209027154103\n",
      "Episode 963, Reward: -33.84506173119909, Moving Avg Reward: -21.967680191971336, Replay Buffer Size: 34263\n",
      "Current Epsilon: 0.013156542237201536\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 963, Reward: -33.84506173119909, Moving Avg Reward: -21.967680191971336, Replay Buffer Size: 34263\n",
      "Current Epsilon: 0.013156542237201536\n",
      "Episode 964: Won\n",
      "Episode 964, Avg Value Loss: 3.752567410469055, Avg Policy Loss: 3.7452225468375464\n",
      "Episode 964, Reward: -25.18870527799512, Moving Avg Reward: -21.712631165694003, Replay Buffer Size: 34274\n",
      "Current Epsilon: 0.013090759526015528\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 964, Reward: -25.18870527799512, Moving Avg Reward: -21.712631165694003, Replay Buffer Size: 34274\n",
      "Current Epsilon: 0.013090759526015528\n",
      "Episode 965: Won\n",
      "Episode 965, Avg Value Loss: 3.729059590233697, Avg Policy Loss: 3.8463530275556774\n",
      "Episode 965, Reward: -24.904669671321344, Moving Avg Reward: -21.697192398948488, Replay Buffer Size: 34283\n",
      "Current Epsilon: 0.01302530572838545\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 965, Reward: -24.904669671321344, Moving Avg Reward: -21.697192398948488, Replay Buffer Size: 34283\n",
      "Current Epsilon: 0.01302530572838545\n",
      "Episode 966: Won\n",
      "Episode 966, Avg Value Loss: 4.0152586698532104, Avg Policy Loss: 3.5758508682250976\n",
      "Episode 966, Reward: -28.97269426006875, Moving Avg Reward: -21.70399186154241, Replay Buffer Size: 34293\n",
      "Current Epsilon: 0.012960179199743523\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 966, Reward: -28.97269426006875, Moving Avg Reward: -21.70399186154241, Replay Buffer Size: 34293\n",
      "Current Epsilon: 0.012960179199743523\n",
      "Episode 967: Won\n",
      "Episode 967, Avg Value Loss: 3.276143117384477, Avg Policy Loss: 3.8735621625726875\n",
      "Episode 967, Reward: -27.674355236483137, Moving Avg Reward: -21.790233589990056, Replay Buffer Size: 34304\n",
      "Current Epsilon: 0.012895378303744804\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 967, Reward: -27.674355236483137, Moving Avg Reward: -21.790233589990056, Replay Buffer Size: 34304\n",
      "Current Epsilon: 0.012895378303744804\n",
      "Episode 968: Won\n",
      "Episode 968, Avg Value Loss: 2.944372129440308, Avg Policy Loss: 3.919273781776428\n",
      "Episode 968, Reward: -28.733292319797414, Moving Avg Reward: -21.792131085790125, Replay Buffer Size: 34314\n",
      "Current Epsilon: 0.01283090141222608\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 968, Reward: -28.733292319797414, Moving Avg Reward: -21.792131085790125, Replay Buffer Size: 34314\n",
      "Current Epsilon: 0.01283090141222608\n",
      "Episode 969: Won\n",
      "Episode 969, Avg Value Loss: 3.434569390118122, Avg Policy Loss: 3.862108075618744\n",
      "Episode 969, Reward: -36.00581339284738, Moving Avg Reward: -21.87768859111041, Replay Buffer Size: 34394\n",
      "Current Epsilon: 0.012766746905164949\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 969, Reward: -36.00581339284738, Moving Avg Reward: -21.87768859111041, Replay Buffer Size: 34394\n",
      "Current Epsilon: 0.012766746905164949\n",
      "Episode 970: Lost\n",
      "Episode 970, Avg Value Loss: 3.4743695987595453, Avg Policy Loss: 3.9442420403162637\n",
      "Episode 970, Reward: 17.43702034378642, Moving Avg Reward: -21.876740064070397, Replay Buffer Size: 34430\n",
      "Current Epsilon: 0.012702913170639124\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 970, Reward: 17.43702034378642, Moving Avg Reward: -21.876740064070397, Replay Buffer Size: 34430\n",
      "Current Epsilon: 0.012702913170639124\n",
      "Episode 971: Won\n",
      "Episode 971, Avg Value Loss: 3.8008820533752443, Avg Policy Loss: 3.8699002027511598\n",
      "Episode 971, Reward: -27.441056751505492, Moving Avg Reward: -21.74831806685342, Replay Buffer Size: 34440\n",
      "Current Epsilon: 0.012639398604785928\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 971, Reward: -27.441056751505492, Moving Avg Reward: -21.74831806685342, Replay Buffer Size: 34440\n",
      "Current Epsilon: 0.012639398604785928\n",
      "Episode 972: Won\n",
      "Episode 972, Avg Value Loss: 3.594099409878254, Avg Policy Loss: 3.8715888381004335\n",
      "Episode 972, Reward: -13.415812009682275, Moving Avg Reward: -21.60493483822924, Replay Buffer Size: 34520\n",
      "Current Epsilon: 0.012576201611761997\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 972, Reward: -13.415812009682275, Moving Avg Reward: -21.60493483822924, Replay Buffer Size: 34520\n",
      "Current Epsilon: 0.012576201611761997\n",
      "Episode 973: Won\n",
      "Episode 973, Avg Value Loss: 3.0530439890347996, Avg Policy Loss: 4.203256680415227\n",
      "Episode 973, Reward: -28.973091677570096, Moving Avg Reward: -21.643160761532048, Replay Buffer Size: 34533\n",
      "Current Epsilon: 0.012513320603703188\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 973, Reward: -28.973091677570096, Moving Avg Reward: -21.643160761532048, Replay Buffer Size: 34533\n",
      "Current Epsilon: 0.012513320603703188\n",
      "Episode 974: Won\n",
      "Episode 974, Avg Value Loss: 3.438020520740085, Avg Policy Loss: 3.7657679186926947\n",
      "Episode 974, Reward: 19.17938332575973, Moving Avg Reward: -21.185264805820385, Replay Buffer Size: 34560\n",
      "Current Epsilon: 0.012450754000684672\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 974, Reward: 19.17938332575973, Moving Avg Reward: -21.185264805820385, Replay Buffer Size: 34560\n",
      "Current Epsilon: 0.012450754000684672\n",
      "Episode 975: Won\n",
      "Episode 975, Avg Value Loss: 3.059941963715987, Avg Policy Loss: 3.9549309340390293\n",
      "Episode 975, Reward: -25.464719727298885, Moving Avg Reward: -21.199337784615285, Replay Buffer Size: 34571\n",
      "Current Epsilon: 0.012388500230681249\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 975, Reward: -25.464719727298885, Moving Avg Reward: -21.199337784615285, Replay Buffer Size: 34571\n",
      "Current Epsilon: 0.012388500230681249\n",
      "Episode 976: Won\n",
      "Episode 976, Avg Value Loss: 3.456940511862437, Avg Policy Loss: 3.755344351132711\n",
      "Episode 976, Reward: -31.595731511694247, Moving Avg Reward: -21.19005660378092, Replay Buffer Size: 34583\n",
      "Current Epsilon: 0.012326557729527843\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 976, Reward: -31.595731511694247, Moving Avg Reward: -21.19005660378092, Replay Buffer Size: 34583\n",
      "Current Epsilon: 0.012326557729527843\n",
      "Episode 977: Won\n",
      "Episode 977, Avg Value Loss: 3.5124701499938964, Avg Policy Loss: 3.709855079650879\n",
      "Episode 977, Reward: -25.94434426051687, Moving Avg Reward: -21.18640934216069, Replay Buffer Size: 34593\n",
      "Current Epsilon: 0.012264924940880204\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 977, Reward: -25.94434426051687, Moving Avg Reward: -21.18640934216069, Replay Buffer Size: 34593\n",
      "Current Epsilon: 0.012264924940880204\n",
      "Episode 978: Won\n",
      "Episode 978, Avg Value Loss: 3.131120777130127, Avg Policy Loss: 3.763598680496216\n",
      "Episode 978, Reward: -25.11074339399746, Moving Avg Reward: -21.36508041628885, Replay Buffer Size: 34603\n",
      "Current Epsilon: 0.012203600316175803\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 978, Reward: -25.11074339399746, Moving Avg Reward: -21.36508041628885, Replay Buffer Size: 34603\n",
      "Current Epsilon: 0.012203600316175803\n",
      "Episode 979: Won\n",
      "Episode 979, Avg Value Loss: 3.3541409307056003, Avg Policy Loss: 3.9295903046925864\n",
      "Episode 979, Reward: -28.464487109609237, Moving Avg Reward: -21.748425287384944, Replay Buffer Size: 34612\n",
      "Current Epsilon: 0.012142582314594924\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 979, Reward: -28.464487109609237, Moving Avg Reward: -21.748425287384944, Replay Buffer Size: 34612\n",
      "Current Epsilon: 0.012142582314594924\n",
      "Episode 980: Won\n",
      "Episode 980, Avg Value Loss: 3.203836056921217, Avg Policy Loss: 3.945770025253296\n",
      "Episode 980, Reward: -26.392766690608767, Moving Avg Reward: -21.78210886491463, Replay Buffer Size: 34621\n",
      "Current Epsilon: 0.01208186940302195\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 980, Reward: -26.392766690608767, Moving Avg Reward: -21.78210886491463, Replay Buffer Size: 34621\n",
      "Current Epsilon: 0.01208186940302195\n",
      "Episode 981: Won\n",
      "Episode 981, Avg Value Loss: 2.9490538477897643, Avg Policy Loss: 4.033954977989197\n",
      "Episode 981, Reward: -27.413892583726952, Moving Avg Reward: -21.790102430863282, Replay Buffer Size: 34631\n",
      "Current Epsilon: 0.01202146005600684\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 981, Reward: -27.413892583726952, Moving Avg Reward: -21.790102430863282, Replay Buffer Size: 34631\n",
      "Current Epsilon: 0.01202146005600684\n",
      "Episode 982: Won\n",
      "Episode 982, Avg Value Loss: 3.880447283387184, Avg Policy Loss: 3.737894296646118\n",
      "Episode 982, Reward: -25.809600506179052, Moving Avg Reward: -22.048110374460165, Replay Buffer Size: 34639\n",
      "Current Epsilon: 0.011961352755726806\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 982, Reward: -25.809600506179052, Moving Avg Reward: -22.048110374460165, Replay Buffer Size: 34639\n",
      "Current Epsilon: 0.011961352755726806\n",
      "Episode 983: Won\n",
      "Episode 983, Avg Value Loss: 3.5334277749061584, Avg Policy Loss: 3.8061093986034393\n",
      "Episode 983, Reward: -26.163946339849865, Moving Avg Reward: -22.00586089764031, Replay Buffer Size: 34647\n",
      "Current Epsilon: 0.01190154599194817\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 983, Reward: -26.163946339849865, Moving Avg Reward: -22.00586089764031, Replay Buffer Size: 34647\n",
      "Current Epsilon: 0.01190154599194817\n",
      "Episode 984: Won\n",
      "Episode 984, Avg Value Loss: 3.2149560848871865, Avg Policy Loss: 3.746881617440118\n",
      "Episode 984, Reward: -27.544432974257745, Moving Avg Reward: -22.317152390976716, Replay Buffer Size: 34656\n",
      "Current Epsilon: 0.01184203826198843\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 984, Reward: -27.544432974257745, Moving Avg Reward: -22.317152390976716, Replay Buffer Size: 34656\n",
      "Current Epsilon: 0.01184203826198843\n",
      "Episode 985: Won\n",
      "Episode 985, Avg Value Loss: 3.5123016595840455, Avg Policy Loss: 3.911045753955841\n",
      "Episode 985, Reward: -32.748922282049996, Moving Avg Reward: -22.563937307936477, Replay Buffer Size: 34736\n",
      "Current Epsilon: 0.011782828070678488\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 985, Reward: -32.748922282049996, Moving Avg Reward: -22.563937307936477, Replay Buffer Size: 34736\n",
      "Current Epsilon: 0.011782828070678488\n",
      "Episode 986: Lost\n",
      "Episode 986, Avg Value Loss: 3.4494348114187066, Avg Policy Loss: 4.068462068384344\n",
      "Episode 986, Reward: -35.42521513093286, Moving Avg Reward: -23.016689459245807, Replay Buffer Size: 34747\n",
      "Current Epsilon: 0.011723913930325095\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 986, Reward: -35.42521513093286, Moving Avg Reward: -23.016689459245807, Replay Buffer Size: 34747\n",
      "Current Epsilon: 0.011723913930325095\n",
      "Episode 987: Lost\n",
      "Episode 987, Avg Value Loss: 3.4453519185384116, Avg Policy Loss: 3.75265900293986\n",
      "Episode 987, Reward: -27.56212153783048, Moving Avg Reward: -23.231774343598076, Replay Buffer Size: 34756\n",
      "Current Epsilon: 0.01166529436067347\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 987, Reward: -27.56212153783048, Moving Avg Reward: -23.231774343598076, Replay Buffer Size: 34756\n",
      "Current Epsilon: 0.01166529436067347\n",
      "Episode 988: Lost\n",
      "Episode 988, Avg Value Loss: 3.1271692836606824, Avg Policy Loss: 3.889318253542926\n",
      "Episode 988, Reward: -26.984196482882425, Moving Avg Reward: -23.21565356111214, Replay Buffer Size: 34793\n",
      "Current Epsilon: 0.011606967888870102\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 988, Reward: -26.984196482882425, Moving Avg Reward: -23.21565356111214, Replay Buffer Size: 34793\n",
      "Current Epsilon: 0.011606967888870102\n",
      "Episode 989: Lost\n",
      "Episode 989, Avg Value Loss: 3.210097074508667, Avg Policy Loss: 3.9077249490297756\n",
      "Episode 989, Reward: -28.156044899415086, Moving Avg Reward: -23.68110970856665, Replay Buffer Size: 34806\n",
      "Current Epsilon: 0.01154893304942575\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 989, Reward: -28.156044899415086, Moving Avg Reward: -23.68110970856665, Replay Buffer Size: 34806\n",
      "Current Epsilon: 0.01154893304942575\n",
      "Episode 990: Lost\n",
      "Episode 990, Avg Value Loss: 2.8065993189811707, Avg Policy Loss: 4.023044526576996\n",
      "Episode 990, Reward: -30.89172421704367, Moving Avg Reward: -23.745663440425265, Replay Buffer Size: 34818\n",
      "Current Epsilon: 0.011491188384178622\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 990, Reward: -30.89172421704367, Moving Avg Reward: -23.745663440425265, Replay Buffer Size: 34818\n",
      "Current Epsilon: 0.011491188384178622\n",
      "Episode 991: Lost\n",
      "Episode 991, Avg Value Loss: 2.9477927833795547, Avg Policy Loss: 3.932923823595047\n",
      "Episode 991, Reward: -24.393104639926534, Moving Avg Reward: -24.189986269316172, Replay Buffer Size: 34826\n",
      "Current Epsilon: 0.011433732442257729\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 991, Reward: -24.393104639926534, Moving Avg Reward: -24.189986269316172, Replay Buffer Size: 34826\n",
      "Current Epsilon: 0.011433732442257729\n",
      "Episode 992: Lost\n",
      "Episode 992, Avg Value Loss: 3.3868472248315813, Avg Policy Loss: 3.911556550860405\n",
      "Episode 992, Reward: -63.847729750656015, Moving Avg Reward: -24.54662585457406, Replay Buffer Size: 34906\n",
      "Current Epsilon: 0.01137656378004644\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 992, Reward: -63.847729750656015, Moving Avg Reward: -24.54662585457406, Replay Buffer Size: 34906\n",
      "Current Epsilon: 0.01137656378004644\n",
      "Episode 993: Lost\n",
      "Episode 993, Avg Value Loss: 3.305337797040525, Avg Policy Loss: 4.045516325079876\n",
      "Episode 993, Reward: -28.398127794311215, Moving Avg Reward: -24.500183500633923, Replay Buffer Size: 34929\n",
      "Current Epsilon: 0.011319680961146208\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 993, Reward: -28.398127794311215, Moving Avg Reward: -24.500183500633923, Replay Buffer Size: 34929\n",
      "Current Epsilon: 0.011319680961146208\n",
      "Episode 994: Lost\n",
      "Episode 994, Avg Value Loss: 3.8671131001578436, Avg Policy Loss: 3.9435176849365234\n",
      "Episode 994, Reward: -30.271827580796597, Moving Avg Reward: -24.440695653352794, Replay Buffer Size: 34938\n",
      "Current Epsilon: 0.011263082556340478\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 994, Reward: -30.271827580796597, Moving Avg Reward: -24.440695653352794, Replay Buffer Size: 34938\n",
      "Current Epsilon: 0.011263082556340478\n",
      "Episode 995: Lost\n",
      "Episode 995, Avg Value Loss: 3.8376141527424688, Avg Policy Loss: 4.030286281005196\n",
      "Episode 995, Reward: 15.724571334387537, Moving Avg Reward: -24.458762068514517, Replay Buffer Size: 34961\n",
      "Current Epsilon: 0.011206767143558775\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 995, Reward: 15.724571334387537, Moving Avg Reward: -24.458762068514517, Replay Buffer Size: 34961\n",
      "Current Epsilon: 0.011206767143558775\n",
      "Episode 996: Won\n",
      "Episode 996, Avg Value Loss: 3.6026987969875335, Avg Policy Loss: 3.8191596060991286\n",
      "Episode 996, Reward: -4.20404220492744, Moving Avg Reward: -24.250633742912783, Replay Buffer Size: 35041\n",
      "Current Epsilon: 0.011150733307840981\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 996, Reward: -4.20404220492744, Moving Avg Reward: -24.250633742912783, Replay Buffer Size: 35041\n",
      "Current Epsilon: 0.011150733307840981\n",
      "Episode 997: Won\n",
      "Episode 997, Avg Value Loss: 2.8147765764823327, Avg Policy Loss: 4.001463229839619\n",
      "Episode 997, Reward: -28.71678707948506, Moving Avg Reward: -24.265891697357507, Replay Buffer Size: 35054\n",
      "Current Epsilon: 0.011094979641301777\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 997, Reward: -28.71678707948506, Moving Avg Reward: -24.265891697357507, Replay Buffer Size: 35054\n",
      "Current Epsilon: 0.011094979641301777\n",
      "Episode 998: Won\n",
      "Episode 998, Avg Value Loss: 3.41657073630227, Avg Policy Loss: 3.8301686313417225\n",
      "Episode 998, Reward: 20.236348341392485, Moving Avg Reward: -24.16668986414265, Replay Buffer Size: 35072\n",
      "Current Epsilon: 0.011039504743095268\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 998, Reward: 20.236348341392485, Moving Avg Reward: -24.16668986414265, Replay Buffer Size: 35072\n",
      "Current Epsilon: 0.011039504743095268\n",
      "Episode 999: Won\n",
      "Episode 999, Avg Value Loss: 3.3131318166851997, Avg Policy Loss: 3.907733616232872\n",
      "Episode 999, Reward: -2.0884005282815794, Moving Avg Reward: -23.917332969291532, Replay Buffer Size: 35152\n",
      "Current Epsilon: 0.01098430721937979\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 999, Reward: -2.0884005282815794, Moving Avg Reward: -23.917332969291532, Replay Buffer Size: 35152\n",
      "Current Epsilon: 0.01098430721937979\n",
      "Episode 1000: Won\n",
      "Episode 1000, Avg Value Loss: 3.8394489685694375, Avg Policy Loss: 3.7443453470865884\n",
      "Episode 1000, Reward: -32.90569218351912, Moving Avg Reward: -23.950286284962857, Replay Buffer Size: 35164\n",
      "Current Epsilon: 0.010929385683282892\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1000, Reward: -32.90569218351912, Moving Avg Reward: -23.950286284962857, Replay Buffer Size: 35164\n",
      "Current Epsilon: 0.010929385683282892\n",
      "Episode 1001: Won\n",
      "Episode 1001, Avg Value Loss: 3.2916268587112425, Avg Policy Loss: 4.060934948921203\n",
      "Episode 1001, Reward: -26.521020216590088, Moving Avg Reward: -23.975213772753918, Replay Buffer Size: 35174\n",
      "Current Epsilon: 0.010874738754866477\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1001, Reward: -26.521020216590088, Moving Avg Reward: -23.975213772753918, Replay Buffer Size: 35174\n",
      "Current Epsilon: 0.010874738754866477\n",
      "Episode 1002: Won\n",
      "Episode 1002, Avg Value Loss: 3.1488323152065276, Avg Policy Loss: 3.8972515195608137\n",
      "Episode 1002, Reward: 6.76169326655286, Moving Avg Reward: -23.600525066917836, Replay Buffer Size: 35254\n",
      "Current Epsilon: 0.010820365061092144\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1002, Reward: 6.76169326655286, Moving Avg Reward: -23.600525066917836, Replay Buffer Size: 35254\n",
      "Current Epsilon: 0.010820365061092144\n",
      "Episode 1003: Lost\n",
      "Episode 1003, Avg Value Loss: 4.019581061143142, Avg Policy Loss: 3.807086632801936\n",
      "Episode 1003, Reward: -29.99676600888872, Moving Avg Reward: -23.249321048555572, Replay Buffer Size: 35267\n",
      "Current Epsilon: 0.010766263235786683\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1003, Reward: -29.99676600888872, Moving Avg Reward: -23.249321048555572, Replay Buffer Size: 35267\n",
      "Current Epsilon: 0.010766263235786683\n",
      "Episode 1004: Lost\n",
      "Episode 1004, Avg Value Loss: 3.1584227204322817, Avg Policy Loss: 4.0009866714477536\n",
      "Episode 1004, Reward: -28.38630883687496, Moving Avg Reward: -22.97388140587207, Replay Buffer Size: 35277\n",
      "Current Epsilon: 0.01071243191960775\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1004, Reward: -28.38630883687496, Moving Avg Reward: -22.97388140587207, Replay Buffer Size: 35277\n",
      "Current Epsilon: 0.01071243191960775\n",
      "Episode 1005: Lost\n",
      "Episode 1005, Avg Value Loss: 3.435560005903244, Avg Policy Loss: 3.8818186074495316\n",
      "Episode 1005, Reward: -23.37305028756217, Moving Avg Reward: -22.949996727035263, Replay Buffer Size: 35357\n",
      "Current Epsilon: 0.010658869760009713\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1005, Reward: -23.37305028756217, Moving Avg Reward: -22.949996727035263, Replay Buffer Size: 35357\n",
      "Current Epsilon: 0.010658869760009713\n",
      "Episode 1006: Lost\n",
      "Episode 1006, Avg Value Loss: 3.405434421130589, Avg Policy Loss: 3.7013937064579556\n",
      "Episode 1006, Reward: -21.47785918447449, Moving Avg Reward: -23.065670028683343, Replay Buffer Size: 35364\n",
      "Current Epsilon: 0.010605575411209664\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1006, Reward: -21.47785918447449, Moving Avg Reward: -23.065670028683343, Replay Buffer Size: 35364\n",
      "Current Epsilon: 0.010605575411209664\n",
      "Episode 1007: Lost\n",
      "Episode 1007, Avg Value Loss: 2.9391983449459076, Avg Policy Loss: 3.9632365703582764\n",
      "Episode 1007, Reward: -24.754149707680654, Moving Avg Reward: -23.057540824569568, Replay Buffer Size: 35376\n",
      "Current Epsilon: 0.010552547534153616\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1007, Reward: -24.754149707680654, Moving Avg Reward: -23.057540824569568, Replay Buffer Size: 35376\n",
      "Current Epsilon: 0.010552547534153616\n",
      "Episode 1008: Lost\n",
      "Episode 1008, Avg Value Loss: 3.218383742313759, Avg Policy Loss: 3.8974834470187916\n",
      "Episode 1008, Reward: -35.239439681064546, Moving Avg Reward: -22.71866000129177, Replay Buffer Size: 35427\n",
      "Current Epsilon: 0.010499784796482848\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1008, Reward: -35.239439681064546, Moving Avg Reward: -22.71866000129177, Replay Buffer Size: 35427\n",
      "Current Epsilon: 0.010499784796482848\n",
      "Episode 1009: Won\n",
      "Episode 1009, Avg Value Loss: 2.9728413687811956, Avg Policy Loss: 3.9944008456336126\n",
      "Episode 1009, Reward: -28.928348221599762, Moving Avg Reward: -22.79805609131336, Replay Buffer Size: 35436\n",
      "Current Epsilon: 0.010447285872500434\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1009, Reward: -28.928348221599762, Moving Avg Reward: -22.79805609131336, Replay Buffer Size: 35436\n",
      "Current Epsilon: 0.010447285872500434\n",
      "Episode 1010: Won\n",
      "Episode 1010, Avg Value Loss: 3.338650940358639, Avg Policy Loss: 3.9621704310178756\n",
      "Episode 1010, Reward: -54.38698898161153, Moving Avg Reward: -23.00552994026219, Replay Buffer Size: 35516\n",
      "Current Epsilon: 0.01039504944313793\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1010, Reward: -54.38698898161153, Moving Avg Reward: -23.00552994026219, Replay Buffer Size: 35516\n",
      "Current Epsilon: 0.01039504944313793\n",
      "Episode 1011: Won\n",
      "Episode 1011, Avg Value Loss: 3.260069891810417, Avg Policy Loss: 3.924980717897415\n",
      "Episode 1011, Reward: -54.38166534944037, Moving Avg Reward: -23.212272888966286, Replay Buffer Size: 35596\n",
      "Current Epsilon: 0.010343074195922241\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1011, Reward: -54.38166534944037, Moving Avg Reward: -23.212272888966286, Replay Buffer Size: 35596\n",
      "Current Epsilon: 0.010343074195922241\n",
      "Episode 1012: Draw\n",
      "Episode 1012, Avg Value Loss: 3.789046843846639, Avg Policy Loss: 3.79681924978892\n",
      "Episode 1012, Reward: -33.29996659062822, Moving Avg Reward: -23.580955578921944, Replay Buffer Size: 35608\n",
      "Current Epsilon: 0.01029135882494263\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1012, Reward: -33.29996659062822, Moving Avg Reward: -23.580955578921944, Replay Buffer Size: 35608\n",
      "Current Epsilon: 0.01029135882494263\n",
      "Episode 1013: Lost\n",
      "Episode 1013, Avg Value Loss: 3.118819306294123, Avg Policy Loss: 4.028025448322296\n",
      "Episode 1013, Reward: -29.15940180908661, Moving Avg Reward: -23.54448106238591, Replay Buffer Size: 35620\n",
      "Current Epsilon: 0.010239902030817916\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 1013, Reward: -29.15940180908661, Moving Avg Reward: -23.54448106238591, Replay Buffer Size: 35620\n",
      "Current Epsilon: 0.010239902030817916\n",
      "Episode 1014: Lost\n",
      "Episode 1014, Avg Value Loss: 3.1609608978033066, Avg Policy Loss: 3.4475890696048737\n",
      "Episode 1014, Reward: -25.00855710623176, Moving Avg Reward: -23.55383363717215, Replay Buffer Size: 35628\n",
      "Current Epsilon: 0.010188702520663827\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1014, Reward: -25.00855710623176, Moving Avg Reward: -23.55383363717215, Replay Buffer Size: 35628\n",
      "Current Epsilon: 0.010188702520663827\n",
      "Episode 1015: Lost\n",
      "Episode 1015, Avg Value Loss: 3.350359232723713, Avg Policy Loss: 3.939729982614517\n",
      "Episode 1015, Reward: -0.8000000000000005, Moving Avg Reward: -23.724259725023572, Replay Buffer Size: 35708\n",
      "Current Epsilon: 0.010137759008060509\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1015, Reward: -0.8000000000000005, Moving Avg Reward: -23.724259725023572, Replay Buffer Size: 35708\n",
      "Current Epsilon: 0.010137759008060509\n",
      "Episode 1016: Lost\n",
      "Episode 1016, Avg Value Loss: 3.4497950159013273, Avg Policy Loss: 3.949441799521446\n",
      "Episode 1016, Reward: -9.45285350181645, Moving Avg Reward: -23.727892396585688, Replay Buffer Size: 35788\n",
      "Current Epsilon: 0.010087070213020206\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1016, Reward: -9.45285350181645, Moving Avg Reward: -23.727892396585688, Replay Buffer Size: 35788\n",
      "Current Epsilon: 0.010087070213020206\n",
      "Episode 1017: Draw\n",
      "Episode 1017, Avg Value Loss: 3.2102997850727393, Avg Policy Loss: 4.065650198910688\n",
      "Episode 1017, Reward: -29.32387767370078, Moving Avg Reward: -24.269831173322697, Replay Buffer Size: 35825\n",
      "Current Epsilon: 0.010036634861955105\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1017, Reward: -29.32387767370078, Moving Avg Reward: -24.269831173322697, Replay Buffer Size: 35825\n",
      "Current Epsilon: 0.010036634861955105\n",
      "Episode 1018: Lost\n",
      "Episode 1018, Avg Value Loss: 3.5345812681175413, Avg Policy Loss: 3.9928521031425115\n",
      "Episode 1018, Reward: 18.43797676651571, Moving Avg Reward: -23.764954250944033, Replay Buffer Size: 35867\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1018, Reward: 18.43797676651571, Moving Avg Reward: -23.764954250944033, Replay Buffer Size: 35867\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1019: Won\n",
      "Episode 1019, Avg Value Loss: 3.1211580336093903, Avg Policy Loss: 3.7672908902168274\n",
      "Episode 1019, Reward: -27.391844538224714, Moving Avg Reward: -24.00657131579356, Replay Buffer Size: 35875\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1019, Reward: -27.391844538224714, Moving Avg Reward: -24.00657131579356, Replay Buffer Size: 35875\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1020: Won\n",
      "Episode 1020, Avg Value Loss: 3.9549141270773753, Avg Policy Loss: 3.9233194419315884\n",
      "Episode 1020, Reward: -22.13299361318149, Moving Avg Reward: -24.012395521096774, Replay Buffer Size: 35882\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1020, Reward: -22.13299361318149, Moving Avg Reward: -24.012395521096774, Replay Buffer Size: 35882\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1021: Won\n",
      "Episode 1021, Avg Value Loss: 3.337670526653528, Avg Policy Loss: 3.923666813969612\n",
      "Episode 1021, Reward: -97.7551227069316, Moving Avg Reward: -24.601582303820518, Replay Buffer Size: 35962\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1021, Reward: -97.7551227069316, Moving Avg Reward: -24.601582303820518, Replay Buffer Size: 35962\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1022: Won\n",
      "Episode 1022, Avg Value Loss: 3.057316667503781, Avg Policy Loss: 3.998720791604784\n",
      "Episode 1022, Reward: 18.57703737074809, Moving Avg Reward: -24.17645039302978, Replay Buffer Size: 35980\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1022, Reward: 18.57703737074809, Moving Avg Reward: -24.17645039302978, Replay Buffer Size: 35980\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1023: Won\n",
      "Episode 1023, Avg Value Loss: 3.384569752216339, Avg Policy Loss: 3.955706086754799\n",
      "Episode 1023, Reward: 6.732736975178642, Moving Avg Reward: -24.27585147955658, Replay Buffer Size: 36060\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1023, Reward: 6.732736975178642, Moving Avg Reward: -24.27585147955658, Replay Buffer Size: 36060\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1024: Won\n",
      "Episode 1024, Avg Value Loss: 2.9546133930032905, Avg Policy Loss: 3.932262268933383\n",
      "Episode 1024, Reward: -26.118142563322568, Moving Avg Reward: -24.58483367182511, Replay Buffer Size: 36071\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1024, Reward: -26.118142563322568, Moving Avg Reward: -24.58483367182511, Replay Buffer Size: 36071\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1025: Won\n",
      "Episode 1025, Avg Value Loss: 3.3056007370352747, Avg Policy Loss: 3.9412581861019134\n",
      "Episode 1025, Reward: 4.036234462735469, Moving Avg Reward: -24.08149201891864, Replay Buffer Size: 36151\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1025, Reward: 4.036234462735469, Moving Avg Reward: -24.08149201891864, Replay Buffer Size: 36151\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1026: Lost\n",
      "Episode 1026, Avg Value Loss: 3.4799282451470694, Avg Policy Loss: 3.9367462396621704\n",
      "Episode 1026, Reward: -28.26702366724284, Moving Avg Reward: -24.547854907753774, Replay Buffer Size: 36163\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1026, Reward: -28.26702366724284, Moving Avg Reward: -24.547854907753774, Replay Buffer Size: 36163\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1027: Lost\n",
      "Episode 1027, Avg Value Loss: 3.4061320923268794, Avg Policy Loss: 3.870724669098854\n",
      "Episode 1027, Reward: -21.223341993623823, Moving Avg Reward: -23.80271967225929, Replay Buffer Size: 36243\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1027, Reward: -21.223341993623823, Moving Avg Reward: -23.80271967225929, Replay Buffer Size: 36243\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1028: Lost\n",
      "Episode 1028, Avg Value Loss: 3.07326902449131, Avg Policy Loss: 3.9491947491963706\n",
      "Episode 1028, Reward: -25.86845981182342, Moving Avg Reward: -23.9664972163166, Replay Buffer Size: 36255\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 1028, Reward: -25.86845981182342, Moving Avg Reward: -23.9664972163166, Replay Buffer Size: 36255\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1029: Lost\n",
      "Episode 1029, Avg Value Loss: 2.9743238470771094, Avg Policy Loss: 4.025767001238736\n",
      "Episode 1029, Reward: -29.152184887434267, Moving Avg Reward: -23.972651877850595, Replay Buffer Size: 36266\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1029, Reward: -29.152184887434267, Moving Avg Reward: -23.972651877850595, Replay Buffer Size: 36266\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1030: Lost\n",
      "Episode 1030, Avg Value Loss: 3.1053568005561827, Avg Policy Loss: 3.927816426753998\n",
      "Episode 1030, Reward: -1.0198417852078596, Moving Avg Reward: -23.702056531716877, Replay Buffer Size: 36346\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1030, Reward: -1.0198417852078596, Moving Avg Reward: -23.702056531716877, Replay Buffer Size: 36346\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1031: Lost\n",
      "Episode 1031, Avg Value Loss: 3.4213969791439216, Avg Policy Loss: 3.9693950465027714\n",
      "Episode 1031, Reward: -44.46780225963158, Moving Avg Reward: -23.360731882498325, Replay Buffer Size: 36417\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 1031, Reward: -44.46780225963158, Moving Avg Reward: -23.360731882498325, Replay Buffer Size: 36417\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1032: Lost\n",
      "Episode 1032, Avg Value Loss: 2.763641834259033, Avg Policy Loss: 3.9442039186304267\n",
      "Episode 1032, Reward: -31.180005270206784, Moving Avg Reward: -23.45788000977123, Replay Buffer Size: 36428\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1032, Reward: -31.180005270206784, Moving Avg Reward: -23.45788000977123, Replay Buffer Size: 36428\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1033: Lost\n",
      "Episode 1033, Avg Value Loss: 3.217926706586565, Avg Policy Loss: 4.035250765936715\n",
      "Episode 1033, Reward: -32.176814846130654, Moving Avg Reward: -23.5071395664029, Replay Buffer Size: 36442\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1033, Reward: -32.176814846130654, Moving Avg Reward: -23.5071395664029, Replay Buffer Size: 36442\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1034: Lost\n",
      "Episode 1034, Avg Value Loss: 3.0118288516998293, Avg Policy Loss: 3.974916362762451\n",
      "Episode 1034, Reward: -25.3007250248483, Moving Avg Reward: -23.9137333676626, Replay Buffer Size: 36452\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1034, Reward: -25.3007250248483, Moving Avg Reward: -23.9137333676626, Replay Buffer Size: 36452\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1035: Lost\n",
      "Episode 1035, Avg Value Loss: 2.7430818514390425, Avg Policy Loss: 3.8378125104037197\n",
      "Episode 1035, Reward: -34.000986517962325, Moving Avg Reward: -24.035839351132637, Replay Buffer Size: 36463\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1035, Reward: -34.000986517962325, Moving Avg Reward: -24.035839351132637, Replay Buffer Size: 36463\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1036: Lost\n",
      "Episode 1036, Avg Value Loss: 3.3674278599875316, Avg Policy Loss: 4.211126906531198\n",
      "Episode 1036, Reward: -22.485657679879566, Moving Avg Reward: -23.21532801765602, Replay Buffer Size: 36470\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1036, Reward: -22.485657679879566, Moving Avg Reward: -23.21532801765602, Replay Buffer Size: 36470\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1037: Lost\n",
      "Episode 1037, Avg Value Loss: 3.4740945624142157, Avg Policy Loss: 3.926942156582344\n",
      "Episode 1037, Reward: -32.37968246791534, Moving Avg Reward: -23.22685860707834, Replay Buffer Size: 36511\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1037, Reward: -32.37968246791534, Moving Avg Reward: -23.22685860707834, Replay Buffer Size: 36511\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1038: Lost\n",
      "Episode 1038, Avg Value Loss: 3.2165769168308804, Avg Policy Loss: 3.908871923174177\n",
      "Episode 1038, Reward: 18.370957220226774, Moving Avg Reward: -22.813305619170155, Replay Buffer Size: 36532\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1038, Reward: 18.370957220226774, Moving Avg Reward: -22.813305619170155, Replay Buffer Size: 36532\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1039: Won\n",
      "Episode 1039, Avg Value Loss: 3.3810556801882656, Avg Policy Loss: 3.922510168769143\n",
      "Episode 1039, Reward: -27.036278895651094, Moving Avg Reward: -22.280758083468363, Replay Buffer Size: 36543\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1039, Reward: -27.036278895651094, Moving Avg Reward: -22.280758083468363, Replay Buffer Size: 36543\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1040: Won\n",
      "Episode 1040, Avg Value Loss: 2.9126504560311637, Avg Policy Loss: 4.061712185541789\n",
      "Episode 1040, Reward: -31.071191227079716, Moving Avg Reward: -22.654043094681757, Replay Buffer Size: 36555\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1040, Reward: -31.071191227079716, Moving Avg Reward: -22.654043094681757, Replay Buffer Size: 36555\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1041: Won\n",
      "Episode 1041, Avg Value Loss: 3.405965381115675, Avg Policy Loss: 3.8571977108716964\n",
      "Episode 1041, Reward: -28.653615651250483, Moving Avg Reward: -22.642323607691946, Replay Buffer Size: 36635\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1041, Reward: -28.653615651250483, Moving Avg Reward: -22.642323607691946, Replay Buffer Size: 36635\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1042: Won\n",
      "Episode 1042, Avg Value Loss: 3.2536243200302124, Avg Policy Loss: 4.015772104263306\n",
      "Episode 1042, Reward: -19.33587545022513, Moving Avg Reward: -22.44271621695108, Replay Buffer Size: 36641\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1042, Reward: -19.33587545022513, Moving Avg Reward: -22.44271621695108, Replay Buffer Size: 36641\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1043: Won\n",
      "Episode 1043, Avg Value Loss: 3.4755829789421777, Avg Policy Loss: 3.985768448222767\n",
      "Episode 1043, Reward: -27.215738823158624, Moving Avg Reward: -22.54861585014131, Replay Buffer Size: 36652\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1043, Reward: -27.215738823158624, Moving Avg Reward: -22.54861585014131, Replay Buffer Size: 36652\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1044: Won\n",
      "Episode 1044, Avg Value Loss: 3.4361347541213036, Avg Policy Loss: 3.9946140378713606\n",
      "Episode 1044, Reward: -4.017764254695732, Moving Avg Reward: -22.54570299510091, Replay Buffer Size: 36732\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1044, Reward: -4.017764254695732, Moving Avg Reward: -22.54570299510091, Replay Buffer Size: 36732\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1045: Lost\n",
      "Episode 1045, Avg Value Loss: 4.498308592372471, Avg Policy Loss: 3.72758682568868\n",
      "Episode 1045, Reward: -25.807365375449777, Moving Avg Reward: -22.52911077026439, Replay Buffer Size: 36741\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1045, Reward: -25.807365375449777, Moving Avg Reward: -22.52911077026439, Replay Buffer Size: 36741\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1046: Lost\n",
      "Episode 1046, Avg Value Loss: 3.3925574749708174, Avg Policy Loss: 3.986383831501007\n",
      "Episode 1046, Reward: 4.114192389572011, Moving Avg Reward: -22.195369544615254, Replay Buffer Size: 36821\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1046, Reward: 4.114192389572011, Moving Avg Reward: -22.195369544615254, Replay Buffer Size: 36821\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1047: Lost\n",
      "Episode 1047, Avg Value Loss: 3.3210676327347755, Avg Policy Loss: 3.906519201397896\n",
      "Episode 1047, Reward: -42.54184680473224, Moving Avg Reward: -22.374960791288252, Replay Buffer Size: 36901\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1047, Reward: -42.54184680473224, Moving Avg Reward: -22.374960791288252, Replay Buffer Size: 36901\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1048: Draw\n",
      "Episode 1048, Avg Value Loss: 3.3455676461787935, Avg Policy Loss: 3.9115453375146743\n",
      "Episode 1048, Reward: 18.51495075084187, Moving Avg Reward: -21.711157439266728, Replay Buffer Size: 36948\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1048, Reward: 18.51495075084187, Moving Avg Reward: -21.711157439266728, Replay Buffer Size: 36948\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1049: Won\n",
      "Episode 1049, Avg Value Loss: 3.6660938913171943, Avg Policy Loss: 3.7779687317934902\n",
      "Episode 1049, Reward: -23.565643560309034, Moving Avg Reward: -22.104562548408648, Replay Buffer Size: 36959\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1049, Reward: -23.565643560309034, Moving Avg Reward: -22.104562548408648, Replay Buffer Size: 36959\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1050: Won\n",
      "Episode 1050, Avg Value Loss: 3.60989730656147, Avg Policy Loss: 3.9472137868404387\n",
      "Episode 1050, Reward: -89.28255542774873, Moving Avg Reward: -22.632322616729766, Replay Buffer Size: 37039\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1050, Reward: -89.28255542774873, Moving Avg Reward: -22.632322616729766, Replay Buffer Size: 37039\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1051: Won\n",
      "Episode 1051, Avg Value Loss: 3.2431960456073283, Avg Policy Loss: 3.921855166554451\n",
      "Episode 1051, Reward: -8.459139177309064, Moving Avg Reward: -22.81581400850286, Replay Buffer Size: 37119\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 1051, Reward: -8.459139177309064, Moving Avg Reward: -22.81581400850286, Replay Buffer Size: 37119\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1052: Draw\n",
      "Episode 1052, Avg Value Loss: 3.3827124413322, Avg Policy Loss: 3.930175591917599\n",
      "Episode 1052, Reward: -35.96324938296839, Moving Avg Reward: -23.070937670447798, Replay Buffer Size: 37153\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 1052, Reward: -35.96324938296839, Moving Avg Reward: -23.070937670447798, Replay Buffer Size: 37153\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1053: Lost\n",
      "Episode 1053, Avg Value Loss: 2.736767189843314, Avg Policy Loss: 4.1044003282274515\n",
      "Episode 1053, Reward: -21.239789477539865, Moving Avg Reward: -23.467220773314448, Replay Buffer Size: 37160\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1053, Reward: -21.239789477539865, Moving Avg Reward: -23.467220773314448, Replay Buffer Size: 37160\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1054: Lost\n",
      "Episode 1054, Avg Value Loss: 3.4251910209655763, Avg Policy Loss: 3.970993718504906\n",
      "Episode 1054, Reward: 8.652658937173355, Moving Avg Reward: -23.030877905940603, Replay Buffer Size: 37240\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 1054, Reward: 8.652658937173355, Moving Avg Reward: -23.030877905940603, Replay Buffer Size: 37240\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1055: Lost\n",
      "Episode 1055, Avg Value Loss: 3.0762812048196793, Avg Policy Loss: 3.9517350991566977\n",
      "Episode 1055, Reward: -30.172006640703394, Moving Avg Reward: -22.958001728875374, Replay Buffer Size: 37252\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1055, Reward: -30.172006640703394, Moving Avg Reward: -22.958001728875374, Replay Buffer Size: 37252\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1056: Lost\n",
      "Episode 1056, Avg Value Loss: 3.319819753820246, Avg Policy Loss: 4.037200169129805\n",
      "Episode 1056, Reward: -30.376163611032656, Moving Avg Reward: -22.993876102016728, Replay Buffer Size: 37263\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1056, Reward: -30.376163611032656, Moving Avg Reward: -22.993876102016728, Replay Buffer Size: 37263\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1057: Lost\n",
      "Episode 1057, Avg Value Loss: 3.352334403991699, Avg Policy Loss: 3.954374870657921\n",
      "Episode 1057, Reward: -75.37118831363644, Moving Avg Reward: -23.48152440618885, Replay Buffer Size: 37343\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1057, Reward: -75.37118831363644, Moving Avg Reward: -23.48152440618885, Replay Buffer Size: 37343\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1058: Lost\n",
      "Episode 1058, Avg Value Loss: 3.3291844084407343, Avg Policy Loss: 3.9426275744582666\n",
      "Episode 1058, Reward: -11.939558514614776, Moving Avg Reward: -23.599328761668012, Replay Buffer Size: 37409\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1058, Reward: -11.939558514614776, Moving Avg Reward: -23.599328761668012, Replay Buffer Size: 37409\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1059: Won\n",
      "Episode 1059, Avg Value Loss: 3.506453964445326, Avg Policy Loss: 3.9344107574886746\n",
      "Episode 1059, Reward: -21.86771232699299, Moving Avg Reward: -23.59188655195321, Replay Buffer Size: 37418\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1059, Reward: -21.86771232699299, Moving Avg Reward: -23.59188655195321, Replay Buffer Size: 37418\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1060: Won\n",
      "Episode 1060, Avg Value Loss: 3.188718266785145, Avg Policy Loss: 3.993657273054123\n",
      "Episode 1060, Reward: -65.08731999867494, Moving Avg Reward: -23.96647728446624, Replay Buffer Size: 37498\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1060, Reward: -65.08731999867494, Moving Avg Reward: -23.96647728446624, Replay Buffer Size: 37498\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1061: Won\n",
      "Episode 1061, Avg Value Loss: 3.117734968662262, Avg Policy Loss: 3.9802913665771484\n",
      "Episode 1061, Reward: -26.197957459769732, Moving Avg Reward: -23.998268813346684, Replay Buffer Size: 37508\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1061, Reward: -26.197957459769732, Moving Avg Reward: -23.998268813346684, Replay Buffer Size: 37508\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1062: Won\n",
      "Episode 1062, Avg Value Loss: 3.3200105958514743, Avg Policy Loss: 4.1379029750823975\n",
      "Episode 1062, Reward: -23.962550748541155, Moving Avg Reward: -23.894527861662986, Replay Buffer Size: 37517\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1062, Reward: -23.962550748541155, Moving Avg Reward: -23.894527861662986, Replay Buffer Size: 37517\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1063: Won\n",
      "Episode 1063, Avg Value Loss: 3.0961009732314517, Avg Policy Loss: 3.9363704323768616\n",
      "Episode 1063, Reward: 15.139004434105807, Moving Avg Reward: -23.404687200009935, Replay Buffer Size: 37545\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1063, Reward: 15.139004434105807, Moving Avg Reward: -23.404687200009935, Replay Buffer Size: 37545\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1064: Won\n",
      "Episode 1064, Avg Value Loss: 3.2926670496280375, Avg Policy Loss: 3.9224801246936503\n",
      "Episode 1064, Reward: -29.127004438209724, Moving Avg Reward: -23.44407019161208, Replay Buffer Size: 37558\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1064, Reward: -29.127004438209724, Moving Avg Reward: -23.44407019161208, Replay Buffer Size: 37558\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1065: Won\n",
      "Episode 1065, Avg Value Loss: 3.17447320792986, Avg Policy Loss: 3.96122612123904\n",
      "Episode 1065, Reward: 19.227433961819443, Moving Avg Reward: -23.002749155280675, Replay Buffer Size: 37581\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1065, Reward: 19.227433961819443, Moving Avg Reward: -23.002749155280675, Replay Buffer Size: 37581\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1066: Won\n",
      "Episode 1066, Avg Value Loss: 2.8963462471961976, Avg Policy Loss: 3.8929195642471313\n",
      "Episode 1066, Reward: -28.496719114807988, Moving Avg Reward: -22.997989403828065, Replay Buffer Size: 37591\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1066, Reward: -28.496719114807988, Moving Avg Reward: -22.997989403828065, Replay Buffer Size: 37591\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1067: Won\n",
      "Episode 1067, Avg Value Loss: 2.6024416387081146, Avg Policy Loss: 3.9529888331890106\n",
      "Episode 1067, Reward: -9.32985213387634, Moving Avg Reward: -22.814544372802008, Replay Buffer Size: 37599\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1067, Reward: -9.32985213387634, Moving Avg Reward: -22.814544372802008, Replay Buffer Size: 37599\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1068: Won\n",
      "Episode 1068, Avg Value Loss: 3.4914703723546623, Avg Policy Loss: 3.896253656696629\n",
      "Episode 1068, Reward: -34.406525206753535, Moving Avg Reward: -22.87127670167157, Replay Buffer Size: 37636\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 1068, Reward: -34.406525206753535, Moving Avg Reward: -22.87127670167157, Replay Buffer Size: 37636\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1069: Won\n",
      "Episode 1069, Avg Value Loss: 3.536121892077582, Avg Policy Loss: 4.019826173782349\n",
      "Episode 1069, Reward: 12.842885366162953, Moving Avg Reward: -22.382789714081458, Replay Buffer Size: 37664\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1069, Reward: 12.842885366162953, Moving Avg Reward: -22.382789714081458, Replay Buffer Size: 37664\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1070: Won\n",
      "Episode 1070, Avg Value Loss: 2.9736811389093813, Avg Policy Loss: 4.010417015656181\n",
      "Episode 1070, Reward: 19.396592280287056, Moving Avg Reward: -22.363193994716447, Replay Buffer Size: 37687\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1070, Reward: 19.396592280287056, Moving Avg Reward: -22.363193994716447, Replay Buffer Size: 37687\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1071: Won\n",
      "Episode 1071, Avg Value Loss: 2.780715832343468, Avg Policy Loss: 3.9297636288862963\n",
      "Episode 1071, Reward: -34.28199327409904, Moving Avg Reward: -22.431603359942383, Replay Buffer Size: 37700\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 1071, Reward: -34.28199327409904, Moving Avg Reward: -22.431603359942383, Replay Buffer Size: 37700\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1072: Won\n",
      "Episode 1072, Avg Value Loss: 4.3386593948711045, Avg Policy Loss: 3.767304398796775\n",
      "Episode 1072, Reward: -27.30649674941936, Moving Avg Reward: -22.57051020733976, Replay Buffer Size: 37711\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1072, Reward: -27.30649674941936, Moving Avg Reward: -22.57051020733976, Replay Buffer Size: 37711\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1073: Won\n",
      "Episode 1073, Avg Value Loss: 3.074441514231942, Avg Policy Loss: 3.99176946553317\n",
      "Episode 1073, Reward: -39.731565659904355, Moving Avg Reward: -22.6780949471631, Replay Buffer Size: 37733\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1073, Reward: -39.731565659904355, Moving Avg Reward: -22.6780949471631, Replay Buffer Size: 37733\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1074: Won\n",
      "Episode 1074, Avg Value Loss: 3.3146852016448975, Avg Policy Loss: 3.9649364709854127\n",
      "Episode 1074, Reward: -26.446210794148083, Moving Avg Reward: -23.134350888362178, Replay Buffer Size: 37743\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1074, Reward: -26.446210794148083, Moving Avg Reward: -23.134350888362178, Replay Buffer Size: 37743\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1075: Won\n",
      "Episode 1075, Avg Value Loss: 3.3506692975759504, Avg Policy Loss: 4.0051492869853975\n",
      "Episode 1075, Reward: -12.379715464407356, Moving Avg Reward: -23.003500845733264, Replay Buffer Size: 37823\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1075, Reward: -12.379715464407356, Moving Avg Reward: -23.003500845733264, Replay Buffer Size: 37823\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1076: Won\n",
      "Episode 1076, Avg Value Loss: 3.360944077372551, Avg Policy Loss: 3.968146634101868\n",
      "Episode 1076, Reward: 3.573567693111311, Moving Avg Reward: -22.65180785368521, Replay Buffer Size: 37903\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1076, Reward: 3.573567693111311, Moving Avg Reward: -22.65180785368521, Replay Buffer Size: 37903\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1077: Won\n",
      "Episode 1077, Avg Value Loss: 2.393651032447815, Avg Policy Loss: 4.062491464614868\n",
      "Episode 1077, Reward: -28.18090829002044, Moving Avg Reward: -22.67417349398025, Replay Buffer Size: 37913\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1077, Reward: -28.18090829002044, Moving Avg Reward: -22.67417349398025, Replay Buffer Size: 37913\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1078: Won\n",
      "Episode 1078, Avg Value Loss: 3.314996786551042, Avg Policy Loss: 4.041911242224954\n",
      "Episode 1078, Reward: -37.78187739534067, Moving Avg Reward: -22.800884833993678, Replay Buffer Size: 37968\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 1078, Reward: -37.78187739534067, Moving Avg Reward: -22.800884833993678, Replay Buffer Size: 37968\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1079: Won\n",
      "Episode 1079, Avg Value Loss: 2.885440981388092, Avg Policy Loss: 3.983819103240967\n",
      "Episode 1079, Reward: -32.32491106157564, Moving Avg Reward: -22.83948907351334, Replay Buffer Size: 37978\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1079, Reward: -32.32491106157564, Moving Avg Reward: -22.83948907351334, Replay Buffer Size: 37978\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1080: Lost\n",
      "Episode 1080, Avg Value Loss: 3.281637752928385, Avg Policy Loss: 3.98237369700176\n",
      "Episode 1080, Reward: -57.6195474782538, Moving Avg Reward: -23.151756881389794, Replay Buffer Size: 38019\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1080, Reward: -57.6195474782538, Moving Avg Reward: -23.151756881389794, Replay Buffer Size: 38019\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1081: Lost\n",
      "Episode 1081, Avg Value Loss: 3.379907176608131, Avg Policy Loss: 4.007468882061186\n",
      "Episode 1081, Reward: 17.945365171253968, Moving Avg Reward: -22.698164303839985, Replay Buffer Size: 38040\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1081, Reward: 17.945365171253968, Moving Avg Reward: -22.698164303839985, Replay Buffer Size: 38040\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1082: Won\n",
      "Episode 1082, Avg Value Loss: 3.1940739022360907, Avg Policy Loss: 4.012028084860908\n",
      "Episode 1082, Reward: -33.9399572385339, Moving Avg Reward: -22.77946787116353, Replay Buffer Size: 38049\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 1082, Reward: -33.9399572385339, Moving Avg Reward: -22.77946787116353, Replay Buffer Size: 38049\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1083: Won\n",
      "Episode 1083, Avg Value Loss: 3.4597724922001363, Avg Policy Loss: 3.9440986096858976\n",
      "Episode 1083, Reward: -18.925221724621466, Moving Avg Reward: -22.707080625011244, Replay Buffer Size: 38129\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1083, Reward: -18.925221724621466, Moving Avg Reward: -22.707080625011244, Replay Buffer Size: 38129\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1084: Won\n",
      "Episode 1084, Avg Value Loss: 3.5272994518280028, Avg Policy Loss: 4.1511778116226195\n",
      "Episode 1084, Reward: -28.034095893690242, Moving Avg Reward: -22.71197725420557, Replay Buffer Size: 38139\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1084, Reward: -28.034095893690242, Moving Avg Reward: -22.71197725420557, Replay Buffer Size: 38139\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1085: Won\n",
      "Episode 1085, Avg Value Loss: 3.5252571381055393, Avg Policy Loss: 3.9153238993424635\n",
      "Episode 1085, Reward: -31.7680059571813, Moving Avg Reward: -22.702168090956885, Replay Buffer Size: 38152\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1085, Reward: -31.7680059571813, Moving Avg Reward: -22.702168090956885, Replay Buffer Size: 38152\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1086: Won\n",
      "Episode 1086, Avg Value Loss: 3.2870454013347628, Avg Policy Loss: 3.998739105463028\n",
      "Episode 1086, Reward: -34.51930803336512, Moving Avg Reward: -22.69310901998121, Replay Buffer Size: 38232\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1086, Reward: -34.51930803336512, Moving Avg Reward: -22.69310901998121, Replay Buffer Size: 38232\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1087: Lost\n",
      "Episode 1087, Avg Value Loss: 3.425190145319158, Avg Policy Loss: 4.031320160085505\n",
      "Episode 1087, Reward: 18.472624725890974, Moving Avg Reward: -22.23276155734399, Replay Buffer Size: 38254\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1087, Reward: 18.472624725890974, Moving Avg Reward: -22.23276155734399, Replay Buffer Size: 38254\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1088: Won\n",
      "Episode 1088, Avg Value Loss: 3.1579253503254483, Avg Policy Loss: 3.941603881972177\n",
      "Episode 1088, Reward: -30.377193138291908, Moving Avg Reward: -22.26669152389809, Replay Buffer Size: 38268\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1088, Reward: -30.377193138291908, Moving Avg Reward: -22.26669152389809, Replay Buffer Size: 38268\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1089: Won\n",
      "Episode 1089, Avg Value Loss: 3.3706631578505037, Avg Policy Loss: 4.03222508430481\n",
      "Episode 1089, Reward: -0.7949276485031879, Moving Avg Reward: -21.993080351388972, Replay Buffer Size: 38348\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1089, Reward: -0.7949276485031879, Moving Avg Reward: -21.993080351388972, Replay Buffer Size: 38348\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1090: Won\n",
      "Episode 1090, Avg Value Loss: 2.6894861459732056, Avg Policy Loss: 3.9654244422912597\n",
      "Episode 1090, Reward: -24.356330025146274, Moving Avg Reward: -21.927726409469997, Replay Buffer Size: 38358\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1090, Reward: -24.356330025146274, Moving Avg Reward: -21.927726409469997, Replay Buffer Size: 38358\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1091: Won\n",
      "Episode 1091, Avg Value Loss: 3.652462113987316, Avg Policy Loss: 4.013600999658758\n",
      "Episode 1091, Reward: -30.577316710202737, Moving Avg Reward: -21.98956853017276, Replay Buffer Size: 38369\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 1091, Reward: -30.577316710202737, Moving Avg Reward: -21.98956853017276, Replay Buffer Size: 38369\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1092: Won\n",
      "Episode 1092, Avg Value Loss: 3.321377130655142, Avg Policy Loss: 4.046225676169763\n",
      "Episode 1092, Reward: -38.57893117303736, Moving Avg Reward: -21.736880544396573, Replay Buffer Size: 38382\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1092, Reward: -38.57893117303736, Moving Avg Reward: -21.736880544396573, Replay Buffer Size: 38382\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1093: Won\n",
      "Episode 1093, Avg Value Loss: 2.934462606906891, Avg Policy Loss: 4.171959489583969\n",
      "Episode 1093, Reward: -24.42539479442239, Moving Avg Reward: -21.69715321439768, Replay Buffer Size: 38390\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1093, Reward: -24.42539479442239, Moving Avg Reward: -21.69715321439768, Replay Buffer Size: 38390\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1094: Won\n",
      "Episode 1094, Avg Value Loss: 3.5206799164414404, Avg Policy Loss: 4.010530936717987\n",
      "Episode 1094, Reward: -38.88020992958582, Moving Avg Reward: -21.78323703788557, Replay Buffer Size: 38470\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1094, Reward: -38.88020992958582, Moving Avg Reward: -21.78323703788557, Replay Buffer Size: 38470\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1095: Lost\n",
      "Episode 1095, Avg Value Loss: 2.988672012090683, Avg Policy Loss: 3.8151586055755615\n",
      "Episode 1095, Reward: -29.862821151676464, Moving Avg Reward: -22.239110962746214, Replay Buffer Size: 38480\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1095, Reward: -29.862821151676464, Moving Avg Reward: -22.239110962746214, Replay Buffer Size: 38480\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1096: Lost\n",
      "Episode 1096, Avg Value Loss: 3.3449980318546295, Avg Policy Loss: 3.8350456058979034\n",
      "Episode 1096, Reward: -24.443257512585827, Moving Avg Reward: -22.441503115822798, Replay Buffer Size: 38488\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1096, Reward: -24.443257512585827, Moving Avg Reward: -22.441503115822798, Replay Buffer Size: 38488\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1097: Lost\n",
      "Episode 1097, Avg Value Loss: 3.6846165381945095, Avg Policy Loss: 4.199080485564012\n",
      "Episode 1097, Reward: -32.533200545212026, Moving Avg Reward: -22.479667250480063, Replay Buffer Size: 38501\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 1097, Reward: -32.533200545212026, Moving Avg Reward: -22.479667250480063, Replay Buffer Size: 38501\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1098: Lost\n",
      "Episode 1098, Avg Value Loss: 3.2918616646812078, Avg Policy Loss: 4.032365197227115\n",
      "Episode 1098, Reward: 18.61906941024954, Moving Avg Reward: -22.49584003979149, Replay Buffer Size: 38522\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1098, Reward: 18.61906941024954, Moving Avg Reward: -22.49584003979149, Replay Buffer Size: 38522\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1099: Won\n",
      "Episode 1099, Avg Value Loss: 3.687273383140564, Avg Policy Loss: 3.8797596295674643\n",
      "Episode 1099, Reward: -26.118696761013794, Moving Avg Reward: -22.736143002118812, Replay Buffer Size: 38534\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1099, Reward: -26.118696761013794, Moving Avg Reward: -22.736143002118812, Replay Buffer Size: 38534\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1100: Won\n",
      "Episode 1100, Avg Value Loss: 3.6238859860520614, Avg Policy Loss: 4.010846050162065\n",
      "Episode 1100, Reward: -30.67415097669781, Moving Avg Reward: -22.713827590050606, Replay Buffer Size: 38572\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1100, Reward: -30.67415097669781, Moving Avg Reward: -22.713827590050606, Replay Buffer Size: 38572\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1101: Won\n",
      "Episode 1101, Avg Value Loss: 2.883865311741829, Avg Policy Loss: 4.093320369720459\n",
      "Episode 1101, Reward: -27.74716854336798, Moving Avg Reward: -22.72608907331838, Replay Buffer Size: 38580\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1101, Reward: -27.74716854336798, Moving Avg Reward: -22.72608907331838, Replay Buffer Size: 38580\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1102: Won\n",
      "Episode 1102, Avg Value Loss: 2.8754274606704713, Avg Policy Loss: 3.96588876247406\n",
      "Episode 1102, Reward: -25.144520114754457, Moving Avg Reward: -23.045151207131457, Replay Buffer Size: 38590\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1102, Reward: -25.144520114754457, Moving Avg Reward: -23.045151207131457, Replay Buffer Size: 38590\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1103: Won\n",
      "Episode 1103, Avg Value Loss: 3.3597996611344185, Avg Policy Loss: 4.066000888222142\n",
      "Episode 1103, Reward: 18.168915525024133, Moving Avg Reward: -22.56349439179233, Replay Buffer Size: 38609\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1103, Reward: 18.168915525024133, Moving Avg Reward: -22.56349439179233, Replay Buffer Size: 38609\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1104: Won\n",
      "Episode 1104, Avg Value Loss: 3.3981700404123827, Avg Policy Loss: 3.9176167303865608\n",
      "Episode 1104, Reward: -45.564354209615736, Moving Avg Reward: -22.735274845519733, Replay Buffer Size: 38653\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1104, Reward: -45.564354209615736, Moving Avg Reward: -22.735274845519733, Replay Buffer Size: 38653\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1105: Won\n",
      "Episode 1105, Avg Value Loss: 3.329360524813334, Avg Policy Loss: 4.070522387822469\n",
      "Episode 1105, Reward: -31.61040933576744, Moving Avg Reward: -22.81764843600179, Replay Buffer Size: 38662\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1105, Reward: -31.61040933576744, Moving Avg Reward: -22.81764843600179, Replay Buffer Size: 38662\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1106: Won\n",
      "Episode 1106, Avg Value Loss: 3.813034564256668, Avg Policy Loss: 4.121572434902191\n",
      "Episode 1106, Reward: -24.050878325870745, Moving Avg Reward: -22.84337862741575, Replay Buffer Size: 38670\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1106, Reward: -24.050878325870745, Moving Avg Reward: -22.84337862741575, Replay Buffer Size: 38670\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1107: Won\n",
      "Episode 1107, Avg Value Loss: 3.14516184065077, Avg Policy Loss: 4.088422563340929\n",
      "Episode 1107, Reward: -30.877636564575745, Moving Avg Reward: -22.904613495984705, Replay Buffer Size: 38679\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1107, Reward: -30.877636564575745, Moving Avg Reward: -22.904613495984705, Replay Buffer Size: 38679\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1108: Won\n",
      "Episode 1108, Avg Value Loss: 3.9645753587995256, Avg Policy Loss: 3.744823523930141\n",
      "Episode 1108, Reward: -21.77805695322065, Moving Avg Reward: -22.769999668706262, Replay Buffer Size: 38686\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1108, Reward: -21.77805695322065, Moving Avg Reward: -22.769999668706262, Replay Buffer Size: 38686\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1109: Won\n",
      "Episode 1109, Avg Value Loss: 3.7792002814156667, Avg Policy Loss: 3.997180938720703\n",
      "Episode 1109, Reward: -31.851066390686476, Moving Avg Reward: -22.799226850397126, Replay Buffer Size: 38700\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1109, Reward: -31.851066390686476, Moving Avg Reward: -22.799226850397126, Replay Buffer Size: 38700\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1110: Won\n",
      "Episode 1110, Avg Value Loss: 3.5122461542487144, Avg Policy Loss: 4.053245204687118\n",
      "Episode 1110, Reward: -31.70010577298756, Moving Avg Reward: -22.572358018310894, Replay Buffer Size: 38780\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1110, Reward: -31.70010577298756, Moving Avg Reward: -22.572358018310894, Replay Buffer Size: 38780\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1111: Lost\n",
      "Episode 1111, Avg Value Loss: 3.349902556492732, Avg Policy Loss: 4.0212597113389235\n",
      "Episode 1111, Reward: -27.822564238302625, Moving Avg Reward: -22.30676700719951, Replay Buffer Size: 38793\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1111, Reward: -27.822564238302625, Moving Avg Reward: -22.30676700719951, Replay Buffer Size: 38793\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1112: Lost\n",
      "Episode 1112, Avg Value Loss: 3.1492414474487305, Avg Policy Loss: 4.117738048235576\n",
      "Episode 1112, Reward: -20.610055823289514, Moving Avg Reward: -22.179867899526126, Replay Buffer Size: 38799\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 1112, Reward: -20.610055823289514, Moving Avg Reward: -22.179867899526126, Replay Buffer Size: 38799\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1113: Lost\n",
      "Episode 1113, Avg Value Loss: 4.15868353843689, Avg Policy Loss: 3.892305588722229\n",
      "Episode 1113, Reward: -34.97019775388533, Moving Avg Reward: -22.237975858974117, Replay Buffer Size: 38809\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1113, Reward: -34.97019775388533, Moving Avg Reward: -22.237975858974117, Replay Buffer Size: 38809\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1114: Lost\n",
      "Episode 1114, Avg Value Loss: 3.3651230957197105, Avg Policy Loss: 3.9911602891009785\n",
      "Episode 1114, Reward: 15.091516693507765, Moving Avg Reward: -21.836975120976717, Replay Buffer Size: 38832\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 1114, Reward: 15.091516693507765, Moving Avg Reward: -21.836975120976717, Replay Buffer Size: 38832\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1115: Won\n",
      "Episode 1115, Avg Value Loss: 3.728112675926902, Avg Policy Loss: 4.23385925726457\n",
      "Episode 1115, Reward: -25.546342311199545, Moving Avg Reward: -22.084438544088716, Replay Buffer Size: 38843\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 1115, Reward: -25.546342311199545, Moving Avg Reward: -22.084438544088716, Replay Buffer Size: 38843\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1116: Won\n",
      "Episode 1116, Avg Value Loss: 3.5239682472669163, Avg Policy Loss: 3.9616708755493164\n",
      "Episode 1116, Reward: -33.29014218688758, Moving Avg Reward: -22.322811430939428, Replay Buffer Size: 38856\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1116, Reward: -33.29014218688758, Moving Avg Reward: -22.322811430939428, Replay Buffer Size: 38856\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1117: Won\n",
      "Episode 1117, Avg Value Loss: 3.6027249813079836, Avg Policy Loss: 4.0427474737167355\n",
      "Episode 1117, Reward: -28.829155488814354, Moving Avg Reward: -22.317864209090562, Replay Buffer Size: 38866\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1117, Reward: -28.829155488814354, Moving Avg Reward: -22.317864209090562, Replay Buffer Size: 38866\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1118: Won\n",
      "Episode 1118, Avg Value Loss: 3.5476148823897042, Avg Policy Loss: 4.097540100415547\n",
      "Episode 1118, Reward: -31.616433322838752, Moving Avg Reward: -22.8184083099841, Replay Buffer Size: 38878\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 1118, Reward: -31.616433322838752, Moving Avg Reward: -22.8184083099841, Replay Buffer Size: 38878\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1119: Won\n",
      "Episode 1119, Avg Value Loss: 3.270367256551981, Avg Policy Loss: 4.020379567146302\n",
      "Episode 1119, Reward: -14.602778047149982, Moving Avg Reward: -22.69051764507335, Replay Buffer Size: 38958\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 1119, Reward: -14.602778047149982, Moving Avg Reward: -22.69051764507335, Replay Buffer Size: 38958\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1120: Won\n",
      "Episode 1120, Avg Value Loss: 3.220977535098791, Avg Policy Loss: 4.056614920496941\n",
      "Episode 1120, Reward: -64.92580588549072, Moving Avg Reward: -23.11844576779645, Replay Buffer Size: 39038\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1120, Reward: -64.92580588549072, Moving Avg Reward: -23.11844576779645, Replay Buffer Size: 39038\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1121: Draw\n",
      "Episode 1121, Avg Value Loss: 5.99542760848999, Avg Policy Loss: 3.9467084407806396\n",
      "Episode 1121, Reward: 9.99, Moving Avg Reward: -22.040994540727134, Replay Buffer Size: 39039\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1121, Reward: 9.99, Moving Avg Reward: -22.040994540727134, Replay Buffer Size: 39039\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1122: Won\n",
      "Episode 1122, Avg Value Loss: 3.525062322616577, Avg Policy Loss: 4.0175700187683105\n",
      "Episode 1122, Reward: 9.99, Moving Avg Reward: -22.12686491443462, Replay Buffer Size: 39040\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1122, Reward: 9.99, Moving Avg Reward: -22.12686491443462, Replay Buffer Size: 39040\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1123: Won\n",
      "Episode 1123, Avg Value Loss: 3.2007011897861957, Avg Policy Loss: 4.0203405052423475\n",
      "Episode 1123, Reward: -95.2030524486629, Moving Avg Reward: -23.146222808673038, Replay Buffer Size: 39120\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1123, Reward: -95.2030524486629, Moving Avg Reward: -23.146222808673038, Replay Buffer Size: 39120\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1124: Won\n",
      "Episode 1124, Avg Value Loss: 2.6563100264622617, Avg Policy Loss: 4.063654110981868\n",
      "Episode 1124, Reward: -24.94928993072771, Moving Avg Reward: -23.134534282347087, Replay Buffer Size: 39133\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1124, Reward: -24.94928993072771, Moving Avg Reward: -23.134534282347087, Replay Buffer Size: 39133\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1125: Won\n",
      "Episode 1125, Avg Value Loss: 3.30750098079443, Avg Policy Loss: 4.085303783416748\n",
      "Episode 1125, Reward: -23.616567865251955, Moving Avg Reward: -23.41106230562696, Replay Buffer Size: 39141\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1125, Reward: -23.616567865251955, Moving Avg Reward: -23.41106230562696, Replay Buffer Size: 39141\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1126: Won\n",
      "Episode 1126, Avg Value Loss: 3.2599114874998727, Avg Policy Loss: 4.153896013895671\n",
      "Episode 1126, Reward: -30.15235642656357, Moving Avg Reward: -23.42991563322016, Replay Buffer Size: 39153\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1126, Reward: -30.15235642656357, Moving Avg Reward: -23.42991563322016, Replay Buffer Size: 39153\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1127: Won\n",
      "Episode 1127, Avg Value Loss: 3.6318204565481707, Avg Policy Loss: 4.002586798234419\n",
      "Episode 1127, Reward: -41.7656567685342, Moving Avg Reward: -23.635338780969267, Replay Buffer Size: 39197\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 1127, Reward: -41.7656567685342, Moving Avg Reward: -23.635338780969267, Replay Buffer Size: 39197\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1128: Lost\n",
      "Episode 1128, Avg Value Loss: 2.912833235480569, Avg Policy Loss: 3.9114636291157114\n",
      "Episode 1128, Reward: -25.96557499379161, Moving Avg Reward: -23.63630993278895, Replay Buffer Size: 39208\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1128, Reward: -25.96557499379161, Moving Avg Reward: -23.63630993278895, Replay Buffer Size: 39208\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1129: Lost\n",
      "Episode 1129, Avg Value Loss: 3.521505756811662, Avg Policy Loss: 4.03850041736256\n",
      "Episode 1129, Reward: -26.266197630624116, Moving Avg Reward: -23.607450060220845, Replay Buffer Size: 39219\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1129, Reward: -26.266197630624116, Moving Avg Reward: -23.607450060220845, Replay Buffer Size: 39219\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1130: Lost\n",
      "Episode 1130, Avg Value Loss: 3.2311078820909773, Avg Policy Loss: 4.067307608468192\n",
      "Episode 1130, Reward: -21.666821916014428, Moving Avg Reward: -23.813919861528905, Replay Buffer Size: 39226\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1130, Reward: -21.666821916014428, Moving Avg Reward: -23.813919861528905, Replay Buffer Size: 39226\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1131: Lost\n",
      "Episode 1131, Avg Value Loss: 2.8186899561148424, Avg Policy Loss: 4.1289059198819675\n",
      "Episode 1131, Reward: -32.11804525847799, Moving Avg Reward: -23.690422291517372, Replay Buffer Size: 39239\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1131, Reward: -32.11804525847799, Moving Avg Reward: -23.690422291517372, Replay Buffer Size: 39239\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1132: Lost\n",
      "Episode 1132, Avg Value Loss: 3.295481790195812, Avg Policy Loss: 3.9513640620491723\n",
      "Episode 1132, Reward: -27.107665771244562, Moving Avg Reward: -23.649698896527756, Replay Buffer Size: 39250\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1132, Reward: -27.107665771244562, Moving Avg Reward: -23.649698896527756, Replay Buffer Size: 39250\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1133: Lost\n",
      "Episode 1133, Avg Value Loss: 3.463337659090757, Avg Policy Loss: 4.068330958485603\n",
      "Episode 1133, Reward: -23.49907551519402, Moving Avg Reward: -23.562921503218387, Replay Buffer Size: 39330\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1133, Reward: -23.49907551519402, Moving Avg Reward: -23.562921503218387, Replay Buffer Size: 39330\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1134: Lost\n",
      "Episode 1134, Avg Value Loss: 2.375389128923416, Avg Policy Loss: 3.9758759140968323\n",
      "Episode 1134, Reward: -27.929934002317985, Moving Avg Reward: -23.589213592993087, Replay Buffer Size: 39338\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1134, Reward: -27.929934002317985, Moving Avg Reward: -23.589213592993087, Replay Buffer Size: 39338\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1135: Lost\n",
      "Episode 1135, Avg Value Loss: 2.7980327285253086, Avg Policy Loss: 4.192417997580308\n",
      "Episode 1135, Reward: 12.508642790939495, Moving Avg Reward: -23.124117299904064, Replay Buffer Size: 39364\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1135, Reward: 12.508642790939495, Moving Avg Reward: -23.124117299904064, Replay Buffer Size: 39364\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1136: Won\n",
      "Episode 1136, Avg Value Loss: 3.349615676701069, Avg Policy Loss: 4.0821437507867815\n",
      "Episode 1136, Reward: -1.585681769389887, Moving Avg Reward: -22.915117540799166, Replay Buffer Size: 39444\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1136, Reward: -1.585681769389887, Moving Avg Reward: -22.915117540799166, Replay Buffer Size: 39444\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1137: Won\n",
      "Episode 1137, Avg Value Loss: 3.0161750486918857, Avg Policy Loss: 4.061314037867954\n",
      "Episode 1137, Reward: -22.946582828233385, Moving Avg Reward: -22.82078654440235, Replay Buffer Size: 39451\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1137, Reward: -22.946582828233385, Moving Avg Reward: -22.82078654440235, Replay Buffer Size: 39451\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1138: Won\n",
      "Episode 1138, Avg Value Loss: 3.2834239155054092, Avg Policy Loss: 4.00758495926857\n",
      "Episode 1138, Reward: -37.14157328464929, Moving Avg Reward: -23.375911849451114, Replay Buffer Size: 39491\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1138, Reward: -37.14157328464929, Moving Avg Reward: -23.375911849451114, Replay Buffer Size: 39491\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1139: Won\n",
      "Episode 1139, Avg Value Loss: 3.54923155605793, Avg Policy Loss: 4.005492353439331\n",
      "Episode 1139, Reward: -39.60395153706754, Moving Avg Reward: -23.501588575865277, Replay Buffer Size: 39571\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1139, Reward: -39.60395153706754, Moving Avg Reward: -23.501588575865277, Replay Buffer Size: 39571\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1140: Lost\n",
      "Episode 1140, Avg Value Loss: 3.294290799943228, Avg Policy Loss: 4.116001492454892\n",
      "Episode 1140, Reward: -24.818236281878256, Moving Avg Reward: -23.43905902641326, Replay Buffer Size: 39634\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1140, Reward: -24.818236281878256, Moving Avg Reward: -23.43905902641326, Replay Buffer Size: 39634\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1141: Won\n",
      "Episode 1141, Avg Value Loss: 3.7927225828170776, Avg Policy Loss: 4.1757714450359344\n",
      "Episode 1141, Reward: -26.701857404512975, Moving Avg Reward: -23.419541443945885, Replay Buffer Size: 39642\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1141, Reward: -26.701857404512975, Moving Avg Reward: -23.419541443945885, Replay Buffer Size: 39642\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1142: Won\n",
      "Episode 1142, Avg Value Loss: 3.4312033653259277, Avg Policy Loss: 3.966062903404236\n",
      "Episode 1142, Reward: -23.04716103809026, Moving Avg Reward: -23.456654299824535, Replay Buffer Size: 39652\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1142, Reward: -23.04716103809026, Moving Avg Reward: -23.456654299824535, Replay Buffer Size: 39652\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1143: Won\n",
      "Episode 1143, Avg Value Loss: 3.668743466337522, Avg Policy Loss: 4.016134699185689\n",
      "Episode 1143, Reward: -35.29039332121665, Moving Avg Reward: -23.537400844805116, Replay Buffer Size: 39664\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1143, Reward: -35.29039332121665, Moving Avg Reward: -23.537400844805116, Replay Buffer Size: 39664\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1144: Won\n",
      "Episode 1144, Avg Value Loss: 3.511190290749073, Avg Policy Loss: 4.021049612760544\n",
      "Episode 1144, Reward: -5.450497455795689, Moving Avg Reward: -23.55172817681611, Replay Buffer Size: 39744\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1144, Reward: -5.450497455795689, Moving Avg Reward: -23.55172817681611, Replay Buffer Size: 39744\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1145: Won\n",
      "Episode 1145, Avg Value Loss: 3.0020935013890266, Avg Policy Loss: 4.094627895951271\n",
      "Episode 1145, Reward: -0.2665251965725791, Moving Avg Reward: -23.296319775027346, Replay Buffer Size: 39824\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1145, Reward: -0.2665251965725791, Moving Avg Reward: -23.296319775027346, Replay Buffer Size: 39824\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1146: Draw\n",
      "Episode 1146, Avg Value Loss: 3.173067806661129, Avg Policy Loss: 4.076774048805237\n",
      "Episode 1146, Reward: -20.931858393430126, Moving Avg Reward: -23.546780282857366, Replay Buffer Size: 39904\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1146, Reward: -20.931858393430126, Moving Avg Reward: -23.546780282857366, Replay Buffer Size: 39904\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1147: Draw\n",
      "Episode 1147, Avg Value Loss: 3.3321966379880905, Avg Policy Loss: 4.174953192472458\n",
      "Episode 1147, Reward: -50.983715811248864, Moving Avg Reward: -23.631198972922533, Replay Buffer Size: 39944\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1147, Reward: -50.983715811248864, Moving Avg Reward: -23.631198972922533, Replay Buffer Size: 39944\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1148: Lost\n",
      "Episode 1148, Avg Value Loss: 3.2563247211277484, Avg Policy Loss: 4.117177632451058\n",
      "Episode 1148, Reward: -41.831298751399615, Moving Avg Reward: -24.234661467944942, Replay Buffer Size: 40024\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1148, Reward: -41.831298751399615, Moving Avg Reward: -24.234661467944942, Replay Buffer Size: 40024\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1149: Lost\n",
      "Episode 1149, Avg Value Loss: 3.4069869041442873, Avg Policy Loss: 4.071375751495362\n",
      "Episode 1149, Reward: -29.849021783950118, Moving Avg Reward: -24.29749525018136, Replay Buffer Size: 40034\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1149, Reward: -29.849021783950118, Moving Avg Reward: -24.29749525018136, Replay Buffer Size: 40034\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1150: Lost\n",
      "Episode 1150, Avg Value Loss: 2.7971959222446787, Avg Policy Loss: 4.064556642012163\n",
      "Episode 1150, Reward: -32.48360608743922, Moving Avg Reward: -23.729505756778263, Replay Buffer Size: 40045\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1150, Reward: -32.48360608743922, Moving Avg Reward: -23.729505756778263, Replay Buffer Size: 40045\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1151: Lost\n",
      "Episode 1151, Avg Value Loss: 3.205898490819064, Avg Policy Loss: 4.268380685286089\n",
      "Episode 1151, Reward: -23.295342431484087, Moving Avg Reward: -23.877867789320018, Replay Buffer Size: 40056\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1151, Reward: -23.295342431484087, Moving Avg Reward: -23.877867789320018, Replay Buffer Size: 40056\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1152: Lost\n",
      "Episode 1152, Avg Value Loss: 4.340211391448975, Avg Policy Loss: 3.8660261971609935\n",
      "Episode 1152, Reward: -21.849077918020104, Moving Avg Reward: -23.736726074670532, Replay Buffer Size: 40063\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1152, Reward: -21.849077918020104, Moving Avg Reward: -23.736726074670532, Replay Buffer Size: 40063\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1153: Lost\n",
      "Episode 1153, Avg Value Loss: 3.5747738643126055, Avg Policy Loss: 4.174098838459361\n",
      "Episode 1153, Reward: -30.061575416359503, Moving Avg Reward: -23.82494393405873, Replay Buffer Size: 40074\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1153, Reward: -30.061575416359503, Moving Avg Reward: -23.82494393405873, Replay Buffer Size: 40074\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1154: Lost\n",
      "Episode 1154, Avg Value Loss: 3.2850305646657945, Avg Policy Loss: 4.032687002420426\n",
      "Episode 1154, Reward: -0.693903094374132, Moving Avg Reward: -23.918409554374204, Replay Buffer Size: 40154\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1154, Reward: -0.693903094374132, Moving Avg Reward: -23.918409554374204, Replay Buffer Size: 40154\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1155: Lost\n",
      "Episode 1155, Avg Value Loss: 3.3777060955762863, Avg Policy Loss: 4.072709712386131\n",
      "Episode 1155, Reward: -33.86727532244209, Moving Avg Reward: -23.955362241191594, Replay Buffer Size: 40234\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1155, Reward: -33.86727532244209, Moving Avg Reward: -23.955362241191594, Replay Buffer Size: 40234\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1156: Draw\n",
      "Episode 1156, Avg Value Loss: 2.4029582291841507, Avg Policy Loss: 4.326324641704559\n",
      "Episode 1156, Reward: -24.238515101981527, Moving Avg Reward: -23.893985756101085, Replay Buffer Size: 40242\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1156, Reward: -24.238515101981527, Moving Avg Reward: -23.893985756101085, Replay Buffer Size: 40242\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1157: Lost\n",
      "Episode 1157, Avg Value Loss: 3.183846131960551, Avg Policy Loss: 4.116205819447836\n",
      "Episode 1157, Reward: 9.85, Moving Avg Reward: -23.041773872964715, Replay Buffer Size: 40257\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1157, Reward: 9.85, Moving Avg Reward: -23.041773872964715, Replay Buffer Size: 40257\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1158: Won\n",
      "Episode 1158, Avg Value Loss: 2.424623111883799, Avg Policy Loss: 4.1743390162785845\n",
      "Episode 1158, Reward: -17.157960900078557, Moving Avg Reward: -23.093957896819358, Replay Buffer Size: 40263\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1158, Reward: -17.157960900078557, Moving Avg Reward: -23.093957896819358, Replay Buffer Size: 40263\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1159: Won\n",
      "Episode 1159, Avg Value Loss: 3.43414749882438, Avg Policy Loss: 4.048217556693337\n",
      "Episode 1159, Reward: -30.46321142011923, Moving Avg Reward: -23.179912887750614, Replay Buffer Size: 40274\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1159, Reward: -30.46321142011923, Moving Avg Reward: -23.179912887750614, Replay Buffer Size: 40274\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1160: Won\n",
      "Episode 1160, Avg Value Loss: 4.163720945517222, Avg Policy Loss: 3.915266275405884\n",
      "Episode 1160, Reward: -29.17055697075498, Moving Avg Reward: -22.820745257471415, Replay Buffer Size: 40286\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1160, Reward: -29.17055697075498, Moving Avg Reward: -22.820745257471415, Replay Buffer Size: 40286\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1161: Won\n",
      "Episode 1161, Avg Value Loss: 3.2711342022969174, Avg Policy Loss: 4.14644193649292\n",
      "Episode 1161, Reward: -30.68754818426244, Moving Avg Reward: -22.86564116471634, Replay Buffer Size: 40299\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1161, Reward: -30.68754818426244, Moving Avg Reward: -22.86564116471634, Replay Buffer Size: 40299\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1162: Won\n",
      "Episode 1162, Avg Value Loss: 3.011633136055686, Avg Policy Loss: 4.143479390577837\n",
      "Episode 1162, Reward: -28.15815414852578, Moving Avg Reward: -22.907597198716186, Replay Buffer Size: 40310\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1162, Reward: -28.15815414852578, Moving Avg Reward: -22.907597198716186, Replay Buffer Size: 40310\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1163: Won\n",
      "Episode 1163, Avg Value Loss: 3.6814861779517316, Avg Policy Loss: 3.969701300276087\n",
      "Episode 1163, Reward: -21.95821275853561, Moving Avg Reward: -23.278569370642604, Replay Buffer Size: 40357\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1163, Reward: -21.95821275853561, Moving Avg Reward: -23.278569370642604, Replay Buffer Size: 40357\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1164: Won\n",
      "Episode 1164, Avg Value Loss: 3.5320732402801513, Avg Policy Loss: 4.209100513458252\n",
      "Episode 1164, Reward: 19.195548973335836, Moving Avg Reward: -22.79534383652714, Replay Buffer Size: 40382\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1164, Reward: 19.195548973335836, Moving Avg Reward: -22.79534383652714, Replay Buffer Size: 40382\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1165: Won\n",
      "Episode 1165, Avg Value Loss: 3.6665276090304055, Avg Policy Loss: 3.9888441363970437\n",
      "Episode 1165, Reward: -28.70092028890533, Moving Avg Reward: -23.274627379034392, Replay Buffer Size: 40394\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1165, Reward: -28.70092028890533, Moving Avg Reward: -23.274627379034392, Replay Buffer Size: 40394\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1166: Won\n",
      "Episode 1166, Avg Value Loss: 2.9498248270579746, Avg Policy Loss: 4.0111240318843295\n",
      "Episode 1166, Reward: -24.380714995260405, Moving Avg Reward: -23.233467337838917, Replay Buffer Size: 40401\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 1166, Reward: -24.380714995260405, Moving Avg Reward: -23.233467337838917, Replay Buffer Size: 40401\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1167: Won\n",
      "Episode 1167, Avg Value Loss: 3.162846096924373, Avg Policy Loss: 4.148807942867279\n",
      "Episode 1167, Reward: -50.93513146856858, Moving Avg Reward: -23.649520131185838, Replay Buffer Size: 40429\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1167, Reward: -50.93513146856858, Moving Avg Reward: -23.649520131185838, Replay Buffer Size: 40429\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1168: Won\n",
      "Episode 1168, Avg Value Loss: 3.2828186750411987, Avg Policy Loss: 4.097983029755679\n",
      "Episode 1168, Reward: -39.42082096134777, Moving Avg Reward: -23.69966308873178, Replay Buffer Size: 40473\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1168, Reward: -39.42082096134777, Moving Avg Reward: -23.69966308873178, Replay Buffer Size: 40473\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1169: Won\n",
      "Episode 1169, Avg Value Loss: 3.292694485412454, Avg Policy Loss: 4.143963548372376\n",
      "Episode 1169, Reward: -50.69081172704246, Moving Avg Reward: -24.335000059663834, Replay Buffer Size: 40526\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1169, Reward: -50.69081172704246, Moving Avg Reward: -24.335000059663834, Replay Buffer Size: 40526\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1170: Won\n",
      "Episode 1170, Avg Value Loss: 4.007627218961716, Avg Policy Loss: 4.021508723497391\n",
      "Episode 1170, Reward: -23.960303814572733, Moving Avg Reward: -24.768569020612436, Replay Buffer Size: 40534\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1170, Reward: -23.960303814572733, Moving Avg Reward: -24.768569020612436, Replay Buffer Size: 40534\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1171: Lost\n",
      "Episode 1171, Avg Value Loss: 2.8874685118595758, Avg Policy Loss: 4.026305099328359\n",
      "Episode 1171, Reward: -34.16203050340031, Moving Avg Reward: -24.767369392905444, Replay Buffer Size: 40546\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1171, Reward: -34.16203050340031, Moving Avg Reward: -24.767369392905444, Replay Buffer Size: 40546\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1172: Lost\n",
      "Episode 1172, Avg Value Loss: 3.1488136507570745, Avg Policy Loss: 4.097907927632332\n",
      "Episode 1172, Reward: 3.013746680521429, Moving Avg Reward: -24.46416695860604, Replay Buffer Size: 40626\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1172, Reward: 3.013746680521429, Moving Avg Reward: -24.46416695860604, Replay Buffer Size: 40626\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1173: Lost\n",
      "Episode 1173, Avg Value Loss: 3.6482985615730286, Avg Policy Loss: 4.321032285690308\n",
      "Episode 1173, Reward: -25.361516019730168, Moving Avg Reward: -24.320466462204298, Replay Buffer Size: 40634\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1173, Reward: -25.361516019730168, Moving Avg Reward: -24.320466462204298, Replay Buffer Size: 40634\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1174: Lost\n",
      "Episode 1174, Avg Value Loss: 2.818784290552139, Avg Policy Loss: 3.9318602800369264\n",
      "Episode 1174, Reward: -29.570376339333706, Moving Avg Reward: -24.35170811765615, Replay Buffer Size: 40644\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1174, Reward: -29.570376339333706, Moving Avg Reward: -24.35170811765615, Replay Buffer Size: 40644\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1175: Lost\n",
      "Episode 1175, Avg Value Loss: 2.9983193332498725, Avg Policy Loss: 3.967034686695446\n",
      "Episode 1175, Reward: -24.55317065990633, Moving Avg Reward: -24.473442669611142, Replay Buffer Size: 40655\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1175, Reward: -24.55317065990633, Moving Avg Reward: -24.473442669611142, Replay Buffer Size: 40655\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1176: Lost\n",
      "Episode 1176, Avg Value Loss: 3.6500094069374933, Avg Policy Loss: 4.080492231580946\n",
      "Episode 1176, Reward: -24.915622443004178, Moving Avg Reward: -24.7583345709723, Replay Buffer Size: 40664\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1176, Reward: -24.915622443004178, Moving Avg Reward: -24.7583345709723, Replay Buffer Size: 40664\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1177: Lost\n",
      "Episode 1177, Avg Value Loss: 3.3901627980172635, Avg Policy Loss: 4.059095257520676\n",
      "Episode 1177, Reward: -13.1891139648331, Moving Avg Reward: -24.608416627720427, Replay Buffer Size: 40744\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1177, Reward: -13.1891139648331, Moving Avg Reward: -24.608416627720427, Replay Buffer Size: 40744\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1178: Lost\n",
      "Episode 1178, Avg Value Loss: 3.9215511083602905, Avg Policy Loss: 3.9236603577931723\n",
      "Episode 1178, Reward: -25.98414144139982, Moving Avg Reward: -24.490439268181017, Replay Buffer Size: 40756\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1178, Reward: -25.98414144139982, Moving Avg Reward: -24.490439268181017, Replay Buffer Size: 40756\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1179: Lost\n",
      "Episode 1179, Avg Value Loss: 3.4910234808921814, Avg Policy Loss: 4.123612940311432\n",
      "Episode 1179, Reward: -27.849268272963617, Moving Avg Reward: -24.4456828402949, Replay Buffer Size: 40764\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1179, Reward: -27.849268272963617, Moving Avg Reward: -24.4456828402949, Replay Buffer Size: 40764\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1180: Lost\n",
      "Episode 1180, Avg Value Loss: 3.4312657872835794, Avg Policy Loss: 3.9942490220069886\n",
      "Episode 1180, Reward: -66.04635028546807, Moving Avg Reward: -24.52995086836704, Replay Buffer Size: 40824\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1180, Reward: -66.04635028546807, Moving Avg Reward: -24.52995086836704, Replay Buffer Size: 40824\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1181: Lost\n",
      "Episode 1181, Avg Value Loss: 2.7729261994361876, Avg Policy Loss: 4.056934571266174\n",
      "Episode 1181, Reward: -27.506527943562944, Moving Avg Reward: -24.98446979951521, Replay Buffer Size: 40834\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1181, Reward: -27.506527943562944, Moving Avg Reward: -24.98446979951521, Replay Buffer Size: 40834\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1182: Lost\n",
      "Episode 1182, Avg Value Loss: 3.7802774722759542, Avg Policy Loss: 4.103867970980131\n",
      "Episode 1182, Reward: -38.54258112252553, Moving Avg Reward: -25.03049603835512, Replay Buffer Size: 40847\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1182, Reward: -38.54258112252553, Moving Avg Reward: -25.03049603835512, Replay Buffer Size: 40847\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1183: Lost\n",
      "Episode 1183, Avg Value Loss: 3.7172044813632965, Avg Policy Loss: 4.040121346712112\n",
      "Episode 1183, Reward: -25.860538090935584, Moving Avg Reward: -25.099849202018262, Replay Buffer Size: 40855\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1183, Reward: -25.860538090935584, Moving Avg Reward: -25.099849202018262, Replay Buffer Size: 40855\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1184: Lost\n",
      "Episode 1184, Avg Value Loss: 4.002754423353407, Avg Policy Loss: 4.16003696123759\n",
      "Episode 1184, Reward: -27.2923426240227, Moving Avg Reward: -25.092431669321595, Replay Buffer Size: 40864\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1184, Reward: -27.2923426240227, Moving Avg Reward: -25.092431669321595, Replay Buffer Size: 40864\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1185: Lost\n",
      "Episode 1185, Avg Value Loss: 3.2458042688667774, Avg Policy Loss: 4.074414214491844\n",
      "Episode 1185, Reward: -32.840402506105214, Moving Avg Reward: -25.10315563481083, Replay Buffer Size: 40944\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1185, Reward: -32.840402506105214, Moving Avg Reward: -25.10315563481083, Replay Buffer Size: 40944\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1186: Lost\n",
      "Episode 1186, Avg Value Loss: 3.1054094742084373, Avg Policy Loss: 4.116858408368867\n",
      "Episode 1186, Reward: 15.923650140137278, Moving Avg Reward: -24.598726053075808, Replay Buffer Size: 40973\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1186, Reward: 15.923650140137278, Moving Avg Reward: -24.598726053075808, Replay Buffer Size: 40973\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1187: Won\n",
      "Episode 1187, Avg Value Loss: 3.2540205206189836, Avg Policy Loss: 4.194314820425851\n",
      "Episode 1187, Reward: -24.225099139372862, Moving Avg Reward: -25.025703291728448, Replay Buffer Size: 40980\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1187, Reward: -24.225099139372862, Moving Avg Reward: -25.025703291728448, Replay Buffer Size: 40980\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1188: Won\n",
      "Episode 1188, Avg Value Loss: 2.954715622795953, Avg Policy Loss: 4.0328176551395\n",
      "Episode 1188, Reward: -25.75466636282983, Moving Avg Reward: -24.979478023973826, Replay Buffer Size: 40989\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1188, Reward: -25.75466636282983, Moving Avg Reward: -24.979478023973826, Replay Buffer Size: 40989\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1189: Won\n",
      "Episode 1189, Avg Value Loss: 3.083847904205322, Avg Policy Loss: 4.080226564407349\n",
      "Episode 1189, Reward: -26.471744196474866, Moving Avg Reward: -25.236246189453542, Replay Buffer Size: 40999\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1189, Reward: -26.471744196474866, Moving Avg Reward: -25.236246189453542, Replay Buffer Size: 40999\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1190: Won\n",
      "Episode 1190, Avg Value Loss: 3.143357951532711, Avg Policy Loss: 4.1129922288836855\n",
      "Episode 1190, Reward: -30.3404734282554, Moving Avg Reward: -25.29608762348463, Replay Buffer Size: 41065\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1190, Reward: -30.3404734282554, Moving Avg Reward: -25.29608762348463, Replay Buffer Size: 41065\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1191: Won\n",
      "Episode 1191, Avg Value Loss: 3.734640876452128, Avg Policy Loss: 4.2374752627478705\n",
      "Episode 1191, Reward: -27.101732330172798, Moving Avg Reward: -25.261331779684333, Replay Buffer Size: 41074\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1191, Reward: -27.101732330172798, Moving Avg Reward: -25.261331779684333, Replay Buffer Size: 41074\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1192: Won\n",
      "Episode 1192, Avg Value Loss: 3.6714003533124924, Avg Policy Loss: 3.9725957087108066\n",
      "Episode 1192, Reward: -52.81210896955597, Moving Avg Reward: -25.403663557649516, Replay Buffer Size: 41130\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1192, Reward: -52.81210896955597, Moving Avg Reward: -25.403663557649516, Replay Buffer Size: 41130\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1193: Lost\n",
      "Episode 1193, Avg Value Loss: 3.18256676197052, Avg Policy Loss: 4.147104813502385\n",
      "Episode 1193, Reward: -34.70260756466503, Moving Avg Reward: -25.50643568535194, Replay Buffer Size: 41143\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1193, Reward: -34.70260756466503, Moving Avg Reward: -25.50643568535194, Replay Buffer Size: 41143\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1194: Lost\n",
      "Episode 1194, Avg Value Loss: 3.195374690569364, Avg Policy Loss: 3.8919748159555287\n",
      "Episode 1194, Reward: -34.99088450562438, Moving Avg Reward: -25.467542431112324, Replay Buffer Size: 41156\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1194, Reward: -34.99088450562438, Moving Avg Reward: -25.467542431112324, Replay Buffer Size: 41156\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1195: Lost\n",
      "Episode 1195, Avg Value Loss: 3.2862319548924765, Avg Policy Loss: 4.197520653406779\n",
      "Episode 1195, Reward: -31.057286216633564, Moving Avg Reward: -25.4794870817619, Replay Buffer Size: 41168\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1195, Reward: -31.057286216633564, Moving Avg Reward: -25.4794870817619, Replay Buffer Size: 41168\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1196: Lost\n",
      "Episode 1196, Avg Value Loss: 2.9294869850079217, Avg Policy Loss: 4.088848392168681\n",
      "Episode 1196, Reward: 18.19066262705552, Moving Avg Reward: -25.053147880365486, Replay Buffer Size: 41192\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1196, Reward: 18.19066262705552, Moving Avg Reward: -25.053147880365486, Replay Buffer Size: 41192\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1197: Won\n",
      "Episode 1197, Avg Value Loss: 3.5133030712604523, Avg Policy Loss: 4.153604090213776\n",
      "Episode 1197, Reward: -21.873351677191494, Moving Avg Reward: -24.94654939168528, Replay Buffer Size: 41200\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1197, Reward: -21.873351677191494, Moving Avg Reward: -24.94654939168528, Replay Buffer Size: 41200\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1198: Won\n",
      "Episode 1198, Avg Value Loss: 3.488239516104971, Avg Policy Loss: 4.233794757298061\n",
      "Episode 1198, Reward: -53.89169856866863, Moving Avg Reward: -25.671657071474463, Replay Buffer Size: 41228\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1198, Reward: -53.89169856866863, Moving Avg Reward: -25.671657071474463, Replay Buffer Size: 41228\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1199: Won\n",
      "Episode 1199, Avg Value Loss: 2.9607964356740317, Avg Policy Loss: 4.1316591368781195\n",
      "Episode 1199, Reward: -30.54650315212689, Moving Avg Reward: -25.71593513538559, Replay Buffer Size: 41237\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1199, Reward: -30.54650315212689, Moving Avg Reward: -25.71593513538559, Replay Buffer Size: 41237\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1200: Won\n",
      "Episode 1200, Avg Value Loss: 2.633923496518816, Avg Policy Loss: 3.9984752450670515\n",
      "Episode 1200, Reward: -22.16920116839503, Moving Avg Reward: -25.63088563730257, Replay Buffer Size: 41244\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1200, Reward: -22.16920116839503, Moving Avg Reward: -25.63088563730257, Replay Buffer Size: 41244\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1201: Won\n",
      "Episode 1201, Avg Value Loss: 3.6930085288153753, Avg Policy Loss: 4.264809025658502\n",
      "Episode 1201, Reward: -23.052162286309922, Moving Avg Reward: -25.58393557473198, Replay Buffer Size: 41253\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1201, Reward: -23.052162286309922, Moving Avg Reward: -25.58393557473198, Replay Buffer Size: 41253\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1202: Won\n",
      "Episode 1202, Avg Value Loss: 3.6820398677479136, Avg Policy Loss: 4.023865548047152\n",
      "Episode 1202, Reward: -31.494013866040362, Moving Avg Reward: -25.647430512244842, Replay Buffer Size: 41264\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1202, Reward: -31.494013866040362, Moving Avg Reward: -25.647430512244842, Replay Buffer Size: 41264\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1203: Won\n",
      "Episode 1203, Avg Value Loss: 3.4521740451455116, Avg Policy Loss: 4.1791105538606645\n",
      "Episode 1203, Reward: -120.61836474459304, Moving Avg Reward: -27.035303314941014, Replay Buffer Size: 41344\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1203, Reward: -120.61836474459304, Moving Avg Reward: -27.035303314941014, Replay Buffer Size: 41344\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1204: Lost\n",
      "Episode 1204, Avg Value Loss: 3.2520348951220512, Avg Policy Loss: 4.148028981685639\n",
      "Episode 1204, Reward: -3.9304224522064546, Moving Avg Reward: -26.618963997366926, Replay Buffer Size: 41424\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1204, Reward: -3.9304224522064546, Moving Avg Reward: -26.618963997366926, Replay Buffer Size: 41424\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1205: Draw\n",
      "Episode 1205, Avg Value Loss: 3.5427878007292746, Avg Policy Loss: 4.161519482731819\n",
      "Episode 1205, Reward: -69.25930035529278, Moving Avg Reward: -26.995452907562186, Replay Buffer Size: 41504\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1205, Reward: -69.25930035529278, Moving Avg Reward: -26.995452907562186, Replay Buffer Size: 41504\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1206: Draw\n",
      "Episode 1206, Avg Value Loss: 3.281911070857729, Avg Policy Loss: 4.123395528112139\n",
      "Episode 1206, Reward: -7.217755710004424, Moving Avg Reward: -26.82712168140352, Replay Buffer Size: 41532\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1206, Reward: -7.217755710004424, Moving Avg Reward: -26.82712168140352, Replay Buffer Size: 41532\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1207: Won\n",
      "Episode 1207, Avg Value Loss: 3.979540917608473, Avg Policy Loss: 4.031286928388807\n",
      "Episode 1207, Reward: 16.543725635723995, Moving Avg Reward: -26.352908059400516, Replay Buffer Size: 41550\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1207, Reward: 16.543725635723995, Moving Avg Reward: -26.352908059400516, Replay Buffer Size: 41550\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1208: Won\n",
      "Episode 1208, Avg Value Loss: 3.134091453254223, Avg Policy Loss: 4.183478957414627\n",
      "Episode 1208, Reward: -31.44559225860702, Moving Avg Reward: -26.44958341245438, Replay Buffer Size: 41630\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1208, Reward: -31.44559225860702, Moving Avg Reward: -26.44958341245438, Replay Buffer Size: 41630\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1209: Won\n",
      "Episode 1209, Avg Value Loss: 3.440229735591195, Avg Policy Loss: 4.203898375684565\n",
      "Episode 1209, Reward: -52.69682817462709, Moving Avg Reward: -26.658041030293788, Replay Buffer Size: 41652\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1209, Reward: -52.69682817462709, Moving Avg Reward: -26.658041030293788, Replay Buffer Size: 41652\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1210: Won\n",
      "Episode 1210, Avg Value Loss: 3.349674646671002, Avg Policy Loss: 4.237022491601797\n",
      "Episode 1210, Reward: -36.112023282826144, Moving Avg Reward: -26.702160205392175, Replay Buffer Size: 41665\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1210, Reward: -36.112023282826144, Moving Avg Reward: -26.702160205392175, Replay Buffer Size: 41665\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1211: Won\n",
      "Episode 1211, Avg Value Loss: 3.2722119078040124, Avg Policy Loss: 4.168488922715187\n",
      "Episode 1211, Reward: -56.57458128435884, Moving Avg Reward: -26.989680375852735, Replay Buffer Size: 41745\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1211, Reward: -56.57458128435884, Moving Avg Reward: -26.989680375852735, Replay Buffer Size: 41745\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1212: Lost\n",
      "Episode 1212, Avg Value Loss: 3.261043961231525, Avg Policy Loss: 4.214054015966562\n",
      "Episode 1212, Reward: -30.630926969398136, Moving Avg Reward: -27.089889087313818, Replay Buffer Size: 41758\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.68\n",
      "Episode 1212, Reward: -30.630926969398136, Moving Avg Reward: -27.089889087313818, Replay Buffer Size: 41758\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1213: Lost\n",
      "Episode 1213, Avg Value Loss: 3.3065146297216415, Avg Policy Loss: 4.112204965949059\n",
      "Episode 1213, Reward: -22.696308911081537, Moving Avg Reward: -26.96715019888578, Replay Buffer Size: 41838\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 1213, Reward: -22.696308911081537, Moving Avg Reward: -26.96715019888578, Replay Buffer Size: 41838\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1214: Lost\n",
      "Episode 1214, Avg Value Loss: 3.304584671469296, Avg Policy Loss: 3.9642037503859577\n",
      "Episode 1214, Reward: -30.727136129758264, Moving Avg Reward: -27.425336727118438, Replay Buffer Size: 41855\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1214, Reward: -30.727136129758264, Moving Avg Reward: -27.425336727118438, Replay Buffer Size: 41855\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1215: Lost\n",
      "Episode 1215, Avg Value Loss: 3.16178437769413, Avg Policy Loss: 4.175409558415413\n",
      "Episode 1215, Reward: -58.770129667858015, Moving Avg Reward: -27.75757460068502, Replay Buffer Size: 41935\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1215, Reward: -58.770129667858015, Moving Avg Reward: -27.75757460068502, Replay Buffer Size: 41935\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1216: Lost\n",
      "Episode 1216, Avg Value Loss: 2.9045374542474747, Avg Policy Loss: 4.146844118833542\n",
      "Episode 1216, Reward: -23.158685587282562, Moving Avg Reward: -27.65626003468897, Replay Buffer Size: 41943\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1216, Reward: -23.158685587282562, Moving Avg Reward: -27.65626003468897, Replay Buffer Size: 41943\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1217: Lost\n",
      "Episode 1217, Avg Value Loss: 3.5619834264119468, Avg Policy Loss: 4.203623245159785\n",
      "Episode 1217, Reward: 20.329456130177775, Moving Avg Reward: -27.164673918499048, Replay Buffer Size: 41967\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1217, Reward: 20.329456130177775, Moving Avg Reward: -27.164673918499048, Replay Buffer Size: 41967\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1218: Won\n",
      "Episode 1218, Avg Value Loss: 2.885483438318426, Avg Policy Loss: 4.315708853981712\n",
      "Episode 1218, Reward: -28.55984284411188, Moving Avg Reward: -27.134108013711785, Replay Buffer Size: 41978\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1218, Reward: -28.55984284411188, Moving Avg Reward: -27.134108013711785, Replay Buffer Size: 41978\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1219: Won\n",
      "Episode 1219, Avg Value Loss: 3.4851484497388205, Avg Policy Loss: 4.036847114562988\n",
      "Episode 1219, Reward: -22.47287210858117, Moving Avg Reward: -27.212808954326096, Replay Buffer Size: 41990\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 1219, Reward: -22.47287210858117, Moving Avg Reward: -27.212808954326096, Replay Buffer Size: 41990\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1220: Won\n",
      "Episode 1220, Avg Value Loss: 3.6279953585730658, Avg Policy Loss: 4.316771825154622\n",
      "Episode 1220, Reward: -21.3187263184624, Moving Avg Reward: -26.776738158655814, Replay Buffer Size: 41999\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 1220, Reward: -21.3187263184624, Moving Avg Reward: -26.776738158655814, Replay Buffer Size: 41999\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1221: Won\n",
      "Episode 1221, Avg Value Loss: 3.019515132904053, Avg Policy Loss: 4.134019432067871\n",
      "Episode 1221, Reward: 13.81594749887954, Moving Avg Reward: -26.73847868366701, Replay Buffer Size: 42024\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1221, Reward: 13.81594749887954, Moving Avg Reward: -26.73847868366701, Replay Buffer Size: 42024\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1222: Won\n",
      "Episode 1222, Avg Value Loss: 4.0586010951262255, Avg Policy Loss: 3.99246228658236\n",
      "Episode 1222, Reward: -30.619395262298703, Moving Avg Reward: -27.144572636290007, Replay Buffer Size: 42037\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1222, Reward: -30.619395262298703, Moving Avg Reward: -27.144572636290007, Replay Buffer Size: 42037\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1223: Won\n",
      "Episode 1223, Avg Value Loss: 3.5018324984444513, Avg Policy Loss: 4.161966032452053\n",
      "Episode 1223, Reward: -6.751873478260629, Moving Avg Reward: -26.260060846585983, Replay Buffer Size: 42046\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1223, Reward: -6.751873478260629, Moving Avg Reward: -26.260060846585983, Replay Buffer Size: 42046\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1224: Won\n",
      "Episode 1224, Avg Value Loss: 3.4601893390434375, Avg Policy Loss: 4.073679664860601\n",
      "Episode 1224, Reward: -48.60924074482259, Moving Avg Reward: -26.49666035472693, Replay Buffer Size: 42115\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1224, Reward: -48.60924074482259, Moving Avg Reward: -26.49666035472693, Replay Buffer Size: 42115\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1225: Won\n",
      "Episode 1225, Avg Value Loss: 3.259151266171382, Avg Policy Loss: 4.081359056326059\n",
      "Episode 1225, Reward: -29.275674579794693, Moving Avg Reward: -26.553251421872364, Replay Buffer Size: 42128\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1225, Reward: -29.275674579794693, Moving Avg Reward: -26.553251421872364, Replay Buffer Size: 42128\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1226: Won\n",
      "Episode 1226, Avg Value Loss: 3.098514336126822, Avg Policy Loss: 4.231374996679801\n",
      "Episode 1226, Reward: 16.16630986020524, Moving Avg Reward: -26.090064759004676, Replay Buffer Size: 42155\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 1226, Reward: 16.16630986020524, Moving Avg Reward: -26.090064759004676, Replay Buffer Size: 42155\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1227: Won\n",
      "Episode 1227, Avg Value Loss: 3.2210787504911425, Avg Policy Loss: 4.183015769720077\n",
      "Episode 1227, Reward: 1.819036734616433, Moving Avg Reward: -25.654217823973166, Replay Buffer Size: 42235\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1227, Reward: 1.819036734616433, Moving Avg Reward: -25.654217823973166, Replay Buffer Size: 42235\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1228: Won\n",
      "Episode 1228, Avg Value Loss: 3.1958914778449317, Avg Policy Loss: 4.2587079351598565\n",
      "Episode 1228, Reward: -29.67120509587339, Moving Avg Reward: -25.691274124993985, Replay Buffer Size: 42246\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 1228, Reward: -29.67120509587339, Moving Avg Reward: -25.691274124993985, Replay Buffer Size: 42246\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1229: Won\n",
      "Episode 1229, Avg Value Loss: 3.072630931349362, Avg Policy Loss: 4.059236021602855\n",
      "Episode 1229, Reward: -51.2600570249578, Moving Avg Reward: -25.941212718937322, Replay Buffer Size: 42297\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1229, Reward: -51.2600570249578, Moving Avg Reward: -25.941212718937322, Replay Buffer Size: 42297\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1230: Won\n",
      "Episode 1230, Avg Value Loss: 3.3465797702471414, Avg Policy Loss: 4.126335223515828\n",
      "Episode 1230, Reward: -28.123695088331054, Moving Avg Reward: -26.00578145066049, Replay Buffer Size: 42309\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1230, Reward: -28.123695088331054, Moving Avg Reward: -26.00578145066049, Replay Buffer Size: 42309\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1231: Lost\n",
      "Episode 1231, Avg Value Loss: 3.1998720921576025, Avg Policy Loss: 4.137040373682976\n",
      "Episode 1231, Reward: 8.964135068323047, Moving Avg Reward: -25.594959647392486, Replay Buffer Size: 42389\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1231, Reward: 8.964135068323047, Moving Avg Reward: -25.594959647392486, Replay Buffer Size: 42389\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1232: Lost\n",
      "Episode 1232, Avg Value Loss: 2.8055028404508318, Avg Policy Loss: 4.029974290302822\n",
      "Episode 1232, Reward: -22.078544827693186, Moving Avg Reward: -25.544668437956965, Replay Buffer Size: 42396\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1232, Reward: -22.078544827693186, Moving Avg Reward: -25.544668437956965, Replay Buffer Size: 42396\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1233: Lost\n",
      "Episode 1233, Avg Value Loss: 3.0312785042656794, Avg Policy Loss: 4.130161815219456\n",
      "Episode 1233, Reward: -27.190409070856173, Moving Avg Reward: -25.58158177351359, Replay Buffer Size: 42405\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 1233, Reward: -27.190409070856173, Moving Avg Reward: -25.58158177351359, Replay Buffer Size: 42405\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1234: Lost\n",
      "Episode 1234, Avg Value Loss: 3.2094740186418806, Avg Policy Loss: 4.344944000244141\n",
      "Episode 1234, Reward: -19.3434896377299, Moving Avg Reward: -25.495717329867702, Replay Buffer Size: 42412\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1234, Reward: -19.3434896377299, Moving Avg Reward: -25.495717329867702, Replay Buffer Size: 42412\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1235: Lost\n",
      "Episode 1235, Avg Value Loss: 3.3830953046679495, Avg Policy Loss: 4.145563071966171\n",
      "Episode 1235, Reward: 2.7655904560718745, Moving Avg Reward: -25.59314785321638, Replay Buffer Size: 42492\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1235, Reward: 2.7655904560718745, Moving Avg Reward: -25.59314785321638, Replay Buffer Size: 42492\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1236: Lost\n",
      "Episode 1236, Avg Value Loss: 3.366620797377366, Avg Policy Loss: 4.191242098808289\n",
      "Episode 1236, Reward: 16.92339918741655, Moving Avg Reward: -25.408057043648313, Replay Buffer Size: 42518\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1236, Reward: 16.92339918741655, Moving Avg Reward: -25.408057043648313, Replay Buffer Size: 42518\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1237: Won\n",
      "Episode 1237, Avg Value Loss: 3.234260762865479, Avg Policy Loss: 4.232869280351175\n",
      "Episode 1237, Reward: -8.925830329099165, Moving Avg Reward: -25.267849518656966, Replay Buffer Size: 42592\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1237, Reward: -8.925830329099165, Moving Avg Reward: -25.267849518656966, Replay Buffer Size: 42592\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1238: Won\n",
      "Episode 1238, Avg Value Loss: 4.100994691252708, Avg Policy Loss: 4.210978090763092\n",
      "Episode 1238, Reward: -25.418800406739052, Moving Avg Reward: -25.150621789877867, Replay Buffer Size: 42600\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1238, Reward: -25.418800406739052, Moving Avg Reward: -25.150621789877867, Replay Buffer Size: 42600\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1239: Won\n",
      "Episode 1239, Avg Value Loss: 3.0479335635900497, Avg Policy Loss: 4.049209743738174\n",
      "Episode 1239, Reward: -27.145541220415357, Moving Avg Reward: -25.026037686711344, Replay Buffer Size: 42608\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1239, Reward: -27.145541220415357, Moving Avg Reward: -25.026037686711344, Replay Buffer Size: 42608\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1240: Won\n",
      "Episode 1240, Avg Value Loss: 3.121020221710205, Avg Policy Loss: 4.324489450454712\n",
      "Episode 1240, Reward: -25.367345216955172, Moving Avg Reward: -25.031528776062117, Replay Buffer Size: 42618\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1240, Reward: -25.367345216955172, Moving Avg Reward: -25.031528776062117, Replay Buffer Size: 42618\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1241: Won\n",
      "Episode 1241, Avg Value Loss: 3.2007181231792154, Avg Policy Loss: 4.020995873671311\n",
      "Episode 1241, Reward: -35.70120309305907, Moving Avg Reward: -25.121522232947576, Replay Buffer Size: 42631\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1241, Reward: -35.70120309305907, Moving Avg Reward: -25.121522232947576, Replay Buffer Size: 42631\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1242: Won\n",
      "Episode 1242, Avg Value Loss: 3.6436308452061246, Avg Policy Loss: 4.3177867616925925\n",
      "Episode 1242, Reward: -22.274547399268016, Moving Avg Reward: -25.11379609655936, Replay Buffer Size: 42638\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.68\n",
      "Episode 1242, Reward: -22.274547399268016, Moving Avg Reward: -25.11379609655936, Replay Buffer Size: 42638\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1243: Won\n",
      "Episode 1243, Avg Value Loss: 3.4087269968456693, Avg Policy Loss: 4.161486625671387\n",
      "Episode 1243, Reward: -24.628952665025206, Moving Avg Reward: -25.00718168999744, Replay Buffer Size: 42647\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1243, Reward: -24.628952665025206, Moving Avg Reward: -25.00718168999744, Replay Buffer Size: 42647\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1244: Won\n",
      "Episode 1244, Avg Value Loss: 3.334470518430074, Avg Policy Loss: 4.153057389789158\n",
      "Episode 1244, Reward: -48.31472511113186, Moving Avg Reward: -25.435823966550803, Replay Buffer Size: 42692\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1244, Reward: -48.31472511113186, Moving Avg Reward: -25.435823966550803, Replay Buffer Size: 42692\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1245: Won\n",
      "Episode 1245, Avg Value Loss: 3.5800896187623343, Avg Policy Loss: 3.975664178530375\n",
      "Episode 1245, Reward: -31.84922182854836, Moving Avg Reward: -25.75165093287056, Replay Buffer Size: 42704\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1245, Reward: -31.84922182854836, Moving Avg Reward: -25.75165093287056, Replay Buffer Size: 42704\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1246: Won\n",
      "Episode 1246, Avg Value Loss: 3.476077651232481, Avg Policy Loss: 4.199383261799812\n",
      "Episode 1246, Reward: -75.37112429962565, Moving Avg Reward: -26.296043591932513, Replay Buffer Size: 42784\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1246, Reward: -75.37112429962565, Moving Avg Reward: -26.296043591932513, Replay Buffer Size: 42784\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1247: Lost\n",
      "Episode 1247, Avg Value Loss: 3.164345852259932, Avg Policy Loss: 4.263617367580019\n",
      "Episode 1247, Reward: 17.39458211642669, Moving Avg Reward: -25.61226061265576, Replay Buffer Size: 42813\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1247, Reward: 17.39458211642669, Moving Avg Reward: -25.61226061265576, Replay Buffer Size: 42813\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1248: Won\n",
      "Episode 1248, Avg Value Loss: 3.840289980173111, Avg Policy Loss: 4.216645777225494\n",
      "Episode 1248, Reward: -22.23295574946075, Moving Avg Reward: -25.41627718263637, Replay Buffer Size: 42821\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1248, Reward: -22.23295574946075, Moving Avg Reward: -25.41627718263637, Replay Buffer Size: 42821\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1249: Won\n",
      "Episode 1249, Avg Value Loss: 3.6898708078596325, Avg Policy Loss: 4.335258748796251\n",
      "Episode 1249, Reward: -27.342871215398006, Moving Avg Reward: -25.391215676950857, Replay Buffer Size: 42830\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1249, Reward: -27.342871215398006, Moving Avg Reward: -25.391215676950857, Replay Buffer Size: 42830\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1250: Won\n",
      "Episode 1250, Avg Value Loss: 2.787052273750305, Avg Policy Loss: 4.1348390817642215\n",
      "Episode 1250, Reward: -25.795470897007363, Moving Avg Reward: -25.324334325046536, Replay Buffer Size: 42840\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1250, Reward: -25.795470897007363, Moving Avg Reward: -25.324334325046536, Replay Buffer Size: 42840\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1251: Won\n",
      "Episode 1251, Avg Value Loss: 2.895938813686371, Avg Policy Loss: 4.240045441521539\n",
      "Episode 1251, Reward: -28.157888159461823, Moving Avg Reward: -25.372959782326316, Replay Buffer Size: 42849\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1251, Reward: -28.157888159461823, Moving Avg Reward: -25.372959782326316, Replay Buffer Size: 42849\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1252: Won\n",
      "Episode 1252, Avg Value Loss: 3.683905854821205, Avg Policy Loss: 4.241626501083374\n",
      "Episode 1252, Reward: -24.09083276894772, Moving Avg Reward: -25.395377330835586, Replay Buffer Size: 42857\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1252, Reward: -24.09083276894772, Moving Avg Reward: -25.395377330835586, Replay Buffer Size: 42857\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1253: Won\n",
      "Episode 1253, Avg Value Loss: 3.124122122923533, Avg Policy Loss: 4.129846036434174\n",
      "Episode 1253, Reward: -27.530906882754294, Moving Avg Reward: -25.37007064549953, Replay Buffer Size: 42869\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1253, Reward: -27.530906882754294, Moving Avg Reward: -25.37007064549953, Replay Buffer Size: 42869\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1254: Won\n",
      "Episode 1254, Avg Value Loss: 2.8074908786349826, Avg Policy Loss: 4.331037097507053\n",
      "Episode 1254, Reward: -22.255470099471115, Moving Avg Reward: -25.585686315550497, Replay Buffer Size: 42878\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 1254, Reward: -22.255470099471115, Moving Avg Reward: -25.585686315550497, Replay Buffer Size: 42878\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1255: Won\n",
      "Episode 1255, Avg Value Loss: 3.4494377225637436, Avg Policy Loss: 4.146044182777405\n",
      "Episode 1255, Reward: -36.258133296177846, Moving Avg Reward: -25.609594895287856, Replay Buffer Size: 42958\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 1255, Reward: -36.258133296177846, Moving Avg Reward: -25.609594895287856, Replay Buffer Size: 42958\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1256: Won\n",
      "Episode 1256, Avg Value Loss: 3.267769310209486, Avg Policy Loss: 4.170664416419135\n",
      "Episode 1256, Reward: -26.027818375923466, Moving Avg Reward: -25.627487928027275, Replay Buffer Size: 42967\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 1256, Reward: -26.027818375923466, Moving Avg Reward: -25.627487928027275, Replay Buffer Size: 42967\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1257: Lost\n",
      "Episode 1257, Avg Value Loss: 3.0254988935258655, Avg Policy Loss: 4.152437766393025\n",
      "Episode 1257, Reward: -27.520816848435253, Moving Avg Reward: -26.001196096511627, Replay Buffer Size: 42976\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1257, Reward: -27.520816848435253, Moving Avg Reward: -26.001196096511627, Replay Buffer Size: 42976\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1258: Lost\n",
      "Episode 1258, Avg Value Loss: 3.3060126185417174, Avg Policy Loss: 4.168631601333618\n",
      "Episode 1258, Reward: -28.981803923187904, Moving Avg Reward: -26.119434526742715, Replay Buffer Size: 42986\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1258, Reward: -28.981803923187904, Moving Avg Reward: -26.119434526742715, Replay Buffer Size: 42986\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1259: Lost\n",
      "Episode 1259, Avg Value Loss: 3.9467870394388833, Avg Policy Loss: 4.1968446837531195\n",
      "Episode 1259, Reward: -27.009005870258065, Moving Avg Reward: -26.084892471244107, Replay Buffer Size: 42995\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1259, Reward: -27.009005870258065, Moving Avg Reward: -26.084892471244107, Replay Buffer Size: 42995\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1260: Lost\n",
      "Episode 1260, Avg Value Loss: 3.2692896870599277, Avg Policy Loss: 4.236709757127624\n",
      "Episode 1260, Reward: -19.87002275792397, Moving Avg Reward: -25.9918871291158, Replay Buffer Size: 43064\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1260, Reward: -19.87002275792397, Moving Avg Reward: -25.9918871291158, Replay Buffer Size: 43064\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1261: Won\n",
      "Episode 1261, Avg Value Loss: 3.3949625889460244, Avg Policy Loss: 4.282174229621887\n",
      "Episode 1261, Reward: -30.744927829707457, Moving Avg Reward: -25.992460925570253, Replay Buffer Size: 43076\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1261, Reward: -30.744927829707457, Moving Avg Reward: -25.992460925570253, Replay Buffer Size: 43076\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1262: Won\n",
      "Episode 1262, Avg Value Loss: 2.9730155865351358, Avg Policy Loss: 4.256139715512593\n",
      "Episode 1262, Reward: -26.246060003888083, Moving Avg Reward: -25.97333998412388, Replay Buffer Size: 43088\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1262, Reward: -26.246060003888083, Moving Avg Reward: -25.97333998412388, Replay Buffer Size: 43088\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1263: Won\n",
      "Episode 1263, Avg Value Loss: 3.0821437935034433, Avg Policy Loss: 4.212205529212952\n",
      "Episode 1263, Reward: -31.34936983661123, Moving Avg Reward: -26.067251554904633, Replay Buffer Size: 43100\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1263, Reward: -31.34936983661123, Moving Avg Reward: -26.067251554904633, Replay Buffer Size: 43100\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1264: Won\n",
      "Episode 1264, Avg Value Loss: 2.7360673121043613, Avg Policy Loss: 4.336838585989816\n",
      "Episode 1264, Reward: -21.165275630035147, Moving Avg Reward: -26.470859800938346, Replay Buffer Size: 43107\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1264, Reward: -21.165275630035147, Moving Avg Reward: -26.470859800938346, Replay Buffer Size: 43107\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1265: Won\n",
      "Episode 1265, Avg Value Loss: 3.462872529029846, Avg Policy Loss: 4.172847485542297\n",
      "Episode 1265, Reward: -26.363220864125807, Moving Avg Reward: -26.447482806690545, Replay Buffer Size: 43117\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1265, Reward: -26.363220864125807, Moving Avg Reward: -26.447482806690545, Replay Buffer Size: 43117\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1266: Won\n",
      "Episode 1266, Avg Value Loss: 3.6642576364370494, Avg Policy Loss: 4.053797520123995\n",
      "Episode 1266, Reward: -30.374992478660133, Moving Avg Reward: -26.507425581524547, Replay Buffer Size: 43130\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1266, Reward: -30.374992478660133, Moving Avg Reward: -26.507425581524547, Replay Buffer Size: 43130\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1267: Won\n",
      "Episode 1267, Avg Value Loss: 3.0968718429406485, Avg Policy Loss: 4.325700283050537\n",
      "Episode 1267, Reward: -29.004553226389024, Moving Avg Reward: -26.288119799102745, Replay Buffer Size: 43142\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1267, Reward: -29.004553226389024, Moving Avg Reward: -26.288119799102745, Replay Buffer Size: 43142\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1268: Won\n",
      "Episode 1268, Avg Value Loss: 3.2305520406136146, Avg Policy Loss: 4.063333878150353\n",
      "Episode 1268, Reward: -29.214892738729237, Moving Avg Reward: -26.186060516876566, Replay Buffer Size: 43155\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1268, Reward: -29.214892738729237, Moving Avg Reward: -26.186060516876566, Replay Buffer Size: 43155\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1269: Won\n",
      "Episode 1269, Avg Value Loss: 2.7676329985260963, Avg Policy Loss: 4.388273060321808\n",
      "Episode 1269, Reward: -24.968956114751933, Moving Avg Reward: -25.92884196075366, Replay Buffer Size: 43163\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1269, Reward: -24.968956114751933, Moving Avg Reward: -25.92884196075366, Replay Buffer Size: 43163\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1270: Won\n",
      "Episode 1270, Avg Value Loss: 3.150418719178752, Avg Policy Loss: 4.3022162945646985\n",
      "Episode 1270, Reward: -47.94492097015958, Moving Avg Reward: -26.16868813230953, Replay Buffer Size: 43239\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1270, Reward: -47.94492097015958, Moving Avg Reward: -26.16868813230953, Replay Buffer Size: 43239\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1271: Won\n",
      "Episode 1271, Avg Value Loss: 3.40499453923919, Avg Policy Loss: 4.16922556812113\n",
      "Episode 1271, Reward: -48.2262043977018, Moving Avg Reward: -26.309329871252544, Replay Buffer Size: 43283\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1271, Reward: -48.2262043977018, Moving Avg Reward: -26.309329871252544, Replay Buffer Size: 43283\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1272: Won\n",
      "Episode 1272, Avg Value Loss: 2.820468172430992, Avg Policy Loss: 4.339756548404694\n",
      "Episode 1272, Reward: -29.671374751880904, Moving Avg Reward: -26.636181085576567, Replay Buffer Size: 43291\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1272, Reward: -29.671374751880904, Moving Avg Reward: -26.636181085576567, Replay Buffer Size: 43291\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1273: Won\n",
      "Episode 1273, Avg Value Loss: 3.834232435776637, Avg Policy Loss: 4.127206343870896\n",
      "Episode 1273, Reward: -34.901788560272564, Moving Avg Reward: -26.731583810981988, Replay Buffer Size: 43304\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1273, Reward: -34.901788560272564, Moving Avg Reward: -26.731583810981988, Replay Buffer Size: 43304\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1274: Won\n",
      "Episode 1274, Avg Value Loss: 3.160923134196888, Avg Policy Loss: 4.327473943883723\n",
      "Episode 1274, Reward: -28.711764443790663, Moving Avg Reward: -26.722997692026556, Replay Buffer Size: 43315\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1274, Reward: -28.711764443790663, Moving Avg Reward: -26.722997692026556, Replay Buffer Size: 43315\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1275: Won\n",
      "Episode 1275, Avg Value Loss: 3.3637395054101944, Avg Policy Loss: 4.220569069186847\n",
      "Episode 1275, Reward: -23.656369951144185, Moving Avg Reward: -26.714029684938943, Replay Buffer Size: 43363\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1275, Reward: -23.656369951144185, Moving Avg Reward: -26.714029684938943, Replay Buffer Size: 43363\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1276: Won\n",
      "Episode 1276, Avg Value Loss: 2.956187824408213, Avg Policy Loss: 4.372098843256633\n",
      "Episode 1276, Reward: -36.918419466470866, Moving Avg Reward: -26.83405765517361, Replay Buffer Size: 43375\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1276, Reward: -36.918419466470866, Moving Avg Reward: -26.83405765517361, Replay Buffer Size: 43375\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1277: Won\n",
      "Episode 1277, Avg Value Loss: 3.7023820143479567, Avg Policy Loss: 4.258801056788518\n",
      "Episode 1277, Reward: -33.28442071497698, Moving Avg Reward: -27.035010722675047, Replay Buffer Size: 43388\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1277, Reward: -33.28442071497698, Moving Avg Reward: -27.035010722675047, Replay Buffer Size: 43388\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1278: Won\n",
      "Episode 1278, Avg Value Loss: 2.999865106173924, Avg Policy Loss: 4.647618974958148\n",
      "Episode 1278, Reward: -21.823064412315695, Moving Avg Reward: -26.993399952384202, Replay Buffer Size: 43395\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1278, Reward: -21.823064412315695, Moving Avg Reward: -26.993399952384202, Replay Buffer Size: 43395\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1279: Won\n",
      "Episode 1279, Avg Value Loss: 3.243154591984219, Avg Policy Loss: 4.148101815470943\n",
      "Episode 1279, Reward: 16.049675932560923, Moving Avg Reward: -26.554410510328953, Replay Buffer Size: 43422\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1279, Reward: 16.049675932560923, Moving Avg Reward: -26.554410510328953, Replay Buffer Size: 43422\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1280: Won\n",
      "Episode 1280, Avg Value Loss: 2.4198368879464955, Avg Policy Loss: 4.303269459651067\n",
      "Episode 1280, Reward: -33.2975844558613, Moving Avg Reward: -26.22692285203289, Replay Buffer Size: 43435\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1280, Reward: -33.2975844558613, Moving Avg Reward: -26.22692285203289, Replay Buffer Size: 43435\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1281: Won\n",
      "Episode 1281, Avg Value Loss: 3.4752094447612762, Avg Policy Loss: 4.29220986366272\n",
      "Episode 1281, Reward: -35.34800310360013, Moving Avg Reward: -26.305337603633255, Replay Buffer Size: 43447\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1281, Reward: -35.34800310360013, Moving Avg Reward: -26.305337603633255, Replay Buffer Size: 43447\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1282: Won\n",
      "Episode 1282, Avg Value Loss: 3.4831417340498705, Avg Policy Loss: 4.205449003439683\n",
      "Episode 1282, Reward: -26.721763249799388, Moving Avg Reward: -26.187129424906, Replay Buffer Size: 43499\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1282, Reward: -26.721763249799388, Moving Avg Reward: -26.187129424906, Replay Buffer Size: 43499\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1283: Won\n",
      "Episode 1283, Avg Value Loss: 3.2732621958622565, Avg Policy Loss: 4.275953526680286\n",
      "Episode 1283, Reward: -65.92394197182486, Moving Avg Reward: -26.587763463714897, Replay Buffer Size: 43551\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1283, Reward: -65.92394197182486, Moving Avg Reward: -26.587763463714897, Replay Buffer Size: 43551\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1284: Won\n",
      "Episode 1284, Avg Value Loss: 3.3323322385549545, Avg Policy Loss: 4.192278861999512\n",
      "Episode 1284, Reward: -27.604971060864575, Moving Avg Reward: -26.590889748083313, Replay Buffer Size: 43559\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1284, Reward: -27.604971060864575, Moving Avg Reward: -26.590889748083313, Replay Buffer Size: 43559\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1285: Won\n",
      "Episode 1285, Avg Value Loss: 3.296300308406353, Avg Policy Loss: 4.264258554577827\n",
      "Episode 1285, Reward: -23.53060512890677, Moving Avg Reward: -26.497791774311327, Replay Buffer Size: 43639\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1285, Reward: -23.53060512890677, Moving Avg Reward: -26.497791774311327, Replay Buffer Size: 43639\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1286: Lost\n",
      "Episode 1286, Avg Value Loss: 3.4873119984100116, Avg Policy Loss: 4.251732089626255\n",
      "Episode 1286, Reward: -52.20922689166165, Moving Avg Reward: -27.179120544629317, Replay Buffer Size: 43706\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1286, Reward: -52.20922689166165, Moving Avg Reward: -27.179120544629317, Replay Buffer Size: 43706\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1287: Lost\n",
      "Episode 1287, Avg Value Loss: 3.1449838541448116, Avg Policy Loss: 4.240260821580887\n",
      "Episode 1287, Reward: -52.48204035764531, Moving Avg Reward: -27.46168995681204, Replay Buffer Size: 43786\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1287, Reward: -52.48204035764531, Moving Avg Reward: -27.46168995681204, Replay Buffer Size: 43786\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1288: Lost\n",
      "Episode 1288, Avg Value Loss: 3.427482748031616, Avg Policy Loss: 4.23513089120388\n",
      "Episode 1288, Reward: -48.07157572896103, Moving Avg Reward: -27.684859050473356, Replay Buffer Size: 43866\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1288, Reward: -48.07157572896103, Moving Avg Reward: -27.684859050473356, Replay Buffer Size: 43866\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1289: Draw\n",
      "Episode 1289, Avg Value Loss: 2.6616976857185364, Avg Policy Loss: 4.234267771244049\n",
      "Episode 1289, Reward: -23.769125463669177, Moving Avg Reward: -27.6578328631453, Replay Buffer Size: 43874\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1289, Reward: -23.769125463669177, Moving Avg Reward: -27.6578328631453, Replay Buffer Size: 43874\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1290: Lost\n",
      "Episode 1290, Avg Value Loss: 3.441376967089517, Avg Policy Loss: 4.257168003490993\n",
      "Episode 1290, Reward: -16.79753184938422, Moving Avg Reward: -27.522403447356588, Replay Buffer Size: 43930\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1290, Reward: -16.79753184938422, Moving Avg Reward: -27.522403447356588, Replay Buffer Size: 43930\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1291: Won\n",
      "Episode 1291, Avg Value Loss: 3.5655322762636037, Avg Policy Loss: 4.196185818085303\n",
      "Episode 1291, Reward: 15.998253060350244, Moving Avg Reward: -27.091403593451354, Replay Buffer Size: 43956\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1291, Reward: 15.998253060350244, Moving Avg Reward: -27.091403593451354, Replay Buffer Size: 43956\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1292: Won\n",
      "Episode 1292, Avg Value Loss: 3.3456227779388428, Avg Policy Loss: 4.275773286819458\n",
      "Episode 1292, Reward: -31.54154877587783, Moving Avg Reward: -26.878697991514574, Replay Buffer Size: 43968\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1292, Reward: -31.54154877587783, Moving Avg Reward: -26.878697991514574, Replay Buffer Size: 43968\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1293: Won\n",
      "Episode 1293, Avg Value Loss: 3.4334170569976172, Avg Policy Loss: 4.127914160490036\n",
      "Episode 1293, Reward: -47.63141538346626, Moving Avg Reward: -27.00798606970259, Replay Buffer Size: 43992\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.68\n",
      "Episode 1293, Reward: -47.63141538346626, Moving Avg Reward: -27.00798606970259, Replay Buffer Size: 43992\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1294: Won\n",
      "Episode 1294, Avg Value Loss: 3.96136376592848, Avg Policy Loss: 4.227261437310113\n",
      "Episode 1294, Reward: -23.56358247756895, Moving Avg Reward: -26.893713049422036, Replay Buffer Size: 44001\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 1294, Reward: -23.56358247756895, Moving Avg Reward: -26.893713049422036, Replay Buffer Size: 44001\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1295: Won\n",
      "Episode 1295, Avg Value Loss: 3.3140088074347553, Avg Policy Loss: 4.284479169284596\n",
      "Episode 1295, Reward: -6.371718961056645, Moving Avg Reward: -26.646857376866265, Replay Buffer Size: 44069\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1295, Reward: -6.371718961056645, Moving Avg Reward: -26.646857376866265, Replay Buffer Size: 44069\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1296: Won\n",
      "Episode 1296, Avg Value Loss: 2.7504971772432327, Avg Policy Loss: 4.1423042714595795\n",
      "Episode 1296, Reward: -25.38022751455662, Moving Avg Reward: -27.08256627828239, Replay Buffer Size: 44077\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.68\n",
      "Episode 1296, Reward: -25.38022751455662, Moving Avg Reward: -27.08256627828239, Replay Buffer Size: 44077\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1297: Won\n",
      "Episode 1297, Avg Value Loss: 3.2446172803640367, Avg Policy Loss: 4.216319823265076\n",
      "Episode 1297, Reward: 6.298662239320192, Moving Avg Reward: -26.80084613911727, Replay Buffer Size: 44157\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 1297, Reward: 6.298662239320192, Moving Avg Reward: -26.80084613911727, Replay Buffer Size: 44157\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1298: Won\n",
      "Episode 1298, Avg Value Loss: 4.225792451338335, Avg Policy Loss: 4.193898786198009\n",
      "Episode 1298, Reward: -33.099605406615225, Moving Avg Reward: -26.592925207496737, Replay Buffer Size: 44168\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1298, Reward: -33.099605406615225, Moving Avg Reward: -26.592925207496737, Replay Buffer Size: 44168\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1299: Won\n",
      "Episode 1299, Avg Value Loss: 3.557566112942166, Avg Policy Loss: 4.115154398812188\n",
      "Episode 1299, Reward: -21.186570903708557, Moving Avg Reward: -26.499325885012553, Replay Buffer Size: 44177\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1299, Reward: -21.186570903708557, Moving Avg Reward: -26.499325885012553, Replay Buffer Size: 44177\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1300: Won\n",
      "Episode 1300, Avg Value Loss: 3.4742564473833357, Avg Policy Loss: 4.2314252853393555\n",
      "Episode 1300, Reward: -20.769024958428584, Moving Avg Reward: -26.48532412291289, Replay Buffer Size: 44184\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1300, Reward: -20.769024958428584, Moving Avg Reward: -26.48532412291289, Replay Buffer Size: 44184\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1301: Won\n",
      "Episode 1301, Avg Value Loss: 3.7642269849777223, Avg Policy Loss: 4.004376935958862\n",
      "Episode 1301, Reward: -30.250856025791457, Moving Avg Reward: -26.557311060307697, Replay Buffer Size: 44194\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1301, Reward: -30.250856025791457, Moving Avg Reward: -26.557311060307697, Replay Buffer Size: 44194\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1302: Won\n",
      "Episode 1302, Avg Value Loss: 2.968352794647217, Avg Policy Loss: 4.317428641849094\n",
      "Episode 1302, Reward: -22.70489239926264, Moving Avg Reward: -26.46941984563992, Replay Buffer Size: 44203\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1302, Reward: -22.70489239926264, Moving Avg Reward: -26.46941984563992, Replay Buffer Size: 44203\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1303: Won\n",
      "Episode 1303, Avg Value Loss: 3.388949789784171, Avg Policy Loss: 4.330140568993309\n",
      "Episode 1303, Reward: -29.89413309538191, Moving Avg Reward: -25.56217752914781, Replay Buffer Size: 44214\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1303, Reward: -29.89413309538191, Moving Avg Reward: -25.56217752914781, Replay Buffer Size: 44214\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1304: Won\n",
      "Episode 1304, Avg Value Loss: 2.807202604081896, Avg Policy Loss: 4.196303261650933\n",
      "Episode 1304, Reward: -21.924320215278605, Moving Avg Reward: -25.742116506778533, Replay Buffer Size: 44223\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1304, Reward: -21.924320215278605, Moving Avg Reward: -25.742116506778533, Replay Buffer Size: 44223\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1305: Lost\n",
      "Episode 1305, Avg Value Loss: 3.1105366230010985, Avg Policy Loss: 4.259235668182373\n",
      "Episode 1305, Reward: -24.497123064783004, Moving Avg Reward: -25.294494733873435, Replay Buffer Size: 44233\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 1305, Reward: -24.497123064783004, Moving Avg Reward: -25.294494733873435, Replay Buffer Size: 44233\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1306: Lost\n",
      "Episode 1306, Avg Value Loss: 3.8321471214294434, Avg Policy Loss: 4.121723794937134\n",
      "Episode 1306, Reward: -29.203472412588006, Moving Avg Reward: -25.514351900899268, Replay Buffer Size: 44243\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1306, Reward: -29.203472412588006, Moving Avg Reward: -25.514351900899268, Replay Buffer Size: 44243\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1307: Lost\n",
      "Episode 1307, Avg Value Loss: 3.8272518316904702, Avg Policy Loss: 4.3136671384175616\n",
      "Episode 1307, Reward: -25.125095803217697, Moving Avg Reward: -25.93104011528869, Replay Buffer Size: 44252\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1307, Reward: -25.125095803217697, Moving Avg Reward: -25.93104011528869, Replay Buffer Size: 44252\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1308: Lost\n",
      "Episode 1308, Avg Value Loss: 3.3308076479218225, Avg Policy Loss: 4.286167675798589\n",
      "Episode 1308, Reward: -49.678802736276005, Moving Avg Reward: -26.113372220065376, Replay Buffer Size: 44296\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1308, Reward: -49.678802736276005, Moving Avg Reward: -26.113372220065376, Replay Buffer Size: 44296\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1309: Lost\n",
      "Episode 1309, Avg Value Loss: 3.5472412356790506, Avg Policy Loss: 4.24237673687485\n",
      "Episode 1309, Reward: 16.01193813250372, Moving Avg Reward: -25.42628455699407, Replay Buffer Size: 44349\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1309, Reward: 16.01193813250372, Moving Avg Reward: -25.42628455699407, Replay Buffer Size: 44349\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1310: Won\n",
      "Episode 1310, Avg Value Loss: 3.3347377512190075, Avg Policy Loss: 4.306076844533284\n",
      "Episode 1310, Reward: -24.655641101248627, Moving Avg Reward: -25.31172073517829, Replay Buffer Size: 44358\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1310, Reward: -24.655641101248627, Moving Avg Reward: -25.31172073517829, Replay Buffer Size: 44358\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1311: Won\n",
      "Episode 1311, Avg Value Loss: 3.2113391811197456, Avg Policy Loss: 4.057918180118907\n",
      "Episode 1311, Reward: -30.873146989899197, Moving Avg Reward: -25.054706392233697, Replay Buffer Size: 44369\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1311, Reward: -30.873146989899197, Moving Avg Reward: -25.054706392233697, Replay Buffer Size: 44369\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1312: Won\n",
      "Episode 1312, Avg Value Loss: 3.84251860777537, Avg Policy Loss: 4.168752776251899\n",
      "Episode 1312, Reward: -22.984316609598995, Moving Avg Reward: -24.978240288635703, Replay Buffer Size: 44378\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1312, Reward: -22.984316609598995, Moving Avg Reward: -24.978240288635703, Replay Buffer Size: 44378\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1313: Won\n",
      "Episode 1313, Avg Value Loss: 3.1497117772698404, Avg Policy Loss: 4.224853873252869\n",
      "Episode 1313, Reward: 4.321344302252122, Moving Avg Reward: -24.708063756502366, Replay Buffer Size: 44458\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1313, Reward: 4.321344302252122, Moving Avg Reward: -24.708063756502366, Replay Buffer Size: 44458\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1314: Won\n",
      "Episode 1314, Avg Value Loss: 3.281528155505657, Avg Policy Loss: 4.23824434876442\n",
      "Episode 1314, Reward: -63.41500495690588, Moving Avg Reward: -25.034942444773847, Replay Buffer Size: 44538\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1314, Reward: -63.41500495690588, Moving Avg Reward: -25.034942444773847, Replay Buffer Size: 44538\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1315: Lost\n",
      "Episode 1315, Avg Value Loss: 2.680795705318451, Avg Policy Loss: 4.229414176940918\n",
      "Episode 1315, Reward: -26.567718112611175, Moving Avg Reward: -24.712918329221374, Replay Buffer Size: 44548\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1315, Reward: -26.567718112611175, Moving Avg Reward: -24.712918329221374, Replay Buffer Size: 44548\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1316: Lost\n",
      "Episode 1316, Avg Value Loss: 2.9450379461050034, Avg Policy Loss: 4.186171591281891\n",
      "Episode 1316, Reward: -29.845361702899375, Moving Avg Reward: -24.779785090377544, Replay Buffer Size: 44556\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1316, Reward: -29.845361702899375, Moving Avg Reward: -24.779785090377544, Replay Buffer Size: 44556\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1317: Lost\n",
      "Episode 1317, Avg Value Loss: 3.4589765690267087, Avg Policy Loss: 4.262279415130616\n",
      "Episode 1317, Reward: -17.336021053629377, Moving Avg Reward: -25.15643986221562, Replay Buffer Size: 44636\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1317, Reward: -17.336021053629377, Moving Avg Reward: -25.15643986221562, Replay Buffer Size: 44636\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1318: Lost\n",
      "Episode 1318, Avg Value Loss: 2.97997864484787, Avg Policy Loss: 4.376201868057251\n",
      "Episode 1318, Reward: -27.215669383858607, Moving Avg Reward: -25.142998127613087, Replay Buffer Size: 44646\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1318, Reward: -27.215669383858607, Moving Avg Reward: -25.142998127613087, Replay Buffer Size: 44646\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1319: Lost\n",
      "Episode 1319, Avg Value Loss: 3.718213215470314, Avg Policy Loss: 4.440154075622559\n",
      "Episode 1319, Reward: -23.068482078670996, Moving Avg Reward: -25.14895422731399, Replay Buffer Size: 44654\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1319, Reward: -23.068482078670996, Moving Avg Reward: -25.14895422731399, Replay Buffer Size: 44654\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1320: Lost\n",
      "Episode 1320, Avg Value Loss: 3.5349687735239663, Avg Policy Loss: 4.223981539408366\n",
      "Episode 1320, Reward: -24.752498646306854, Moving Avg Reward: -25.18329195059243, Replay Buffer Size: 44663\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1320, Reward: -24.752498646306854, Moving Avg Reward: -25.18329195059243, Replay Buffer Size: 44663\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1321: Lost\n",
      "Episode 1321, Avg Value Loss: 3.4606760839621225, Avg Policy Loss: 4.27656336625417\n",
      "Episode 1321, Reward: -34.40678489338265, Moving Avg Reward: -25.665519274515056, Replay Buffer Size: 44675\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1321, Reward: -34.40678489338265, Moving Avg Reward: -25.665519274515056, Replay Buffer Size: 44675\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1322: Lost\n",
      "Episode 1322, Avg Value Loss: 4.212485641241074, Avg Policy Loss: 4.224190592765808\n",
      "Episode 1322, Reward: -30.17633604714939, Moving Avg Reward: -25.661088682363562, Replay Buffer Size: 44687\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1322, Reward: -30.17633604714939, Moving Avg Reward: -25.661088682363562, Replay Buffer Size: 44687\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1323: Lost\n",
      "Episode 1323, Avg Value Loss: 3.260371359911832, Avg Policy Loss: 4.054532939737493\n",
      "Episode 1323, Reward: -30.32501181431887, Moving Avg Reward: -25.896820065724143, Replay Buffer Size: 44698\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1323, Reward: -30.32501181431887, Moving Avg Reward: -25.896820065724143, Replay Buffer Size: 44698\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1324: Lost\n",
      "Episode 1324, Avg Value Loss: 3.3218024752356787, Avg Policy Loss: 4.3777291558005595\n",
      "Episode 1324, Reward: -25.734169565066424, Moving Avg Reward: -25.66806935392658, Replay Buffer Size: 44709\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1324, Reward: -25.734169565066424, Moving Avg Reward: -25.66806935392658, Replay Buffer Size: 44709\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1325: Lost\n",
      "Episode 1325, Avg Value Loss: 3.4722634646627637, Avg Policy Loss: 4.253965250651041\n",
      "Episode 1325, Reward: 9.670214117293627, Moving Avg Reward: -25.278610466955698, Replay Buffer Size: 44754\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1325, Reward: 9.670214117293627, Moving Avg Reward: -25.278610466955698, Replay Buffer Size: 44754\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1326: Won\n",
      "Episode 1326, Avg Value Loss: 3.474124014377594, Avg Policy Loss: 4.079263508319855\n",
      "Episode 1326, Reward: -38.27966617013923, Moving Avg Reward: -25.823070227259137, Replay Buffer Size: 44766\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1326, Reward: -38.27966617013923, Moving Avg Reward: -25.823070227259137, Replay Buffer Size: 44766\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1327: Won\n",
      "Episode 1327, Avg Value Loss: 3.1733686550339657, Avg Policy Loss: 4.283459147410606\n",
      "Episode 1327, Reward: -53.33857757897721, Moving Avg Reward: -26.374646370395077, Replay Buffer Size: 44833\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1327, Reward: -53.33857757897721, Moving Avg Reward: -26.374646370395077, Replay Buffer Size: 44833\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1328: Won\n",
      "Episode 1328, Avg Value Loss: 3.495965410768986, Avg Policy Loss: 4.302318581938744\n",
      "Episode 1328, Reward: -34.67047065586628, Moving Avg Reward: -26.424639025995, Replay Buffer Size: 44913\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1328, Reward: -34.67047065586628, Moving Avg Reward: -26.424639025995, Replay Buffer Size: 44913\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1329: Lost\n",
      "Episode 1329, Avg Value Loss: 3.1023086071014405, Avg Policy Loss: 4.381696462631226\n",
      "Episode 1329, Reward: -35.28069556495355, Moving Avg Reward: -26.264845411394955, Replay Buffer Size: 44923\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1329, Reward: -35.28069556495355, Moving Avg Reward: -26.264845411394955, Replay Buffer Size: 44923\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1330: Lost\n",
      "Episode 1330, Avg Value Loss: 3.053342878818512, Avg Policy Loss: 4.406351566314697\n",
      "Episode 1330, Reward: -24.61510894650445, Moving Avg Reward: -26.229759549976688, Replay Buffer Size: 44931\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1330, Reward: -24.61510894650445, Moving Avg Reward: -26.229759549976688, Replay Buffer Size: 44931\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1331: Lost\n",
      "Episode 1331, Avg Value Loss: 2.9419695337613425, Avg Policy Loss: 4.442837794621785\n",
      "Episode 1331, Reward: -29.841546420121553, Moving Avg Reward: -26.61781636486114, Replay Buffer Size: 44943\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1331, Reward: -29.841546420121553, Moving Avg Reward: -26.61781636486114, Replay Buffer Size: 44943\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1332: Lost\n",
      "Episode 1332, Avg Value Loss: 3.7583352506160734, Avg Policy Loss: 4.241022673249245\n",
      "Episode 1332, Reward: 6.954315500442952, Moving Avg Reward: -26.32748776157978, Replay Buffer Size: 45023\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1332, Reward: 6.954315500442952, Moving Avg Reward: -26.32748776157978, Replay Buffer Size: 45023\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1333: Lost\n",
      "Episode 1333, Avg Value Loss: 3.431404535472393, Avg Policy Loss: 4.267509296536446\n",
      "Episode 1333, Reward: -102.49201591372616, Moving Avg Reward: -27.080503830008475, Replay Buffer Size: 45103\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1333, Reward: -102.49201591372616, Moving Avg Reward: -27.080503830008475, Replay Buffer Size: 45103\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1334: Draw\n",
      "Episode 1334, Avg Value Loss: 3.568210350142585, Avg Policy Loss: 4.292507595486111\n",
      "Episode 1334, Reward: -25.392801993924675, Moving Avg Reward: -27.14099695357043, Replay Buffer Size: 45112\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 1334, Reward: -25.392801993924675, Moving Avg Reward: -27.14099695357043, Replay Buffer Size: 45112\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1335: Lost\n",
      "Episode 1335, Avg Value Loss: 4.15729033946991, Avg Policy Loss: 4.265938758850098\n",
      "Episode 1335, Reward: -17.957988678858623, Moving Avg Reward: -27.348232744919734, Replay Buffer Size: 45118\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.67\n",
      "Episode 1335, Reward: -17.957988678858623, Moving Avg Reward: -27.348232744919734, Replay Buffer Size: 45118\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1336: Lost\n",
      "Episode 1336, Avg Value Loss: 3.6296956539154053, Avg Policy Loss: 4.467594536868009\n",
      "Episode 1336, Reward: -24.3209330019926, Moving Avg Reward: -27.76067606681383, Replay Buffer Size: 45129\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1336, Reward: -24.3209330019926, Moving Avg Reward: -27.76067606681383, Replay Buffer Size: 45129\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1337: Lost\n",
      "Episode 1337, Avg Value Loss: 3.5011256337165833, Avg Policy Loss: 4.348202873678768\n",
      "Episode 1337, Reward: 9.83, Moving Avg Reward: -27.57311776352284, Replay Buffer Size: 45146\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1337, Reward: 9.83, Moving Avg Reward: -27.57311776352284, Replay Buffer Size: 45146\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1338: Won\n",
      "Episode 1338, Avg Value Loss: 2.862346190672654, Avg Policy Loss: 4.2200117478003865\n",
      "Episode 1338, Reward: -28.48271084516471, Moving Avg Reward: -27.60375686790709, Replay Buffer Size: 45159\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1338, Reward: -28.48271084516471, Moving Avg Reward: -27.60375686790709, Replay Buffer Size: 45159\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1339: Won\n",
      "Episode 1339, Avg Value Loss: 3.5603892538282604, Avg Policy Loss: 4.229282379150391\n",
      "Episode 1339, Reward: -22.461128201084804, Moving Avg Reward: -27.556912737713784, Replay Buffer Size: 45168\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1339, Reward: -22.461128201084804, Moving Avg Reward: -27.556912737713784, Replay Buffer Size: 45168\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1340: Won\n",
      "Episode 1340, Avg Value Loss: 3.9424545526504517, Avg Policy Loss: 4.238191843032837\n",
      "Episode 1340, Reward: -30.674943140800067, Moving Avg Reward: -27.609988716952234, Replay Buffer Size: 45178\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1340, Reward: -30.674943140800067, Moving Avg Reward: -27.609988716952234, Replay Buffer Size: 45178\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1341: Won\n",
      "Episode 1341, Avg Value Loss: 3.569826068196978, Avg Policy Loss: 4.353777422223772\n",
      "Episode 1341, Reward: -36.62650869416213, Moving Avg Reward: -27.619241772963264, Replay Buffer Size: 45213\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1341, Reward: -36.62650869416213, Moving Avg Reward: -27.619241772963264, Replay Buffer Size: 45213\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1342: Won\n",
      "Episode 1342, Avg Value Loss: 3.3397838920354843, Avg Policy Loss: 4.171081990003586\n",
      "Episode 1342, Reward: -24.988648119069, Moving Avg Reward: -27.64638278016127, Replay Buffer Size: 45221\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1342, Reward: -24.988648119069, Moving Avg Reward: -27.64638278016127, Replay Buffer Size: 45221\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1343: Won\n",
      "Episode 1343, Avg Value Loss: 3.303897749293934, Avg Policy Loss: 4.201343124563044\n",
      "Episode 1343, Reward: -28.941282937048186, Moving Avg Reward: -27.689506082881508, Replay Buffer Size: 45232\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1343, Reward: -28.941282937048186, Moving Avg Reward: -27.689506082881508, Replay Buffer Size: 45232\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1344: Won\n",
      "Episode 1344, Avg Value Loss: 3.3497707180678846, Avg Policy Loss: 4.362228143215179\n",
      "Episode 1344, Reward: -25.3413819964061, Moving Avg Reward: -27.45977265173424, Replay Buffer Size: 45312\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1344, Reward: -25.3413819964061, Moving Avg Reward: -27.45977265173424, Replay Buffer Size: 45312\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1345: Lost\n",
      "Episode 1345, Avg Value Loss: 2.456218822435899, Avg Policy Loss: 4.411378448659724\n",
      "Episode 1345, Reward: -34.34893601517882, Moving Avg Reward: -27.48476979360055, Replay Buffer Size: 45323\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1345, Reward: -34.34893601517882, Moving Avg Reward: -27.48476979360055, Replay Buffer Size: 45323\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1346: Lost\n",
      "Episode 1346, Avg Value Loss: 3.2834735810756683, Avg Policy Loss: 4.372038473685582\n",
      "Episode 1346, Reward: -45.31164577300821, Moving Avg Reward: -27.184175008334382, Replay Buffer Size: 45371\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1346, Reward: -45.31164577300821, Moving Avg Reward: -27.184175008334382, Replay Buffer Size: 45371\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1347: Lost\n",
      "Episode 1347, Avg Value Loss: 3.065508117278417, Avg Policy Loss: 4.24838262796402\n",
      "Episode 1347, Reward: -30.939930248611454, Moving Avg Reward: -27.667520131984762, Replay Buffer Size: 45383\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1347, Reward: -30.939930248611454, Moving Avg Reward: -27.667520131984762, Replay Buffer Size: 45383\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1348: Lost\n",
      "Episode 1348, Avg Value Loss: 3.4587358066013882, Avg Policy Loss: 4.38304877281189\n",
      "Episode 1348, Reward: 12.591004174640757, Moving Avg Reward: -27.319280532743747, Replay Buffer Size: 45411\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1348, Reward: 12.591004174640757, Moving Avg Reward: -27.319280532743747, Replay Buffer Size: 45411\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1349: Won\n",
      "Episode 1349, Avg Value Loss: 3.4128212928771973, Avg Policy Loss: 4.3313760076250345\n",
      "Episode 1349, Reward: -21.789921635266673, Moving Avg Reward: -27.263751036942434, Replay Buffer Size: 45418\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1349, Reward: -21.789921635266673, Moving Avg Reward: -27.263751036942434, Replay Buffer Size: 45418\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1350: Won\n",
      "Episode 1350, Avg Value Loss: 3.4641671505841343, Avg Policy Loss: 4.3420233726501465\n",
      "Episode 1350, Reward: -30.988026698686163, Moving Avg Reward: -27.315676594959218, Replay Buffer Size: 45429\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1350, Reward: -30.988026698686163, Moving Avg Reward: -27.315676594959218, Replay Buffer Size: 45429\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1351: Won\n",
      "Episode 1351, Avg Value Loss: 2.4335527420043945, Avg Policy Loss: 4.286861525641547\n",
      "Episode 1351, Reward: -29.1432684117936, Moving Avg Reward: -27.325530397482538, Replay Buffer Size: 45438\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1351, Reward: -29.1432684117936, Moving Avg Reward: -27.325530397482538, Replay Buffer Size: 45438\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1352: Won\n",
      "Episode 1352, Avg Value Loss: 3.2877010504404702, Avg Policy Loss: 4.005274375279744\n",
      "Episode 1352, Reward: -27.109833842788216, Moving Avg Reward: -27.35572040822094, Replay Buffer Size: 45447\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1352, Reward: -27.109833842788216, Moving Avg Reward: -27.35572040822094, Replay Buffer Size: 45447\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1353: Won\n",
      "Episode 1353, Avg Value Loss: 3.1923131197690964, Avg Policy Loss: 4.248443186283112\n",
      "Episode 1353, Reward: -27.38525265603708, Moving Avg Reward: -27.354263865953772, Replay Buffer Size: 45455\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1353, Reward: -27.38525265603708, Moving Avg Reward: -27.354263865953772, Replay Buffer Size: 45455\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1354: Won\n",
      "Episode 1354, Avg Value Loss: 4.120899132319859, Avg Policy Loss: 4.649933201926095\n",
      "Episode 1354, Reward: -24.75556555849706, Moving Avg Reward: -27.37926482054403, Replay Buffer Size: 45462\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1354, Reward: -24.75556555849706, Moving Avg Reward: -27.37926482054403, Replay Buffer Size: 45462\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1355: Won\n",
      "Episode 1355, Avg Value Loss: 3.264616446061568, Avg Policy Loss: 4.104770790446889\n",
      "Episode 1355, Reward: -31.706776873842557, Moving Avg Reward: -27.333751256320674, Replay Buffer Size: 45473\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1355, Reward: -31.706776873842557, Moving Avg Reward: -27.333751256320674, Replay Buffer Size: 45473\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1356: Won\n",
      "Episode 1356, Avg Value Loss: 3.3965058837618147, Avg Policy Loss: 4.371444225311279\n",
      "Episode 1356, Reward: -23.06390690838318, Moving Avg Reward: -27.304112141645273, Replay Buffer Size: 45480\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1356, Reward: -23.06390690838318, Moving Avg Reward: -27.304112141645273, Replay Buffer Size: 45480\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1357: Won\n",
      "Episode 1357, Avg Value Loss: 3.0349403467289235, Avg Policy Loss: 4.429515982783118\n",
      "Episode 1357, Reward: -75.06901270849671, Moving Avg Reward: -27.77959410024588, Replay Buffer Size: 45523\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1357, Reward: -75.06901270849671, Moving Avg Reward: -27.77959410024588, Replay Buffer Size: 45523\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1358: Won\n",
      "Episode 1358, Avg Value Loss: 3.2377849320570626, Avg Policy Loss: 4.496929049491882\n",
      "Episode 1358, Reward: -29.013088649121197, Moving Avg Reward: -27.779906947505218, Replay Buffer Size: 45535\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1358, Reward: -29.013088649121197, Moving Avg Reward: -27.779906947505218, Replay Buffer Size: 45535\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1359: Won\n",
      "Episode 1359, Avg Value Loss: 3.273585567474365, Avg Policy Loss: 4.483022041320801\n",
      "Episode 1359, Reward: 19.21845150359382, Moving Avg Reward: -27.317632373766696, Replay Buffer Size: 45560\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1359, Reward: 19.21845150359382, Moving Avg Reward: -27.317632373766696, Replay Buffer Size: 45560\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1360: Won\n",
      "Episode 1360, Avg Value Loss: 3.040483444929123, Avg Policy Loss: 4.233404755592346\n",
      "Episode 1360, Reward: -25.292405309401417, Moving Avg Reward: -27.371856199281474, Replay Buffer Size: 45568\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1360, Reward: -25.292405309401417, Moving Avg Reward: -27.371856199281474, Replay Buffer Size: 45568\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1361: Won\n",
      "Episode 1361, Avg Value Loss: 3.144324952905828, Avg Policy Loss: 4.404118364507502\n",
      "Episode 1361, Reward: -28.810370674683895, Moving Avg Reward: -27.352510627731235, Replay Buffer Size: 45579\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1361, Reward: -28.810370674683895, Moving Avg Reward: -27.352510627731235, Replay Buffer Size: 45579\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1362: Won\n",
      "Episode 1362, Avg Value Loss: 4.095829350607736, Avg Policy Loss: 4.216273461069379\n",
      "Episode 1362, Reward: -35.24646691053309, Moving Avg Reward: -27.442514696797684, Replay Buffer Size: 45593\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1362, Reward: -35.24646691053309, Moving Avg Reward: -27.442514696797684, Replay Buffer Size: 45593\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1363: Won\n",
      "Episode 1363, Avg Value Loss: 3.317513565222422, Avg Policy Loss: 4.237780928611755\n",
      "Episode 1363, Reward: -28.476624226584512, Moving Avg Reward: -27.41378724069742, Replay Buffer Size: 45605\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1363, Reward: -28.476624226584512, Moving Avg Reward: -27.41378724069742, Replay Buffer Size: 45605\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1364: Won\n",
      "Episode 1364, Avg Value Loss: 3.5290664732456207, Avg Policy Loss: 4.315602016448975\n",
      "Episode 1364, Reward: -24.988103868119957, Moving Avg Reward: -27.452015523078266, Replay Buffer Size: 45645\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1364, Reward: -24.988103868119957, Moving Avg Reward: -27.452015523078266, Replay Buffer Size: 45645\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1365: Won\n",
      "Episode 1365, Avg Value Loss: 3.4571463763713837, Avg Policy Loss: 4.247108976046245\n",
      "Episode 1365, Reward: -28.837846714374066, Moving Avg Reward: -27.47676178158075, Replay Buffer Size: 45657\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1365, Reward: -28.837846714374066, Moving Avg Reward: -27.47676178158075, Replay Buffer Size: 45657\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1366: Won\n",
      "Episode 1366, Avg Value Loss: 3.3362242850390347, Avg Policy Loss: 4.4050094864585185\n",
      "Episode 1366, Reward: -26.92328022480327, Moving Avg Reward: -27.44224465904218, Replay Buffer Size: 45668\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1366, Reward: -26.92328022480327, Moving Avg Reward: -27.44224465904218, Replay Buffer Size: 45668\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1367: Won\n",
      "Episode 1367, Avg Value Loss: 3.712470041380988, Avg Policy Loss: 4.427349196539985\n",
      "Episode 1367, Reward: -31.520081072049678, Moving Avg Reward: -27.46739993749879, Replay Buffer Size: 45677\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 1367, Reward: -31.520081072049678, Moving Avg Reward: -27.46739993749879, Replay Buffer Size: 45677\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1368: Won\n",
      "Episode 1368, Avg Value Loss: 3.2960033893585203, Avg Policy Loss: 4.413698959350586\n",
      "Episode 1368, Reward: -24.372251794826365, Moving Avg Reward: -27.41897352805976, Replay Buffer Size: 45687\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1368, Reward: -24.372251794826365, Moving Avg Reward: -27.41897352805976, Replay Buffer Size: 45687\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1369: Won\n",
      "Episode 1369, Avg Value Loss: 3.4737172052264214, Avg Policy Loss: 4.357946405808131\n",
      "Episode 1369, Reward: -30.946211047749323, Moving Avg Reward: -27.478746077389737, Replay Buffer Size: 45735\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1369, Reward: -30.946211047749323, Moving Avg Reward: -27.478746077389737, Replay Buffer Size: 45735\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1370: Lost\n",
      "Episode 1370, Avg Value Loss: 3.705713319778442, Avg Policy Loss: 4.47900071144104\n",
      "Episode 1370, Reward: -25.14742145222931, Moving Avg Reward: -27.25077108221043, Replay Buffer Size: 45745\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1370, Reward: -25.14742145222931, Moving Avg Reward: -27.25077108221043, Replay Buffer Size: 45745\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1371: Lost\n",
      "Episode 1371, Avg Value Loss: 3.5050801038742065, Avg Policy Loss: 4.369207906723022\n",
      "Episode 1371, Reward: -26.896306155461666, Moving Avg Reward: -27.037472099788022, Replay Buffer Size: 45755\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1371, Reward: -26.896306155461666, Moving Avg Reward: -27.037472099788022, Replay Buffer Size: 45755\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1372: Lost\n",
      "Episode 1372, Avg Value Loss: 4.14060001373291, Avg Policy Loss: 4.525381374359131\n",
      "Episode 1372, Reward: -27.833150889278965, Moving Avg Reward: -27.019089861162005, Replay Buffer Size: 45765\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1372, Reward: -27.833150889278965, Moving Avg Reward: -27.019089861162005, Replay Buffer Size: 45765\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1373: Lost\n",
      "Episode 1373, Avg Value Loss: 3.3488313605388007, Avg Policy Loss: 4.328773816426595\n",
      "Episode 1373, Reward: -32.037070174881165, Moving Avg Reward: -26.990442677308103, Replay Buffer Size: 45777\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1373, Reward: -32.037070174881165, Moving Avg Reward: -26.990442677308103, Replay Buffer Size: 45777\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1374: Lost\n",
      "Episode 1374, Avg Value Loss: 4.250497142473857, Avg Policy Loss: 4.227434317270915\n",
      "Episode 1374, Reward: -31.08390783722554, Moving Avg Reward: -27.014164111242444, Replay Buffer Size: 45789\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1374, Reward: -31.08390783722554, Moving Avg Reward: -27.014164111242444, Replay Buffer Size: 45789\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1375: Lost\n",
      "Episode 1375, Avg Value Loss: 3.3229597873157926, Avg Policy Loss: 4.325584332148234\n",
      "Episode 1375, Reward: 18.005961688757452, Moving Avg Reward: -26.59754079484343, Replay Buffer Size: 45807\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1375, Reward: 18.005961688757452, Moving Avg Reward: -26.59754079484343, Replay Buffer Size: 45807\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1376: Won\n",
      "Episode 1376, Avg Value Loss: 3.6553989350795746, Avg Policy Loss: 4.300948458058493\n",
      "Episode 1376, Reward: -29.03534661389694, Moving Avg Reward: -26.518710066317684, Replay Buffer Size: 45835\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1376, Reward: -29.03534661389694, Moving Avg Reward: -26.518710066317684, Replay Buffer Size: 45835\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1377: Won\n",
      "Episode 1377, Avg Value Loss: 3.885170042514801, Avg Policy Loss: 4.323977053165436\n",
      "Episode 1377, Reward: -26.584672139247992, Moving Avg Reward: -26.4517125805604, Replay Buffer Size: 45843\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1377, Reward: -26.584672139247992, Moving Avg Reward: -26.4517125805604, Replay Buffer Size: 45843\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1378: Won\n",
      "Episode 1378, Avg Value Loss: 2.7570945024490356, Avg Policy Loss: 4.423711034986708\n",
      "Episode 1378, Reward: -25.662810240897045, Moving Avg Reward: -26.490110038846215, Replay Buffer Size: 45852\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1378, Reward: -25.662810240897045, Moving Avg Reward: -26.490110038846215, Replay Buffer Size: 45852\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1379: Won\n",
      "Episode 1379, Avg Value Loss: 3.5610935904762964, Avg Policy Loss: 4.401476946744052\n",
      "Episode 1379, Reward: -27.073551774890618, Moving Avg Reward: -26.921342315920732, Replay Buffer Size: 45863\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1379, Reward: -27.073551774890618, Moving Avg Reward: -26.921342315920732, Replay Buffer Size: 45863\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1380: Won\n",
      "Episode 1380, Avg Value Loss: 3.36911127269268, Avg Policy Loss: 4.388512778282165\n",
      "Episode 1380, Reward: -36.03533447802907, Moving Avg Reward: -26.94871981614241, Replay Buffer Size: 45943\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1380, Reward: -36.03533447802907, Moving Avg Reward: -26.94871981614241, Replay Buffer Size: 45943\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1381: Won\n",
      "Episode 1381, Avg Value Loss: 3.1942491114139555, Avg Policy Loss: 4.396532672643661\n",
      "Episode 1381, Reward: 1.0363770112002764, Moving Avg Reward: -26.584876014994407, Replay Buffer Size: 46023\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1381, Reward: 1.0363770112002764, Moving Avg Reward: -26.584876014994407, Replay Buffer Size: 46023\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1382: Draw\n",
      "Episode 1382, Avg Value Loss: 3.8302937746047974, Avg Policy Loss: 4.062858164310455\n",
      "Episode 1382, Reward: -36.38529413227916, Moving Avg Reward: -26.681511323819205, Replay Buffer Size: 46035\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1382, Reward: -36.38529413227916, Moving Avg Reward: -26.681511323819205, Replay Buffer Size: 46035\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1383: Lost\n",
      "Episode 1383, Avg Value Loss: 3.425334175427755, Avg Policy Loss: 4.436445554097493\n",
      "Episode 1383, Reward: -30.023677015687575, Moving Avg Reward: -26.32250867425783, Replay Buffer Size: 46047\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1383, Reward: -30.023677015687575, Moving Avg Reward: -26.32250867425783, Replay Buffer Size: 46047\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1384: Lost\n",
      "Episode 1384, Avg Value Loss: 3.3505311787128447, Avg Policy Loss: 4.425485047698021\n",
      "Episode 1384, Reward: -9.57764913503033, Moving Avg Reward: -26.142235454999486, Replay Buffer Size: 46127\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1384, Reward: -9.57764913503033, Moving Avg Reward: -26.142235454999486, Replay Buffer Size: 46127\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1385: Lost\n",
      "Episode 1385, Avg Value Loss: 3.597931195795536, Avg Policy Loss: 4.269532927870751\n",
      "Episode 1385, Reward: -4.010052608978397, Moving Avg Reward: -25.947029929800202, Replay Buffer Size: 46207\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1385, Reward: -4.010052608978397, Moving Avg Reward: -25.947029929800202, Replay Buffer Size: 46207\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1386: Draw\n",
      "Episode 1386, Avg Value Loss: 3.2852066622840033, Avg Policy Loss: 4.232201496760051\n",
      "Episode 1386, Reward: -22.421063158468165, Moving Avg Reward: -25.64914829246827, Replay Buffer Size: 46216\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1386, Reward: -22.421063158468165, Moving Avg Reward: -25.64914829246827, Replay Buffer Size: 46216\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1387: Lost\n",
      "Episode 1387, Avg Value Loss: 3.4175446182489395, Avg Policy Loss: 4.366210493445396\n",
      "Episode 1387, Reward: -3.958234015437742, Moving Avg Reward: -25.16391022904619, Replay Buffer Size: 46296\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1387, Reward: -3.958234015437742, Moving Avg Reward: -25.16391022904619, Replay Buffer Size: 46296\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1388: Lost\n",
      "Episode 1388, Avg Value Loss: 3.600937843322754, Avg Policy Loss: 4.488317693982806\n",
      "Episode 1388, Reward: -23.261026914375222, Moving Avg Reward: -24.915804740900334, Replay Buffer Size: 46303\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1388, Reward: -23.261026914375222, Moving Avg Reward: -24.915804740900334, Replay Buffer Size: 46303\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1389: Lost\n",
      "Episode 1389, Avg Value Loss: 3.4195903145497843, Avg Policy Loss: 4.33853522808321\n",
      "Episode 1389, Reward: -28.9273413428308, Moving Avg Reward: -24.967386899691956, Replay Buffer Size: 46365\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1389, Reward: -28.9273413428308, Moving Avg Reward: -24.967386899691956, Replay Buffer Size: 46365\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1390: Won\n",
      "Episode 1390, Avg Value Loss: 2.812087525924047, Avg Policy Loss: 4.124144474665324\n",
      "Episode 1390, Reward: -32.99527418186432, Moving Avg Reward: -25.129364323016752, Replay Buffer Size: 46377\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1390, Reward: -32.99527418186432, Moving Avg Reward: -25.129364323016752, Replay Buffer Size: 46377\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1391: Won\n",
      "Episode 1391, Avg Value Loss: 3.0886614635586738, Avg Policy Loss: 4.360879063606262\n",
      "Episode 1391, Reward: -22.49551234144117, Moving Avg Reward: -25.514301977034666, Replay Buffer Size: 46457\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1391, Reward: -22.49551234144117, Moving Avg Reward: -25.514301977034666, Replay Buffer Size: 46457\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1392: Won\n",
      "Episode 1392, Avg Value Loss: 3.202286273241043, Avg Policy Loss: 4.424227318167686\n",
      "Episode 1392, Reward: -0.8000000000000005, Moving Avg Reward: -25.206886489275885, Replay Buffer Size: 46537\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1392, Reward: -0.8000000000000005, Moving Avg Reward: -25.206886489275885, Replay Buffer Size: 46537\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1393: Draw\n",
      "Episode 1393, Avg Value Loss: 3.3018310725688935, Avg Policy Loss: 4.373330387473106\n",
      "Episode 1393, Reward: -122.49413217412767, Moving Avg Reward: -25.955513657182497, Replay Buffer Size: 46617\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1393, Reward: -122.49413217412767, Moving Avg Reward: -25.955513657182497, Replay Buffer Size: 46617\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1394: Draw\n",
      "Episode 1394, Avg Value Loss: 2.8335961997509003, Avg Policy Loss: 4.43815153837204\n",
      "Episode 1394, Reward: 16.524831962224837, Moving Avg Reward: -25.55462951278456, Replay Buffer Size: 46641\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1394, Reward: 16.524831962224837, Moving Avg Reward: -25.55462951278456, Replay Buffer Size: 46641\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1395: Won\n",
      "Episode 1395, Avg Value Loss: 3.257112577557564, Avg Policy Loss: 4.407567262649536\n",
      "Episode 1395, Reward: -33.71155605099351, Moving Avg Reward: -25.82802788368393, Replay Buffer Size: 46653\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1395, Reward: -33.71155605099351, Moving Avg Reward: -25.82802788368393, Replay Buffer Size: 46653\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1396: Won\n",
      "Episode 1396, Avg Value Loss: 3.1745330095291138, Avg Policy Loss: 4.338444113731384\n",
      "Episode 1396, Reward: -27.95381110311376, Moving Avg Reward: -25.8537637195695, Replay Buffer Size: 46665\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1396, Reward: -27.95381110311376, Moving Avg Reward: -25.8537637195695, Replay Buffer Size: 46665\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1397: Won\n",
      "Episode 1397, Avg Value Loss: 3.448974920809269, Avg Policy Loss: 4.381877499818802\n",
      "Episode 1397, Reward: -41.955609499246314, Moving Avg Reward: -26.336306436955166, Replay Buffer Size: 46745\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1397, Reward: -41.955609499246314, Moving Avg Reward: -26.336306436955166, Replay Buffer Size: 46745\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1398: Won\n",
      "Episode 1398, Avg Value Loss: 3.675350228945414, Avg Policy Loss: 4.100657204786937\n",
      "Episode 1398, Reward: -25.43207030137777, Moving Avg Reward: -26.259631085902793, Replay Buffer Size: 46757\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1398, Reward: -25.43207030137777, Moving Avg Reward: -26.259631085902793, Replay Buffer Size: 46757\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1399: Won\n",
      "Episode 1399, Avg Value Loss: 3.1931624195792456, Avg Policy Loss: 4.391832091591575\n",
      "Episode 1399, Reward: -32.52512254429653, Moving Avg Reward: -26.373016602308677, Replay Buffer Size: 46801\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1399, Reward: -32.52512254429653, Moving Avg Reward: -26.373016602308677, Replay Buffer Size: 46801\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1400: Lost\n",
      "Episode 1400, Avg Value Loss: 3.232458683103323, Avg Policy Loss: 4.361702537536621\n",
      "Episode 1400, Reward: -52.39279352753767, Moving Avg Reward: -26.68925428799977, Replay Buffer Size: 46881\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1400, Reward: -52.39279352753767, Moving Avg Reward: -26.68925428799977, Replay Buffer Size: 46881\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1401: Lost\n",
      "Episode 1401, Avg Value Loss: 3.3215674087405205, Avg Policy Loss: 4.433625143766403\n",
      "Episode 1401, Reward: -0.5334473053301054, Moving Avg Reward: -26.392080200795153, Replay Buffer Size: 46961\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.68\n",
      "Episode 1401, Reward: -0.5334473053301054, Moving Avg Reward: -26.392080200795153, Replay Buffer Size: 46961\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1402: Draw\n",
      "Episode 1402, Avg Value Loss: 3.4334019497036934, Avg Policy Loss: 4.350310069322586\n",
      "Episode 1402, Reward: -22.915833284301428, Moving Avg Reward: -26.39418960964554, Replay Buffer Size: 47041\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.68\n",
      "Episode 1402, Reward: -22.915833284301428, Moving Avg Reward: -26.39418960964554, Replay Buffer Size: 47041\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1403: Draw\n",
      "Episode 1403, Avg Value Loss: 4.141089569438588, Avg Policy Loss: 4.279328259554776\n",
      "Episode 1403, Reward: -23.940266797051102, Moving Avg Reward: -26.334650946662233, Replay Buffer Size: 47052\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1403, Reward: -23.940266797051102, Moving Avg Reward: -26.334650946662233, Replay Buffer Size: 47052\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1404: Lost\n",
      "Episode 1404, Avg Value Loss: 2.5715456704298654, Avg Policy Loss: 4.390491972366969\n",
      "Episode 1404, Reward: 17.25099447793806, Moving Avg Reward: -25.942897799730062, Replay Buffer Size: 47076\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1404, Reward: 17.25099447793806, Moving Avg Reward: -25.942897799730062, Replay Buffer Size: 47076\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1405: Won\n",
      "Episode 1405, Avg Value Loss: 3.1470471262931823, Avg Policy Loss: 4.5767436027526855\n",
      "Episode 1405, Reward: -30.062243167284528, Moving Avg Reward: -25.99854900075508, Replay Buffer Size: 47086\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1405, Reward: -30.062243167284528, Moving Avg Reward: -25.99854900075508, Replay Buffer Size: 47086\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1406: Won\n",
      "Episode 1406, Avg Value Loss: 2.967890426516533, Avg Policy Loss: 4.209328919649124\n",
      "Episode 1406, Reward: -26.910814497558988, Moving Avg Reward: -25.975622421604793, Replay Buffer Size: 47094\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1406, Reward: -26.910814497558988, Moving Avg Reward: -25.975622421604793, Replay Buffer Size: 47094\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1407: Won\n",
      "Episode 1407, Avg Value Loss: 3.201357990503311, Avg Policy Loss: 4.389220443367958\n",
      "Episode 1407, Reward: 4.807370685472284, Moving Avg Reward: -25.676297756717887, Replay Buffer Size: 47174\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1407, Reward: 4.807370685472284, Moving Avg Reward: -25.676297756717887, Replay Buffer Size: 47174\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1408: Won\n",
      "Episode 1408, Avg Value Loss: 3.4933708757162094, Avg Policy Loss: 4.3874602999006\n",
      "Episode 1408, Reward: -53.95612773240629, Moving Avg Reward: -25.71907100667919, Replay Buffer Size: 47202\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1408, Reward: -53.95612773240629, Moving Avg Reward: -25.71907100667919, Replay Buffer Size: 47202\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1409: Won\n",
      "Episode 1409, Avg Value Loss: 3.0965472724702625, Avg Policy Loss: 4.439996030595568\n",
      "Episode 1409, Reward: -28.05833306032017, Moving Avg Reward: -26.159773718607436, Replay Buffer Size: 47211\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1409, Reward: -28.05833306032017, Moving Avg Reward: -26.159773718607436, Replay Buffer Size: 47211\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1410: Won\n",
      "Episode 1410, Avg Value Loss: 2.951572088094858, Avg Policy Loss: 4.484634252694937\n",
      "Episode 1410, Reward: -27.76193369839042, Moving Avg Reward: -26.190836644578848, Replay Buffer Size: 47224\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1410, Reward: -27.76193369839042, Moving Avg Reward: -26.190836644578848, Replay Buffer Size: 47224\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1411: Won\n",
      "Episode 1411, Avg Value Loss: 3.2254016771912575, Avg Policy Loss: 4.4120295971632\n",
      "Episode 1411, Reward: -1.685688137623072, Moving Avg Reward: -25.89896205605609, Replay Buffer Size: 47304\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1411, Reward: -1.685688137623072, Moving Avg Reward: -25.89896205605609, Replay Buffer Size: 47304\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1412: Lost\n",
      "Episode 1412, Avg Value Loss: 2.9712367607997012, Avg Policy Loss: 4.30031339938824\n",
      "Episode 1412, Reward: -37.819210345459595, Moving Avg Reward: -26.047310993414694, Replay Buffer Size: 47317\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1412, Reward: -37.819210345459595, Moving Avg Reward: -26.047310993414694, Replay Buffer Size: 47317\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1413: Lost\n",
      "Episode 1413, Avg Value Loss: 3.5235820338129997, Avg Policy Loss: 4.40917517542839\n",
      "Episode 1413, Reward: -42.87156653944204, Moving Avg Reward: -26.519240101831627, Replay Buffer Size: 47397\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1413, Reward: -42.87156653944204, Moving Avg Reward: -26.519240101831627, Replay Buffer Size: 47397\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1414: Lost\n",
      "Episode 1414, Avg Value Loss: 3.5205526451269784, Avg Policy Loss: 4.343171834945679\n",
      "Episode 1414, Reward: -37.65135822958018, Moving Avg Reward: -26.261603634558373, Replay Buffer Size: 47409\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1414, Reward: -37.65135822958018, Moving Avg Reward: -26.261603634558373, Replay Buffer Size: 47409\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1415: Lost\n",
      "Episode 1415, Avg Value Loss: 3.127978216518055, Avg Policy Loss: 4.551832632585005\n",
      "Episode 1415, Reward: -29.436810380903843, Moving Avg Reward: -26.290294557241303, Replay Buffer Size: 47420\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1415, Reward: -29.436810380903843, Moving Avg Reward: -26.290294557241303, Replay Buffer Size: 47420\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1416: Lost\n",
      "Episode 1416, Avg Value Loss: 3.266835469465989, Avg Policy Loss: 4.186742782592773\n",
      "Episode 1416, Reward: -30.43088958605812, Moving Avg Reward: -26.296149836072896, Replay Buffer Size: 47433\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 1416, Reward: -30.43088958605812, Moving Avg Reward: -26.296149836072896, Replay Buffer Size: 47433\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1417: Lost\n",
      "Episode 1417, Avg Value Loss: 3.4373104439841375, Avg Policy Loss: 4.2963427437676325\n",
      "Episode 1417, Reward: -25.447443835534493, Moving Avg Reward: -26.377264063891943, Replay Buffer Size: 47442\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1417, Reward: -25.447443835534493, Moving Avg Reward: -26.377264063891943, Replay Buffer Size: 47442\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1418: Lost\n",
      "Episode 1418, Avg Value Loss: 3.348239024480184, Avg Policy Loss: 4.461586952209473\n",
      "Episode 1418, Reward: -27.72225013785441, Moving Avg Reward: -26.382329871431903, Replay Buffer Size: 47454\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1418, Reward: -27.72225013785441, Moving Avg Reward: -26.382329871431903, Replay Buffer Size: 47454\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1419: Lost\n",
      "Episode 1419, Avg Value Loss: 3.1649515628814697, Avg Policy Loss: 4.524702668190002\n",
      "Episode 1419, Reward: -29.283628552153935, Moving Avg Reward: -26.44448133616673, Replay Buffer Size: 47466\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1419, Reward: -29.283628552153935, Moving Avg Reward: -26.44448133616673, Replay Buffer Size: 47466\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1420: Lost\n",
      "Episode 1420, Avg Value Loss: 3.0515131685468884, Avg Policy Loss: 4.496470822228326\n",
      "Episode 1420, Reward: -5.652600162706573, Moving Avg Reward: -26.253482351330728, Replay Buffer Size: 47475\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1420, Reward: -5.652600162706573, Moving Avg Reward: -26.253482351330728, Replay Buffer Size: 47475\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1421: Lost\n",
      "Episode 1421, Avg Value Loss: 3.3353793706212724, Avg Policy Loss: 4.4074502944946286\n",
      "Episode 1421, Reward: -21.52032215774993, Moving Avg Reward: -26.124617723974396, Replay Buffer Size: 47510\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1421, Reward: -21.52032215774993, Moving Avg Reward: -26.124617723974396, Replay Buffer Size: 47510\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1422: Lost\n",
      "Episode 1422, Avg Value Loss: 3.2201319535573325, Avg Policy Loss: 4.490943392117818\n",
      "Episode 1422, Reward: -32.69892602352235, Moving Avg Reward: -26.14984362373813, Replay Buffer Size: 47522\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 1422, Reward: -32.69892602352235, Moving Avg Reward: -26.14984362373813, Replay Buffer Size: 47522\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1423: Lost\n",
      "Episode 1423, Avg Value Loss: 3.188773587346077, Avg Policy Loss: 4.518664598464966\n",
      "Episode 1423, Reward: -24.163799857288712, Moving Avg Reward: -26.08823150416783, Replay Buffer Size: 47530\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1423, Reward: -24.163799857288712, Moving Avg Reward: -26.08823150416783, Replay Buffer Size: 47530\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1424: Lost\n",
      "Episode 1424, Avg Value Loss: 3.480805272857348, Avg Policy Loss: 4.2979557911554975\n",
      "Episode 1424, Reward: 17.53556006912046, Moving Avg Reward: -25.65553420782596, Replay Buffer Size: 47554\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1424, Reward: 17.53556006912046, Moving Avg Reward: -25.65553420782596, Replay Buffer Size: 47554\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1425: Won\n",
      "Episode 1425, Avg Value Loss: 3.2875350162386896, Avg Policy Loss: 4.44740674495697\n",
      "Episode 1425, Reward: -31.32393493045613, Moving Avg Reward: -26.06547569830346, Replay Buffer Size: 47634\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1425, Reward: -31.32393493045613, Moving Avg Reward: -26.06547569830346, Replay Buffer Size: 47634\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1426: Won\n",
      "Episode 1426, Avg Value Loss: 3.476464530825615, Avg Policy Loss: 4.396209201216697\n",
      "Episode 1426, Reward: -27.41824816587008, Moving Avg Reward: -25.956861518260766, Replay Buffer Size: 47714\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1426, Reward: -27.41824816587008, Moving Avg Reward: -25.956861518260766, Replay Buffer Size: 47714\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1427: Draw\n",
      "Episode 1427, Avg Value Loss: 3.1847978432973227, Avg Policy Loss: 4.438285706837972\n",
      "Episode 1427, Reward: -118.31578112180632, Moving Avg Reward: -26.606633553689054, Replay Buffer Size: 47789\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1427, Reward: -118.31578112180632, Moving Avg Reward: -26.606633553689054, Replay Buffer Size: 47789\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1428: Lost\n",
      "Episode 1428, Avg Value Loss: 3.3131812870502473, Avg Policy Loss: 4.433920928835869\n",
      "Episode 1428, Reward: 2.2107408267177924, Moving Avg Reward: -26.23782143886321, Replay Buffer Size: 47869\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1428, Reward: 2.2107408267177924, Moving Avg Reward: -26.23782143886321, Replay Buffer Size: 47869\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1429: Lost\n",
      "Episode 1429, Avg Value Loss: 2.7437461813290915, Avg Policy Loss: 4.387738307317098\n",
      "Episode 1429, Reward: -25.965798992774445, Moving Avg Reward: -26.144672473141423, Replay Buffer Size: 47881\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1429, Reward: -25.965798992774445, Moving Avg Reward: -26.144672473141423, Replay Buffer Size: 47881\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1430: Lost\n",
      "Episode 1430, Avg Value Loss: 3.2707902416586876, Avg Policy Loss: 4.450342518942697\n",
      "Episode 1430, Reward: -17.790045634823244, Moving Avg Reward: -26.076421840024608, Replay Buffer Size: 47937\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1430, Reward: -17.790045634823244, Moving Avg Reward: -26.076421840024608, Replay Buffer Size: 47937\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1431: Won\n",
      "Episode 1431, Avg Value Loss: 2.9062309190630913, Avg Policy Loss: 4.5908936858177185\n",
      "Episode 1431, Reward: -21.45090150022444, Moving Avg Reward: -25.99251539082565, Replay Buffer Size: 47945\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1431, Reward: -21.45090150022444, Moving Avg Reward: -25.99251539082565, Replay Buffer Size: 47945\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1432: Won\n",
      "Episode 1432, Avg Value Loss: 3.2867411739296384, Avg Policy Loss: 4.402313298649258\n",
      "Episode 1432, Reward: 18.899490185619744, Moving Avg Reward: -25.873063643973875, Replay Buffer Size: 47981\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1432, Reward: 18.899490185619744, Moving Avg Reward: -25.873063643973875, Replay Buffer Size: 47981\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1433: Won\n",
      "Episode 1433, Avg Value Loss: 3.4251339435577393, Avg Policy Loss: 4.5039873123168945\n",
      "Episode 1433, Reward: -28.370374017991743, Moving Avg Reward: -25.131847225016532, Replay Buffer Size: 47991\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1433, Reward: -28.370374017991743, Moving Avg Reward: -25.131847225016532, Replay Buffer Size: 47991\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1434: Won\n",
      "Episode 1434, Avg Value Loss: 3.345854300719041, Avg Policy Loss: 4.486182029430683\n",
      "Episode 1434, Reward: -36.37232788884509, Moving Avg Reward: -25.24164248396573, Replay Buffer Size: 48004\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1434, Reward: -36.37232788884509, Moving Avg Reward: -25.24164248396573, Replay Buffer Size: 48004\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1435: Won\n",
      "Episode 1435, Avg Value Loss: 3.085382342338562, Avg Policy Loss: 4.397302329540253\n",
      "Episode 1435, Reward: -28.175321431156867, Moving Avg Reward: -25.343815811488717, Replay Buffer Size: 48016\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1435, Reward: -28.175321431156867, Moving Avg Reward: -25.343815811488717, Replay Buffer Size: 48016\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1436: Won\n",
      "Episode 1436, Avg Value Loss: 3.0772311795841563, Avg Policy Loss: 4.343494328585538\n",
      "Episode 1436, Reward: -31.379938593202482, Moving Avg Reward: -25.414405867400816, Replay Buffer Size: 48027\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1436, Reward: -31.379938593202482, Moving Avg Reward: -25.414405867400816, Replay Buffer Size: 48027\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1437: Won\n",
      "Episode 1437, Avg Value Loss: 3.262572580575943, Avg Policy Loss: 4.476845932006836\n",
      "Episode 1437, Reward: -32.4214209036948, Moving Avg Reward: -25.836920076437767, Replay Buffer Size: 48037\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1437, Reward: -32.4214209036948, Moving Avg Reward: -25.836920076437767, Replay Buffer Size: 48037\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1438: Won\n",
      "Episode 1438, Avg Value Loss: 3.5724930233425565, Avg Policy Loss: 4.393040498097737\n",
      "Episode 1438, Reward: -32.005982296404156, Moving Avg Reward: -25.872152790950157, Replay Buffer Size: 48046\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1438, Reward: -32.005982296404156, Moving Avg Reward: -25.872152790950157, Replay Buffer Size: 48046\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1439: Won\n",
      "Episode 1439, Avg Value Loss: 3.238313318083161, Avg Policy Loss: 4.526202132827358\n",
      "Episode 1439, Reward: -71.82730182215488, Moving Avg Reward: -26.365814527160854, Replay Buffer Size: 48122\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1439, Reward: -71.82730182215488, Moving Avg Reward: -26.365814527160854, Replay Buffer Size: 48122\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1440: Won\n",
      "Episode 1440, Avg Value Loss: 3.4564613103866577, Avg Policy Loss: 4.463825139132413\n",
      "Episode 1440, Reward: 17.0721065290963, Moving Avg Reward: -25.888344030461894, Replay Buffer Size: 48144\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1440, Reward: 17.0721065290963, Moving Avg Reward: -25.888344030461894, Replay Buffer Size: 48144\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1441: Won\n",
      "Episode 1441, Avg Value Loss: 3.347508728504181, Avg Policy Loss: 4.480850172042847\n",
      "Episode 1441, Reward: -32.08363479218779, Moving Avg Reward: -25.842915291442146, Replay Buffer Size: 48154\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1441, Reward: -32.08363479218779, Moving Avg Reward: -25.842915291442146, Replay Buffer Size: 48154\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1442: Won\n",
      "Episode 1442, Avg Value Loss: 3.3302784065405526, Avg Policy Loss: 4.5405211846033735\n",
      "Episode 1442, Reward: -32.709746831242036, Moving Avg Reward: -25.92012627856388, Replay Buffer Size: 48166\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1442, Reward: -32.709746831242036, Moving Avg Reward: -25.92012627856388, Replay Buffer Size: 48166\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1443: Won\n",
      "Episode 1443, Avg Value Loss: 4.226249868219549, Avg Policy Loss: 4.4941347729076035\n",
      "Episode 1443, Reward: -27.435859629791214, Moving Avg Reward: -25.90507204549131, Replay Buffer Size: 48177\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1443, Reward: -27.435859629791214, Moving Avg Reward: -25.90507204549131, Replay Buffer Size: 48177\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1444: Won\n",
      "Episode 1444, Avg Value Loss: 3.32910632789135, Avg Policy Loss: 4.412819483876229\n",
      "Episode 1444, Reward: -87.02346064801665, Moving Avg Reward: -26.52189283200742, Replay Buffer Size: 48257\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1444, Reward: -87.02346064801665, Moving Avg Reward: -26.52189283200742, Replay Buffer Size: 48257\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1445: Won\n",
      "Episode 1445, Avg Value Loss: 3.152676950395107, Avg Policy Loss: 4.458146277070045\n",
      "Episode 1445, Reward: -7.082426216413312, Moving Avg Reward: -26.249227734019765, Replay Buffer Size: 48337\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1445, Reward: -7.082426216413312, Moving Avg Reward: -26.249227734019765, Replay Buffer Size: 48337\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1446: Draw\n",
      "Episode 1446, Avg Value Loss: 3.1166712015867235, Avg Policy Loss: 4.4146239966154095\n",
      "Episode 1446, Reward: 0.0040780003489073735, Moving Avg Reward: -25.796070496286198, Replay Buffer Size: 48417\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1446, Reward: 0.0040780003489073735, Moving Avg Reward: -25.796070496286198, Replay Buffer Size: 48417\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1447: Draw\n",
      "Episode 1447, Avg Value Loss: 3.2360271267592906, Avg Policy Loss: 4.452336648106575\n",
      "Episode 1447, Reward: -46.867266965233284, Moving Avg Reward: -25.955343863452413, Replay Buffer Size: 48497\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1447, Reward: -46.867266965233284, Moving Avg Reward: -25.955343863452413, Replay Buffer Size: 48497\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1448: Lost\n",
      "Episode 1448, Avg Value Loss: 3.19767659381032, Avg Policy Loss: 4.39627543091774\n",
      "Episode 1448, Reward: -54.035599422769145, Moving Avg Reward: -26.621609899426517, Replay Buffer Size: 48577\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1448, Reward: -54.035599422769145, Moving Avg Reward: -26.621609899426517, Replay Buffer Size: 48577\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1449: Lost\n",
      "Episode 1449, Avg Value Loss: 3.3581720987955728, Avg Policy Loss: 4.470226168632507\n",
      "Episode 1449, Reward: -34.47465305277439, Moving Avg Reward: -26.74845721360159, Replay Buffer Size: 48589\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1449, Reward: -34.47465305277439, Moving Avg Reward: -26.74845721360159, Replay Buffer Size: 48589\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1450: Lost\n",
      "Episode 1450, Avg Value Loss: 3.4756272733211517, Avg Policy Loss: 4.315281927585602\n",
      "Episode 1450, Reward: -27.338329230173912, Moving Avg Reward: -26.71196023891647, Replay Buffer Size: 48597\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1450, Reward: -27.338329230173912, Moving Avg Reward: -26.71196023891647, Replay Buffer Size: 48597\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1451: Lost\n",
      "Episode 1451, Avg Value Loss: 3.996024741066827, Avg Policy Loss: 4.368717564476861\n",
      "Episode 1451, Reward: -27.46174388646871, Moving Avg Reward: -26.69514499366322, Replay Buffer Size: 48606\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1451, Reward: -27.46174388646871, Moving Avg Reward: -26.69514499366322, Replay Buffer Size: 48606\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1452: Lost\n",
      "Episode 1452, Avg Value Loss: 3.0799274668097496, Avg Policy Loss: 4.490578040480614\n",
      "Episode 1452, Reward: -26.89454814130424, Moving Avg Reward: -26.69299213664838, Replay Buffer Size: 48686\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1452, Reward: -26.89454814130424, Moving Avg Reward: -26.69299213664838, Replay Buffer Size: 48686\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1453: Lost\n",
      "Episode 1453, Avg Value Loss: 3.411606654524803, Avg Policy Loss: 4.503990739583969\n",
      "Episode 1453, Reward: -28.465669858529793, Moving Avg Reward: -26.703796308673304, Replay Buffer Size: 48726\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1453, Reward: -28.465669858529793, Moving Avg Reward: -26.703796308673304, Replay Buffer Size: 48726\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1454: Lost\n",
      "Episode 1454, Avg Value Loss: 2.943322256207466, Avg Policy Loss: 4.30398432413737\n",
      "Episode 1454, Reward: -25.82905238713029, Moving Avg Reward: -26.714531176959635, Replay Buffer Size: 48738\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1454, Reward: -25.82905238713029, Moving Avg Reward: -26.714531176959635, Replay Buffer Size: 48738\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1455: Lost\n",
      "Episode 1455, Avg Value Loss: 3.345361089706421, Avg Policy Loss: 4.6281674861907955\n",
      "Episode 1455, Reward: -23.928374400431032, Moving Avg Reward: -26.63674715222552, Replay Buffer Size: 48748\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1455, Reward: -23.928374400431032, Moving Avg Reward: -26.63674715222552, Replay Buffer Size: 48748\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1456: Lost\n",
      "Episode 1456, Avg Value Loss: 3.5580787857373557, Avg Policy Loss: 4.453185359636943\n",
      "Episode 1456, Reward: -27.127461764624236, Moving Avg Reward: -26.67738270078793, Replay Buffer Size: 48760\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1456, Reward: -27.127461764624236, Moving Avg Reward: -26.67738270078793, Replay Buffer Size: 48760\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1457: Lost\n",
      "Episode 1457, Avg Value Loss: 3.0989796139977197, Avg Policy Loss: 4.3903562805869365\n",
      "Episode 1457, Reward: -25.089716720802034, Moving Avg Reward: -26.17758974091098, Replay Buffer Size: 48771\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1457, Reward: -25.089716720802034, Moving Avg Reward: -26.17758974091098, Replay Buffer Size: 48771\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1458: Lost\n",
      "Episode 1458, Avg Value Loss: 3.260207115858793, Avg Policy Loss: 4.447736948728561\n",
      "Episode 1458, Reward: -19.44019552351417, Moving Avg Reward: -26.081860809654913, Replay Buffer Size: 48851\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1458, Reward: -19.44019552351417, Moving Avg Reward: -26.081860809654913, Replay Buffer Size: 48851\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1459: Lost\n",
      "Episode 1459, Avg Value Loss: 2.6953010678291323, Avg Policy Loss: 4.520675468444824\n",
      "Episode 1459, Reward: -26.195850026063894, Moving Avg Reward: -26.53600382495149, Replay Buffer Size: 48861\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1459, Reward: -26.195850026063894, Moving Avg Reward: -26.53600382495149, Replay Buffer Size: 48861\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1460: Lost\n",
      "Episode 1460, Avg Value Loss: 3.2232802510261536, Avg Policy Loss: 4.630987564722697\n",
      "Episode 1460, Reward: -31.51841398173159, Moving Avg Reward: -26.598263911674795, Replay Buffer Size: 48873\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 1460, Reward: -31.51841398173159, Moving Avg Reward: -26.598263911674795, Replay Buffer Size: 48873\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1461: Lost\n",
      "Episode 1461, Avg Value Loss: 3.252443688420149, Avg Policy Loss: 4.437223237294417\n",
      "Episode 1461, Reward: -45.891910719321, Moving Avg Reward: -26.76907931212116, Replay Buffer Size: 48925\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1461, Reward: -45.891910719321, Moving Avg Reward: -26.76907931212116, Replay Buffer Size: 48925\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1462: Won\n",
      "Episode 1462, Avg Value Loss: 2.959395243571355, Avg Policy Loss: 4.402079985691951\n",
      "Episode 1462, Reward: -30.382655190197873, Moving Avg Reward: -26.72044119491781, Replay Buffer Size: 48938\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1462, Reward: -30.382655190197873, Moving Avg Reward: -26.72044119491781, Replay Buffer Size: 48938\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1463: Won\n",
      "Episode 1463, Avg Value Loss: 3.161499715768374, Avg Policy Loss: 4.405654467069185\n",
      "Episode 1463, Reward: 18.005107612041307, Moving Avg Reward: -26.255623876531555, Replay Buffer Size: 48977\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1463, Reward: 18.005107612041307, Moving Avg Reward: -26.255623876531555, Replay Buffer Size: 48977\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1464: Won\n",
      "Episode 1464, Avg Value Loss: 3.5743928485446506, Avg Policy Loss: 4.46996545791626\n",
      "Episode 1464, Reward: -20.204946863218986, Moving Avg Reward: -26.207792306482542, Replay Buffer Size: 48986\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1464, Reward: -20.204946863218986, Moving Avg Reward: -26.207792306482542, Replay Buffer Size: 48986\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1465: Won\n",
      "Episode 1465, Avg Value Loss: 3.2145569579941884, Avg Policy Loss: 4.424812646139236\n",
      "Episode 1465, Reward: 8.692670328885516, Moving Avg Reward: -25.83248713604995, Replay Buffer Size: 49028\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1465, Reward: 8.692670328885516, Moving Avg Reward: -25.83248713604995, Replay Buffer Size: 49028\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1466: Won\n",
      "Episode 1466, Avg Value Loss: 4.1067543559604225, Avg Policy Loss: 4.53502098719279\n",
      "Episode 1466, Reward: -26.06134020148798, Moving Avg Reward: -25.823867735816794, Replay Buffer Size: 49037\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1466, Reward: -26.06134020148798, Moving Avg Reward: -25.823867735816794, Replay Buffer Size: 49037\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1467: Won\n",
      "Episode 1467, Avg Value Loss: 3.1520928382873534, Avg Policy Loss: 4.281091928482056\n",
      "Episode 1467, Reward: -29.60544898458228, Moving Avg Reward: -25.804721414942122, Replay Buffer Size: 49047\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1467, Reward: -29.60544898458228, Moving Avg Reward: -25.804721414942122, Replay Buffer Size: 49047\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1468: Won\n",
      "Episode 1468, Avg Value Loss: 2.8994222709110806, Avg Policy Loss: 4.436045169830322\n",
      "Episode 1468, Reward: -21.20140285285779, Moving Avg Reward: -25.773012925522433, Replay Buffer Size: 49054\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1468, Reward: -21.20140285285779, Moving Avg Reward: -25.773012925522433, Replay Buffer Size: 49054\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1469: Won\n",
      "Episode 1469, Avg Value Loss: 3.1235531121492386, Avg Policy Loss: 4.479212701320648\n",
      "Episode 1469, Reward: -26.124019042705616, Moving Avg Reward: -25.724791005472, Replay Buffer Size: 49062\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1469, Reward: -26.124019042705616, Moving Avg Reward: -25.724791005472, Replay Buffer Size: 49062\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1470: Won\n",
      "Episode 1470, Avg Value Loss: 3.307292103767395, Avg Policy Loss: 4.366731119155884\n",
      "Episode 1470, Reward: -23.16152930108379, Moving Avg Reward: -25.704932083960543, Replay Buffer Size: 49072\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1470, Reward: -23.16152930108379, Moving Avg Reward: -25.704932083960543, Replay Buffer Size: 49072\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1471: Won\n",
      "Episode 1471, Avg Value Loss: 3.0147967977183208, Avg Policy Loss: 4.494214841297695\n",
      "Episode 1471, Reward: -33.42347092513231, Moving Avg Reward: -25.770203731657247, Replay Buffer Size: 49086\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1471, Reward: -33.42347092513231, Moving Avg Reward: -25.770203731657247, Replay Buffer Size: 49086\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1472: Won\n",
      "Episode 1472, Avg Value Loss: 2.8521668165922165, Avg Policy Loss: 4.380242586135864\n",
      "Episode 1472, Reward: -22.245088935305027, Moving Avg Reward: -25.714323112117505, Replay Buffer Size: 49094\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1472, Reward: -22.245088935305027, Moving Avg Reward: -25.714323112117505, Replay Buffer Size: 49094\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1473: Won\n",
      "Episode 1473, Avg Value Loss: 3.6273611545562745, Avg Policy Loss: 4.445815944671631\n",
      "Episode 1473, Reward: 19.94238232159383, Moving Avg Reward: -25.194528587152753, Replay Buffer Size: 49114\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1473, Reward: 19.94238232159383, Moving Avg Reward: -25.194528587152753, Replay Buffer Size: 49114\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1474: Won\n",
      "Episode 1474, Avg Value Loss: 3.6205848285130093, Avg Policy Loss: 4.354731219155448\n",
      "Episode 1474, Reward: -20.26494266666618, Moving Avg Reward: -25.08633893544716, Replay Buffer Size: 49121\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 1474, Reward: -20.26494266666618, Moving Avg Reward: -25.08633893544716, Replay Buffer Size: 49121\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1475: Won\n",
      "Episode 1475, Avg Value Loss: 4.416858239607378, Avg Policy Loss: 4.400102745402943\n",
      "Episode 1475, Reward: -26.013084178013447, Moving Avg Reward: -25.526529394114878, Replay Buffer Size: 49132\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1475, Reward: -26.013084178013447, Moving Avg Reward: -25.526529394114878, Replay Buffer Size: 49132\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1476: Won\n",
      "Episode 1476, Avg Value Loss: 3.0013624338003306, Avg Policy Loss: 4.58927924816425\n",
      "Episode 1476, Reward: -29.49179234572768, Moving Avg Reward: -25.53109385143318, Replay Buffer Size: 49145\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1476, Reward: -29.49179234572768, Moving Avg Reward: -25.53109385143318, Replay Buffer Size: 49145\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1477: Won\n",
      "Episode 1477, Avg Value Loss: 3.0876376903974094, Avg Policy Loss: 4.4126009207505446\n",
      "Episode 1477, Reward: -33.572970102545604, Moving Avg Reward: -25.600976831066163, Replay Buffer Size: 49158\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1477, Reward: -33.572970102545604, Moving Avg Reward: -25.600976831066163, Replay Buffer Size: 49158\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1478: Won\n",
      "Episode 1478, Avg Value Loss: 3.4791701237360635, Avg Policy Loss: 4.413411736488342\n",
      "Episode 1478, Reward: -29.806843463834742, Moving Avg Reward: -25.642417163295537, Replay Buffer Size: 49170\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1478, Reward: -29.806843463834742, Moving Avg Reward: -25.642417163295537, Replay Buffer Size: 49170\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1479: Won\n",
      "Episode 1479, Avg Value Loss: 3.5581159248948095, Avg Policy Loss: 4.463618063926697\n",
      "Episode 1479, Reward: -19.118075037834515, Moving Avg Reward: -25.56286239592498, Replay Buffer Size: 49250\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1479, Reward: -19.118075037834515, Moving Avg Reward: -25.56286239592498, Replay Buffer Size: 49250\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1480: Won\n",
      "Episode 1480, Avg Value Loss: 3.944476736916436, Avg Policy Loss: 4.39746438132392\n",
      "Episode 1480, Reward: -25.32392246909746, Moving Avg Reward: -25.45574827583566, Replay Buffer Size: 49259\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1480, Reward: -25.32392246909746, Moving Avg Reward: -25.45574827583566, Replay Buffer Size: 49259\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1481: Won\n",
      "Episode 1481, Avg Value Loss: 3.5557731355939595, Avg Policy Loss: 4.436848490578788\n",
      "Episode 1481, Reward: 10.95058417970785, Moving Avg Reward: -25.356606204150584, Replay Buffer Size: 49294\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1481, Reward: 10.95058417970785, Moving Avg Reward: -25.356606204150584, Replay Buffer Size: 49294\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1482: Won\n",
      "Episode 1482, Avg Value Loss: 2.754333943128586, Avg Policy Loss: 4.453876209259033\n",
      "Episode 1482, Reward: -32.57622264532985, Moving Avg Reward: -25.31851548928109, Replay Buffer Size: 49304\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1482, Reward: -32.57622264532985, Moving Avg Reward: -25.31851548928109, Replay Buffer Size: 49304\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1483: Won\n",
      "Episode 1483, Avg Value Loss: 3.135489649242825, Avg Policy Loss: 4.413987159729004\n",
      "Episode 1483, Reward: -26.468793894363884, Moving Avg Reward: -25.282966658067853, Replay Buffer Size: 49313\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1483, Reward: -26.468793894363884, Moving Avg Reward: -25.282966658067853, Replay Buffer Size: 49313\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1484: Won\n",
      "Episode 1484, Avg Value Loss: 3.8326738940344915, Avg Policy Loss: 4.491998301612006\n",
      "Episode 1484, Reward: -5.5222118797710635, Moving Avg Reward: -25.24241228551526, Replay Buffer Size: 49322\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1484, Reward: -5.5222118797710635, Moving Avg Reward: -25.24241228551526, Replay Buffer Size: 49322\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1485: Won\n",
      "Episode 1485, Avg Value Loss: 2.9593502559832165, Avg Policy Loss: 4.354149035045078\n",
      "Episode 1485, Reward: 24.870000000000005, Moving Avg Reward: -24.953611759425474, Replay Buffer Size: 49336\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1485, Reward: 24.870000000000005, Moving Avg Reward: -24.953611759425474, Replay Buffer Size: 49336\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1486: Won\n",
      "Episode 1486, Avg Value Loss: 3.4317320783933005, Avg Policy Loss: 4.485021948814392\n",
      "Episode 1486, Reward: -31.121975755112416, Moving Avg Reward: -25.040620885391924, Replay Buffer Size: 49348\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1486, Reward: -31.121975755112416, Moving Avg Reward: -25.040620885391924, Replay Buffer Size: 49348\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1487: Won\n",
      "Episode 1487, Avg Value Loss: 3.3297626048326494, Avg Policy Loss: 4.51415176987648\n",
      "Episode 1487, Reward: -10.959170460073945, Moving Avg Reward: -25.11063024983828, Replay Buffer Size: 49428\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1487, Reward: -10.959170460073945, Moving Avg Reward: -25.11063024983828, Replay Buffer Size: 49428\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1488: Won\n",
      "Episode 1488, Avg Value Loss: 3.38588787317276, Avg Policy Loss: 4.494155359268189\n",
      "Episode 1488, Reward: -29.03420407681147, Moving Avg Reward: -25.168362021462645, Replay Buffer Size: 49438\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1488, Reward: -29.03420407681147, Moving Avg Reward: -25.168362021462645, Replay Buffer Size: 49438\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1489: Won\n",
      "Episode 1489, Avg Value Loss: 3.192081424925062, Avg Policy Loss: 4.475355625152588\n",
      "Episode 1489, Reward: -26.663480066079543, Moving Avg Reward: -25.14572340869513, Replay Buffer Size: 49447\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1489, Reward: -26.663480066079543, Moving Avg Reward: -25.14572340869513, Replay Buffer Size: 49447\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1490: Won\n",
      "Episode 1490, Avg Value Loss: 3.256082632324912, Avg Policy Loss: 4.361422061920166\n",
      "Episode 1490, Reward: -28.8587160710436, Moving Avg Reward: -25.104357827586924, Replay Buffer Size: 49458\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1490, Reward: -28.8587160710436, Moving Avg Reward: -25.104357827586924, Replay Buffer Size: 49458\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1491: Won\n",
      "Episode 1491, Avg Value Loss: 3.4131693243980408, Avg Policy Loss: 4.664216876029968\n",
      "Episode 1491, Reward: -27.903199028516283, Moving Avg Reward: -25.158434694457675, Replay Buffer Size: 49466\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1491, Reward: -27.903199028516283, Moving Avg Reward: -25.158434694457675, Replay Buffer Size: 49466\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1492: Won\n",
      "Episode 1492, Avg Value Loss: 3.1866925027635364, Avg Policy Loss: 4.506843566894531\n",
      "Episode 1492, Reward: -29.808585181196598, Moving Avg Reward: -25.448520546269638, Replay Buffer Size: 49475\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1492, Reward: -29.808585181196598, Moving Avg Reward: -25.448520546269638, Replay Buffer Size: 49475\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1493: Won\n",
      "Episode 1493, Avg Value Loss: 3.148275050249967, Avg Policy Loss: 4.543089823289351\n",
      "Episode 1493, Reward: 17.710852596954496, Moving Avg Reward: -24.04647069855882, Replay Buffer Size: 49497\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1493, Reward: 17.710852596954496, Moving Avg Reward: -24.04647069855882, Replay Buffer Size: 49497\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1494: Won\n",
      "Episode 1494, Avg Value Loss: 3.0999152206239247, Avg Policy Loss: 4.484089647020612\n",
      "Episode 1494, Reward: 16.37511250444111, Moving Avg Reward: -24.047967893136658, Replay Buffer Size: 49518\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1494, Reward: 16.37511250444111, Moving Avg Reward: -24.047967893136658, Replay Buffer Size: 49518\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1495: Won\n",
      "Episode 1495, Avg Value Loss: 3.200240284204483, Avg Policy Loss: 4.515756857395172\n",
      "Episode 1495, Reward: -27.98991483420859, Moving Avg Reward: -23.99075148096881, Replay Buffer Size: 49598\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 1495, Reward: -27.98991483420859, Moving Avg Reward: -23.99075148096881, Replay Buffer Size: 49598\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1496: Won\n",
      "Episode 1496, Avg Value Loss: 3.144669181108475, Avg Policy Loss: 4.593069958686828\n",
      "Episode 1496, Reward: -54.60134776433421, Moving Avg Reward: -24.257226847581013, Replay Buffer Size: 49678\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1496, Reward: -54.60134776433421, Moving Avg Reward: -24.257226847581013, Replay Buffer Size: 49678\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1497: Draw\n",
      "Episode 1497, Avg Value Loss: 3.7561737060546876, Avg Policy Loss: 4.348033881187439\n",
      "Episode 1497, Reward: -29.738427155456133, Moving Avg Reward: -24.135055024143107, Replay Buffer Size: 49688\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1497, Reward: -29.738427155456133, Moving Avg Reward: -24.135055024143107, Replay Buffer Size: 49688\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1498: Lost\n",
      "Episode 1498, Avg Value Loss: 3.710459311803182, Avg Policy Loss: 4.291481018066406\n",
      "Episode 1498, Reward: -29.361958124379125, Moving Avg Reward: -24.174353902373124, Replay Buffer Size: 49697\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1498, Reward: -29.361958124379125, Moving Avg Reward: -24.174353902373124, Replay Buffer Size: 49697\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1499: Lost\n",
      "Episode 1499, Avg Value Loss: 3.0056866142484875, Avg Policy Loss: 4.4992660946316185\n",
      "Episode 1499, Reward: -23.83599863132698, Moving Avg Reward: -24.08746266324343, Replay Buffer Size: 49706\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1499, Reward: -23.83599863132698, Moving Avg Reward: -24.08746266324343, Replay Buffer Size: 49706\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1500: Lost\n",
      "Episode 1500, Avg Value Loss: 3.278747396916151, Avg Policy Loss: 4.4771841913461685\n",
      "Episode 1500, Reward: 4.792689913534632, Moving Avg Reward: -23.515607828832703, Replay Buffer Size: 49786\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1500, Reward: 4.792689913534632, Moving Avg Reward: -23.515607828832703, Replay Buffer Size: 49786\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1501: Lost\n",
      "Episode 1501, Avg Value Loss: 3.6346199247572155, Avg Policy Loss: 4.524111641777886\n",
      "Episode 1501, Reward: -28.75285054547789, Moving Avg Reward: -23.797801861234188, Replay Buffer Size: 49795\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1501, Reward: -28.75285054547789, Moving Avg Reward: -23.797801861234188, Replay Buffer Size: 49795\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1502: Lost\n",
      "Episode 1502, Avg Value Loss: 3.285062657462226, Avg Policy Loss: 4.457525041368273\n",
      "Episode 1502, Reward: -23.863866123911908, Moving Avg Reward: -23.807282189630293, Replay Buffer Size: 49804\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1502, Reward: -23.863866123911908, Moving Avg Reward: -23.807282189630293, Replay Buffer Size: 49804\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1503: Lost\n",
      "Episode 1503, Avg Value Loss: 3.5202353459138136, Avg Policy Loss: 4.569769932673528\n",
      "Episode 1503, Reward: -33.07381473371137, Moving Avg Reward: -23.898617668996895, Replay Buffer Size: 49817\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1503, Reward: -33.07381473371137, Moving Avg Reward: -23.898617668996895, Replay Buffer Size: 49817\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1504: Lost\n",
      "Episode 1504, Avg Value Loss: 2.2147515501294817, Avg Policy Loss: 4.482143061501639\n",
      "Episode 1504, Reward: -22.191911856618177, Moving Avg Reward: -24.29304673234246, Replay Buffer Size: 49824\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1504, Reward: -22.191911856618177, Moving Avg Reward: -24.29304673234246, Replay Buffer Size: 49824\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1505: Lost\n",
      "Episode 1505, Avg Value Loss: 2.715789571404457, Avg Policy Loss: 4.718171834945679\n",
      "Episode 1505, Reward: -24.649710845776102, Moving Avg Reward: -24.23892140912737, Replay Buffer Size: 49832\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.69\n",
      "Episode 1505, Reward: -24.649710845776102, Moving Avg Reward: -24.23892140912737, Replay Buffer Size: 49832\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1506: Lost\n",
      "Episode 1506, Avg Value Loss: 3.293771115216342, Avg Policy Loss: 4.666439836675471\n",
      "Episode 1506, Reward: -29.414866248937727, Moving Avg Reward: -24.26396192664116, Replay Buffer Size: 49843\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1506, Reward: -29.414866248937727, Moving Avg Reward: -24.26396192664116, Replay Buffer Size: 49843\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1507: Lost\n",
      "Episode 1507, Avg Value Loss: 3.19097176194191, Avg Policy Loss: 4.524196588993073\n",
      "Episode 1507, Reward: -74.35340084407679, Moving Avg Reward: -25.05556964193665, Replay Buffer Size: 49923\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1507, Reward: -74.35340084407679, Moving Avg Reward: -25.05556964193665, Replay Buffer Size: 49923\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1508: Lost\n",
      "Episode 1508, Avg Value Loss: 3.527938329256498, Avg Policy Loss: 4.575932924564068\n",
      "Episode 1508, Reward: -39.29273104299141, Moving Avg Reward: -24.9089356750425, Replay Buffer Size: 49936\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1508, Reward: -39.29273104299141, Moving Avg Reward: -24.9089356750425, Replay Buffer Size: 49936\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1509: Lost\n",
      "Episode 1509, Avg Value Loss: 3.0907948513825736, Avg Policy Loss: 4.4967267115910845\n",
      "Episode 1509, Reward: -36.38345950885416, Moving Avg Reward: -24.992186939527837, Replay Buffer Size: 49948\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1509, Reward: -36.38345950885416, Moving Avg Reward: -24.992186939527837, Replay Buffer Size: 49948\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1510: Lost\n",
      "Episode 1510, Avg Value Loss: 3.2830862998962402, Avg Policy Loss: 4.627902189890544\n",
      "Episode 1510, Reward: -23.294524546333314, Moving Avg Reward: -24.947512848007264, Replay Buffer Size: 49957\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1510, Reward: -23.294524546333314, Moving Avg Reward: -24.947512848007264, Replay Buffer Size: 49957\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1511: Lost\n",
      "Episode 1511, Avg Value Loss: 3.7837230183861474, Avg Policy Loss: 4.620459946719083\n",
      "Episode 1511, Reward: -26.8026918822991, Moving Avg Reward: -25.198682885454026, Replay Buffer Size: 49968\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1511, Reward: -26.8026918822991, Moving Avg Reward: -25.198682885454026, Replay Buffer Size: 49968\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1512: Lost\n",
      "Episode 1512, Avg Value Loss: 3.526359333097935, Avg Policy Loss: 4.481473475694656\n",
      "Episode 1512, Reward: -9.158740520645384, Moving Avg Reward: -24.91207818720589, Replay Buffer Size: 50048\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1512, Reward: -9.158740520645384, Moving Avg Reward: -24.91207818720589, Replay Buffer Size: 50048\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1513: Lost\n",
      "Episode 1513, Avg Value Loss: 3.544249332868136, Avg Policy Loss: 4.592232704162598\n",
      "Episode 1513, Reward: -31.400073867177717, Moving Avg Reward: -24.797363260483245, Replay Buffer Size: 50061\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1513, Reward: -31.400073867177717, Moving Avg Reward: -24.797363260483245, Replay Buffer Size: 50061\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1514: Lost\n",
      "Episode 1514, Avg Value Loss: 3.862132489681244, Avg Policy Loss: 4.466940820217133\n",
      "Episode 1514, Reward: -23.041902588286433, Moving Avg Reward: -24.651268704070304, Replay Buffer Size: 50069\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1514, Reward: -23.041902588286433, Moving Avg Reward: -24.651268704070304, Replay Buffer Size: 50069\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1515: Lost\n",
      "Episode 1515, Avg Value Loss: 3.4784469074673123, Avg Policy Loss: 4.490865654415554\n",
      "Episode 1515, Reward: 18.84534535240296, Moving Avg Reward: -24.16844714673724, Replay Buffer Size: 50087\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1515, Reward: 18.84534535240296, Moving Avg Reward: -24.16844714673724, Replay Buffer Size: 50087\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1516: Won\n",
      "Episode 1516, Avg Value Loss: 3.241064201701771, Avg Policy Loss: 4.484926960685036\n",
      "Episode 1516, Reward: -25.02818119313183, Moving Avg Reward: -24.114420062807973, Replay Buffer Size: 50098\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1516, Reward: -25.02818119313183, Moving Avg Reward: -24.114420062807973, Replay Buffer Size: 50098\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1517: Won\n",
      "Episode 1517, Avg Value Loss: 3.8803526560465493, Avg Policy Loss: 4.546717166900635\n",
      "Episode 1517, Reward: -19.21484243671351, Moving Avg Reward: -24.052094048819757, Replay Buffer Size: 50104\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1517, Reward: -19.21484243671351, Moving Avg Reward: -24.052094048819757, Replay Buffer Size: 50104\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1518: Won\n",
      "Episode 1518, Avg Value Loss: 3.2397936314344404, Avg Policy Loss: 4.479106771945953\n",
      "Episode 1518, Reward: 18.234888869134767, Moving Avg Reward: -23.592522658749868, Replay Buffer Size: 50144\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1518, Reward: 18.234888869134767, Moving Avg Reward: -23.592522658749868, Replay Buffer Size: 50144\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1519: Won\n",
      "Episode 1519, Avg Value Loss: 3.3500714540481566, Avg Policy Loss: 4.616270136833191\n",
      "Episode 1519, Reward: -40.37492313012043, Moving Avg Reward: -23.70343560452953, Replay Buffer Size: 50224\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1519, Reward: -40.37492313012043, Moving Avg Reward: -23.70343560452953, Replay Buffer Size: 50224\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1520: Won\n",
      "Episode 1520, Avg Value Loss: 3.5022791822751365, Avg Policy Loss: 4.568554441134135\n",
      "Episode 1520, Reward: -26.385934041879402, Moving Avg Reward: -23.91076894332126, Replay Buffer Size: 50236\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1520, Reward: -26.385934041879402, Moving Avg Reward: -23.91076894332126, Replay Buffer Size: 50236\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1521: Won\n",
      "Episode 1521, Avg Value Loss: 3.092919132113457, Avg Policy Loss: 4.59815753698349\n",
      "Episode 1521, Reward: 3.4821226744569334, Moving Avg Reward: -23.660744494999193, Replay Buffer Size: 50316\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1521, Reward: 3.4821226744569334, Moving Avg Reward: -23.660744494999193, Replay Buffer Size: 50316\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1522: Lost\n",
      "Episode 1522, Avg Value Loss: 3.8970422983169555, Avg Policy Loss: 4.368040943145752\n",
      "Episode 1522, Reward: -28.473496263533715, Moving Avg Reward: -23.61849019739931, Replay Buffer Size: 50326\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1522, Reward: -28.473496263533715, Moving Avg Reward: -23.61849019739931, Replay Buffer Size: 50326\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1523: Lost\n",
      "Episode 1523, Avg Value Loss: 3.885246843099594, Avg Policy Loss: 4.579482754071553\n",
      "Episode 1523, Reward: -35.779936804636, Moving Avg Reward: -23.73465156687278, Replay Buffer Size: 50338\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1523, Reward: -35.779936804636, Moving Avg Reward: -23.73465156687278, Replay Buffer Size: 50338\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1524: Lost\n",
      "Episode 1524, Avg Value Loss: 2.9144423246383666, Avg Policy Loss: 4.581727409362793\n",
      "Episode 1524, Reward: -25.577944040488532, Moving Avg Reward: -24.165786607968872, Replay Buffer Size: 50348\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1524, Reward: -25.577944040488532, Moving Avg Reward: -24.165786607968872, Replay Buffer Size: 50348\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1525: Lost\n",
      "Episode 1525, Avg Value Loss: 2.878852436939875, Avg Policy Loss: 4.637226025263469\n",
      "Episode 1525, Reward: -29.599209546787954, Moving Avg Reward: -24.14853935413219, Replay Buffer Size: 50360\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1525, Reward: -29.599209546787954, Moving Avg Reward: -24.14853935413219, Replay Buffer Size: 50360\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1526: Lost\n",
      "Episode 1526, Avg Value Loss: 3.590617367199489, Avg Policy Loss: 4.6948379789079935\n",
      "Episode 1526, Reward: -23.665422698162565, Moving Avg Reward: -24.111011099455112, Replay Buffer Size: 50367\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1526, Reward: -23.665422698162565, Moving Avg Reward: -24.111011099455112, Replay Buffer Size: 50367\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1527: Lost\n",
      "Episode 1527, Avg Value Loss: 3.0859063069025674, Avg Policy Loss: 4.593182563781738\n",
      "Episode 1527, Reward: -29.522816600169264, Moving Avg Reward: -23.223081454238745, Replay Buffer Size: 50376\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1527, Reward: -29.522816600169264, Moving Avg Reward: -23.223081454238745, Replay Buffer Size: 50376\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1528: Lost\n",
      "Episode 1528, Avg Value Loss: 3.5898515780766806, Avg Policy Loss: 4.594143350919087\n",
      "Episode 1528, Reward: -35.047906701528525, Moving Avg Reward: -23.595667929521206, Replay Buffer Size: 50388\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1528, Reward: -35.047906701528525, Moving Avg Reward: -23.595667929521206, Replay Buffer Size: 50388\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1529: Lost\n",
      "Episode 1529, Avg Value Loss: 3.087689777215322, Avg Policy Loss: 4.632736762364705\n",
      "Episode 1529, Reward: -37.72823501829245, Moving Avg Reward: -23.71329228977639, Replay Buffer Size: 50400\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1529, Reward: -37.72823501829245, Moving Avg Reward: -23.71329228977639, Replay Buffer Size: 50400\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1530: Lost\n",
      "Episode 1530, Avg Value Loss: 3.3300754853657315, Avg Policy Loss: 4.589433874402728\n",
      "Episode 1530, Reward: -21.339496383092218, Moving Avg Reward: -23.748786797259076, Replay Buffer Size: 50407\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1530, Reward: -21.339496383092218, Moving Avg Reward: -23.748786797259076, Replay Buffer Size: 50407\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1531: Lost\n",
      "Episode 1531, Avg Value Loss: 3.449904974301656, Avg Policy Loss: 4.3258382479349775\n",
      "Episode 1531, Reward: 9.85, Moving Avg Reward: -23.43577778225683, Replay Buffer Size: 50422\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1531, Reward: 9.85, Moving Avg Reward: -23.43577778225683, Replay Buffer Size: 50422\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1532: Won\n",
      "Episode 1532, Avg Value Loss: 3.2020416378974916, Avg Policy Loss: 4.498221111297608\n",
      "Episode 1532, Reward: -28.930300610118316, Moving Avg Reward: -23.914075690214208, Replay Buffer Size: 50432\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1532, Reward: -28.930300610118316, Moving Avg Reward: -23.914075690214208, Replay Buffer Size: 50432\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1533: Won\n",
      "Episode 1533, Avg Value Loss: 3.0719300714032403, Avg Policy Loss: 4.653054714202881\n",
      "Episode 1533, Reward: -39.5423160914698, Moving Avg Reward: -24.025795110948998, Replay Buffer Size: 50461\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1533, Reward: -39.5423160914698, Moving Avg Reward: -24.025795110948998, Replay Buffer Size: 50461\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1534: Won\n",
      "Episode 1534, Avg Value Loss: 2.790395639159463, Avg Policy Loss: 4.643156571821733\n",
      "Episode 1534, Reward: -30.29732020078046, Moving Avg Reward: -23.96504503406835, Replay Buffer Size: 50472\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1534, Reward: -30.29732020078046, Moving Avg Reward: -23.96504503406835, Replay Buffer Size: 50472\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1535: Won\n",
      "Episode 1535, Avg Value Loss: 3.5287740770727396, Avg Policy Loss: 4.4919606521725655\n",
      "Episode 1535, Reward: -56.62267658301409, Moving Avg Reward: -24.24951858558692, Replay Buffer Size: 50536\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1535, Reward: -56.62267658301409, Moving Avg Reward: -24.24951858558692, Replay Buffer Size: 50536\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1536: Won\n",
      "Episode 1536, Avg Value Loss: 3.2720048969442193, Avg Policy Loss: 4.518275997855446\n",
      "Episode 1536, Reward: -27.147757541465687, Moving Avg Reward: -24.20719677506955, Replay Buffer Size: 50558\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1536, Reward: -27.147757541465687, Moving Avg Reward: -24.20719677506955, Replay Buffer Size: 50558\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1537: Won\n",
      "Episode 1537, Avg Value Loss: 3.471990504860878, Avg Policy Loss: 4.539919602870941\n",
      "Episode 1537, Reward: 7.462741957389673, Moving Avg Reward: -23.80835514645871, Replay Buffer Size: 50638\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 1537, Reward: 7.462741957389673, Moving Avg Reward: -23.80835514645871, Replay Buffer Size: 50638\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1538: Lost\n",
      "Episode 1538, Avg Value Loss: 3.0666354656219483, Avg Policy Loss: 4.491125917434692\n",
      "Episode 1538, Reward: -27.09677699553884, Moving Avg Reward: -23.75926309345005, Replay Buffer Size: 50648\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1538, Reward: -27.09677699553884, Moving Avg Reward: -23.75926309345005, Replay Buffer Size: 50648\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1539: Lost\n",
      "Episode 1539, Avg Value Loss: 3.4160568200051786, Avg Policy Loss: 4.52899449467659\n",
      "Episode 1539, Reward: -15.023661305599202, Moving Avg Reward: -23.19122668828449, Replay Buffer Size: 50728\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1539, Reward: -15.023661305599202, Moving Avg Reward: -23.19122668828449, Replay Buffer Size: 50728\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1540: Lost\n",
      "Episode 1540, Avg Value Loss: 3.3580928087234496, Avg Policy Loss: 4.4777051448822025\n",
      "Episode 1540, Reward: -23.441004816061966, Moving Avg Reward: -23.596357801736076, Replay Buffer Size: 50738\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1540, Reward: -23.441004816061966, Moving Avg Reward: -23.596357801736076, Replay Buffer Size: 50738\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1541: Lost\n",
      "Episode 1541, Avg Value Loss: 3.608844071626663, Avg Policy Loss: 4.593717336654663\n",
      "Episode 1541, Reward: -31.650793044640118, Moving Avg Reward: -23.5920293842606, Replay Buffer Size: 50750\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1541, Reward: -31.650793044640118, Moving Avg Reward: -23.5920293842606, Replay Buffer Size: 50750\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1542: Lost\n",
      "Episode 1542, Avg Value Loss: 3.400642932785882, Avg Policy Loss: 4.639333110385471\n",
      "Episode 1542, Reward: -53.62146880419708, Moving Avg Reward: -23.801146603990148, Replay Buffer Size: 50795\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1542, Reward: -53.62146880419708, Moving Avg Reward: -23.801146603990148, Replay Buffer Size: 50795\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1543: Lost\n",
      "Episode 1543, Avg Value Loss: 3.156377633412679, Avg Policy Loss: 4.620019594828288\n",
      "Episode 1543, Reward: -24.128212218723863, Moving Avg Reward: -23.768070129879483, Replay Buffer Size: 50804\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1543, Reward: -24.128212218723863, Moving Avg Reward: -23.768070129879483, Replay Buffer Size: 50804\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1544: Lost\n",
      "Episode 1544, Avg Value Loss: 3.185330663621426, Avg Policy Loss: 4.568726554512978\n",
      "Episode 1544, Reward: -41.53831567865498, Moving Avg Reward: -23.313218680185866, Replay Buffer Size: 50884\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.70\n",
      "Episode 1544, Reward: -41.53831567865498, Moving Avg Reward: -23.313218680185866, Replay Buffer Size: 50884\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1545: Lost\n",
      "Episode 1545, Avg Value Loss: 3.011553731831637, Avg Policy Loss: 4.7380266189575195\n",
      "Episode 1545, Reward: -30.541774455104267, Moving Avg Reward: -23.54781216257277, Replay Buffer Size: 50895\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1545, Reward: -30.541774455104267, Moving Avg Reward: -23.54781216257277, Replay Buffer Size: 50895\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1546: Lost\n",
      "Episode 1546, Avg Value Loss: 3.3192058756947516, Avg Policy Loss: 4.575277811288833\n",
      "Episode 1546, Reward: 0.8757054082350006, Moving Avg Reward: -23.53909588849391, Replay Buffer Size: 50975\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1546, Reward: 0.8757054082350006, Moving Avg Reward: -23.53909588849391, Replay Buffer Size: 50975\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1547: Lost\n",
      "Episode 1547, Avg Value Loss: 3.3321440145373344, Avg Policy Loss: 4.527527105808258\n",
      "Episode 1547, Reward: -20.882681423892024, Moving Avg Reward: -23.279250033080498, Replay Buffer Size: 51055\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1547, Reward: -20.882681423892024, Moving Avg Reward: -23.279250033080498, Replay Buffer Size: 51055\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1548: Draw\n",
      "Episode 1548, Avg Value Loss: 3.189685272425413, Avg Policy Loss: 4.658553111553192\n",
      "Episode 1548, Reward: -0.8000000000000005, Moving Avg Reward: -22.746894038852812, Replay Buffer Size: 51135\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1548, Reward: -0.8000000000000005, Moving Avg Reward: -22.746894038852812, Replay Buffer Size: 51135\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1549: Draw\n",
      "Episode 1549, Avg Value Loss: 2.961650165644559, Avg Policy Loss: 4.515519532290372\n",
      "Episode 1549, Reward: -31.772184457529914, Moving Avg Reward: -22.719869352900364, Replay Buffer Size: 51146\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1549, Reward: -31.772184457529914, Moving Avg Reward: -22.719869352900364, Replay Buffer Size: 51146\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1550: Lost\n",
      "Episode 1550, Avg Value Loss: 3.099623441696167, Avg Policy Loss: 4.781868197701194\n",
      "Episode 1550, Reward: -24.011086501212567, Moving Avg Reward: -22.68659692561075, Replay Buffer Size: 51157\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1550, Reward: -24.011086501212567, Moving Avg Reward: -22.68659692561075, Replay Buffer Size: 51157\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1551: Lost\n",
      "Episode 1551, Avg Value Loss: 3.138132241639224, Avg Policy Loss: 4.422720779072154\n",
      "Episode 1551, Reward: -24.88567541903617, Moving Avg Reward: -22.660836240936423, Replay Buffer Size: 51168\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1551, Reward: -24.88567541903617, Moving Avg Reward: -22.660836240936423, Replay Buffer Size: 51168\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1552: Lost\n",
      "Episode 1552, Avg Value Loss: 3.337340945005417, Avg Policy Loss: 4.540610265731812\n",
      "Episode 1552, Reward: 0.8821348921165584, Moving Avg Reward: -22.383069410602214, Replay Buffer Size: 51248\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1552, Reward: 0.8821348921165584, Moving Avg Reward: -22.383069410602214, Replay Buffer Size: 51248\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1553: Lost\n",
      "Episode 1553, Avg Value Loss: 2.8822595477104187, Avg Policy Loss: 4.673579394817352\n",
      "Episode 1553, Reward: -27.822088596297057, Moving Avg Reward: -22.376633597979886, Replay Buffer Size: 51256\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1553, Reward: -27.822088596297057, Moving Avg Reward: -22.376633597979886, Replay Buffer Size: 51256\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1554: Lost\n",
      "Episode 1554, Avg Value Loss: 2.8772832697088067, Avg Policy Loss: 4.646196322007612\n",
      "Episode 1554, Reward: -35.64943489043435, Moving Avg Reward: -22.47483742301293, Replay Buffer Size: 51267\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1554, Reward: -35.64943489043435, Moving Avg Reward: -22.47483742301293, Replay Buffer Size: 51267\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1555: Lost\n",
      "Episode 1555, Avg Value Loss: 3.138046322045503, Avg Policy Loss: 4.624369992150201\n",
      "Episode 1555, Reward: 16.78537348625953, Moving Avg Reward: -22.067699944146025, Replay Buffer Size: 51294\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1555, Reward: 16.78537348625953, Moving Avg Reward: -22.067699944146025, Replay Buffer Size: 51294\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1556: Won\n",
      "Episode 1556, Avg Value Loss: 3.295682340226275, Avg Policy Loss: 4.615696237442341\n",
      "Episode 1556, Reward: -56.651040142894466, Moving Avg Reward: -22.362935727928722, Replay Buffer Size: 51341\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1556, Reward: -56.651040142894466, Moving Avg Reward: -22.362935727928722, Replay Buffer Size: 51341\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1557: Won\n",
      "Episode 1557, Avg Value Loss: 2.9571655657556324, Avg Policy Loss: 4.554542541503906\n",
      "Episode 1557, Reward: -27.44980012462809, Moving Avg Reward: -22.386536561966988, Replay Buffer Size: 51350\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.71\n",
      "Episode 1557, Reward: -27.44980012462809, Moving Avg Reward: -22.386536561966988, Replay Buffer Size: 51350\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1558: Won\n",
      "Episode 1558, Avg Value Loss: 2.997429363189205, Avg Policy Loss: 4.508151669656077\n",
      "Episode 1558, Reward: -18.90899636933128, Moving Avg Reward: -22.381224570425157, Replay Buffer Size: 51381\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1558, Reward: -18.90899636933128, Moving Avg Reward: -22.381224570425157, Replay Buffer Size: 51381\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1559: Won\n",
      "Episode 1559, Avg Value Loss: 3.46550425440073, Avg Policy Loss: 4.609627074003219\n",
      "Episode 1559, Reward: -24.59712010249085, Moving Avg Reward: -22.36523727118943, Replay Buffer Size: 51461\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1559, Reward: -24.59712010249085, Moving Avg Reward: -22.36523727118943, Replay Buffer Size: 51461\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1560: Won\n",
      "Episode 1560, Avg Value Loss: 3.0277525583902993, Avg Policy Loss: 4.674796263376872\n",
      "Episode 1560, Reward: -33.51521985848626, Moving Avg Reward: -22.38520532995697, Replay Buffer Size: 51473\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1560, Reward: -33.51521985848626, Moving Avg Reward: -22.38520532995697, Replay Buffer Size: 51473\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1561: Won\n",
      "Episode 1561, Avg Value Loss: 3.398263919353485, Avg Policy Loss: 4.422777843475342\n",
      "Episode 1561, Reward: -28.2479502941879, Moving Avg Reward: -22.208765725705643, Replay Buffer Size: 51483\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1561, Reward: -28.2479502941879, Moving Avg Reward: -22.208765725705643, Replay Buffer Size: 51483\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1562: Won\n",
      "Episode 1562, Avg Value Loss: 3.4800832088177023, Avg Policy Loss: 4.640447213099553\n",
      "Episode 1562, Reward: -28.88507388469019, Moving Avg Reward: -22.19378991265057, Replay Buffer Size: 51496\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1562, Reward: -28.88507388469019, Moving Avg Reward: -22.19378991265057, Replay Buffer Size: 51496\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1563: Lost\n",
      "Episode 1563, Avg Value Loss: 4.252363469865587, Avg Policy Loss: 4.6770483122931585\n",
      "Episode 1563, Reward: -28.301746141352496, Moving Avg Reward: -22.6568584501845, Replay Buffer Size: 51505\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1563, Reward: -28.301746141352496, Moving Avg Reward: -22.6568584501845, Replay Buffer Size: 51505\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1564: Lost\n",
      "Episode 1564, Avg Value Loss: 3.2860179424285887, Avg Policy Loss: 4.431285858154297\n",
      "Episode 1564, Reward: -26.327952306421146, Moving Avg Reward: -22.71808850461652, Replay Buffer Size: 51515\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1564, Reward: -26.327952306421146, Moving Avg Reward: -22.71808850461652, Replay Buffer Size: 51515\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1565: Lost\n",
      "Episode 1565, Avg Value Loss: 2.3087213784456253, Avg Policy Loss: 4.567575752735138\n",
      "Episode 1565, Reward: -23.64802579299385, Moving Avg Reward: -23.041495465835315, Replay Buffer Size: 51523\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.72\n",
      "Episode 1565, Reward: -23.64802579299385, Moving Avg Reward: -23.041495465835315, Replay Buffer Size: 51523\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1566: Lost\n",
      "Episode 1566, Avg Value Loss: 3.265580342366145, Avg Policy Loss: 4.69863117658175\n",
      "Episode 1566, Reward: -30.609604590613223, Moving Avg Reward: -23.086978109726573, Replay Buffer Size: 51536\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1566, Reward: -30.609604590613223, Moving Avg Reward: -23.086978109726573, Replay Buffer Size: 51536\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1567: Lost\n",
      "Episode 1567, Avg Value Loss: 3.3629101339508507, Avg Policy Loss: 4.619547900031595\n",
      "Episode 1567, Reward: 14.2904210883751, Moving Avg Reward: -22.648019408996998, Replay Buffer Size: 51570\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1567, Reward: 14.2904210883751, Moving Avg Reward: -22.648019408996998, Replay Buffer Size: 51570\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1568: Won\n",
      "Episode 1568, Avg Value Loss: 3.5106031461195513, Avg Policy Loss: 4.559072061018511\n",
      "Episode 1568, Reward: -30.090836176962625, Moving Avg Reward: -22.736913742238045, Replay Buffer Size: 51581\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1568, Reward: -30.090836176962625, Moving Avg Reward: -22.736913742238045, Replay Buffer Size: 51581\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1569: Won\n",
      "Episode 1569, Avg Value Loss: 3.2738758524258933, Avg Policy Loss: 4.698570887247722\n",
      "Episode 1569, Reward: -29.96443217796424, Moving Avg Reward: -22.77531787359063, Replay Buffer Size: 51593\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1569, Reward: -29.96443217796424, Moving Avg Reward: -22.77531787359063, Replay Buffer Size: 51593\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1570: Won\n",
      "Episode 1570, Avg Value Loss: 3.763401734828949, Avg Policy Loss: 4.642119407653809\n",
      "Episode 1570, Reward: -26.21612906648818, Moving Avg Reward: -22.805863871244675, Replay Buffer Size: 51603\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1570, Reward: -26.21612906648818, Moving Avg Reward: -22.805863871244675, Replay Buffer Size: 51603\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1571: Won\n",
      "Episode 1571, Avg Value Loss: 3.2630360381943837, Avg Policy Loss: 4.69405106135777\n",
      "Episode 1571, Reward: -36.070483895063, Moving Avg Reward: -22.83233400094398, Replay Buffer Size: 51617\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1571, Reward: -36.070483895063, Moving Avg Reward: -22.83233400094398, Replay Buffer Size: 51617\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1572: Won\n",
      "Episode 1572, Avg Value Loss: 3.2792721182107925, Avg Policy Loss: 4.56805049777031\n",
      "Episode 1572, Reward: 4.2317905001501686, Moving Avg Reward: -22.56756520658943, Replay Buffer Size: 51697\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1572, Reward: 4.2317905001501686, Moving Avg Reward: -22.56756520658943, Replay Buffer Size: 51697\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1573: Won\n",
      "Episode 1573, Avg Value Loss: 3.0336720835078848, Avg Policy Loss: 4.509038708426735\n",
      "Episode 1573, Reward: -28.350231943396963, Moving Avg Reward: -23.050491349239337, Replay Buffer Size: 51708\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1573, Reward: -28.350231943396963, Moving Avg Reward: -23.050491349239337, Replay Buffer Size: 51708\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1574: Won\n",
      "Episode 1574, Avg Value Loss: 3.4008492112159727, Avg Policy Loss: 4.627807378768921\n",
      "Episode 1574, Reward: 15.312380854323562, Moving Avg Reward: -22.694718114029442, Replay Buffer Size: 51748\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1574, Reward: 15.312380854323562, Moving Avg Reward: -22.694718114029442, Replay Buffer Size: 51748\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1575: Won\n",
      "Episode 1575, Avg Value Loss: 3.3603983342647554, Avg Policy Loss: 4.544361436367035\n",
      "Episode 1575, Reward: -25.182050667356005, Moving Avg Reward: -22.68640777892287, Replay Buffer Size: 51828\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1575, Reward: -25.182050667356005, Moving Avg Reward: -22.68640777892287, Replay Buffer Size: 51828\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1576: Won\n",
      "Episode 1576, Avg Value Loss: 3.106979641047391, Avg Policy Loss: 4.755727811293169\n",
      "Episode 1576, Reward: -30.51987750797501, Moving Avg Reward: -22.696688630545342, Replay Buffer Size: 51839\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1576, Reward: -30.51987750797501, Moving Avg Reward: -22.696688630545342, Replay Buffer Size: 51839\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1577: Won\n",
      "Episode 1577, Avg Value Loss: 3.219043492525816, Avg Policy Loss: 4.59836847782135\n",
      "Episode 1577, Reward: -100.48674606569953, Moving Avg Reward: -23.36582639017688, Replay Buffer Size: 51919\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1577, Reward: -100.48674606569953, Moving Avg Reward: -23.36582639017688, Replay Buffer Size: 51919\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1578: Lost\n",
      "Episode 1578, Avg Value Loss: 3.1857813469001224, Avg Policy Loss: 4.660436817577907\n",
      "Episode 1578, Reward: 14.385558231022214, Moving Avg Reward: -22.923902373228316, Replay Buffer Size: 51947\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1578, Reward: 14.385558231022214, Moving Avg Reward: -22.923902373228316, Replay Buffer Size: 51947\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1579: Won\n",
      "Episode 1579, Avg Value Loss: 2.9516679415336022, Avg Policy Loss: 4.731902342576247\n",
      "Episode 1579, Reward: -30.854153875710526, Moving Avg Reward: -23.04126316160707, Replay Buffer Size: 51960\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1579, Reward: -30.854153875710526, Moving Avg Reward: -23.04126316160707, Replay Buffer Size: 51960\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1580: Won\n",
      "Episode 1580, Avg Value Loss: 3.970566064119339, Avg Policy Loss: 4.785725235939026\n",
      "Episode 1580, Reward: -30.84110898022984, Moving Avg Reward: -23.0964350267184, Replay Buffer Size: 51972\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1580, Reward: -30.84110898022984, Moving Avg Reward: -23.0964350267184, Replay Buffer Size: 51972\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1581: Won\n",
      "Episode 1581, Avg Value Loss: 3.654067248106003, Avg Policy Loss: 4.337791383266449\n",
      "Episode 1581, Reward: -24.38902311456771, Moving Avg Reward: -23.44983109966115, Replay Buffer Size: 51980\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1581, Reward: -24.38902311456771, Moving Avg Reward: -23.44983109966115, Replay Buffer Size: 51980\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1582: Won\n",
      "Episode 1582, Avg Value Loss: 3.654643562104967, Avg Policy Loss: 4.736529721154107\n",
      "Episode 1582, Reward: -26.156600017417375, Moving Avg Reward: -23.385634873382028, Replay Buffer Size: 51989\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1582, Reward: -26.156600017417375, Moving Avg Reward: -23.385634873382028, Replay Buffer Size: 51989\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1583: Won\n",
      "Episode 1583, Avg Value Loss: 2.5735700130462646, Avg Policy Loss: 4.7444944977760315\n",
      "Episode 1583, Reward: -25.09625100963253, Moving Avg Reward: -23.37190944453471, Replay Buffer Size: 51997\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1583, Reward: -25.09625100963253, Moving Avg Reward: -23.37190944453471, Replay Buffer Size: 51997\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1584: Won\n",
      "Episode 1584, Avg Value Loss: 3.3095959782600404, Avg Policy Loss: 4.565038935343424\n",
      "Episode 1584, Reward: -67.88286211777411, Moving Avg Reward: -23.995515946914743, Replay Buffer Size: 52042\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1584, Reward: -67.88286211777411, Moving Avg Reward: -23.995515946914743, Replay Buffer Size: 52042\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1585: Won\n",
      "Episode 1585, Avg Value Loss: 3.0954911817203867, Avg Policy Loss: 4.632452011108398\n",
      "Episode 1585, Reward: -28.076920001657275, Moving Avg Reward: -24.524985146931307, Replay Buffer Size: 52053\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1585, Reward: -28.076920001657275, Moving Avg Reward: -24.524985146931307, Replay Buffer Size: 52053\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1586: Won\n",
      "Episode 1586, Avg Value Loss: 3.0871624037623406, Avg Policy Loss: 4.669620913267136\n",
      "Episode 1586, Reward: -61.45916169721045, Moving Avg Reward: -24.828357006352288, Replay Buffer Size: 52133\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1586, Reward: -61.45916169721045, Moving Avg Reward: -24.828357006352288, Replay Buffer Size: 52133\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1587: Lost\n",
      "Episode 1587, Avg Value Loss: 2.9043456415335336, Avg Policy Loss: 4.533556302388509\n",
      "Episode 1587, Reward: -34.03806844108746, Moving Avg Reward: -25.05914598616243, Replay Buffer Size: 52145\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1587, Reward: -34.03806844108746, Moving Avg Reward: -25.05914598616243, Replay Buffer Size: 52145\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1588: Lost\n",
      "Episode 1588, Avg Value Loss: 3.064281630516052, Avg Policy Loss: 4.82850284576416\n",
      "Episode 1588, Reward: -27.782767262344297, Moving Avg Reward: -25.046631618017756, Replay Buffer Size: 52155\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1588, Reward: -27.782767262344297, Moving Avg Reward: -25.046631618017756, Replay Buffer Size: 52155\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1589: Lost\n",
      "Episode 1589, Avg Value Loss: 3.1797657879916104, Avg Policy Loss: 4.643310546875\n",
      "Episode 1589, Reward: -25.995499216675547, Moving Avg Reward: -25.039951809523714, Replay Buffer Size: 52166\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1589, Reward: -25.995499216675547, Moving Avg Reward: -25.039951809523714, Replay Buffer Size: 52166\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1590: Lost\n",
      "Episode 1590, Avg Value Loss: 3.470930242538452, Avg Policy Loss: 4.4762232303619385\n",
      "Episode 1590, Reward: -33.120326386338064, Moving Avg Reward: -25.082567912676662, Replay Buffer Size: 52176\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1590, Reward: -33.120326386338064, Moving Avg Reward: -25.082567912676662, Replay Buffer Size: 52176\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1591: Lost\n",
      "Episode 1591, Avg Value Loss: 3.385336272052077, Avg Policy Loss: 4.678986916776563\n",
      "Episode 1591, Reward: -66.05754287021878, Moving Avg Reward: -25.46411135109369, Replay Buffer Size: 52237\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1591, Reward: -66.05754287021878, Moving Avg Reward: -25.46411135109369, Replay Buffer Size: 52237\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1592: Lost\n",
      "Episode 1592, Avg Value Loss: 2.4653462717930474, Avg Policy Loss: 4.806683659553528\n",
      "Episode 1592, Reward: -31.677205215403266, Moving Avg Reward: -25.482797551435755, Replay Buffer Size: 52249\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1592, Reward: -31.677205215403266, Moving Avg Reward: -25.482797551435755, Replay Buffer Size: 52249\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1593: Lost\n",
      "Episode 1593, Avg Value Loss: 2.735256476835771, Avg Policy Loss: 4.609226443550804\n",
      "Episode 1593, Reward: -33.21357037336848, Moving Avg Reward: -25.992041781138987, Replay Buffer Size: 52260\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1593, Reward: -33.21357037336848, Moving Avg Reward: -25.992041781138987, Replay Buffer Size: 52260\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1594: Lost\n",
      "Episode 1594, Avg Value Loss: 3.4569503929879932, Avg Policy Loss: 4.600265423456828\n",
      "Episode 1594, Reward: 9.82, Moving Avg Reward: -26.057592906183395, Replay Buffer Size: 52278\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1594, Reward: 9.82, Moving Avg Reward: -26.057592906183395, Replay Buffer Size: 52278\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1595: Won\n",
      "Episode 1595, Avg Value Loss: 3.34212868979999, Avg Policy Loss: 4.677186480590275\n",
      "Episode 1595, Reward: -12.718710317107229, Moving Avg Reward: -25.904880861012384, Replay Buffer Size: 52334\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1595, Reward: -12.718710317107229, Moving Avg Reward: -25.904880861012384, Replay Buffer Size: 52334\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1596: Won\n",
      "Episode 1596, Avg Value Loss: 3.623819424555852, Avg Policy Loss: 4.700439453125\n",
      "Episode 1596, Reward: -31.29501960667031, Moving Avg Reward: -25.671817579435743, Replay Buffer Size: 52347\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1596, Reward: -31.29501960667031, Moving Avg Reward: -25.671817579435743, Replay Buffer Size: 52347\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1597: Won\n",
      "Episode 1597, Avg Value Loss: 3.1362301468849183, Avg Policy Loss: 4.807292366027832\n",
      "Episode 1597, Reward: -32.931639437219374, Moving Avg Reward: -25.70374970225338, Replay Buffer Size: 52357\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1597, Reward: -32.931639437219374, Moving Avg Reward: -25.70374970225338, Replay Buffer Size: 52357\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1598: Won\n",
      "Episode 1598, Avg Value Loss: 3.7602773308753967, Avg Policy Loss: 4.52701997756958\n",
      "Episode 1598, Reward: 12.707390462151125, Moving Avg Reward: -25.283056216388076, Replay Buffer Size: 52380\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1598, Reward: 12.707390462151125, Moving Avg Reward: -25.283056216388076, Replay Buffer Size: 52380\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1599: Won\n",
      "Episode 1599, Avg Value Loss: 2.7231139540672302, Avg Policy Loss: 4.700836579004924\n",
      "Episode 1599, Reward: -27.889274839464548, Moving Avg Reward: -25.323588978469456, Replay Buffer Size: 52392\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1599, Reward: -27.889274839464548, Moving Avg Reward: -25.323588978469456, Replay Buffer Size: 52392\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1600: Won\n",
      "Episode 1600, Avg Value Loss: 3.223420013080944, Avg Policy Loss: 4.536916212602095\n",
      "Episode 1600, Reward: -30.998647317955523, Moving Avg Reward: -25.681502350784356, Replay Buffer Size: 52403\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1600, Reward: -30.998647317955523, Moving Avg Reward: -25.681502350784356, Replay Buffer Size: 52403\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1601: Won\n",
      "Episode 1601, Avg Value Loss: 2.645977459170602, Avg Policy Loss: 4.8063227046619765\n",
      "Episode 1601, Reward: -34.4193079509664, Moving Avg Reward: -25.738166924839238, Replay Buffer Size: 52414\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1601, Reward: -34.4193079509664, Moving Avg Reward: -25.738166924839238, Replay Buffer Size: 52414\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1602: Won\n",
      "Episode 1602, Avg Value Loss: 3.1772368055369173, Avg Policy Loss: 4.6498315419469565\n",
      "Episode 1602, Reward: -57.29397417663741, Moving Avg Reward: -26.072468005366492, Replay Buffer Size: 52470\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1602, Reward: -57.29397417663741, Moving Avg Reward: -26.072468005366492, Replay Buffer Size: 52470\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1603: Won\n",
      "Episode 1603, Avg Value Loss: 3.469329576194286, Avg Policy Loss: 4.631127399206162\n",
      "Episode 1603, Reward: -43.98908207918831, Moving Avg Reward: -26.181620678821265, Replay Buffer Size: 52550\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1603, Reward: -43.98908207918831, Moving Avg Reward: -26.181620678821265, Replay Buffer Size: 52550\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1604: Lost\n",
      "Episode 1604, Avg Value Loss: 2.7471384635338416, Avg Policy Loss: 4.749991453610933\n",
      "Episode 1604, Reward: -26.372312675972964, Moving Avg Reward: -26.22342468701481, Replay Buffer Size: 52563\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1604, Reward: -26.372312675972964, Moving Avg Reward: -26.22342468701481, Replay Buffer Size: 52563\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1605: Lost\n",
      "Episode 1605, Avg Value Loss: 3.2243435382843018, Avg Policy Loss: 4.778578417641776\n",
      "Episode 1605, Reward: -24.668219863665126, Moving Avg Reward: -26.22360977719369, Replay Buffer Size: 52570\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1605, Reward: -24.668219863665126, Moving Avg Reward: -26.22360977719369, Replay Buffer Size: 52570\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1606: Lost\n",
      "Episode 1606, Avg Value Loss: 3.1047456115484238, Avg Policy Loss: 4.6585600733757015\n",
      "Episode 1606, Reward: 5.499669295185994, Moving Avg Reward: -25.874464421752464, Replay Buffer Size: 52650\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1606, Reward: 5.499669295185994, Moving Avg Reward: -25.874464421752464, Replay Buffer Size: 52650\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1607: Lost\n",
      "Episode 1607, Avg Value Loss: 3.019291355059697, Avg Policy Loss: 4.728495010962853\n",
      "Episode 1607, Reward: -34.655187704391565, Moving Avg Reward: -25.47748229035561, Replay Buffer Size: 52663\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1607, Reward: -34.655187704391565, Moving Avg Reward: -25.47748229035561, Replay Buffer Size: 52663\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1608: Lost\n",
      "Episode 1608, Avg Value Loss: 2.6695974005593195, Avg Policy Loss: 4.777392069498698\n",
      "Episode 1608, Reward: -26.946406055682345, Moving Avg Reward: -25.354019040482523, Replay Buffer Size: 52672\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1608, Reward: -26.946406055682345, Moving Avg Reward: -25.354019040482523, Replay Buffer Size: 52672\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1609: Lost\n",
      "Episode 1609, Avg Value Loss: 4.397982627153397, Avg Policy Loss: 4.559911727905273\n",
      "Episode 1609, Reward: -23.608914173126294, Moving Avg Reward: -25.22627358712524, Replay Buffer Size: 52680\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1609, Reward: -23.608914173126294, Moving Avg Reward: -25.22627358712524, Replay Buffer Size: 52680\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1610: Lost\n",
      "Episode 1610, Avg Value Loss: 3.1411675044468472, Avg Policy Loss: 4.66820308140346\n",
      "Episode 1610, Reward: -25.102178237118046, Moving Avg Reward: -25.24435012403309, Replay Buffer Size: 52687\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1610, Reward: -25.102178237118046, Moving Avg Reward: -25.24435012403309, Replay Buffer Size: 52687\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1611: Lost\n",
      "Episode 1611, Avg Value Loss: 3.602510118484497, Avg Policy Loss: 4.700830698013306\n",
      "Episode 1611, Reward: -26.62807830372568, Moving Avg Reward: -25.242603988247353, Replay Buffer Size: 52697\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1611, Reward: -26.62807830372568, Moving Avg Reward: -25.242603988247353, Replay Buffer Size: 52697\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1612: Lost\n",
      "Episode 1612, Avg Value Loss: 2.775255968173345, Avg Policy Loss: 4.628892103830974\n",
      "Episode 1612, Reward: -32.26515693033822, Moving Avg Reward: -25.473668152344285, Replay Buffer Size: 52709\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1612, Reward: -32.26515693033822, Moving Avg Reward: -25.473668152344285, Replay Buffer Size: 52709\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1613: Lost\n",
      "Episode 1613, Avg Value Loss: 3.362957502901554, Avg Policy Loss: 4.617761474847794\n",
      "Episode 1613, Reward: 4.670729322651626, Moving Avg Reward: -25.112960120445987, Replay Buffer Size: 52789\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1613, Reward: 4.670729322651626, Moving Avg Reward: -25.112960120445987, Replay Buffer Size: 52789\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1614: Lost\n",
      "Episode 1614, Avg Value Loss: 3.493537127971649, Avg Policy Loss: 4.842393398284912\n",
      "Episode 1614, Reward: -28.14656970726177, Moving Avg Reward: -25.164006791635742, Replay Buffer Size: 52797\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1614, Reward: -28.14656970726177, Moving Avg Reward: -25.164006791635742, Replay Buffer Size: 52797\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1615: Lost\n",
      "Episode 1615, Avg Value Loss: 2.6124845097462335, Avg Policy Loss: 4.698037028312683\n",
      "Episode 1615, Reward: -30.78236462861249, Moving Avg Reward: -25.660283891445893, Replay Buffer Size: 52809\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1615, Reward: -30.78236462861249, Moving Avg Reward: -25.660283891445893, Replay Buffer Size: 52809\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1616: Lost\n",
      "Episode 1616, Avg Value Loss: 3.0470109352698693, Avg Policy Loss: 4.663165752704327\n",
      "Episode 1616, Reward: -29.861862218006614, Moving Avg Reward: -25.70862070169464, Replay Buffer Size: 52822\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1616, Reward: -29.861862218006614, Moving Avg Reward: -25.70862070169464, Replay Buffer Size: 52822\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1617: Lost\n",
      "Episode 1617, Avg Value Loss: 3.09582533286168, Avg Policy Loss: 4.795460517589863\n",
      "Episode 1617, Reward: -35.03403591033289, Moving Avg Reward: -25.866812636430833, Replay Buffer Size: 52835\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1617, Reward: -35.03403591033289, Moving Avg Reward: -25.866812636430833, Replay Buffer Size: 52835\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1618: Lost\n",
      "Episode 1618, Avg Value Loss: 3.087871836870909, Avg Policy Loss: 4.654530048370361\n",
      "Episode 1618, Reward: -28.645116598985485, Moving Avg Reward: -26.335612691112036, Replay Buffer Size: 52915\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1618, Reward: -28.645116598985485, Moving Avg Reward: -26.335612691112036, Replay Buffer Size: 52915\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1619: Lost\n",
      "Episode 1619, Avg Value Loss: 3.262979324047382, Avg Policy Loss: 4.745677801278921\n",
      "Episode 1619, Reward: -34.29097175506092, Moving Avg Reward: -26.27477317736145, Replay Buffer Size: 52928\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1619, Reward: -34.29097175506092, Moving Avg Reward: -26.27477317736145, Replay Buffer Size: 52928\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1620: Lost\n",
      "Episode 1620, Avg Value Loss: 3.3736491799354553, Avg Policy Loss: 4.671365698178609\n",
      "Episode 1620, Reward: -28.344475802427358, Moving Avg Reward: -26.29435859496693, Replay Buffer Size: 52940\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1620, Reward: -28.344475802427358, Moving Avg Reward: -26.29435859496693, Replay Buffer Size: 52940\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1621: Lost\n",
      "Episode 1621, Avg Value Loss: 2.77087394396464, Avg Policy Loss: 4.635599613189697\n",
      "Episode 1621, Reward: -30.912381412678734, Moving Avg Reward: -26.638303635838284, Replay Buffer Size: 52952\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1621, Reward: -30.912381412678734, Moving Avg Reward: -26.638303635838284, Replay Buffer Size: 52952\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1622: Lost\n",
      "Episode 1622, Avg Value Loss: 3.5109949429829914, Avg Policy Loss: 4.619701552391052\n",
      "Episode 1622, Reward: -1.6363976741601007, Moving Avg Reward: -26.369932649944545, Replay Buffer Size: 53012\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1622, Reward: -1.6363976741601007, Moving Avg Reward: -26.369932649944545, Replay Buffer Size: 53012\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1623: Won\n",
      "Episode 1623, Avg Value Loss: 3.4363093331456183, Avg Policy Loss: 4.679637002944946\n",
      "Episode 1623, Reward: -17.321429755994977, Moving Avg Reward: -26.185347579458135, Replay Buffer Size: 53092\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1623, Reward: -17.321429755994977, Moving Avg Reward: -26.185347579458135, Replay Buffer Size: 53092\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1624: Won\n",
      "Episode 1624, Avg Value Loss: 2.6838433742523193, Avg Policy Loss: 4.787045478820801\n",
      "Episode 1624, Reward: -26.846772330272664, Moving Avg Reward: -26.198035862355972, Replay Buffer Size: 53102\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1624, Reward: -26.846772330272664, Moving Avg Reward: -26.198035862355972, Replay Buffer Size: 53102\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1625: Won\n",
      "Episode 1625, Avg Value Loss: 3.9941052993138633, Avg Policy Loss: 4.64309291044871\n",
      "Episode 1625, Reward: -31.566335504023982, Moving Avg Reward: -26.217707121928335, Replay Buffer Size: 53114\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1625, Reward: -31.566335504023982, Moving Avg Reward: -26.217707121928335, Replay Buffer Size: 53114\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1626: Won\n",
      "Episode 1626, Avg Value Loss: 3.2431566454470158, Avg Policy Loss: 4.644906669855118\n",
      "Episode 1626, Reward: 4.507653107083968, Moving Avg Reward: -25.935976363875866, Replay Buffer Size: 53194\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1626, Reward: 4.507653107083968, Moving Avg Reward: -25.935976363875866, Replay Buffer Size: 53194\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1627: Lost\n",
      "Episode 1627, Avg Value Loss: 2.7516838022640773, Avg Policy Loss: 4.622338976178851\n",
      "Episode 1627, Reward: 15.477571245712861, Moving Avg Reward: -25.485972485417047, Replay Buffer Size: 53229\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1627, Reward: 15.477571245712861, Moving Avg Reward: -25.485972485417047, Replay Buffer Size: 53229\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1628: Won\n",
      "Episode 1628, Avg Value Loss: 3.187923463908109, Avg Policy Loss: 4.673876177180897\n",
      "Episode 1628, Reward: -35.4445553610487, Moving Avg Reward: -25.489938972012247, Replay Buffer Size: 53251\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1628, Reward: -35.4445553610487, Moving Avg Reward: -25.489938972012247, Replay Buffer Size: 53251\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1629: Won\n",
      "Episode 1629, Avg Value Loss: 2.7837088841658373, Avg Policy Loss: 4.710960498222938\n",
      "Episode 1629, Reward: -28.071922151520425, Moving Avg Reward: -25.393375843344526, Replay Buffer Size: 53264\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1629, Reward: -28.071922151520425, Moving Avg Reward: -25.393375843344526, Replay Buffer Size: 53264\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1630: Won\n",
      "Episode 1630, Avg Value Loss: 4.026954910971901, Avg Policy Loss: 4.587682030417702\n",
      "Episode 1630, Reward: -29.98930423662909, Moving Avg Reward: -25.4798739218799, Replay Buffer Size: 53275\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1630, Reward: -29.98930423662909, Moving Avg Reward: -25.4798739218799, Replay Buffer Size: 53275\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1631: Won\n",
      "Episode 1631, Avg Value Loss: 3.293886970728636, Avg Policy Loss: 4.64075339436531\n",
      "Episode 1631, Reward: -57.70882713402087, Moving Avg Reward: -26.155462193220107, Replay Buffer Size: 53355\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1631, Reward: -57.70882713402087, Moving Avg Reward: -26.155462193220107, Replay Buffer Size: 53355\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1632: Won\n",
      "Episode 1632, Avg Value Loss: 3.33846323688825, Avg Policy Loss: 4.818528453509013\n",
      "Episode 1632, Reward: -33.436698285221674, Moving Avg Reward: -26.200526169971134, Replay Buffer Size: 53367\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1632, Reward: -33.436698285221674, Moving Avg Reward: -26.200526169971134, Replay Buffer Size: 53367\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1633: Won\n",
      "Episode 1633, Avg Value Loss: 2.6194089140210832, Avg Policy Loss: 4.717975207737514\n",
      "Episode 1633, Reward: -41.108694190640286, Moving Avg Reward: -26.21618995096284, Replay Buffer Size: 53381\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1633, Reward: -41.108694190640286, Moving Avg Reward: -26.21618995096284, Replay Buffer Size: 53381\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1634: Lost\n",
      "Episode 1634, Avg Value Loss: 4.359843860973012, Avg Policy Loss: 4.583158709786155\n",
      "Episode 1634, Reward: -27.789863340418677, Moving Avg Reward: -26.19111538235923, Replay Buffer Size: 53392\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1634, Reward: -27.789863340418677, Moving Avg Reward: -26.19111538235923, Replay Buffer Size: 53392\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1635: Lost\n",
      "Episode 1635, Avg Value Loss: 3.647430968284607, Avg Policy Loss: 4.477607011795044\n",
      "Episode 1635, Reward: -24.185168661200287, Moving Avg Reward: -25.866740303141093, Replay Buffer Size: 53402\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1635, Reward: -24.185168661200287, Moving Avg Reward: -25.866740303141093, Replay Buffer Size: 53402\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1636: Lost\n",
      "Episode 1636, Avg Value Loss: 4.248815488815308, Avg Policy Loss: 4.510583209991455\n",
      "Episode 1636, Reward: 9.950000000000001, Moving Avg Reward: -25.495762727726436, Replay Buffer Size: 53407\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1636, Reward: 9.950000000000001, Moving Avg Reward: -25.495762727726436, Replay Buffer Size: 53407\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1637: Won\n",
      "Episode 1637, Avg Value Loss: 2.806863715251287, Avg Policy Loss: 4.794983744621277\n",
      "Episode 1637, Reward: -30.84577993989577, Moving Avg Reward: -25.87884794669929, Replay Buffer Size: 53419\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1637, Reward: -30.84577993989577, Moving Avg Reward: -25.87884794669929, Replay Buffer Size: 53419\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1638: Won\n",
      "Episode 1638, Avg Value Loss: 3.92823449048129, Avg Policy Loss: 4.621908361261541\n",
      "Episode 1638, Reward: -26.55694452028451, Moving Avg Reward: -25.873449621946747, Replay Buffer Size: 53430\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1638, Reward: -26.55694452028451, Moving Avg Reward: -25.873449621946747, Replay Buffer Size: 53430\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1639: Won\n",
      "Episode 1639, Avg Value Loss: 3.3891450383446435, Avg Policy Loss: 4.64201511036266\n",
      "Episode 1639, Reward: -30.598198800433366, Moving Avg Reward: -26.029194996895086, Replay Buffer Size: 53441\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.73\n",
      "Episode 1639, Reward: -30.598198800433366, Moving Avg Reward: -26.029194996895086, Replay Buffer Size: 53441\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1640: Won\n",
      "Episode 1640, Avg Value Loss: 2.9486140608787537, Avg Policy Loss: 4.646194577217102\n",
      "Episode 1640, Reward: -37.33276632637948, Moving Avg Reward: -26.168112611998264, Replay Buffer Size: 53453\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1640, Reward: -37.33276632637948, Moving Avg Reward: -26.168112611998264, Replay Buffer Size: 53453\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1641: Won\n",
      "Episode 1641, Avg Value Loss: 3.1506503224372864, Avg Policy Loss: 4.765961249669393\n",
      "Episode 1641, Reward: -17.721948789733403, Moving Avg Reward: -26.02882416944919, Replay Buffer Size: 53459\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1641, Reward: -17.721948789733403, Moving Avg Reward: -26.02882416944919, Replay Buffer Size: 53459\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1642: Won\n",
      "Episode 1642, Avg Value Loss: 3.1459681193033853, Avg Policy Loss: 4.664794524510701\n",
      "Episode 1642, Reward: -31.493863541763425, Moving Avg Reward: -25.807548116824854, Replay Buffer Size: 53507\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1642, Reward: -31.493863541763425, Moving Avg Reward: -25.807548116824854, Replay Buffer Size: 53507\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1643: Won\n",
      "Episode 1643, Avg Value Loss: 2.7478707134723663, Avg Policy Loss: 4.601978778839111\n",
      "Episode 1643, Reward: -24.641385778135156, Moving Avg Reward: -25.81267985241897, Replay Buffer Size: 53515\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1643, Reward: -24.641385778135156, Moving Avg Reward: -25.81267985241897, Replay Buffer Size: 53515\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1644: Won\n",
      "Episode 1644, Avg Value Loss: 3.6364716103202417, Avg Policy Loss: 4.552435724358809\n",
      "Episode 1644, Reward: 19.800834465273812, Moving Avg Reward: -25.199288350979682, Replay Buffer Size: 53534\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1644, Reward: 19.800834465273812, Moving Avg Reward: -25.199288350979682, Replay Buffer Size: 53534\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1645: Won\n",
      "Episode 1645, Avg Value Loss: 3.147463947534561, Avg Policy Loss: 4.6789326667785645\n",
      "Episode 1645, Reward: -24.857933422919825, Moving Avg Reward: -25.14244994065784, Replay Buffer Size: 53542\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1645, Reward: -24.857933422919825, Moving Avg Reward: -25.14244994065784, Replay Buffer Size: 53542\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1646: Won\n",
      "Episode 1646, Avg Value Loss: 2.8738427294625177, Avg Policy Loss: 4.8008742862277565\n",
      "Episode 1646, Reward: -25.342284714611168, Moving Avg Reward: -25.404629841886305, Replay Buffer Size: 53551\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1646, Reward: -25.342284714611168, Moving Avg Reward: -25.404629841886305, Replay Buffer Size: 53551\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1647: Won\n",
      "Episode 1647, Avg Value Loss: 2.822040398915609, Avg Policy Loss: 4.797036647796631\n",
      "Episode 1647, Reward: -28.706257642172076, Moving Avg Reward: -25.482865604069097, Replay Buffer Size: 53560\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1647, Reward: -28.706257642172076, Moving Avg Reward: -25.482865604069097, Replay Buffer Size: 53560\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1648: Won\n",
      "Episode 1648, Avg Value Loss: 3.3319082657496133, Avg Policy Loss: 4.359107441372341\n",
      "Episode 1648, Reward: -26.908673295692033, Moving Avg Reward: -25.74395233702602, Replay Buffer Size: 53569\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1648, Reward: -26.908673295692033, Moving Avg Reward: -25.74395233702602, Replay Buffer Size: 53569\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1649: Won\n",
      "Episode 1649, Avg Value Loss: 3.180028150975704, Avg Policy Loss: 4.7308307707309725\n",
      "Episode 1649, Reward: -25.058939860390534, Moving Avg Reward: -25.676819891054627, Replay Buffer Size: 53649\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1649, Reward: -25.058939860390534, Moving Avg Reward: -25.676819891054627, Replay Buffer Size: 53649\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1650: Won\n",
      "Episode 1650, Avg Value Loss: 3.3370203100144864, Avg Policy Loss: 4.630180352926255\n",
      "Episode 1650, Reward: 0.45862160140716846, Moving Avg Reward: -25.43212281002843, Replay Buffer Size: 53729\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1650, Reward: 0.45862160140716846, Moving Avg Reward: -25.43212281002843, Replay Buffer Size: 53729\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1651: Draw\n",
      "Episode 1651, Avg Value Loss: 2.873724418878555, Avg Policy Loss: 4.572965860366821\n",
      "Episode 1651, Reward: -29.984637246477504, Moving Avg Reward: -25.483112428302846, Replay Buffer Size: 53739\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1651, Reward: -29.984637246477504, Moving Avg Reward: -25.483112428302846, Replay Buffer Size: 53739\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1652: Lost\n",
      "Episode 1652, Avg Value Loss: 3.265689079463482, Avg Policy Loss: 4.717975926399231\n",
      "Episode 1652, Reward: -107.08290636027007, Moving Avg Reward: -26.562762840826714, Replay Buffer Size: 53819\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1652, Reward: -107.08290636027007, Moving Avg Reward: -26.562762840826714, Replay Buffer Size: 53819\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1653: Lost\n",
      "Episode 1653, Avg Value Loss: 3.38082363208135, Avg Policy Loss: 4.645610173543294\n",
      "Episode 1653, Reward: -25.5519234750941, Moving Avg Reward: -26.54006118961468, Replay Buffer Size: 53831\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1653, Reward: -25.5519234750941, Moving Avg Reward: -26.54006118961468, Replay Buffer Size: 53831\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1654: Lost\n",
      "Episode 1654, Avg Value Loss: 3.395720852166414, Avg Policy Loss: 4.734013795852661\n",
      "Episode 1654, Reward: 3.6905009461418556, Moving Avg Reward: -26.146661831248917, Replay Buffer Size: 53911\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1654, Reward: 3.6905009461418556, Moving Avg Reward: -26.146661831248917, Replay Buffer Size: 53911\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1655: Lost\n",
      "Episode 1655, Avg Value Loss: 2.8209862609704337, Avg Policy Loss: 4.706383029619853\n",
      "Episode 1655, Reward: -28.05096988899303, Moving Avg Reward: -26.59502526500144, Replay Buffer Size: 53923\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1655, Reward: -28.05096988899303, Moving Avg Reward: -26.59502526500144, Replay Buffer Size: 53923\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1656: Lost\n",
      "Episode 1656, Avg Value Loss: 3.214997798204422, Avg Policy Loss: 4.649122873942058\n",
      "Episode 1656, Reward: -26.390513438574928, Moving Avg Reward: -26.292419997958245, Replay Buffer Size: 53935\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.80\n",
      "Episode 1656, Reward: -26.390513438574928, Moving Avg Reward: -26.292419997958245, Replay Buffer Size: 53935\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1657: Lost\n",
      "Episode 1657, Avg Value Loss: 3.7504572570323944, Avg Policy Loss: 4.50774359703064\n",
      "Episode 1657, Reward: -25.490284228864894, Moving Avg Reward: -26.272824839000613, Replay Buffer Size: 53943\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1657, Reward: -25.490284228864894, Moving Avg Reward: -26.272824839000613, Replay Buffer Size: 53943\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1658: Lost\n",
      "Episode 1658, Avg Value Loss: 3.179899162054062, Avg Policy Loss: 4.8353600025177\n",
      "Episode 1658, Reward: -33.3398876587212, Moving Avg Reward: -26.417133751894518, Replay Buffer Size: 53953\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1658, Reward: -33.3398876587212, Moving Avg Reward: -26.417133751894518, Replay Buffer Size: 53953\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1659: Lost\n",
      "Episode 1659, Avg Value Loss: 2.8702474707051326, Avg Policy Loss: 4.6918669499849015\n",
      "Episode 1659, Reward: 18.829430080169164, Moving Avg Reward: -25.982868250067916, Replay Buffer Size: 53972\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1659, Reward: 18.829430080169164, Moving Avg Reward: -25.982868250067916, Replay Buffer Size: 53972\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1660: Won\n",
      "Episode 1660, Avg Value Loss: 3.8342155516147614, Avg Policy Loss: 4.9095237255096436\n",
      "Episode 1660, Reward: -27.191733403778443, Moving Avg Reward: -25.91963338552084, Replay Buffer Size: 53980\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1660, Reward: -27.191733403778443, Moving Avg Reward: -25.91963338552084, Replay Buffer Size: 53980\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1661: Won\n",
      "Episode 1661, Avg Value Loss: 3.710655391216278, Avg Policy Loss: 4.749024709065755\n",
      "Episode 1661, Reward: -28.958263611195083, Moving Avg Reward: -25.92673651869091, Replay Buffer Size: 53992\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1661, Reward: -28.958263611195083, Moving Avg Reward: -25.92673651869091, Replay Buffer Size: 53992\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1662: Won\n",
      "Episode 1662, Avg Value Loss: 3.095161183178425, Avg Policy Loss: 4.725102210044861\n",
      "Episode 1662, Reward: -36.85435120479134, Moving Avg Reward: -26.006429291891923, Replay Buffer Size: 54032\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1662, Reward: -36.85435120479134, Moving Avg Reward: -26.006429291891923, Replay Buffer Size: 54032\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1663: Won\n",
      "Episode 1663, Avg Value Loss: 3.1386781769829826, Avg Policy Loss: 4.70499866073196\n",
      "Episode 1663, Reward: -27.706307133485076, Moving Avg Reward: -26.000474901813245, Replay Buffer Size: 54069\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1663, Reward: -27.706307133485076, Moving Avg Reward: -26.000474901813245, Replay Buffer Size: 54069\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1664: Won\n",
      "Episode 1664, Avg Value Loss: 3.242417660626498, Avg Policy Loss: 4.6778434840115635\n",
      "Episode 1664, Reward: -27.696285463643882, Moving Avg Reward: -26.01415823338548, Replay Buffer Size: 54080\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1664, Reward: -27.696285463643882, Moving Avg Reward: -26.01415823338548, Replay Buffer Size: 54080\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1665: Won\n",
      "Episode 1665, Avg Value Loss: 3.227014176356487, Avg Policy Loss: 4.7458104353684645\n",
      "Episode 1665, Reward: -29.556665665873066, Moving Avg Reward: -26.07324463211427, Replay Buffer Size: 54158\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1665, Reward: -29.556665665873066, Moving Avg Reward: -26.07324463211427, Replay Buffer Size: 54158\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1666: Lost\n",
      "Episode 1666, Avg Value Loss: 3.326257641498859, Avg Policy Loss: 4.729853214361729\n",
      "Episode 1666, Reward: -37.33283569500744, Moving Avg Reward: -26.14047694315821, Replay Buffer Size: 54197\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1666, Reward: -37.33283569500744, Moving Avg Reward: -26.14047694315821, Replay Buffer Size: 54197\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1667: Lost\n",
      "Episode 1667, Avg Value Loss: 2.8554972112178802, Avg Policy Loss: 4.953279376029968\n",
      "Episode 1667, Reward: -26.107257562395095, Moving Avg Reward: -26.54445372966591, Replay Buffer Size: 54205\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.81\n",
      "Episode 1667, Reward: -26.107257562395095, Moving Avg Reward: -26.54445372966591, Replay Buffer Size: 54205\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1668: Lost\n",
      "Episode 1668, Avg Value Loss: 3.0613459475112683, Avg Policy Loss: 4.80941154017593\n",
      "Episode 1668, Reward: -15.063362395308415, Moving Avg Reward: -26.394178991849373, Replay Buffer Size: 54238\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1668, Reward: -15.063362395308415, Moving Avg Reward: -26.394178991849373, Replay Buffer Size: 54238\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1669: Won\n",
      "Episode 1669, Avg Value Loss: 3.0903976792874546, Avg Policy Loss: 4.780386758887249\n",
      "Episode 1669, Reward: -43.90721939016744, Moving Avg Reward: -26.533606863971404, Replay Buffer Size: 54261\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1669, Reward: -43.90721939016744, Moving Avg Reward: -26.533606863971404, Replay Buffer Size: 54261\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1670: Won\n",
      "Episode 1670, Avg Value Loss: 3.131982900726963, Avg Policy Loss: 4.758693245095267\n",
      "Episode 1670, Reward: -38.747649476066165, Moving Avg Reward: -26.658922068067184, Replay Buffer Size: 54332\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1670, Reward: -38.747649476066165, Moving Avg Reward: -26.658922068067184, Replay Buffer Size: 54332\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1671: Won\n",
      "Episode 1671, Avg Value Loss: 2.8469190171786716, Avg Policy Loss: 4.708959681647165\n",
      "Episode 1671, Reward: -37.44319568738946, Moving Avg Reward: -26.672649185990448, Replay Buffer Size: 54346\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1671, Reward: -37.44319568738946, Moving Avg Reward: -26.672649185990448, Replay Buffer Size: 54346\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1672: Won\n",
      "Episode 1672, Avg Value Loss: 3.25696662068367, Avg Policy Loss: 4.688028335571289\n",
      "Episode 1672, Reward: -35.361818868038306, Moving Avg Reward: -27.068585279672334, Replay Buffer Size: 54358\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1672, Reward: -35.361818868038306, Moving Avg Reward: -27.068585279672334, Replay Buffer Size: 54358\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1673: Won\n",
      "Episode 1673, Avg Value Loss: 3.2644041925668716, Avg Policy Loss: 4.701103746891022\n",
      "Episode 1673, Reward: -27.723075119681802, Moving Avg Reward: -27.06231371143518, Replay Buffer Size: 54366\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.80\n",
      "Episode 1673, Reward: -27.723075119681802, Moving Avg Reward: -27.06231371143518, Replay Buffer Size: 54366\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1674: Won\n",
      "Episode 1674, Avg Value Loss: 2.7389039234681563, Avg Policy Loss: 4.806187152862549\n",
      "Episode 1674, Reward: -33.27542197008306, Moving Avg Reward: -27.548191739679247, Replay Buffer Size: 54377\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1674, Reward: -33.27542197008306, Moving Avg Reward: -27.548191739679247, Replay Buffer Size: 54377\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1675: Won\n",
      "Episode 1675, Avg Value Loss: 2.9523250552324147, Avg Policy Loss: 4.879698973435622\n",
      "Episode 1675, Reward: 9.870000000000001, Moving Avg Reward: -27.197671233005686, Replay Buffer Size: 54390\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1675, Reward: 9.870000000000001, Moving Avg Reward: -27.197671233005686, Replay Buffer Size: 54390\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1676: Won\n",
      "Episode 1676, Avg Value Loss: 3.1229626767337324, Avg Policy Loss: 4.707099449634552\n",
      "Episode 1676, Reward: 4.782042002503392, Moving Avg Reward: -26.844652037900904, Replay Buffer Size: 54470\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1676, Reward: 4.782042002503392, Moving Avg Reward: -26.844652037900904, Replay Buffer Size: 54470\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1677: Won\n",
      "Episode 1677, Avg Value Loss: 3.411754379669825, Avg Policy Loss: 4.741511265436809\n",
      "Episode 1677, Reward: -31.370950318541574, Moving Avg Reward: -26.153494080429322, Replay Buffer Size: 54482\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1677, Reward: -31.370950318541574, Moving Avg Reward: -26.153494080429322, Replay Buffer Size: 54482\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1678: Won\n",
      "Episode 1678, Avg Value Loss: 3.35123985906442, Avg Policy Loss: 4.786698309580485\n",
      "Episode 1678, Reward: -39.046987370749115, Moving Avg Reward: -26.687819536447037, Replay Buffer Size: 54542\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.80\n",
      "Episode 1678, Reward: -39.046987370749115, Moving Avg Reward: -26.687819536447037, Replay Buffer Size: 54542\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1679: Lost\n",
      "Episode 1679, Avg Value Loss: 3.400736772097074, Avg Policy Loss: 4.845682400923509\n",
      "Episode 1679, Reward: -29.49460858106915, Moving Avg Reward: -26.674224083500622, Replay Buffer Size: 54555\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1679, Reward: -29.49460858106915, Moving Avg Reward: -26.674224083500622, Replay Buffer Size: 54555\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1680: Lost\n",
      "Episode 1680, Avg Value Loss: 2.9905874729156494, Avg Policy Loss: 4.70382377079555\n",
      "Episode 1680, Reward: -19.730438263581085, Moving Avg Reward: -26.563117376334134, Replay Buffer Size: 54562\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1680, Reward: -19.730438263581085, Moving Avg Reward: -26.563117376334134, Replay Buffer Size: 54562\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1681: Lost\n",
      "Episode 1681, Avg Value Loss: 3.392166917974299, Avg Policy Loss: 4.684408014470881\n",
      "Episode 1681, Reward: -34.1153610078036, Moving Avg Reward: -26.660380755266498, Replay Buffer Size: 54573\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.81\n",
      "Episode 1681, Reward: -34.1153610078036, Moving Avg Reward: -26.660380755266498, Replay Buffer Size: 54573\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1682: Lost\n",
      "Episode 1682, Avg Value Loss: 2.795160346171435, Avg Policy Loss: 4.870370388031006\n",
      "Episode 1682, Reward: 21.363816442600587, Moving Avg Reward: -26.185176590666316, Replay Buffer Size: 54590\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.80\n",
      "Episode 1682, Reward: 21.363816442600587, Moving Avg Reward: -26.185176590666316, Replay Buffer Size: 54590\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1683: Won\n",
      "Episode 1683, Avg Value Loss: 3.1124063067966037, Avg Policy Loss: 4.929606808556451\n",
      "Episode 1683, Reward: -20.797578144345003, Moving Avg Reward: -26.14218986201344, Replay Buffer Size: 54599\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1683, Reward: -20.797578144345003, Moving Avg Reward: -26.14218986201344, Replay Buffer Size: 54599\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1684: Won\n",
      "Episode 1684, Avg Value Loss: 2.5240918397903442, Avg Policy Loss: 4.658550126211984\n",
      "Episode 1684, Reward: -24.38680865654257, Moving Avg Reward: -25.70722932740112, Replay Buffer Size: 54606\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.81\n",
      "Episode 1684, Reward: -24.38680865654257, Moving Avg Reward: -25.70722932740112, Replay Buffer Size: 54606\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1685: Won\n",
      "Episode 1685, Avg Value Loss: 3.545419931411743, Avg Policy Loss: 4.757974556514195\n",
      "Episode 1685, Reward: -22.06431995049242, Moving Avg Reward: -25.647103326889475, Replay Buffer Size: 54613\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1685, Reward: -22.06431995049242, Moving Avg Reward: -25.647103326889475, Replay Buffer Size: 54613\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1686: Won\n",
      "Episode 1686, Avg Value Loss: 3.3826423585414886, Avg Policy Loss: 4.8241459131240845\n",
      "Episode 1686, Reward: -30.825433438190046, Moving Avg Reward: -25.340766044299272, Replay Buffer Size: 54621\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1686, Reward: -30.825433438190046, Moving Avg Reward: -25.340766044299272, Replay Buffer Size: 54621\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1687: Won\n",
      "Episode 1687, Avg Value Loss: 3.0913565649705776, Avg Policy Loss: 4.815440205966725\n",
      "Episode 1687, Reward: 18.395732837392156, Moving Avg Reward: -24.816428031514473, Replay Buffer Size: 54638\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1687, Reward: 18.395732837392156, Moving Avg Reward: -24.816428031514473, Replay Buffer Size: 54638\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1688: Won\n",
      "Episode 1688, Avg Value Loss: 2.988484528006577, Avg Policy Loss: 4.753758500262005\n",
      "Episode 1688, Reward: -27.681475308586325, Moving Avg Reward: -24.81541511197689, Replay Buffer Size: 54679\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1688, Reward: -27.681475308586325, Moving Avg Reward: -24.81541511197689, Replay Buffer Size: 54679\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1689: Won\n",
      "Episode 1689, Avg Value Loss: 3.139906756579876, Avg Policy Loss: 4.795437049865723\n",
      "Episode 1689, Reward: 1.5043581535663182, Moving Avg Reward: -24.54041653827447, Replay Buffer Size: 54759\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1689, Reward: 1.5043581535663182, Moving Avg Reward: -24.54041653827447, Replay Buffer Size: 54759\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1690: Won\n",
      "Episode 1690, Avg Value Loss: 3.369347188207838, Avg Policy Loss: 4.81928284962972\n",
      "Episode 1690, Reward: -23.017210263766437, Moving Avg Reward: -24.439385377048758, Replay Buffer Size: 54768\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.80\n",
      "Episode 1690, Reward: -23.017210263766437, Moving Avg Reward: -24.439385377048758, Replay Buffer Size: 54768\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1691: Won\n",
      "Episode 1691, Avg Value Loss: 3.6097214818000793, Avg Policy Loss: 4.735731780529022\n",
      "Episode 1691, Reward: -24.26666985755694, Moving Avg Reward: -24.02147664692214, Replay Buffer Size: 54776\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1691, Reward: -24.26666985755694, Moving Avg Reward: -24.02147664692214, Replay Buffer Size: 54776\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1692: Won\n",
      "Episode 1692, Avg Value Loss: 3.5177421834733753, Avg Policy Loss: 4.653541670905219\n",
      "Episode 1692, Reward: -27.03974805972637, Moving Avg Reward: -23.975102075365367, Replay Buffer Size: 54785\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.81\n",
      "Episode 1692, Reward: -27.03974805972637, Moving Avg Reward: -23.975102075365367, Replay Buffer Size: 54785\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1693: Won\n",
      "Episode 1693, Avg Value Loss: 3.6319273312886557, Avg Policy Loss: 4.760797778765361\n",
      "Episode 1693, Reward: -32.06800245508567, Moving Avg Reward: -23.96364639618254, Replay Buffer Size: 54797\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1693, Reward: -32.06800245508567, Moving Avg Reward: -23.96364639618254, Replay Buffer Size: 54797\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1694: Lost\n",
      "Episode 1694, Avg Value Loss: 3.0041515827178955, Avg Policy Loss: 4.832277894020081\n",
      "Episode 1694, Reward: -33.31689053459902, Moving Avg Reward: -24.395015301528527, Replay Buffer Size: 54809\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.82\n",
      "Episode 1694, Reward: -33.31689053459902, Moving Avg Reward: -24.395015301528527, Replay Buffer Size: 54809\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1695: Lost\n",
      "Episode 1695, Avg Value Loss: 3.997931480407715, Avg Policy Loss: 4.753032922744751\n",
      "Episode 1695, Reward: -37.891199048572645, Moving Avg Reward: -24.646740188843186, Replay Buffer Size: 54821\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.81\n",
      "Episode 1695, Reward: -37.891199048572645, Moving Avg Reward: -24.646740188843186, Replay Buffer Size: 54821\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1696: Lost\n",
      "Episode 1696, Avg Value Loss: 2.68180855512619, Avg Policy Loss: 4.790900659561157\n",
      "Episode 1696, Reward: -32.12077215598228, Moving Avg Reward: -24.654997714336304, Replay Buffer Size: 54831\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.80\n",
      "Episode 1696, Reward: -32.12077215598228, Moving Avg Reward: -24.654997714336304, Replay Buffer Size: 54831\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1697: Lost\n",
      "Episode 1697, Avg Value Loss: 3.2502441235951016, Avg Policy Loss: 5.063782691955566\n",
      "Episode 1697, Reward: -24.43623406198933, Moving Avg Reward: -24.570043660584002, Replay Buffer Size: 54838\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.80\n",
      "Episode 1697, Reward: -24.43623406198933, Moving Avg Reward: -24.570043660584002, Replay Buffer Size: 54838\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1698: Lost\n",
      "Episode 1698, Avg Value Loss: 3.1160137388441296, Avg Policy Loss: 4.7159618801540795\n",
      "Episode 1698, Reward: -32.63408639805332, Moving Avg Reward: -25.023458429186043, Replay Buffer Size: 54847\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1698, Reward: -32.63408639805332, Moving Avg Reward: -25.023458429186043, Replay Buffer Size: 54847\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1699: Lost\n",
      "Episode 1699, Avg Value Loss: 2.671961147051591, Avg Policy Loss: 4.673761624556321\n",
      "Episode 1699, Reward: -31.234372559911105, Moving Avg Reward: -25.056909406390513, Replay Buffer Size: 54860\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1699, Reward: -31.234372559911105, Moving Avg Reward: -25.056909406390513, Replay Buffer Size: 54860\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1700: Lost\n",
      "Episode 1700, Avg Value Loss: 3.448003732241117, Avg Policy Loss: 4.772949072030874\n",
      "Episode 1700, Reward: -37.901241655389526, Moving Avg Reward: -25.12593534976485, Replay Buffer Size: 54873\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1700, Reward: -37.901241655389526, Moving Avg Reward: -25.12593534976485, Replay Buffer Size: 54873\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1701: Lost\n",
      "Episode 1701, Avg Value Loss: 3.3297719453510486, Avg Policy Loss: 4.778677589014957\n",
      "Episode 1701, Reward: 19.775440331295368, Moving Avg Reward: -24.583987866942238, Replay Buffer Size: 54892\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.81\n",
      "Episode 1701, Reward: 19.775440331295368, Moving Avg Reward: -24.583987866942238, Replay Buffer Size: 54892\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1702: Won\n",
      "Episode 1702, Avg Value Loss: 3.144149740537008, Avg Policy Loss: 4.774467077520159\n",
      "Episode 1702, Reward: -24.704437678734493, Moving Avg Reward: -24.258092501963212, Replay Buffer Size: 54964\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1702, Reward: -24.704437678734493, Moving Avg Reward: -24.258092501963212, Replay Buffer Size: 54964\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1703: Won\n",
      "Episode 1703, Avg Value Loss: 3.2531471233814955, Avg Policy Loss: 4.740649424493313\n",
      "Episode 1703, Reward: -40.337861832734994, Moving Avg Reward: -24.22158029949868, Replay Buffer Size: 55028\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1703, Reward: -40.337861832734994, Moving Avg Reward: -24.22158029949868, Replay Buffer Size: 55028\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1704: Won\n",
      "Episode 1704, Avg Value Loss: 3.3652649223804474, Avg Policy Loss: 4.670105854670207\n",
      "Episode 1704, Reward: -34.13752692113769, Moving Avg Reward: -24.29923244195032, Replay Buffer Size: 55040\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1704, Reward: -34.13752692113769, Moving Avg Reward: -24.29923244195032, Replay Buffer Size: 55040\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1705: Won\n",
      "Episode 1705, Avg Value Loss: 2.97064913643731, Avg Policy Loss: 4.949639532301161\n",
      "Episode 1705, Reward: -24.603467114809188, Moving Avg Reward: -24.298584914461756, Replay Buffer Size: 55049\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.81\n",
      "Episode 1705, Reward: -24.603467114809188, Moving Avg Reward: -24.298584914461756, Replay Buffer Size: 55049\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1706: Won\n",
      "Episode 1706, Avg Value Loss: 3.1940813759962716, Avg Policy Loss: 4.77834153175354\n",
      "Episode 1706, Reward: -34.40576688581367, Moving Avg Reward: -24.697639276271758, Replay Buffer Size: 55061\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.80\n",
      "Episode 1706, Reward: -34.40576688581367, Moving Avg Reward: -24.697639276271758, Replay Buffer Size: 55061\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1707: Won\n",
      "Episode 1707, Avg Value Loss: 2.8724673986434937, Avg Policy Loss: 4.9042478288922995\n",
      "Episode 1707, Reward: -22.931956016539033, Moving Avg Reward: -24.58040695939323, Replay Buffer Size: 55068\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1707, Reward: -22.931956016539033, Moving Avg Reward: -24.58040695939323, Replay Buffer Size: 55068\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1708: Won\n",
      "Episode 1708, Avg Value Loss: 3.4680323749780655, Avg Policy Loss: 4.718436062335968\n",
      "Episode 1708, Reward: -25.129189945891092, Moving Avg Reward: -24.56223479829532, Replay Buffer Size: 55076\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1708, Reward: -25.129189945891092, Moving Avg Reward: -24.56223479829532, Replay Buffer Size: 55076\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1709: Won\n",
      "Episode 1709, Avg Value Loss: 3.455940452488986, Avg Policy Loss: 4.8583481095053935\n",
      "Episode 1709, Reward: -35.43256917222969, Moving Avg Reward: -24.680471348286357, Replay Buffer Size: 55087\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1709, Reward: -35.43256917222969, Moving Avg Reward: -24.680471348286357, Replay Buffer Size: 55087\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1710: Won\n",
      "Episode 1710, Avg Value Loss: 3.073714163568285, Avg Policy Loss: 4.85897774166531\n",
      "Episode 1710, Reward: -24.27154081941898, Moving Avg Reward: -24.672164974109364, Replay Buffer Size: 55096\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1710, Reward: -24.27154081941898, Moving Avg Reward: -24.672164974109364, Replay Buffer Size: 55096\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1711: Won\n",
      "Episode 1711, Avg Value Loss: 3.0319776346808984, Avg Policy Loss: 4.807783135196619\n",
      "Episode 1711, Reward: 12.953479901914317, Moving Avg Reward: -24.276349392052968, Replay Buffer Size: 55153\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1711, Reward: 12.953479901914317, Moving Avg Reward: -24.276349392052968, Replay Buffer Size: 55153\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1712: Won\n",
      "Episode 1712, Avg Value Loss: 3.2990216933763943, Avg Policy Loss: 4.571046168987568\n",
      "Episode 1712, Reward: -29.903813449141435, Moving Avg Reward: -24.252735957240997, Replay Buffer Size: 55166\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1712, Reward: -29.903813449141435, Moving Avg Reward: -24.252735957240997, Replay Buffer Size: 55166\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1713: Won\n",
      "Episode 1713, Avg Value Loss: 3.1488029190472195, Avg Policy Loss: 4.79930397442409\n",
      "Episode 1713, Reward: -38.401178841488814, Moving Avg Reward: -24.683455038882403, Replay Buffer Size: 55180\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1713, Reward: -38.401178841488814, Moving Avg Reward: -24.683455038882403, Replay Buffer Size: 55180\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1714: Won\n",
      "Episode 1714, Avg Value Loss: 3.2545351803302767, Avg Policy Loss: 4.760665082931519\n",
      "Episode 1714, Reward: -16.305305818816404, Moving Avg Reward: -24.565042399997946, Replay Buffer Size: 55260\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1714, Reward: -16.305305818816404, Moving Avg Reward: -24.565042399997946, Replay Buffer Size: 55260\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1715: Won\n",
      "Episode 1715, Avg Value Loss: 3.298528982533349, Avg Policy Loss: 4.755385292900933\n",
      "Episode 1715, Reward: 20.857820152066473, Moving Avg Reward: -24.048640552191163, Replay Buffer Size: 55278\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1715, Reward: 20.857820152066473, Moving Avg Reward: -24.048640552191163, Replay Buffer Size: 55278\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1716: Won\n",
      "Episode 1716, Avg Value Loss: 3.126552416132642, Avg Policy Loss: 4.805875784390933\n",
      "Episode 1716, Reward: -30.220372880405733, Moving Avg Reward: -24.05222565881515, Replay Buffer Size: 55355\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1716, Reward: -30.220372880405733, Moving Avg Reward: -24.05222565881515, Replay Buffer Size: 55355\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1717: Won\n",
      "Episode 1717, Avg Value Loss: 3.773567823263315, Avg Policy Loss: 4.887834585629976\n",
      "Episode 1717, Reward: -36.83244524362917, Moving Avg Reward: -24.070209752148116, Replay Buffer Size: 55368\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.81\n",
      "Episode 1717, Reward: -36.83244524362917, Moving Avg Reward: -24.070209752148116, Replay Buffer Size: 55368\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1718: Won\n",
      "Episode 1718, Avg Value Loss: 3.796028595704299, Avg Policy Loss: 4.768368647648738\n",
      "Episode 1718, Reward: -33.016045056643165, Moving Avg Reward: -24.11391903672469, Replay Buffer Size: 55381\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1718, Reward: -33.016045056643165, Moving Avg Reward: -24.11391903672469, Replay Buffer Size: 55381\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1719: Won\n",
      "Episode 1719, Avg Value Loss: 3.1708939472834268, Avg Policy Loss: 4.7937222719192505\n",
      "Episode 1719, Reward: -33.91117453634128, Moving Avg Reward: -24.110121064537495, Replay Buffer Size: 55393\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1719, Reward: -33.91117453634128, Moving Avg Reward: -24.110121064537495, Replay Buffer Size: 55393\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1720: Won\n",
      "Episode 1720, Avg Value Loss: 2.9406502842903137, Avg Policy Loss: 5.065410455067952\n",
      "Episode 1720, Reward: -29.900744953771394, Moving Avg Reward: -24.12568375605093, Replay Buffer Size: 55405\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.80\n",
      "Episode 1720, Reward: -29.900744953771394, Moving Avg Reward: -24.12568375605093, Replay Buffer Size: 55405\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1721: Won\n",
      "Episode 1721, Avg Value Loss: 3.467128892739614, Avg Policy Loss: 4.817115028699239\n",
      "Episode 1721, Reward: -28.765358073684745, Moving Avg Reward: -24.104213522660988, Replay Buffer Size: 55417\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1721, Reward: -28.765358073684745, Moving Avg Reward: -24.104213522660988, Replay Buffer Size: 55417\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1722: Won\n",
      "Episode 1722, Avg Value Loss: 3.6063821532509546, Avg Policy Loss: 4.787117524580522\n",
      "Episode 1722, Reward: -29.420293372575166, Moving Avg Reward: -24.38205247964514, Replay Buffer Size: 55428\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1722, Reward: -29.420293372575166, Moving Avg Reward: -24.38205247964514, Replay Buffer Size: 55428\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1723: Won\n",
      "Episode 1723, Avg Value Loss: 4.01115160721999, Avg Policy Loss: 4.807310031010554\n",
      "Episode 1723, Reward: -34.91542593730902, Moving Avg Reward: -24.557992441458282, Replay Buffer Size: 55441\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1723, Reward: -34.91542593730902, Moving Avg Reward: -24.557992441458282, Replay Buffer Size: 55441\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1724: Won\n",
      "Episode 1724, Avg Value Loss: 3.28753799200058, Avg Policy Loss: 4.831298311551412\n",
      "Episode 1724, Reward: -32.226879556568136, Moving Avg Reward: -24.611793513721235, Replay Buffer Size: 55453\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.81\n",
      "Episode 1724, Reward: -32.226879556568136, Moving Avg Reward: -24.611793513721235, Replay Buffer Size: 55453\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1725: Won\n",
      "Episode 1725, Avg Value Loss: 4.5009207963943485, Avg Policy Loss: 4.52936544418335\n",
      "Episode 1725, Reward: -25.102931929257878, Moving Avg Reward: -24.54715947797358, Replay Buffer Size: 55463\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1725, Reward: -25.102931929257878, Moving Avg Reward: -24.54715947797358, Replay Buffer Size: 55463\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1726: Won\n",
      "Episode 1726, Avg Value Loss: 3.712598603505355, Avg Policy Loss: 4.679317144247202\n",
      "Episode 1726, Reward: 9.323200658137807, Moving Avg Reward: -24.49900400246304, Replay Buffer Size: 55489\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1726, Reward: 9.323200658137807, Moving Avg Reward: -24.49900400246304, Replay Buffer Size: 55489\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1727: Won\n",
      "Episode 1727, Avg Value Loss: 2.722322177886963, Avg Policy Loss: 4.744820642471313\n",
      "Episode 1727, Reward: -28.238509073465096, Moving Avg Reward: -24.936164805654823, Replay Buffer Size: 55499\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1727, Reward: -28.238509073465096, Moving Avg Reward: -24.936164805654823, Replay Buffer Size: 55499\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1728: Won\n",
      "Episode 1728, Avg Value Loss: 3.1676202615102134, Avg Policy Loss: 4.906160725487603\n",
      "Episode 1728, Reward: -20.31919664880663, Moving Avg Reward: -24.784911218532397, Replay Buffer Size: 55508\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1728, Reward: -20.31919664880663, Moving Avg Reward: -24.784911218532397, Replay Buffer Size: 55508\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1729: Won\n",
      "Episode 1729, Avg Value Loss: 3.2779024947773325, Avg Policy Loss: 4.7879742275584825\n",
      "Episode 1729, Reward: -25.081731966573088, Moving Avg Reward: -24.755009316682923, Replay Buffer Size: 55519\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1729, Reward: -25.081731966573088, Moving Avg Reward: -24.755009316682923, Replay Buffer Size: 55519\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1730: Won\n",
      "Episode 1730, Avg Value Loss: 3.355732311593725, Avg Policy Loss: 4.704603124172129\n",
      "Episode 1730, Reward: -51.38809853323227, Moving Avg Reward: -24.96899725964895, Replay Buffer Size: 55566\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1730, Reward: -51.38809853323227, Moving Avg Reward: -24.96899725964895, Replay Buffer Size: 55566\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1731: Won\n",
      "Episode 1731, Avg Value Loss: 3.1462525020946157, Avg Policy Loss: 4.744986707513982\n",
      "Episode 1731, Reward: -33.58850077250485, Moving Avg Reward: -24.727793996033792, Replay Buffer Size: 55577\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1731, Reward: -33.58850077250485, Moving Avg Reward: -24.727793996033792, Replay Buffer Size: 55577\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1732: Won\n",
      "Episode 1732, Avg Value Loss: 3.571644122783954, Avg Policy Loss: 4.817405150486873\n",
      "Episode 1732, Reward: -29.61254542383483, Moving Avg Reward: -24.689552467419922, Replay Buffer Size: 55590\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1732, Reward: -29.61254542383483, Moving Avg Reward: -24.689552467419922, Replay Buffer Size: 55590\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1733: Won\n",
      "Episode 1733, Avg Value Loss: 3.1718517981077494, Avg Policy Loss: 4.8273110138742545\n",
      "Episode 1733, Reward: 19.84689142517367, Moving Avg Reward: -24.079996611261787, Replay Buffer Size: 55609\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.81\n",
      "Episode 1733, Reward: 19.84689142517367, Moving Avg Reward: -24.079996611261787, Replay Buffer Size: 55609\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1734: Won\n",
      "Episode 1734, Avg Value Loss: 4.248691827058792, Avg Policy Loss: 5.103973805904388\n",
      "Episode 1734, Reward: -21.87385407367912, Moving Avg Reward: -24.020836518594386, Replay Buffer Size: 55617\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.80\n",
      "Episode 1734, Reward: -21.87385407367912, Moving Avg Reward: -24.020836518594386, Replay Buffer Size: 55617\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1735: Won\n",
      "Episode 1735, Avg Value Loss: 3.4304830763075085, Avg Policy Loss: 4.900164286295573\n",
      "Episode 1735, Reward: -24.675015794271843, Moving Avg Reward: -24.02573498992511, Replay Buffer Size: 55626\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1735, Reward: -24.675015794271843, Moving Avg Reward: -24.02573498992511, Replay Buffer Size: 55626\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1736: Won\n",
      "Episode 1736, Avg Value Loss: 3.1615801465754605, Avg Policy Loss: 4.787922382354736\n",
      "Episode 1736, Reward: -84.88467628117589, Moving Avg Reward: -24.97408175273687, Replay Buffer Size: 55675\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1736, Reward: -84.88467628117589, Moving Avg Reward: -24.97408175273687, Replay Buffer Size: 55675\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1737: Won\n",
      "Episode 1737, Avg Value Loss: 3.3868260582288108, Avg Policy Loss: 4.639070789019267\n",
      "Episode 1737, Reward: -33.04009118725826, Moving Avg Reward: -24.996024865210487, Replay Buffer Size: 55687\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1737, Reward: -33.04009118725826, Moving Avg Reward: -24.996024865210487, Replay Buffer Size: 55687\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1738: Won\n",
      "Episode 1738, Avg Value Loss: 3.53763760970189, Avg Policy Loss: 4.810856745793269\n",
      "Episode 1738, Reward: -39.74827345612691, Moving Avg Reward: -25.127938154568916, Replay Buffer Size: 55700\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.81\n",
      "Episode 1738, Reward: -39.74827345612691, Moving Avg Reward: -25.127938154568916, Replay Buffer Size: 55700\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1739: Won\n",
      "Episode 1739, Avg Value Loss: 3.4525592982769013, Avg Policy Loss: 4.6735235214233395\n",
      "Episode 1739, Reward: -25.72638158624258, Moving Avg Reward: -25.079219982427006, Replay Buffer Size: 55710\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1739, Reward: -25.72638158624258, Moving Avg Reward: -25.079219982427006, Replay Buffer Size: 55710\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1740: Won\n",
      "Episode 1740, Avg Value Loss: 3.230251745744185, Avg Policy Loss: 4.909029700539329\n",
      "Episode 1740, Reward: -29.41612717382614, Moving Avg Reward: -25.000053590901476, Replay Buffer Size: 55721\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1740, Reward: -29.41612717382614, Moving Avg Reward: -25.000053590901476, Replay Buffer Size: 55721\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1741: Won\n",
      "Episode 1741, Avg Value Loss: 3.2324830293655396, Avg Policy Loss: 4.582589626312256\n",
      "Episode 1741, Reward: -25.449624005342706, Moving Avg Reward: -25.077330343057564, Replay Buffer Size: 55731\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1741, Reward: -25.449624005342706, Moving Avg Reward: -25.077330343057564, Replay Buffer Size: 55731\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1742: Won\n",
      "Episode 1742, Avg Value Loss: 3.072151697598971, Avg Policy Loss: 4.860515080965483\n",
      "Episode 1742, Reward: -32.621765496597625, Moving Avg Reward: -25.08860936260591, Replay Buffer Size: 55744\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1742, Reward: -32.621765496597625, Moving Avg Reward: -25.08860936260591, Replay Buffer Size: 55744\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1743: Won\n",
      "Episode 1743, Avg Value Loss: 3.5919786623546055, Avg Policy Loss: 4.767381381988526\n",
      "Episode 1743, Reward: 18.208562960914257, Moving Avg Reward: -24.660109875215415, Replay Buffer Size: 55779\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.81\n",
      "Episode 1743, Reward: 18.208562960914257, Moving Avg Reward: -24.660109875215415, Replay Buffer Size: 55779\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1744: Won\n",
      "Episode 1744, Avg Value Loss: 3.305673163384199, Avg Policy Loss: 4.793796229362488\n",
      "Episode 1744, Reward: 7.826185248867922, Moving Avg Reward: -24.779856367379477, Replay Buffer Size: 55859\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.81\n",
      "Episode 1744, Reward: 7.826185248867922, Moving Avg Reward: -24.779856367379477, Replay Buffer Size: 55859\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1745: Won\n",
      "Episode 1745, Avg Value Loss: 2.5621802144580417, Avg Policy Loss: 4.695041444566515\n",
      "Episode 1745, Reward: -27.63182716551496, Moving Avg Reward: -24.807595304805428, Replay Buffer Size: 55868\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1745, Reward: -27.63182716551496, Moving Avg Reward: -24.807595304805428, Replay Buffer Size: 55868\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1746: Won\n",
      "Episode 1746, Avg Value Loss: 3.3414333377565657, Avg Policy Loss: 4.8731918529588345\n",
      "Episode 1746, Reward: -93.08696910437988, Moving Avg Reward: -25.48504214870311, Replay Buffer Size: 55917\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1746, Reward: -93.08696910437988, Moving Avg Reward: -25.48504214870311, Replay Buffer Size: 55917\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1747: Won\n",
      "Episode 1747, Avg Value Loss: 3.1890011370182036, Avg Policy Loss: 4.797197467088699\n",
      "Episode 1747, Reward: -120.23930467143268, Moving Avg Reward: -26.40037261899572, Replay Buffer Size: 55997\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1747, Reward: -120.23930467143268, Moving Avg Reward: -26.40037261899572, Replay Buffer Size: 55997\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1748: Lost\n",
      "Episode 1748, Avg Value Loss: 2.91463879081938, Avg Policy Loss: 4.955186208089192\n",
      "Episode 1748, Reward: -22.14711590608458, Moving Avg Reward: -26.35275704509965, Replay Buffer Size: 56006\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1748, Reward: -22.14711590608458, Moving Avg Reward: -26.35275704509965, Replay Buffer Size: 56006\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1749: Lost\n",
      "Episode 1749, Avg Value Loss: 3.489911662787199, Avg Policy Loss: 4.806884223222733\n",
      "Episode 1749, Reward: -55.48036015814406, Moving Avg Reward: -26.65697124807718, Replay Buffer Size: 56086\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1749, Reward: -55.48036015814406, Moving Avg Reward: -26.65697124807718, Replay Buffer Size: 56086\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1750: Lost\n",
      "Episode 1750, Avg Value Loss: 3.3666236370801927, Avg Policy Loss: 4.744516670703888\n",
      "Episode 1750, Reward: 5.590028272408186, Moving Avg Reward: -26.605657181367167, Replay Buffer Size: 56166\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1750, Reward: 5.590028272408186, Moving Avg Reward: -26.605657181367167, Replay Buffer Size: 56166\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1751: Draw\n",
      "Episode 1751, Avg Value Loss: 3.244277362823486, Avg Policy Loss: 4.704842224121093\n",
      "Episode 1751, Reward: 13.200439089466208, Moving Avg Reward: -26.17380641800773, Replay Buffer Size: 56191\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1751, Reward: 13.200439089466208, Moving Avg Reward: -26.17380641800773, Replay Buffer Size: 56191\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1752: Won\n",
      "Episode 1752, Avg Value Loss: 3.1708229795098304, Avg Policy Loss: 4.825854927301407\n",
      "Episode 1752, Reward: -57.253667464244316, Moving Avg Reward: -25.675514029047473, Replay Buffer Size: 56271\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1752, Reward: -57.253667464244316, Moving Avg Reward: -25.675514029047473, Replay Buffer Size: 56271\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1753: Won\n",
      "Episode 1753, Avg Value Loss: 3.116018682718277, Avg Policy Loss: 4.745910227298737\n",
      "Episode 1753, Reward: -24.64762295480201, Moving Avg Reward: -25.66647102384455, Replay Buffer Size: 56279\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1753, Reward: -24.64762295480201, Moving Avg Reward: -25.66647102384455, Replay Buffer Size: 56279\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1754: Won\n",
      "Episode 1754, Avg Value Loss: 3.2025104544379492, Avg Policy Loss: 4.841405066576871\n",
      "Episode 1754, Reward: 17.974767193726123, Moving Avg Reward: -25.52362836136871, Replay Buffer Size: 56301\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1754, Reward: 17.974767193726123, Moving Avg Reward: -25.52362836136871, Replay Buffer Size: 56301\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1755: Won\n",
      "Episode 1755, Avg Value Loss: 3.63369083404541, Avg Policy Loss: 4.892263995276557\n",
      "Episode 1755, Reward: -26.611923655242116, Moving Avg Reward: -25.509237899031206, Replay Buffer Size: 56310\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1755, Reward: -26.611923655242116, Moving Avg Reward: -25.509237899031206, Replay Buffer Size: 56310\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1756: Won\n",
      "Episode 1756, Avg Value Loss: 2.8464388102293015, Avg Policy Loss: 4.89959720770518\n",
      "Episode 1756, Reward: -28.355683187102784, Moving Avg Reward: -25.528889596516482, Replay Buffer Size: 56322\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1756, Reward: -28.355683187102784, Moving Avg Reward: -25.528889596516482, Replay Buffer Size: 56322\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1757: Won\n",
      "Episode 1757, Avg Value Loss: 3.365323320031166, Avg Policy Loss: 4.844080030918121\n",
      "Episode 1757, Reward: -24.433362585105204, Moving Avg Reward: -25.518320380078887, Replay Buffer Size: 56330\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1757, Reward: -24.433362585105204, Moving Avg Reward: -25.518320380078887, Replay Buffer Size: 56330\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1758: Won\n",
      "Episode 1758, Avg Value Loss: 3.286222157271012, Avg Policy Loss: 4.888384632442309\n",
      "Episode 1758, Reward: 14.989122985193868, Moving Avg Reward: -25.035030273639737, Replay Buffer Size: 56353\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1758, Reward: 14.989122985193868, Moving Avg Reward: -25.035030273639737, Replay Buffer Size: 56353\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1759: Won\n",
      "Episode 1759, Avg Value Loss: 3.2582931290973316, Avg Policy Loss: 4.869774428280917\n",
      "Episode 1759, Reward: -14.958570374318645, Moving Avg Reward: -25.37291027818461, Replay Buffer Size: 56408\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1759, Reward: -14.958570374318645, Moving Avg Reward: -25.37291027818461, Replay Buffer Size: 56408\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1760: Won\n",
      "Episode 1760, Avg Value Loss: 3.2065040193498136, Avg Policy Loss: 4.8417804598808285\n",
      "Episode 1760, Reward: -87.00732653292414, Moving Avg Reward: -25.97106620947607, Replay Buffer Size: 56488\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1760, Reward: -87.00732653292414, Moving Avg Reward: -25.97106620947607, Replay Buffer Size: 56488\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1761: Won\n",
      "Episode 1761, Avg Value Loss: 3.3072617650032043, Avg Policy Loss: 4.856190246343613\n",
      "Episode 1761, Reward: 6.002657220844576, Moving Avg Reward: -25.621457001155676, Replay Buffer Size: 56568\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1761, Reward: 6.002657220844576, Moving Avg Reward: -25.621457001155676, Replay Buffer Size: 56568\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1762: Draw\n",
      "Episode 1762, Avg Value Loss: 3.2296222679317, Avg Policy Loss: 4.831776750087738\n",
      "Episode 1762, Reward: -9.256200648162002, Moving Avg Reward: -25.345475495589383, Replay Buffer Size: 56648\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1762, Reward: -9.256200648162002, Moving Avg Reward: -25.345475495589383, Replay Buffer Size: 56648\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1763: Draw\n",
      "Episode 1763, Avg Value Loss: 3.247200911695307, Avg Policy Loss: 4.871925614096901\n",
      "Episode 1763, Reward: -28.030230672809875, Moving Avg Reward: -25.34871473098263, Replay Buffer Size: 56659\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1763, Reward: -28.030230672809875, Moving Avg Reward: -25.34871473098263, Replay Buffer Size: 56659\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1764: Lost\n",
      "Episode 1764, Avg Value Loss: 2.935939828554789, Avg Policy Loss: 5.004655520121257\n",
      "Episode 1764, Reward: -37.6842994940405, Moving Avg Reward: -25.448594871286595, Replay Buffer Size: 56671\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1764, Reward: -37.6842994940405, Moving Avg Reward: -25.448594871286595, Replay Buffer Size: 56671\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1765: Lost\n",
      "Episode 1765, Avg Value Loss: 3.3572301169236503, Avg Policy Loss: 4.786233425140381\n",
      "Episode 1765, Reward: -31.55555037806505, Moving Avg Reward: -25.468583718408517, Replay Buffer Size: 56683\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1765, Reward: -31.55555037806505, Moving Avg Reward: -25.468583718408517, Replay Buffer Size: 56683\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1766: Lost\n",
      "Episode 1766, Avg Value Loss: 3.3224988276308234, Avg Policy Loss: 4.823435948111794\n",
      "Episode 1766, Reward: -85.8274976201578, Moving Avg Reward: -25.953530337660016, Replay Buffer Size: 56738\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1766, Reward: -85.8274976201578, Moving Avg Reward: -25.953530337660016, Replay Buffer Size: 56738\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1767: Lost\n",
      "Episode 1767, Avg Value Loss: 2.9173468708992005, Avg Policy Loss: 4.691917133331299\n",
      "Episode 1767, Reward: -34.38289061835502, Moving Avg Reward: -26.036286668219617, Replay Buffer Size: 56748\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1767, Reward: -34.38289061835502, Moving Avg Reward: -26.036286668219617, Replay Buffer Size: 56748\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1768: Lost\n",
      "Episode 1768, Avg Value Loss: 3.5933378496948554, Avg Policy Loss: 4.821629874560298\n",
      "Episode 1768, Reward: -39.45448350808503, Moving Avg Reward: -26.280197879347384, Replay Buffer Size: 56797\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.74\n",
      "Episode 1768, Reward: -39.45448350808503, Moving Avg Reward: -26.280197879347384, Replay Buffer Size: 56797\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1769: Won\n",
      "Episode 1769, Avg Value Loss: 3.50542488694191, Avg Policy Loss: 4.908066391944885\n",
      "Episode 1769, Reward: -24.015208887316263, Moving Avg Reward: -26.08127777431887, Replay Buffer Size: 56805\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1769, Reward: -24.015208887316263, Moving Avg Reward: -26.08127777431887, Replay Buffer Size: 56805\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1770: Won\n",
      "Episode 1770, Avg Value Loss: 3.2196128425144015, Avg Policy Loss: 4.878871622539702\n",
      "Episode 1770, Reward: 14.781807874763214, Moving Avg Reward: -25.545983200810575, Replay Buffer Size: 56826\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1770, Reward: 14.781807874763214, Moving Avg Reward: -25.545983200810575, Replay Buffer Size: 56826\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1771: Won\n",
      "Episode 1771, Avg Value Loss: 2.9859176741706, Avg Policy Loss: 4.869818581475152\n",
      "Episode 1771, Reward: -32.755238510377126, Moving Avg Reward: -25.499103629040455, Replay Buffer Size: 56835\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1771, Reward: -32.755238510377126, Moving Avg Reward: -25.499103629040455, Replay Buffer Size: 56835\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1772: Won\n",
      "Episode 1772, Avg Value Loss: 3.45924844344457, Avg Policy Loss: 4.709027886390686\n",
      "Episode 1772, Reward: -32.58698332581981, Moving Avg Reward: -25.47135527361827, Replay Buffer Size: 56847\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1772, Reward: -32.58698332581981, Moving Avg Reward: -25.47135527361827, Replay Buffer Size: 56847\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1773: Won\n",
      "Episode 1773, Avg Value Loss: 3.113775805383921, Avg Policy Loss: 4.816497892141342\n",
      "Episode 1773, Reward: -38.68069135889909, Moving Avg Reward: -25.580931436010438, Replay Buffer Size: 56927\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1773, Reward: -38.68069135889909, Moving Avg Reward: -25.580931436010438, Replay Buffer Size: 56927\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1774: Won\n",
      "Episode 1774, Avg Value Loss: 3.385859595404731, Avg Policy Loss: 4.882721371120876\n",
      "Episode 1774, Reward: -24.161883274902376, Moving Avg Reward: -25.489796049058633, Replay Buffer Size: 56936\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1774, Reward: -24.161883274902376, Moving Avg Reward: -25.489796049058633, Replay Buffer Size: 56936\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1775: Won\n",
      "Episode 1775, Avg Value Loss: 3.5582287053563704, Avg Policy Loss: 4.774766253001654\n",
      "Episode 1775, Reward: -33.7357077060661, Moving Avg Reward: -25.925853126119293, Replay Buffer Size: 57003\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1775, Reward: -33.7357077060661, Moving Avg Reward: -25.925853126119293, Replay Buffer Size: 57003\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1776: Won\n",
      "Episode 1776, Avg Value Loss: 3.3693347527430606, Avg Policy Loss: 4.889528164496789\n",
      "Episode 1776, Reward: -30.944288256073662, Moving Avg Reward: -26.28311642870506, Replay Buffer Size: 57016\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1776, Reward: -30.944288256073662, Moving Avg Reward: -26.28311642870506, Replay Buffer Size: 57016\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1777: Won\n",
      "Episode 1777, Avg Value Loss: 3.54926689962546, Avg Policy Loss: 4.648759047190349\n",
      "Episode 1777, Reward: -29.461650321588124, Moving Avg Reward: -26.26402342873553, Replay Buffer Size: 57028\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1777, Reward: -29.461650321588124, Moving Avg Reward: -26.26402342873553, Replay Buffer Size: 57028\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1778: Won\n",
      "Episode 1778, Avg Value Loss: 2.4109692335128785, Avg Policy Loss: 4.867618608474731\n",
      "Episode 1778, Reward: -28.809651460681586, Moving Avg Reward: -26.161650069634852, Replay Buffer Size: 57038\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1778, Reward: -28.809651460681586, Moving Avg Reward: -26.161650069634852, Replay Buffer Size: 57038\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1779: Won\n",
      "Episode 1779, Avg Value Loss: 3.5523264691943215, Avg Policy Loss: 4.846437703995478\n",
      "Episode 1779, Reward: 17.205254898810736, Moving Avg Reward: -25.694651434836054, Replay Buffer Size: 57059\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1779, Reward: 17.205254898810736, Moving Avg Reward: -25.694651434836054, Replay Buffer Size: 57059\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1780: Won\n",
      "Episode 1780, Avg Value Loss: 3.762127533555031, Avg Policy Loss: 5.025114119052887\n",
      "Episode 1780, Reward: -29.775704304316278, Moving Avg Reward: -25.7951040952434, Replay Buffer Size: 57067\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.76\n",
      "Episode 1780, Reward: -29.775704304316278, Moving Avg Reward: -25.7951040952434, Replay Buffer Size: 57067\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1781: Won\n",
      "Episode 1781, Avg Value Loss: 3.118995100259781, Avg Policy Loss: 5.009613076845805\n",
      "Episode 1781, Reward: -36.10776752733195, Moving Avg Reward: -25.815028160438686, Replay Buffer Size: 57079\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1781, Reward: -36.10776752733195, Moving Avg Reward: -25.815028160438686, Replay Buffer Size: 57079\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1782: Won\n",
      "Episode 1782, Avg Value Loss: 4.220403770605723, Avg Policy Loss: 4.865610400835673\n",
      "Episode 1782, Reward: -28.949105877087305, Moving Avg Reward: -26.318157383635565, Replay Buffer Size: 57091\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1782, Reward: -28.949105877087305, Moving Avg Reward: -26.318157383635565, Replay Buffer Size: 57091\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1783: Won\n",
      "Episode 1783, Avg Value Loss: 3.096324691405663, Avg Policy Loss: 4.963126475994404\n",
      "Episode 1783, Reward: -29.36931619814826, Moving Avg Reward: -26.4038747641736, Replay Buffer Size: 57104\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1783, Reward: -29.36931619814826, Moving Avg Reward: -26.4038747641736, Replay Buffer Size: 57104\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1784: Won\n",
      "Episode 1784, Avg Value Loss: 3.237000584602356, Avg Policy Loss: 4.98055504329169\n",
      "Episode 1784, Reward: -70.47370965552494, Moving Avg Reward: -26.86474377416343, Replay Buffer Size: 57171\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1784, Reward: -70.47370965552494, Moving Avg Reward: -26.86474377416343, Replay Buffer Size: 57171\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1785: Won\n",
      "Episode 1785, Avg Value Loss: 3.273262718319893, Avg Policy Loss: 4.869594240188599\n",
      "Episode 1785, Reward: -36.07058905608864, Moving Avg Reward: -27.00480646521939, Replay Buffer Size: 57251\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1785, Reward: -36.07058905608864, Moving Avg Reward: -27.00480646521939, Replay Buffer Size: 57251\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1786: Lost\n",
      "Episode 1786, Avg Value Loss: 2.872929330383028, Avg Policy Loss: 4.91647127696446\n",
      "Episode 1786, Reward: -36.56862002446281, Moving Avg Reward: -27.062238331082114, Replay Buffer Size: 57265\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1786, Reward: -36.56862002446281, Moving Avg Reward: -27.062238331082114, Replay Buffer Size: 57265\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1787: Lost\n",
      "Episode 1787, Avg Value Loss: 3.35343140496148, Avg Policy Loss: 4.9129419856601295\n",
      "Episode 1787, Reward: -36.47613054460686, Moving Avg Reward: -27.61095696490211, Replay Buffer Size: 57310\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1787, Reward: -36.47613054460686, Moving Avg Reward: -27.61095696490211, Replay Buffer Size: 57310\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1788: Lost\n",
      "Episode 1788, Avg Value Loss: 3.073098103205363, Avg Policy Loss: 4.8195366859436035\n",
      "Episode 1788, Reward: -23.484180489846516, Moving Avg Reward: -27.568984016714708, Replay Buffer Size: 57319\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.79\n",
      "Episode 1788, Reward: -23.484180489846516, Moving Avg Reward: -27.568984016714708, Replay Buffer Size: 57319\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1789: Lost\n",
      "Episode 1789, Avg Value Loss: 3.1361009857871314, Avg Policy Loss: 4.984309239821001\n",
      "Episode 1789, Reward: -25.6062230201329, Moving Avg Reward: -27.840089828451696, Replay Buffer Size: 57330\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1789, Reward: -25.6062230201329, Moving Avg Reward: -27.840089828451696, Replay Buffer Size: 57330\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1790: Lost\n",
      "Episode 1790, Avg Value Loss: 3.003712935107095, Avg Policy Loss: 4.865430934088571\n",
      "Episode 1790, Reward: -32.37323490465433, Moving Avg Reward: -27.93365007486058, Replay Buffer Size: 57344\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1790, Reward: -32.37323490465433, Moving Avg Reward: -27.93365007486058, Replay Buffer Size: 57344\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1791: Lost\n",
      "Episode 1791, Avg Value Loss: 3.2878620237112046, Avg Policy Loss: 4.816749423742294\n",
      "Episode 1791, Reward: -46.905881119357545, Moving Avg Reward: -28.160042187478584, Replay Buffer Size: 57424\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1791, Reward: -46.905881119357545, Moving Avg Reward: -28.160042187478584, Replay Buffer Size: 57424\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1792: Lost\n",
      "Episode 1792, Avg Value Loss: 4.424641760912809, Avg Policy Loss: 4.825204459103671\n",
      "Episode 1792, Reward: -28.49066293339512, Moving Avg Reward: -28.174551336215277, Replay Buffer Size: 57435\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1792, Reward: -28.49066293339512, Moving Avg Reward: -28.174551336215277, Replay Buffer Size: 57435\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1793: Lost\n",
      "Episode 1793, Avg Value Loss: 3.313589264949163, Avg Policy Loss: 4.853803316752116\n",
      "Episode 1793, Reward: -28.70613061486096, Moving Avg Reward: -28.14093261781303, Replay Buffer Size: 57447\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1793, Reward: -28.70613061486096, Moving Avg Reward: -28.14093261781303, Replay Buffer Size: 57447\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1794: Lost\n",
      "Episode 1794, Avg Value Loss: 3.6681084394454957, Avg Policy Loss: 4.862058115005493\n",
      "Episode 1794, Reward: -28.643638517037623, Moving Avg Reward: -28.09420009763741, Replay Buffer Size: 57457\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1794, Reward: -28.643638517037623, Moving Avg Reward: -28.09420009763741, Replay Buffer Size: 57457\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1795: Lost\n",
      "Episode 1795, Avg Value Loss: 3.1941153809428213, Avg Policy Loss: 4.872906506061554\n",
      "Episode 1795, Reward: -2.720093240686852, Moving Avg Reward: -27.74248903955855, Replay Buffer Size: 57537\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1795, Reward: -2.720093240686852, Moving Avg Reward: -27.74248903955855, Replay Buffer Size: 57537\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1796: Lost\n",
      "Episode 1796, Avg Value Loss: 3.8651815123028226, Avg Policy Loss: 4.898957411448161\n",
      "Episode 1796, Reward: -21.630591946691155, Moving Avg Reward: -27.637587237465645, Replay Buffer Size: 57546\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.75\n",
      "Episode 1796, Reward: -21.630591946691155, Moving Avg Reward: -27.637587237465645, Replay Buffer Size: 57546\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1797: Lost\n",
      "Episode 1797, Avg Value Loss: 3.4441957814352855, Avg Policy Loss: 4.9397053718566895\n",
      "Episode 1797, Reward: -26.744390815427742, Moving Avg Reward: -27.66066880500003, Replay Buffer Size: 57553\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.78\n",
      "Episode 1797, Reward: -26.744390815427742, Moving Avg Reward: -27.66066880500003, Replay Buffer Size: 57553\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1798: Lost\n",
      "Episode 1798, Avg Value Loss: 3.3851711563766003, Avg Policy Loss: 4.885536140203476\n",
      "Episode 1798, Reward: 4.637445620613034, Moving Avg Reward: -27.287953484813364, Replay Buffer Size: 57633\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.77\n",
      "Episode 1798, Reward: 4.637445620613034, Moving Avg Reward: -27.287953484813364, Replay Buffer Size: 57633\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1799: Lost\n",
      "Episode 1799, Avg Value Loss: 2.91922327876091, Avg Policy Loss: 4.823682963848114\n",
      "Episode 1799, Reward: -23.718712707146402, Moving Avg Reward: -27.21279688628572, Replay Buffer Size: 57641\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Loss rate over the last 150 episodes: 0.80\n",
      "Episode 1799, Reward: -23.718712707146402, Moving Avg Reward: -27.21279688628572, Replay Buffer Size: 57641\n",
      "Current Epsilon: 0.00998645168764533\n",
      "Episode 1800: Lost\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-0ad906894ee3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;31m# Set the policy model to evaluation mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monline_policy_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[0maction_player1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepsilon_strategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monline_policy_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-d960722f32a7>\u001b[0m in \u001b[0;36mselect_action\u001b[1;34m(self, model, state_tensor, episode)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Exploitation: Mit Wahrscheinlichkeit 1-epsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m                 \u001b[0mmodel_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m                 \u001b[1;31m# Resetting indices 4 and beyond to zero for defense\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                 \u001b[0mmodel_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alexej\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-a0efeb6f9ae6>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation_fc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhidden_layer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation_fc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alexej\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alexej\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize DDPG Agent and Environment\n",
    "import laserhockey.hockey_env as h_env\n",
    "from gymnasium import spaces\n",
    "\n",
    "env = h_env.HockeyEnv()\n",
    "env.reset(mode=h_env.HockeyEnv.TRAIN_DEFENSE)\n",
    "\n",
    "\n",
    "#env.discrete_action_space = spaces.Discrete(7)\n",
    "\n",
    "# Set the environment mode to TRAIN DEFENSE\n",
    "#reload(lh)\n",
    "#np.set_printoptions(suppress=True)\n",
    "#import laserhockey.laser_hockey_env as lh\n",
    "#env = lh.LaserHockeyEnv()\n",
    "#env.reset(mode=lh.LaserHockeyEnv.TRAIN_DEFENSE)\n",
    "# Set the environment mode to TRAIN DEFENSE\n",
    "#env = h_env.HockeyEnv(mode=h_env.HockeyEnv.TRAIN_DEFENSE)\n",
    "#env.reset()\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_bounds = (env.action_space.low, env.action_space.high)\n",
    "#action_strategy = NormalNoiseDecayStrategy(action_bounds[0], action_bounds[1], initial_noise_ratio=1.2)\n",
    "epsilon_strategy = EpsilonGreedyStrategy(1.0, 0.01, 0.995, env.action_space)\n",
    "\n",
    "# Initialize the Replay Buffer and the DDPG Agent\n",
    "replay_buffer = ReplayBuffer(100000)\n",
    "agent = DDPGAgent(state_dim=state_dim, action_dim=action_dim, action_bounds=action_bounds)\n",
    "\n",
    "# Use the method to apply He initialization\n",
    "agent.he_initialization()\n",
    "\n",
    "num_episodes = 10000\n",
    "batch_size = 128\n",
    "train_start = 1000  \n",
    "update_frequency = 1  # Update the agent every 1 steps\n",
    "\n",
    "losses = []\n",
    "all_rewards = []  # Store all episode rewards for moving average calculation\n",
    "\n",
    "wins =[]\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs = env.reset()\n",
    "    obs_agent1 = env._get_obs()\n",
    "    episode_reward = 0\n",
    "    episode_value_losses = []\n",
    "    episode_policy_losses = []\n",
    "\n",
    "    \n",
    "\n",
    "    # Determine the result for the current episode\n",
    "    last_80_results = wins[-150:]  # Collect results of the last 80 steps\n",
    "    if 1 in last_80_results:\n",
    "        episode_result = \"Won\"\n",
    "    elif -1 in last_80_results:\n",
    "        episode_result = \"Lost\"\n",
    "    else:\n",
    "        episode_result = \"Draw\"\n",
    "\n",
    "    print(f\"Episode {episode + 1}: {episode_result}\")\n",
    "\n",
    "    touched = 0\n",
    "\n",
    "    for step in range(150):\n",
    "        if episode >= 1500:\n",
    "            env.render()\n",
    "        state_tensor = torch.FloatTensor(obs_agent1).unsqueeze(0)\n",
    "\n",
    "        \n",
    "\n",
    "        # Set the policy model to evaluation mode\n",
    "        agent.online_policy_model.eval()\n",
    "        action_player1 = epsilon_strategy.select_action(agent.online_policy_model, state_tensor, episode)\n",
    "\n",
    "        \n",
    "\n",
    "        agent.online_policy_model.train()\n",
    "        \n",
    "        next_obs, reward, done, _, info = env.step(action_player1)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        if info['reward_touch_puck'] > 0 and touched == 0:\n",
    "            touched = 1\n",
    "            reward += (150 - step) * 0.1  # Add the reward for touching the puck for the first time\n",
    "\n",
    "\n",
    "        \n",
    "        reward_closeness = info['reward_closeness_to_puck']\n",
    "        puck_touch = info['reward_touch_puck']\n",
    "\n",
    "        # Add reward for closeness\n",
    "        reward += 5 * reward_closeness\n",
    "\n",
    "        # Add penalty for not touching the puck at all in the current step\n",
    "        reward -= (1 - puck_touch) * 0.01\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "        #Count wins (successes in defense)\n",
    "        wins.append(info['winner'])\n",
    "\n",
    "        obs_agent1 = env._get_obs()\n",
    "        experience = (obs_agent1, action_player1, reward, next_obs, done)\n",
    "        replay_buffer.store(*experience)\n",
    "\n",
    "        # DDPG Training\n",
    "        if episode > train_start and len(replay_buffer) > batch_size and step % update_frequency == 0:\n",
    "            experiences = replay_buffer.sample(batch_size)\n",
    "\n",
    "            \n",
    "            states_np, actions_np, rewards_np, next_states_np, is_terminals_np = [np.array(x) for x in experiences]\n",
    "\n",
    "            # Set all actions from index [4:] to zeros\n",
    "            actions_np[:, 4:] = 0\n",
    "\n",
    "            states, actions, rewards, next_states, is_terminals = [torch.FloatTensor(x_np).to(device) for x_np in [states_np, actions_np, rewards_np, next_states_np, is_terminals_np]]\n",
    "            \n",
    "            #print(actions_np[:, 0:])\n",
    "            #print(actions_np[:, 4:])\n",
    "\n",
    "            value_loss, policy_loss = agent.optimize_model((states, actions, rewards, next_states, is_terminals))\n",
    "\n",
    "            episode_value_losses.append(value_loss)\n",
    "            episode_policy_losses.append(policy_loss)\n",
    "\n",
    "        if step == 79:  \n",
    "            done = True\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "\n",
    "    epsilon_strategy.decay_epsilon()\n",
    "    all_rewards.append(episode_reward)\n",
    "    moving_avg_reward = np.mean(all_rewards[-100:])  # Calculate moving average of the last 100 episode rewards\n",
    "    avg_value_loss = sum(episode_value_losses) / len(episode_value_losses) if episode_value_losses else 0\n",
    "    avg_policy_loss = sum(episode_policy_losses) / len(episode_policy_losses) if episode_policy_losses else 0\n",
    "\n",
    "    print(f\"Episode {episode + 1}, Avg Value Loss: {avg_value_loss}, Avg Policy Loss: {avg_policy_loss}\")\n",
    "    \n",
    "\n",
    "    print(f\"Episode {episode + 1}, Reward: {episode_reward}, Moving Avg Reward: {moving_avg_reward}, Replay Buffer Size: {len(replay_buffer)}\")\n",
    "    print(f\"Current Epsilon: {epsilon_strategy.epsilon}\")\n",
    "\n",
    "    # Calculate loss rate for the last 150 episodes (or less if episode number is < 150)\n",
    "    recent_games = wins[-150 * 80:] if episode >= 150 else wins  # Look back at results of the last 150 episodes (each episode has 80 steps)\n",
    "    recent_losses = [1 for i in range(0, len(recent_games), 80) if -1 in recent_games[i:i+80]].count(1)  # Count how many episodes in the recent games have a loss\n",
    "    loss_rate = recent_losses / (len(recent_games) / 80)  # Calculate loss rate\n",
    "    print(f\"Loss rate over the last {int(len(recent_games)/80)} episodes: {loss_rate:.2f}\")\n",
    "\n",
    "    print(f\"Episode {episode + 1}, Reward: {episode_reward}, Moving Avg Reward: {moving_avg_reward}, Replay Buffer Size: {len(replay_buffer)}\")\n",
    "    print(f\"Current Epsilon: {epsilon_strategy.epsilon}\")\n",
    "\n",
    "# Save the agent's model after training\n",
    "torch.save(agent.online_policy_model.state_dict(), 'online_policy_model_checkpoint.pth')\n",
    "torch.save(agent.online_value_model.state_dict(), 'online_value_model_checkpoint.pth')\n",
    "torch.save(agent.target_policy_model.state_dict(), 'target_policy_model_checkpoint.pth')\n",
    "torch.save(agent.target_value_model.state_dict(), 'target_value_model_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8faa5788",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hindsight Experience Replay Buffer class\n",
    "class HERBuffer:\n",
    "    def __init__(self, buffer_size, goal_selection_strategy, reward_function):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.goal_selection_strategy = goal_selection_strategy\n",
    "        self.reward_function = reward_function\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def store(self, *args):\n",
    "        if len(self.memory) < self.buffer_size:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = args\n",
    "        self.position = (self.position + 1) % self.buffer_size\n",
    "\n",
    "    def store_episode(self, episode):\n",
    "        goal = self.goal_selection_strategy(episode)\n",
    "        \n",
    "        for state, action, _, next_state, done in episode:\n",
    "            reward = self.reward_function(state, action, goal)\n",
    "            self.store(state, action, reward, next_state, goal)\n",
    "            \n",
    "            # HER storage\n",
    "            additional_goals = self.goal_selection_strategy(episode)\n",
    "            for g in additional_goals:\n",
    "                her_reward = self.reward_function(state, action, g)\n",
    "                self.store(state, action, her_reward, next_state, g)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return zip(*random.sample(self.memory, batch_size))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3af3fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 1, Reward: -0.15751856180049464, Moving Avg Reward: -0.15751856180049464, HER Buffer Size: 160\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 2, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 2, Reward: -11.39032215801075, Moving Avg Reward: -5.773920359905623, HER Buffer Size: 242\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 3, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 3, Reward: -11.356689256428453, Moving Avg Reward: -7.634843325413233, HER Buffer Size: 324\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 4, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 4, Reward: -0.39075220754125634, Moving Avg Reward: -5.82382054594524, HER Buffer Size: 484\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 5, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 5, Reward: -11.285190693622859, Moving Avg Reward: -6.916094575480765, HER Buffer Size: 570\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 6, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 6, Reward: -10.292953799428698, Moving Avg Reward: -7.4789044461387535, HER Buffer Size: 672\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 7, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 7, Reward: 0.6912611946703459, Moving Avg Reward: -6.311737926023168, HER Buffer Size: 832\n",
      "Current Epsilon: 0.995\n",
      "Episode 8, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 8, Reward: 0.46120874453906757, Moving Avg Reward: -5.465119592202887, HER Buffer Size: 992\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 9, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 9, Reward: 0.6644054377837054, Moving Avg Reward: -4.7840612555377104, HER Buffer Size: 1152\n",
      "Current Epsilon: 0.995\n",
      "Episode 10, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 10, Reward: 0.3852564273000347, Moving Avg Reward: -4.267129487253936, HER Buffer Size: 1312\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 11, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 11, Reward: -11.252624118521526, Moving Avg Reward: -4.902174453732807, HER Buffer Size: 1400\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 12, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 12, Reward: -11.237862088601334, Moving Avg Reward: -5.430148423305185, HER Buffer Size: 1490\n",
      "Current Epsilon: 0.995\n",
      "Episode 13, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 13, Reward: 0.5283456575003251, Moving Avg Reward: -4.971802724781684, HER Buffer Size: 1650\n",
      "Current Epsilon: 0.995\n",
      "Episode 14, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 14, Reward: 0.4207277925917294, Moving Avg Reward: -4.586621973540725, HER Buffer Size: 1810\n",
      "Current Epsilon: 0.995\n",
      "Episode 15, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 15, Reward: 1.5894108305517145, Moving Avg Reward: -4.174886453267897, HER Buffer Size: 1970\n",
      "Current Epsilon: 0.995\n",
      "Episode 16, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 16, Reward: 0.6894404187394345, Moving Avg Reward: -3.870866023767438, HER Buffer Size: 2130\n",
      "Current Epsilon: 0.995\n",
      "Episode 17, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 17, Reward: 0.7099218725614009, Moving Avg Reward: -3.601407912218683, HER Buffer Size: 2290\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 18, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 18, Reward: 11.034519161224365, Moving Avg Reward: -2.788300852582958, HER Buffer Size: 2360\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 19, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 19, Reward: 0.7342169728924495, Moving Avg Reward: -2.6029051775579366, HER Buffer Size: 2520\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 20, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 20, Reward: 0.6600582010077252, Moving Avg Reward: -2.4397570086296536, HER Buffer Size: 2680\n",
      "Current Epsilon: 0.995\n",
      "Episode 21, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 21, Reward: 0.6408463347596872, Moving Avg Reward: -2.2930616113253994, HER Buffer Size: 2840\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 22, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 22, Reward: -11.33456808287813, Moving Avg Reward: -2.7040391782141597, HER Buffer Size: 2922\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 23, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 23, Reward: 0.6060463547694803, Moving Avg Reward: -2.560122415910523, HER Buffer Size: 3082\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 24, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 24, Reward: 11.01233970489502, Moving Avg Reward: -1.9946031608769592, HER Buffer Size: 3150\n",
      "Current Epsilon: 0.995\n",
      "Episode 25, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 25, Reward: 1.593571504599881, Moving Avg Reward: -1.8510761742578856, HER Buffer Size: 3310\n",
      "Current Epsilon: 0.995\n",
      "Episode 26, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 26, Reward: -0.7066940035587411, Moving Avg Reward: -1.8070614753848415, HER Buffer Size: 3470\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 27, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 27, Reward: 0.6449565036275393, Moving Avg Reward: -1.7162459946806792, HER Buffer Size: 3630\n",
      "Current Epsilon: 0.995\n",
      "Episode 28, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 28, Reward: 0.7521855107179937, Moving Avg Reward: -1.6280877266307265, HER Buffer Size: 3790\n",
      "Current Epsilon: 0.995\n",
      "Episode 29, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 29, Reward: 0.6870512928441631, Moving Avg Reward: -1.548255346648834, HER Buffer Size: 3950\n",
      "Current Epsilon: 0.995\n",
      "Episode 30, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 30, Reward: -0.5557715344132576, Moving Avg Reward: -1.515172552907648, HER Buffer Size: 4110\n",
      "Current Epsilon: 0.995\n",
      "Episode 31, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 31, Reward: 0.5658418517913121, Moving Avg Reward: -1.4480430559818753, HER Buffer Size: 4270\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 32, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 32, Reward: -11.299011394151176, Moving Avg Reward: -1.7558858165496656, HER Buffer Size: 4352\n",
      "Current Epsilon: 0.995\n",
      "Episode 33, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 33, Reward: -1.495861298478984, Moving Avg Reward: -1.7480062856990388, HER Buffer Size: 4512\n",
      "Current Epsilon: 0.995\n",
      "Episode 34, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 34, Reward: 0.6570531361363625, Moving Avg Reward: -1.6772692438803507, HER Buffer Size: 4672\n",
      "Current Epsilon: 0.995\n",
      "Episode 35, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 35, Reward: -1.7311131553928245, Moving Avg Reward: -1.6788076413521356, HER Buffer Size: 4832\n",
      "Current Epsilon: 0.995\n",
      "Episode 36, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 36, Reward: 0.6417840196470367, Moving Avg Reward: -1.6143467618799365, HER Buffer Size: 4992\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 37, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 37, Reward: -10.232039813828763, Moving Avg Reward: -1.8472573849055807, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 38, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 38, Reward: -11.238006432861809, Moving Avg Reward: -2.094382359851797, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 39, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 39, Reward: -11.244700193664551, Moving Avg Reward: -2.3290058940521243, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 40, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 40, Reward: 0.6638729433757677, Moving Avg Reward: -2.2541839231164267, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 41, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 41, Reward: 0.7053696471509412, Moving Avg Reward: -2.1819996896952714, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 42, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 42, Reward: -11.277381881557519, Moving Avg Reward: -2.398556408549134, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 43, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 43, Reward: 0.7089264912754347, Moving Avg Reward: -2.3262893643671676, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 44, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 44, Reward: 0.44478126428003556, Moving Avg Reward: -2.2633104864433675, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 45, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 45, Reward: -11.297601283889724, Moving Avg Reward: -2.4640725041643976, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 46, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 46, Reward: 0.6842381276649663, Moving Avg Reward: -2.3956309686898463, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 47, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 47, Reward: 0.5234312123427662, Moving Avg Reward: -2.3335232627104285, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 48, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 48, Reward: -11.24931408343971, Moving Avg Reward: -2.5192689048089556, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 49, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 49, Reward: -11.273728390727882, Moving Avg Reward: -2.6979313432970966, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 50, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 50, Reward: -0.6523159893532945, Moving Avg Reward: -2.6570190362182204, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 51, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 51, Reward: 0.6621405438657882, Moving Avg Reward: -2.591937475824417, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 52, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 52, Reward: 11.00698235731125, Moving Avg Reward: -2.330419786725654, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 53, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 53, Reward: -11.27514037159286, Moving Avg Reward: -2.499188099647677, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 54, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 54, Reward: -11.303580673278903, Moving Avg Reward: -2.6622324065667735, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 55, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 55, Reward: -0.6044440835411136, Moving Avg Reward: -2.624818073420853, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 56, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 56, Reward: -11.265949017009675, Moving Avg Reward: -2.7791239831277954, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 57, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 57, Reward: -11.274087769439767, Moving Avg Reward: -2.9281584355192334, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 58, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 58, Reward: -11.225328158166548, Moving Avg Reward: -3.0712130859097044, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 59, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 59, Reward: -11.251302529268669, Moving Avg Reward: -3.2098586696954494, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 60, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 60, Reward: -11.24499027942847, Moving Avg Reward: -3.3437775298576664, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 61, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 61, Reward: -11.351135231730186, Moving Avg Reward: -3.4750456889047565, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 62, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 62, Reward: 0.6774217454921178, Moving Avg Reward: -3.408070407704807, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 63, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 63, Reward: -0.015303411290553372, Moving Avg Reward: -3.354216963317279, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 64, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 64, Reward: -9.462750164368174, Moving Avg Reward: -3.4496627945836997, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 65, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 65, Reward: -11.261599979148828, Moving Avg Reward: -3.5698464435770094, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 66, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 66, Reward: 0.5677984093216634, Moving Avg Reward: -3.5071548548967266, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 67, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 67, Reward: 11.00775327091217, Moving Avg Reward: -3.290514435108534, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 68, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 68, Reward: 0.39851499176146893, Moving Avg Reward: -3.2362640023604463, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 69, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 69, Reward: -11.237394560657838, Moving Avg Reward: -3.352222416248814, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 70, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 70, Reward: -0.5620282302011789, Moving Avg Reward: -3.3123624993052765, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 71, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 71, Reward: -11.31762198854059, Moving Avg Reward: -3.425112632956478, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 72, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 72, Reward: -11.298229421509875, Moving Avg Reward: -3.5344614772419423, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 73, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 73, Reward: 11.016641049575806, Moving Avg Reward: -3.3351313056416987, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 74, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 74, Reward: -1.6993898824606282, Moving Avg Reward: -3.3130266918149274, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 75, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 75, Reward: -11.313705238755354, Moving Avg Reward: -3.4197024057741334, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 76, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 76, Reward: -11.30662962368291, Moving Avg Reward: -3.5234777639045123, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 77, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 77, Reward: 0.653151214855576, Moving Avg Reward: -3.4692358291154197, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 78, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 78, Reward: -0.5000821218273637, Moving Avg Reward: -3.43116975594506, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 79, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 79, Reward: -11.25568244345033, Moving Avg Reward: -3.530214220343861, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 80, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 80, Reward: -11.270146962874803, Moving Avg Reward: -3.626963379625498, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 81, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 81, Reward: -11.23817404283404, Moving Avg Reward: -3.720928943368813, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 82, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 82, Reward: -11.277872508722526, Moving Avg Reward: -3.813086791726785, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 83, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 83, Reward: -11.281859251145159, Moving Avg Reward: -3.903072002081223, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 84, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 84, Reward: 2.527988981311859, Moving Avg Reward: -3.8265117522789245, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 85, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 85, Reward: -11.26170075717138, Moving Avg Reward: -3.913984564101188, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 86, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 86, Reward: 0.49227358312387104, Moving Avg Reward: -3.8627490042497343, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 87, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 87, Reward: 0.5663305709090573, Moving Avg Reward: -3.8118400436157254, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 88, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 88, Reward: -10.365419731119731, Moving Avg Reward: -3.886312540064635, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 89, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 89, Reward: -11.220751067473085, Moving Avg Reward: -3.9687219617209095, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 90, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 90, Reward: 0.5651653155347965, Moving Avg Reward: -3.9183454364180688, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 91, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 91, Reward: 0.3006390737533463, Moving Avg Reward: -3.8719829692733274, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 92, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 92, Reward: -1.8195893773884264, Moving Avg Reward: -3.8496743432745784, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 93, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 93, Reward: -11.280205164129507, Moving Avg Reward: -3.9295725241439863, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 94, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 94, Reward: -11.249963459218776, Moving Avg Reward: -4.007449023453292, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 95, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 95, Reward: -11.287932718017373, Moving Avg Reward: -4.084085693922388, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 96, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 96, Reward: 0.4732955055222372, Moving Avg Reward: -4.03661297309484, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 97, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 97, Reward: 0.5190747433492936, Moving Avg Reward: -3.9896471203479935, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 98, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 98, Reward: -11.248004714947278, Moving Avg Reward: -4.063711993762272, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 99, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 99, Reward: 0.7418490037414203, Moving Avg Reward: -4.015170973585467, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 100, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 100, Reward: -11.280869582193874, Moving Avg Reward: -4.087827959671551, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 101, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 101, Reward: -11.387285341877194, Moving Avg Reward: -4.2001256274723175, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 102, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 102, Reward: 0.5585512468822921, Moving Avg Reward: -4.080636893423387, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 103, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 103, Reward: -0.004766636533290146, Moving Avg Reward: -3.967117667224436, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 104, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 104, Reward: 0.5679634699675703, Moving Avg Reward: -3.9575305104493474, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 105, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 105, Reward: -10.269796887599666, Moving Avg Reward: -3.9473765723891154, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 106, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 106, Reward: 0.5637327186239348, Moving Avg Reward: -3.838809707208589, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 107, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 107, Reward: -11.268545216852573, Moving Avg Reward: -3.9584077713238184, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 108, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 108, Reward: -11.284645643854487, Moving Avg Reward: -4.0758663152077546, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 109, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 109, Reward: 0.6734986472493917, Moving Avg Reward: -4.075775383113098, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 110, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 110, Reward: 0.730039975660339, Moving Avg Reward: -4.072327547629493, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 111, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 111, Reward: 0.013936970365047454, Moving Avg Reward: -3.9596619367406287, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 112, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 112, Reward: 0.6364252121729643, Moving Avg Reward: -3.840919063732885, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 113, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 113, Reward: 0.6705236293688972, Moving Avg Reward: -3.8394972840142, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 114, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 114, Reward: -10.241896302221573, Moving Avg Reward: -3.9461235249623328, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 115, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 115, Reward: 0.5952495215062852, Moving Avg Reward: -3.9560651380527867, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 116, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 116, Reward: 0.36262795695277156, Moving Avg Reward: -3.9593332626706537, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 117, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 117, Reward: 0.4365936726275559, Moving Avg Reward: -3.9620665446699923, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 118, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 118, Reward: -0.42665188481692684, Moving Avg Reward: -4.076678255130405, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 119, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 119, Reward: -0.684066607003436, Moving Avg Reward: -4.090861090929363, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 120, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 120, Reward: -11.301885209978325, Moving Avg Reward: -4.210480525039225, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 121, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 121, Reward: -11.267465847425608, Moving Avg Reward: -4.329563646861078, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 122, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 122, Reward: -1.5472179775212649, Moving Avg Reward: -4.231690145807509, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 123, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 123, Reward: -11.259128058649985, Moving Avg Reward: -4.350341889941704, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 124, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 124, Reward: 0.7511213220274482, Moving Avg Reward: -4.452954073770379, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 125, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 125, Reward: -11.284047093812466, Moving Avg Reward: -4.581730259754503, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 126, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 126, Reward: 0.5523536891181859, Moving Avg Reward: -4.569139782827734, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 127, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 127, Reward: -11.289409443167054, Moving Avg Reward: -4.68848344229568, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 128, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 128, Reward: -11.243699361862006, Moving Avg Reward: -4.80844229102148, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Episode 129, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 129, Reward: 0.4693381658349366, Moving Avg Reward: -4.810619422291572, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 130, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 130, Reward: 0.6690094962857775, Moving Avg Reward: -4.798371611984582, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 131, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 131, Reward: 0.5421985866422573, Moving Avg Reward: -4.798608044636072, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 132, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 132, Reward: -11.249914758194578, Moving Avg Reward: -4.798117078276506, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 133, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 133, Reward: 0.5998612378266065, Moving Avg Reward: -4.77715985291345, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 134, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 134, Reward: 0.3620408263285467, Moving Avg Reward: -4.780109976011528, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 135, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 135, Reward: 0.727108966604255, Moving Avg Reward: -4.755527754791557, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 136, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 136, Reward: 0.5996289771080945, Moving Avg Reward: -4.755949305216947, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 137, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 137, Reward: 0.6152582650842401, Moving Avg Reward: -4.647476324427817, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 138, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 138, Reward: -11.256881514088878, Moving Avg Reward: -4.647665075240088, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 139, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 139, Reward: -1.9561997829270539, Moving Avg Reward: -4.554780071132713, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 140, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 140, Reward: 0.4780194948237473, Moving Avg Reward: -4.556638605618233, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 141, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 141, Reward: -11.282979128780157, Moving Avg Reward: -4.676522093377544, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 142, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 142, Reward: 0.35842556224804606, Moving Avg Reward: -4.560164018939488, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 143, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 143, Reward: -11.273969977098002, Moving Avg Reward: -4.6799929836232215, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 144, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 144, Reward: -0.005960120494849981, Moving Avg Reward: -4.684500397470971, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 145, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 145, Reward: 0.006362853050231936, Moving Avg Reward: -4.571460756101572, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Episode 146, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 146, Reward: 0.6984451664765818, Moving Avg Reward: -4.571318685713455, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 147, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 147, Reward: -11.235168829619399, Moving Avg Reward: -4.688904686133077, HER Buffer Size: 5000\n",
      "Current Epsilon: 0.995\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-26c027611f7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mstate_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs_agent1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monline_policy_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0maction_player1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepsilon_strategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monline_policy_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#DDP Agent\n",
    "import random\n",
    "np.set_printoptions(suppress=True)\n",
    "reload(lh)\n",
    "\n",
    "# Set the environment mode to TRAIN DEFENSE\n",
    "env = lh.LaserHockeyEnv()\n",
    "env.reset(mode=lh.LaserHockeyEnv.TRAIN_DEFENSE)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_bounds = (env.action_space.low, env.action_space.high)\n",
    "epsilon_strategy = EpsilonGreedyStrategy(1.0, 0.01, 0.995, env.action_space)\n",
    "\n",
    "# Goal selection strategy for HERBuffer\n",
    "def sample_goals(episode):\n",
    "    goals = []\n",
    "\n",
    "    for experience in episode:\n",
    "        _, _, reward, next_state, _ = experience\n",
    "        if abs(reward) == 0.2:\n",
    "            goals.append(next_state)\n",
    "\n",
    "    return goals\n",
    "\n",
    "def compute_reward(state, action, goal):\n",
    "    _, _, _, _, info = env.step(action)\n",
    "    \n",
    "    if (info.get('reward_closeness_to_puck', 0) > 0) or \\\n",
    "       (info.get('reward_touch_puck', 0) > 0) or \\\n",
    "       (info.get('reward_puck_direction', 0) > 0):\n",
    "        return 0.2\n",
    "    elif (info.get('reward_closeness_to_puck', 0) < 0) or \\\n",
    "         (info.get('reward_touch_puck', 0) < 0) or \\\n",
    "         (info.get('reward_puck_direction', 0) < 0):\n",
    "        return -0.2\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "her_buffer = HERBuffer(buffer_size=5000, goal_selection_strategy=sample_goals, reward_function=compute_reward)\n",
    "\n",
    "agent = DDPGAgent(state_dim=state_dim, action_dim=action_dim, action_bounds=action_bounds)\n",
    "agent.he_initialization()\n",
    "\n",
    "num_episodes = 10000\n",
    "batch_size = 256\n",
    "train_start = 1000 \n",
    "update_frequency = 1\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "wins = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs = env.reset()\n",
    "    obs_agent1 = env._get_obs()\n",
    "    episode_reward = 0\n",
    "    episode_value_losses = []\n",
    "    episode_policy_losses = []\n",
    "\n",
    "   \n",
    "\n",
    "    episode_trajectory = []\n",
    "\n",
    "    for step in range(80):\n",
    "        state_tensor = torch.FloatTensor(obs_agent1).unsqueeze(0)\n",
    "        agent.online_policy_model.eval()\n",
    "        action_player1 = epsilon_strategy.select_action(agent.online_policy_model, state_tensor, episode)\n",
    "\n",
    "        \n",
    "        agent.online_policy_model.train()\n",
    "\n",
    "        next_obs, reward, done, _, info = env.step(action_player1)\n",
    "        reward += info['winner']\n",
    "        reward += info['reward_closeness_to_puck']\n",
    "        reward += info['reward_touch_puck']\n",
    "        reward += info['reward_puck_direction']\n",
    "        episode_reward += reward\n",
    "\n",
    "        #print(f\"info: {info}\")\n",
    "\n",
    "        obs_agent1 = env._get_obs()\n",
    "        experience = (obs_agent1, action_player1, reward, next_obs, done)\n",
    "        #Store trajectories for HER\n",
    "        episode_trajectory.append(experience)\n",
    "        her_buffer.store(*experience)\n",
    "        \n",
    "\n",
    "        # DDPG Training\n",
    "\n",
    "        if episode > train_start and len(her_buffer) > batch_size:\n",
    "\n",
    "            experiences = her_buffer.sample(batch_size)\n",
    "            states, actions, rewards, next_states, is_terminals = [torch.FloatTensor(x_np).to(device) for x_np in [states_np, actions_np, rewards_np, next_states_np, is_terminals_np]]\n",
    "\n",
    "            \n",
    "            value_loss, policy_loss = agent.optimize_model((states, actions, rewards, next_states, is_terminals))\n",
    "            episode_value_losses.append(value_loss)\n",
    "            episode_policy_losses.append(policy_loss)\n",
    "\n",
    "        if step == 79: \n",
    "            done = True\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    \n",
    "    #Store new trajectories in HER\n",
    "    her_buffer.store_episode(episode_trajectory)\n",
    "\n",
    "    if random.random() < 0.15:\n",
    "        gradient = agent.online_policy_model.input_layer.weight.grad\n",
    "        if gradient is not None:\n",
    "            print(f\"Mean gradient of online policy model's input layer: {gradient.mean()}\")\n",
    "\n",
    "        q_gradient = agent.online_value_model.input_layer.weight.grad\n",
    "        if q_gradient is not None:\n",
    "            print(f\"Mean gradient of online value model's input layer: {q_gradient.mean()}\")\n",
    "\n",
    "    epsilon_strategy.decay_epsilon()\n",
    "    all_rewards.append(episode_reward)\n",
    "    moving_avg_reward = np.mean(all_rewards[-100:])\n",
    "    avg_value_loss = sum(episode_value_losses) / len(episode_value_losses) if episode_value_losses else 0\n",
    "    avg_policy_loss = sum(episode_policy_losses) / len(episode_policy_losses) if episode_policy_losses else 0\n",
    "\n",
    "    print(f\"Episode {episode + 1}, Avg Value Loss: {avg_value_loss}, Avg Policy Loss: {avg_policy_loss}\")\n",
    "    print(f\"Episode {episode + 1}, Reward: {episode_reward}, Moving Avg Reward: {moving_avg_reward}, HER Buffer Size: {len(her_buffer)}\")\n",
    "    print(f\"Current Epsilon: {epsilon_strategy.epsilon}\")\n",
    "\n",
    "    recent_games = wins[-150 * 80:] if episode >= 150 else wins\n",
    "    recent_losses = [1 for i in range(0, len(recent_games), 80) if -1 in recent_games[i:i+80]].count(1)\n",
    "    #loss_rate = recent_losses / (len(recent_games) / 80) if len(recent_games) > 0 else 0\n",
    "    #print(f\"Loss rate over the last {int(len(recent_games)/80)} episodes: {loss_rate:.2f}\")\n",
    "\n",
    "\n",
    "    #if loss_rate > 0.80 and episode > train_start:\n",
    "    #    print(\"Bad Agent. Restarting Training...\")\n",
    "    #    agent = DDPGAgent(state_dim=state_dim, action_dim=action_dim, action_bounds=action_bounds)\n",
    "    #    agent.he_initialization()\n",
    "    #    wins.clear()\n",
    "\n",
    "# Save the models\n",
    "torch.save(agent.online_policy_model.state_dict(), \"online_policy_model_defense.pt\")\n",
    "torch.save(agent.online_value_model.state_dict(), \"online_value_model_defense.pt\")\n",
    "torch.save(agent.target_policy_model.state_dict(), \"target_policy_model_defense.pt\")\n",
    "torch.save(agent.target_value_model.state_dict(), \"target_value_model_defense.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef2a741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experience 1:\n",
      "State shape: [ -6.          -0.00520372   0.00200054   0.999998     0.\n",
      "  -0.27195147   0.10002708   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.           0.72931671\n",
      "   4.61147165 -10.9371109   -3.84231043]\n",
      "Action shape: [ 0.        -0.4070022  0.5639671  0.         0.         0.       ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.          -0.00520372   0.00200054   0.999998     0.\n",
      "  -0.27195147   0.10002708   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.           0.72931671\n",
      "   4.61147165 -10.9371109   -3.84231043]\n",
      "Done: False\n",
      "\n",
      "Experience 2:\n",
      "State shape: [ -6.00000286  -0.01078224   0.00652385   0.99997872   0.\n",
      "  -0.30552006   0.22616762   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.           0.51079369\n",
      "   4.53470182 -10.92617416  -3.83846807]\n",
      "Action shape: [ 0.         -0.05837875  0.71119857  0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00000286  -0.01078224   0.00652385   0.99997872   0.\n",
      "  -0.30552006   0.22616762   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.           0.51079369\n",
      "   4.53470182 -10.92617416  -3.83846807]\n",
      "Done: False\n",
      "\n",
      "Experience 3:\n",
      "State shape: [ -6.00001049  -0.01455975   0.01326959   0.99991196   0.\n",
      "  -0.22849996   0.33730412   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.           0.29248905\n",
      "   4.45800924 -10.91524792  -3.83462977]\n",
      "Action shape: [0.         0.10612337 0.62660354 0.         0.         0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00001049  -0.01455975   0.01326959   0.99991196   0.\n",
      "  -0.22849996   0.33730412   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.           0.29248905\n",
      "   4.45800924 -10.91524792  -3.83462977]\n",
      "Done: False\n",
      "\n",
      "Experience 4:\n",
      "State shape: [ -6.00002575  -0.02696848   0.02103016   0.99977884   0.\n",
      "  -0.66604674   0.38808653   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.           0.07440281\n",
      "   4.38139296 -10.90433311  -3.83079529]\n",
      "Action shape: [ 0.         -0.6616714   0.28631863  0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00002575  -0.02696848   0.02103016   0.99977884   0.\n",
      "  -0.66604674   0.38808653   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.           0.07440281\n",
      "   4.38139296 -10.90433311  -3.83079529]\n",
      "Done: False\n",
      "\n",
      "Experience 5:\n",
      "State shape: [ -6.00004768  -0.04738951   0.02846731   0.99959472   0.\n",
      "  -1.06472135   0.37197256   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -0.143466\n",
      "   4.30485392 -10.8934288   -3.82696462]\n",
      "Action shape: [ 0.         -0.61659193 -0.09085283  0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00004768  -0.04738951   0.02846731   0.99959472   0.\n",
      "  -1.06472135   0.37197256   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -0.143466\n",
      "   4.30485392 -10.8934288   -3.82696462]\n",
      "Done: False\n",
      "\n",
      "Experience 6:\n",
      "State shape: [ -6.00007915  -0.06467295   0.0366356    0.99932869   0.\n",
      "  -0.91216815   0.4086321    6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -0.36111641\n",
      "   4.22839117 -10.88253593  -3.82313776]\n",
      "Action shape: [0.         0.19644172 0.20669185 0.         0.         0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00007915  -0.06467295   0.0366356    0.99932869   0.\n",
      "  -0.91216815   0.4086321    6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -0.36111641\n",
      "   4.22839117 -10.88253593  -3.82313776]\n",
      "Done: False\n",
      "\n",
      "Experience 7:\n",
      "State shape: [ -6.00011921  -0.07086849   0.04507494   0.99898361   0.\n",
      "  -0.35937938   0.42232114   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -0.57854939\n",
      "   4.15200472 -10.87165356  -3.81931472]\n",
      "Action shape: [0.         0.8        0.07718086 0.         0.         0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00011921  -0.07086849   0.04507494   0.99898361   0.\n",
      "  -0.35937938   0.42232114   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -0.57854939\n",
      "   4.15200472 -10.87165356  -3.81931472]\n",
      "Done: False\n",
      "\n",
      "Experience 8:\n",
      "State shape: [ -6.00017834  -0.07684231   0.05510907   0.99848034   0.\n",
      "  -0.35766071   0.50233895   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -0.79576492\n",
      "   4.07569456 -10.86078167  -3.81549549]\n",
      "Action shape: [ 0.         -0.00818478  0.4511521   0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00017834  -0.07684231   0.05510907   0.99848034   0.\n",
      "  -0.35766071   0.50233895   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -0.79576492\n",
      "   4.07569456 -10.86078167  -3.81549549]\n",
      "Done: False\n",
      "\n",
      "Experience 9:\n",
      "State shape: [ -6.00022507  -0.07336855   0.06190887   0.99808181   0.\n",
      "   0.13376276   0.34057441   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -1.01276302\n",
      "   3.9994607  -10.84992123  -3.81168008]\n",
      "Action shape: [ 0.          0.72475827 -0.9120518   0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00022507  -0.07336855   0.06190887   0.99808181   0.\n",
      "   0.13376276   0.34057441   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -1.01276302\n",
      "   3.9994607  -10.84992123  -3.81168008]\n",
      "Done: False\n",
      "\n",
      "Experience 10:\n",
      "State shape: [ -6.00026894  -0.07292509   0.06757169   0.99771442   0.\n",
      "  -0.01109874   0.28373638   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -1.22954464\n",
      "   3.92330313 -10.83907127  -3.80786848]\n",
      "Action shape: [ 0.         -0.21279575 -0.3204611   0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00026894  -0.07292509   0.06757169   0.99771442   0.\n",
      "  -0.01109874   0.28373638   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -1.22954464\n",
      "   3.92330313 -10.83907127  -3.80786848]\n",
      "Done: False\n",
      "\n",
      "Experience 11:\n",
      "State shape: [ -6.0002985   -0.06406975   0.07120126   0.99746197   0.\n",
      "   0.42146409   0.18191758   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -1.44610977\n",
      "   3.84722185 -10.82823277  -3.8040607 ]\n",
      "Action shape: [ 0.          0.6470408  -0.57406914  0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.0002985   -0.06406975   0.07120126   0.99746197   0.\n",
      "   0.42146409   0.18191758   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -1.44610977\n",
      "   3.84722185 -10.82823277  -3.8040607 ]\n",
      "Done: False\n",
      "\n",
      "Experience 12:\n",
      "State shape: [ -6.00030279  -0.05402994   0.07173936   0.99742341   0.\n",
      "   0.49881813   0.02697359   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -1.66245747\n",
      "   3.77121687 -10.81740475  -3.80025673]\n",
      "Action shape: [ 0.          0.1283832  -0.87359655  0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00030279  -0.05402994   0.07173936   0.99742341   0.\n",
      "   0.49881813   0.02697359   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -1.66245747\n",
      "   3.77121687 -10.81740475  -3.80025673]\n",
      "Done: False\n",
      "\n",
      "Experience 13:\n",
      "State shape: [ -6.0003233   -0.05500031   0.07415651   0.99724662   0.\n",
      "  -0.06271636   0.12118024   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -1.87858963\n",
      "   3.69528818 -10.80658722  -3.79645658]\n",
      "Action shape: [ 0.        -0.8254612  0.5311507  0.         0.         0.       ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.0003233   -0.05500031   0.07415651   0.99724662   0.\n",
      "  -0.06271636   0.12118024   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -1.87858963\n",
      "   3.69528818 -10.80658722  -3.79645658]\n",
      "Done: False\n",
      "\n",
      "Experience 14:\n",
      "State shape: [ -6.00035381  -0.06783772   0.07755825   0.99698782   0.\n",
      "  -0.66185504   0.17057872   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -2.09450531\n",
      "   3.61943483 -10.79578114  -3.79266024]\n",
      "Action shape: [ 0.         -0.8985475   0.27851576  0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00035381  -0.06783772   0.07755825   0.99698782   0.\n",
      "  -0.66185504   0.17057872   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -2.09450531\n",
      "   3.61943483 -10.79578114  -3.79266024]\n",
      "Done: False\n",
      "\n",
      "Experience 15:\n",
      "State shape: [ -6.00041199  -0.07815313   0.08366821   0.99649367   0.\n",
      "  -0.55166733   0.30649614   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -2.31020498\n",
      "   3.54365778 -10.78498554  -3.78886771]\n",
      "Action shape: [0.         0.14509621 0.76632214 0.         0.         0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00041199  -0.07815313   0.08366821   0.99649367   0.\n",
      "  -0.55166733   0.30649614   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -2.31020498\n",
      "   3.54365778 -10.78498554  -3.78886771]\n",
      "Done: False\n",
      "\n",
      "Experience 16:\n",
      "State shape: [ -6.00044298  -0.08673334   0.08675183   0.99622995   0.\n",
      "  -0.44711998   0.15474387   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -2.52568913\n",
      "   3.46795607 -10.77420044  -3.785079  ]\n",
      "Action shape: [ 0.          0.13995299 -0.85560125  0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00044298  -0.08673334   0.08675183   0.99622995   0.\n",
      "  -0.44711998   0.15474387   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -2.52568913\n",
      "   3.46795607 -10.77420044  -3.785079  ]\n",
      "Done: False\n",
      "\n",
      "Experience 17:\n",
      "State shape: [ -6.00045252  -0.08674192   0.08766441   0.99615006   0.\n",
      "  -0.00576374   0.0458034    6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -2.74095774\n",
      "   3.39233065 -10.76342678  -3.78129387]\n",
      "Action shape: [ 0.          0.64715004 -0.6142221   0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00045252  -0.08674192   0.08766441   0.99615006   0.\n",
      "  -0.00576374   0.0458034    6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -2.74095774\n",
      "   3.39233065 -10.76342678  -3.78129387]\n",
      "Done: False\n",
      "\n",
      "Experience 18:\n",
      "State shape: [ -6.00048447  -0.09723425   0.09073214   0.99587533   0.\n",
      "  -0.54263312   0.15400004   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -2.95601082\n",
      "   3.31678057 -10.75266361  -3.77751255]\n",
      "Action shape: [ 0.         -0.80365044  0.6100283   0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00048447  -0.09723425   0.09073214   0.99587533   0.\n",
      "  -0.54263312   0.15400004   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -2.95601082\n",
      "   3.31678057 -10.75266361  -3.77751255]\n",
      "Done: False\n",
      "\n",
      "Experience 19:\n",
      "State shape: [ -6.00054264  -0.1057601    0.09601527   0.99537986   0.\n",
      "  -0.45733333   0.26531601   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -3.17084885\n",
      "   3.24130583 -10.74191093  -3.77373505]\n",
      "Action shape: [0.         0.1114175  0.62761545 0.         0.         0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00054264  -0.1057601    0.09601527   0.99537986   0.\n",
      "  -0.45733333   0.26531601   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -3.17084885\n",
      "   3.24130583 -10.74191093  -3.77373505]\n",
      "Done: False\n",
      "\n",
      "Experience 20:\n",
      "State shape: [ -6.0005846   -0.10515404   0.09964504   0.99502305   0.\n",
      "   0.00896457   0.18236393   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -3.3854723\n",
      "   3.16590643 -10.73116875  -3.76996136]\n",
      "Action shape: [ 0.          0.684172   -0.46769586  0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.0005846   -0.10515404   0.09964504   0.99502305   0.\n",
      "   0.00896457   0.18236393   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -3.3854723\n",
      "   3.16590643 -10.73116875  -3.76996136]\n",
      "Done: False\n",
      "\n",
      "Experience 21:\n",
      "State shape: [ -6.00060749  -0.11546373   0.10153169   0.99483231   0.\n",
      "  -0.52657598   0.09481294   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -3.59988117\n",
      "   3.09058237 -10.720438    -3.76619148]\n",
      "Action shape: [ 0.        -0.801221  -0.4936251  0.         0.         0.       ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00060749  -0.11546373   0.10153169   0.99483231   0.\n",
      "  -0.52657598   0.09481294   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -3.59988117\n",
      "   3.09058237 -10.720438    -3.76619148]\n",
      "Done: False\n",
      "\n",
      "Experience 22:\n",
      "State shape: [ -6.00063801  -0.11254835   0.10407786   0.99456915   0.\n",
      "   0.13081241   0.12798679   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -3.81407547\n",
      "   3.01533365 -10.70971775  -3.76242542]\n",
      "Action shape: [0.         0.9680853  0.18703896 0.         0.         0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00063801  -0.11254835   0.10407786   0.99456915   0.\n",
      "   0.13081241   0.12798679   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -3.81407547\n",
      "   3.01533365 -10.70971775  -3.76242542]\n",
      "Done: False\n",
      "\n",
      "Experience 23:\n",
      "State shape: [ -6.00065422  -0.11196136   0.10539386   0.99443056   0.\n",
      "   0.02160983   0.06616423   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -4.02805567\n",
      "   2.94016027 -10.69900799  -3.75866294]\n",
      "Action shape: [ 0.         -0.15951698 -0.3485645   0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00065422  -0.11196136   0.10539386   0.99443056   0.\n",
      "   0.02160983   0.06616423   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -4.02805567\n",
      "   2.94016027 -10.69900799  -3.75866294]\n",
      "Done: False\n",
      "\n",
      "Experience 24:\n",
      "State shape: [ -6.00062943  -0.10178423   0.10337697   0.99464225   0.\n",
      "   0.52069551  -0.10139883   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -4.24182177\n",
      "   2.86506224 -10.68830872  -3.75490427]\n",
      "Action shape: [ 0.          0.7475778  -0.94474477  0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00062943  -0.10178423   0.10337697   0.99464225   0.\n",
      "   0.52069551  -0.10139883   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -4.24182177\n",
      "   2.86506224 -10.68830872  -3.75490427]\n",
      "Done: False\n",
      "\n",
      "Experience 25:\n",
      "State shape: [ -6.00064564  -0.09851122   0.10466641   0.99450739   0.\n",
      "   0.15608174   0.06482332   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -4.45537424\n",
      "   2.79003954 -10.67762089  -3.75114942]\n",
      "Action shape: [ 0.        -0.5300951  0.9371845  0.         0.         0.       ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00064564  -0.09851122   0.10466641   0.99450739   0.\n",
      "   0.15608174   0.06482332   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -4.45537424\n",
      "   2.79003954 -10.67762089  -3.75114942]\n",
      "Done: False\n",
      "\n",
      "Experience 26:\n",
      "State shape: [ -6.00067472  -0.10213232   0.10699831   0.9942592    0.\n",
      "  -0.19477491   0.1172535    6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -4.66871309\n",
      "   2.71509123 -10.66694355  -3.74739838]\n",
      "Action shape: [ 0.         -0.52041984  0.2956089   0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00067472  -0.10213232   0.10699831   0.9942592    0.\n",
      "  -0.19477491   0.1172535    6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -4.66871309\n",
      "   2.71509123 -10.66694355  -3.74739838]\n",
      "Done: False\n",
      "\n",
      "Experience 27:\n",
      "State shape: [ -6.00067139  -0.10834455   0.10675054   0.99428584   0.\n",
      "  -0.30916601  -0.01245953   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -4.8818388\n",
      "   2.64021826 -10.6562767   -3.74365091]\n",
      "Action shape: [ 0.         -0.1770276  -0.73134077  0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00067139  -0.10834455   0.10675054   0.99428584   0.\n",
      "  -0.30916601  -0.01245953   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -4.8818388\n",
      "   2.64021826 -10.6562767   -3.74365091]\n",
      "Done: False\n",
      "\n",
      "Experience 28:\n",
      "State shape: [ -6.00067902  -0.11969614   0.10733718   0.99422268   0.\n",
      "  -0.5710209    0.02950152   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -5.09475136\n",
      "   2.56541967 -10.64562035  -3.73990726]\n",
      "Action shape: [ 0.         -0.4011456   0.23658249  0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00067902  -0.11969614   0.10733718   0.99422268   0.\n",
      "  -0.5710209    0.02950152   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -5.09475136\n",
      "   2.56541967 -10.64562035  -3.73990726]\n",
      "Done: False\n",
      "\n",
      "Experience 29:\n",
      "State shape: [ -6.00072956  -0.12244463   0.11123157   0.99379452   0.\n",
      "  -0.16032365   0.19589266   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -5.30745077\n",
      "   2.49069643 -10.63497448  -3.73616743]\n",
      "Action shape: [0.         0.59755725 0.93813723 0.         0.         0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00072956  -0.12244463   0.11123157   0.99379452   0.\n",
      "  -0.16032365   0.19589266   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -5.30745077\n",
      "   2.49069643 -10.63497448  -3.73616743]\n",
      "Done: False\n",
      "\n",
      "Experience 30:\n",
      "State shape: [ -6.00081444  -0.12856436   0.11753631   0.99306859   0.\n",
      "  -0.34304839   0.31732056   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -5.51993752\n",
      "   2.41604757 -10.62434006  -3.73243141]\n",
      "Action shape: [ 0.         -0.27826446  0.684628    0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00081444  -0.12856436   0.11753631   0.99306859   0.\n",
      "  -0.34304839   0.31732056   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -5.51993752\n",
      "   2.41604757 -10.62434006  -3.73243141]\n",
      "Done: False\n",
      "\n",
      "Experience 31:\n",
      "State shape: [ -6.00092363  -0.12676382   0.12512794   0.99214061   0.\n",
      "   0.04541964   0.3824071    6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -5.73221207\n",
      "   2.34147406 -10.61371613  -3.72869897]\n",
      "Action shape: [0.         0.5711127  0.36696726 0.         0.         0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00092363  -0.12676382   0.12512794   0.99214061   0.\n",
      "   0.04541964   0.3824071    6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -5.73221207\n",
      "   2.34147406 -10.61371613  -3.72869897]\n",
      "Done: False\n",
      "\n",
      "Experience 32:\n",
      "State shape: [ -6.0010004   -0.12936831   0.13022993   0.99148382   0.\n",
      "  -0.16019939   0.25720549   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -5.94427395\n",
      "   2.26697493 -10.60310268  -3.72497034]\n",
      "Action shape: [ 0.        -0.3063697 -0.7059047  0.         0.         0.       ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.0010004   -0.12936831   0.13022993   0.99148382   0.\n",
      "  -0.16019939   0.25720549   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -5.94427395\n",
      "   2.26697493 -10.60310268  -3.72497034]\n",
      "Done: False\n",
      "\n",
      "Experience 33:\n",
      "State shape: [ -6.00107574  -0.12644243   0.13500985   0.99084426   0.\n",
      "   0.11820105   0.24112599   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -6.15612411\n",
      "   2.19255018 -10.59249973  -3.72124553]\n",
      "Action shape: [ 0.          0.41185868 -0.09065855  0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00107574  -0.12644243   0.13500985   0.99084426   0.\n",
      "   0.11820105   0.24112599   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -6.15612411\n",
      "   2.19255018 -10.59249973  -3.72124553]\n",
      "Done: False\n",
      "\n",
      "Experience 34:\n",
      "State shape: [ -6.00110149  -0.11999083   0.1366202    0.9906235    0.\n",
      "   0.31312186   0.08127013   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -6.36776209\n",
      "   2.11819983 -10.58190727  -3.71752429]\n",
      "Action shape: [ 0.          0.29525623 -0.9012904   0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00110149  -0.11999083   0.1366202    0.9906235    0.\n",
      "   0.31312186   0.08127013   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -6.36776209\n",
      "   2.11819983 -10.58190727  -3.71752429]\n",
      "Done: False\n",
      "\n",
      "Experience 35:\n",
      "State shape: [ -6.00110531  -0.12014914   0.13684234   0.99059284   0.\n",
      "  -0.00920021   0.01121257   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -6.57918835\n",
      "   2.04392385 -10.5713253   -3.71380687]\n",
      "Action shape: [ 0.         -0.47301447 -0.39499465  0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00110531  -0.12014914   0.13684234   0.99059284   0.\n",
      "  -0.00920021   0.01121257   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -6.57918835\n",
      "   2.04392385 -10.5713253   -3.71380687]\n",
      "Done: False\n",
      "\n",
      "Experience 36:\n",
      "State shape: [ -6.00115061  -0.11689949   0.13959433   0.99020878   0.\n",
      "   0.14628765   0.13893348   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -6.79040337\n",
      "   1.96972227 -10.56075382  -3.71009302]\n",
      "Action shape: [0.         0.23242757 0.7201089  0.         0.         0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00115061  -0.11689949   0.13959433   0.99020878   0.\n",
      "   0.14628765   0.13893348   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -6.79040337\n",
      "   1.96972227 -10.56075382  -3.71009302]\n",
      "Done: False\n",
      "\n",
      "Experience 37:\n",
      "State shape: [ -6.00121689  -0.10861397   0.14354097   0.98964438   0.\n",
      "   0.39111218   0.19933955   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -7.00140762\n",
      "   1.89559507 -10.55019283  -3.70638299]\n",
      "Action shape: [0.         0.37078276 0.3405782  0.         0.         0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00121689  -0.10861397   0.14354097   0.98964438   0.\n",
      "   0.39111218   0.19933955   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -7.00140762\n",
      "   1.89559507 -10.55019283  -3.70638299]\n",
      "Done: False\n",
      "\n",
      "Experience 38:\n",
      "State shape: [ -6.00132132  -0.09587431   0.14954541   0.98875486   0.\n",
      "   0.60170829   0.30349949   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -7.21220016\n",
      "   1.82154131 -10.53964233  -3.70267677]\n",
      "Action shape: [0.         0.32688466 0.5872687  0.         0.         0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00132132  -0.09587431   0.14954541   0.98875486   0.\n",
      "   0.60170829   0.30349949   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -7.21220016\n",
      "   1.82154131 -10.53964233  -3.70267677]\n",
      "Done: False\n",
      "\n",
      "Experience 39:\n",
      "State shape: [ -6.00146961  -0.08475971   0.15765876   0.98749365   0.\n",
      "   0.50805992   0.41054058   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -7.42278194\n",
      "   1.74756193 -10.52910328  -3.69897413]\n",
      "Action shape: [ 0.        -0.1221437  0.6035131  0.         0.         0.       ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00146961  -0.08475971   0.15765876   0.98749365   0.\n",
      "   0.50805992   0.41054058   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -7.42278194\n",
      "   1.74756193 -10.52910328  -3.69897413]\n",
      "Done: False\n",
      "\n",
      "Experience 40:\n",
      "State shape: [ -6.00169325  -0.07924509   0.16914415   0.98559132   0.\n",
      "   0.20822836   0.5820967    6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -7.63315392\n",
      "   1.67365599 -10.51857471  -3.69527531]\n",
      "Action shape: [ 0.         -0.43352032  0.96725816  0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00169325  -0.07924509   0.16914415   0.98559132   0.\n",
      "   0.20822836   0.5820967    6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -7.63315392\n",
      "   1.67365599 -10.51857471  -3.69527531]\n",
      "Done: False\n",
      "\n",
      "Experience 41:\n",
      "State shape: [ -6.00193024  -0.06813002   0.18052312   0.98357074   0.\n",
      "   0.48889688   0.57785249   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -7.84331512\n",
      "   1.59982443 -10.50805664  -3.69158006]\n",
      "Action shape: [ 0.          0.42628086 -0.02392962  0.          0.          0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00193024  -0.06813002   0.18052312   0.98357074   0.\n",
      "   0.48889688   0.57785249   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -7.84331512\n",
      "   1.59982443 -10.50805664  -3.69158006]\n",
      "Done: False\n",
      "\n",
      "Experience 42:\n",
      "State shape: [ -6.00226021  -0.05209923   0.1951968    0.9807641    0.\n",
      "   0.71532124   0.74699068   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -8.05326557\n",
      "   1.5260663  -10.49754906  -3.68788862]\n",
      "Action shape: [0.         0.35350007 0.9536257  0.         0.         0.        ]\n",
      "Reward: -11.006225556078348\n",
      "Next State shape: [ -6.00226021  -0.05209923   0.1951968    0.9807641    0.\n",
      "   0.71532124   0.74699068   6.75694084   3.29298735   0.\n",
      "   1.           0.           0.           0.          -8.05326557\n",
      "   1.5260663  -10.49754906  -3.68788862]\n",
      "Done: True\n",
      "\n",
      "Experience 43:\n",
      "State shape: [ -6.          -0.00887728   0.00066993   0.99999978   0.\n",
      "  -0.4478015    0.03349637   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.           0.48373604\n",
      "   4.66943789 -10.68677425  -3.55238867]\n",
      "Action shape: [ 0.         -0.6701791   0.18885736  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.          -0.00887728   0.00066993   0.99999978   0.\n",
      "  -0.4478015    0.03349637   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.           0.48373604\n",
      "   4.66943789 -10.68677425  -3.55238867]\n",
      "Done: False\n",
      "\n",
      "Experience 44:\n",
      "State shape: [ -6.          -0.01101875  -0.00062701   0.9999998    0.\n",
      "  -0.0994752   -0.06484693   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.           0.27021408\n",
      "   4.59846163 -10.67608738  -3.54883623]\n",
      "Action shape: [ 0.          0.50790113 -0.55447376  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.          -0.01101875  -0.00062701   0.9999998    0.\n",
      "  -0.0994752   -0.06484693   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.           0.27021408\n",
      "   4.59846163 -10.67608738  -3.54883623]\n",
      "Done: False\n",
      "\n",
      "Experience 45:\n",
      "State shape: [ -6.          -0.0126586    0.00023077   0.99999997   0.\n",
      "  -0.08703183   0.04288897   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.           0.05690575\n",
      "   4.52755594 -10.665411    -3.54528737]\n",
      "Action shape: [0.         0.01564525 0.6074306  0.         0.         0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.          -0.0126586    0.00023077   0.99999997   0.\n",
      "  -0.08703183   0.04288897   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.           0.05690575\n",
      "   4.52755594 -10.665411    -3.54528737]\n",
      "Done: False\n",
      "\n",
      "Experience 46:\n",
      "State shape: [ -6.00000048  -0.01183605   0.00277563   0.99999615   0.\n",
      "   0.02618032   0.12724307   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -0.15618896\n",
      "   4.45672083 -10.65474606  -3.54174209]\n",
      "Action shape: [0.         0.16682813 0.47560057 0.         0.         0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00000048  -0.01183605   0.00277563   0.99999615   0.\n",
      "   0.02618032   0.12724307   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -0.15618896\n",
      "   4.45672083 -10.65474606  -3.54174209]\n",
      "Done: False\n",
      "\n",
      "Experience 47:\n",
      "State shape: [ -6.00000095  -0.00226307   0.00377073   0.99999289   0.\n",
      "   0.47280958   0.04975544   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -0.36907101\n",
      "   4.38595724 -10.64409161  -3.53820038]\n",
      "Action shape: [ 0.          0.6692084  -0.43688646  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00000095  -0.00226307   0.00377073   0.99999289   0.\n",
      "   0.47280958   0.04975544   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -0.36907101\n",
      "   4.38595724 -10.64409161  -3.53820038]\n",
      "Done: False\n",
      "\n",
      "Experience 48:\n",
      "State shape: [ -6.           0.00603199   0.00206032   0.99999788   0.\n",
      "   0.42478558  -0.08552083   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -0.58174038\n",
      "   4.31526423 -10.63344765  -3.53466225]\n",
      "Action shape: [ 0.         -0.05772054 -0.7627072   0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.           0.00603199   0.00206032   0.99999788   0.\n",
      "   0.42478558  -0.08552083   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -0.58174038\n",
      "   4.31526423 -10.63344765  -3.53466225]\n",
      "Done: False\n",
      "\n",
      "Experience 49:\n",
      "State shape: [ -6.00000048   0.01680517   0.00303673   0.99999539   0.\n",
      "   0.5329451    0.04882069   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -0.79419708\n",
      "   4.24464178 -10.62281418  -3.53112769]\n",
      "Action shape: [0.         0.17458603 0.7574369  0.         0.         0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00000048   0.01680517   0.00303673   0.99999539   0.\n",
      "   0.5329451    0.04882069   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -0.79419708\n",
      "   4.24464178 -10.62281418  -3.53112769]\n",
      "Done: False\n",
      "\n",
      "Experience 50:\n",
      "State shape: [ -6.           0.03961039   0.00186483   0.99999826   0.\n",
      "   1.14712489  -0.0585952    3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -1.00644112\n",
      "   4.17408991 -10.6121912   -3.52759671]\n",
      "Action shape: [ 0.         0.9351329 -0.6056263  0.         0.         0.       ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.           0.03961039   0.00186483   0.99999826   0.\n",
      "   1.14712489  -0.0585952    3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -1.00644112\n",
      "   4.17408991 -10.6121912   -3.52759671]\n",
      "Done: False\n",
      "\n",
      "Experience 51:\n",
      "State shape: [ -6.           0.05626583  -0.00064998   0.99999979   0.\n",
      "   0.84754324  -0.12574044   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -1.21847248\n",
      "   4.10360861 -10.60157871  -3.52406907]\n",
      "Action shape: [ 0.         -0.4140178  -0.37857458  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.           0.05626583  -0.00064998   0.99999979   0.\n",
      "   0.84754324  -0.12574044   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -1.21847248\n",
      "   4.10360861 -10.60157871  -3.52406907]\n",
      "Done: False\n",
      "\n",
      "Experience 52:\n",
      "State shape: [ -6.           0.06323957  -0.00028411   0.99999996   0.\n",
      "   0.34653586   0.01829323   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -1.43029213\n",
      "   4.03319788 -10.59097767  -3.52054501]\n",
      "Action shape: [ 0.         -0.72443837  0.8120827   0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.           0.06323957  -0.00028411   0.99999996   0.\n",
      "   0.34653586   0.01829323   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -1.43029213\n",
      "   4.03319788 -10.59097767  -3.52054501]\n",
      "Done: False\n",
      "\n",
      "Experience 53:\n",
      "State shape: [ -6.           0.06572104   0.00002096   1.           0.\n",
      "   0.12228625   0.01525364   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -1.64190006\n",
      "   3.96285772 -10.58038712  -3.51702452]\n",
      "Action shape: [ 0.         -0.32523918 -0.01713766  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.           0.06572104   0.00002096   1.           0.\n",
      "   0.12228625   0.01525364   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -1.64190006\n",
      "   3.96285772 -10.58038712  -3.51702452]\n",
      "Done: False\n",
      "\n",
      "Experience 54:\n",
      "State shape: [ -6.           0.06206608   0.0007207    0.99999974   0.\n",
      "  -0.18687809   0.03498679   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -1.85329628\n",
      "   3.89258718 -10.56980705  -3.5135076 ]\n",
      "Action shape: [ 0.         -0.45903474  0.11125833  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.           0.06206608   0.0007207    0.99999974   0.\n",
      "  -0.18687809   0.03498679   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -1.85329628\n",
      "   3.89258718 -10.56980705  -3.5135076 ]\n",
      "Done: False\n",
      "\n",
      "Experience 55:\n",
      "State shape: [ -6.00000048   0.04829741   0.00286144   0.99999591   0.\n",
      "  -0.70099324   0.10703722   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -2.06448078\n",
      "   3.82238722 -10.55923748  -3.50999403]\n",
      "Action shape: [ 0.         -0.7750177   0.40623078  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00000048   0.04829741   0.00286144   0.99999591   0.\n",
      "  -0.70099324   0.10703722   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -2.06448078\n",
      "   3.82238722 -10.55923748  -3.50999403]\n",
      "Done: False\n",
      "\n",
      "Experience 56:\n",
      "State shape: [ -6.00000095   0.02390671   0.00318831   0.99999492   0.\n",
      "  -1.22146428   0.01634392   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -2.27545452\n",
      "   3.75225782 -10.5486784   -3.50648403]\n",
      "Action shape: [ 0.        -0.7999185 -0.5113419  0.         0.         0.       ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00000095   0.02390671   0.00318831   0.99999492   0.\n",
      "  -1.22146428   0.01634392   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -2.27545452\n",
      "   3.75225782 -10.5486784   -3.50648403]\n",
      "Done: False\n",
      "\n",
      "Experience 57:\n",
      "State shape: [ -6.           0.00020313   0.00000315   1.           0.\n",
      "  -1.16646886  -0.15925856   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -2.48621702\n",
      "   3.68219805 -10.53812981  -3.50297761]\n",
      "Action shape: [ 0.          0.04574521 -0.9900722   0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.           0.00020313   0.00000315   1.           0.\n",
      "  -1.16646886  -0.15925856   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -2.48621702\n",
      "   3.68219805 -10.53812981  -3.50297761]\n",
      "Done: False\n",
      "\n",
      "Experience 58:\n",
      "State shape: [ -6.          -0.03347683  -0.00126314   0.9999992    0.\n",
      "  -1.67655611  -0.06331422   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -2.69676876\n",
      "   3.61220884 -10.52759171  -3.49947476]\n",
      "Action shape: [ 0.         -0.7983106   0.54094803  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.          -0.03347683  -0.00126314   0.9999992    0.\n",
      "  -1.67655611  -0.06331422   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -2.69676876\n",
      "   3.61220884 -10.52759171  -3.49947476]\n",
      "Done: False\n",
      "\n",
      "Experience 59:\n",
      "State shape: [ -6.00000095  -0.07734442  -0.0045043    0.99998986   0.\n",
      "  -2.17433929  -0.16205907   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -2.90711021\n",
      "   3.54228926 -10.51706409  -3.49597526]\n",
      "Action shape: [ 0.         -0.79516417 -0.55673766  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00000095  -0.07734442  -0.0045043    0.99998986   0.\n",
      "  -2.17433929  -0.16205907   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -2.90711021\n",
      "   3.54228926 -10.51706409  -3.49597526]\n",
      "Done: False\n",
      "\n",
      "Experience 60:\n",
      "State shape: [ -6.00000429  -0.12973547  -0.00851719   0.99996373   0.\n",
      "  -2.59598351  -0.20064881   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -3.11724091\n",
      "   3.47243929 -10.50654697  -3.49247932]\n",
      "Action shape: [ 0.         -0.69611454 -0.21757452  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00000429  -0.12973547  -0.00851719   0.99996373   0.\n",
      "  -2.59598351  -0.20064881   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -3.11724091\n",
      "   3.47243929 -10.50654697  -3.49247932]\n",
      "Done: False\n",
      "\n",
      "Experience 61:\n",
      "State shape: [ -6.00001335  -0.18462086  -0.01502872   0.99988706   0.\n",
      "  -2.70601463  -0.32559931   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -3.32716179\n",
      "   3.40265989 -10.49604034  -3.48898697]\n",
      "Action shape: [ 0.         -0.24237506 -0.70448893  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00001335  -0.18462086  -0.01502872   0.99988706   0.\n",
      "  -2.70601463  -0.32559931   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -3.32716179\n",
      "   3.40265989 -10.49604034  -3.48898697]\n",
      "Done: False\n",
      "\n",
      "Experience 62:\n",
      "State shape: [ -6.00001907  -0.2402997   -0.01812713   0.99983569   0.\n",
      "  -2.76574302  -0.15494229   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -3.53687286\n",
      "   3.33295012 -10.4855442   -3.48549795]\n",
      "Action shape: [ 0.         -0.17038557  0.96218896  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00001907  -0.2402997   -0.01812713   0.99983569   0.\n",
      "  -2.76574302  -0.15494229   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -3.53687286\n",
      "   3.33295012 -10.4855442   -3.48549795]\n",
      "Done: False\n",
      "\n",
      "Experience 63:\n",
      "State shape: [ -6.00002575  -0.3051219   -0.02093385   0.99978086   0.\n",
      "  -3.22461963  -0.14036241   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -3.74637413\n",
      "   3.26330996 -10.47505856  -3.48201251]\n",
      "Action shape: [ 0.         -0.7695384   0.08220345  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00002575  -0.3051219   -0.02093385   0.99978086   0.\n",
      "  -3.22461963  -0.14036241   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -3.74637413\n",
      "   3.26330996 -10.47505856  -3.48201251]\n",
      "Done: False\n",
      "\n",
      "Experience 64:\n",
      "State shape: [ -6.00002432  -0.35679102  -0.02032796   0.99979337   0.\n",
      "  -2.58702683   0.03030063   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -3.95566559\n",
      "   3.19373941 -10.4645834   -3.47853065]\n",
      "Action shape: [0.        0.8577014 0.9622229 0.        0.        0.       ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00002432  -0.35679102  -0.02032796   0.99979337   0.\n",
      "  -2.58702683   0.03030063   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -3.95566559\n",
      "   3.19373941 -10.4645834   -3.47853065]\n",
      "Done: False\n",
      "\n",
      "Experience 65:\n",
      "State shape: [ -6.00002098  -0.41116333  -0.01902648   0.99981898   0.\n",
      "  -2.72623372   0.06508671   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -4.16474819\n",
      "   3.12423849 -10.45411873  -3.47505212]\n",
      "Action shape: [ 0.         -0.28577158  0.19612893  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00002098  -0.41116333  -0.01902648   0.99981898   0.\n",
      "  -2.72623372   0.06508671   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -4.16474819\n",
      "   3.12423849 -10.45411873  -3.47505212]\n",
      "Done: False\n",
      "\n",
      "Experience 66:\n",
      "State shape: [ -6.00002003  -0.47755098  -0.01852589   0.99982838   0.\n",
      "  -3.32233214   0.02503391   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -4.37362146\n",
      "   3.05480719 -10.44366455  -3.47157717]\n",
      "Action shape: [ 0.         -0.97372156 -0.22582346  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00002003  -0.47755098  -0.01852589   0.99982838   0.\n",
      "  -3.32233214   0.02503391   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -4.37362146\n",
      "   3.05480719 -10.44366455  -3.47157717]\n",
      "Done: False\n",
      "\n",
      "Experience 67:\n",
      "State shape: [ -6.0000267   -0.54841661  -0.0213317    0.99977245   0.\n",
      "  -3.52677274  -0.14031869   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -4.58228588\n",
      "   2.9854455  -10.43322086  -3.46810555]\n",
      "Action shape: [ 0.         -0.40540954 -0.9322818   0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.0000267   -0.54841661  -0.0213317    0.99977245   0.\n",
      "  -3.52677274  -0.14031869   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -4.58228588\n",
      "   2.9854455  -10.43322086  -3.46810555]\n",
      "Done: False\n",
      "\n",
      "Experience 68:\n",
      "State shape: [ -6.00003719  -0.62144899  -0.02524034   0.99968141   0.\n",
      "  -3.62866926  -0.19548507   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -4.79074144\n",
      "   2.91615248 -10.42278767  -3.46463752]\n",
      "Action shape: [ 0.         -0.25806138 -0.31103602  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00003719  -0.62144899  -0.02524034   0.99968141   0.\n",
      "  -3.62866926  -0.19548507   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -4.79074144\n",
      "   2.91615248 -10.42278767  -3.46463752]\n",
      "Done: False\n",
      "\n",
      "Experience 69:\n",
      "State shape: [ -6.00005388  -0.68741941  -0.03027208   0.9995417    0.\n",
      "  -3.26896477  -0.25168425   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -4.99898863\n",
      "   2.84692907 -10.41236496  -3.46117282]\n",
      "Action shape: [ 0.         0.4297202 -0.316859   0.         0.         0.       ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00005388  -0.68741941  -0.03027208   0.9995417    0.\n",
      "  -3.26896477  -0.25168425   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -4.99898863\n",
      "   2.84692907 -10.41236496  -3.46117282]\n",
      "Done: False\n",
      "\n",
      "Experience 70:\n",
      "State shape: [ -6.00006008  -0.76141405  -0.03192552   0.99949025   0.\n",
      "  -3.69002938  -0.08271174   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -5.20702744\n",
      "   2.77777529 -10.40195274  -3.4577117 ]\n",
      "Action shape: [ 0.         -0.72801125  0.95269144  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00006008  -0.76141405  -0.03192552   0.99949025   0.\n",
      "  -3.69002938  -0.08271174   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -5.20702744\n",
      "   2.77777529 -10.40195274  -3.4577117 ]\n",
      "Done: False\n",
      "\n",
      "Experience 71:\n",
      "State shape: [ -6.00006342  -0.83010674  -0.03283724   0.99946071   0.\n",
      "  -3.42928076  -0.04561004   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -5.41485834\n",
      "   2.70869017 -10.39155102  -3.45425391]\n",
      "Action shape: [0.         0.27978647 0.20918474 0.         0.         0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00006342  -0.83010674  -0.03283724   0.99946071   0.\n",
      "  -3.42928076  -0.04561004   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -5.41485834\n",
      "   2.70869017 -10.39155102  -3.45425391]\n",
      "Done: False\n",
      "\n",
      "Experience 72:\n",
      "State shape: [ -6.00006819  -0.90721416  -0.03403333   0.9994207    0.\n",
      "  -3.84832621  -0.05983786   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -5.62248135\n",
      "   2.63967371 -10.38115978  -3.4507997 ]\n",
      "Action shape: [ 0.        -0.7297878 -0.0802185  0.         0.         0.       ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00006819  -0.90721416  -0.03403333   0.9994207    0.\n",
      "  -3.84832621  -0.05983786   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -5.62248135\n",
      "   2.63967371 -10.38115978  -3.4507997 ]\n",
      "Done: False\n",
      "\n",
      "Experience 73:\n",
      "State shape: [ -6.00007677  -0.9886775   -0.0361285    0.99934715   0.\n",
      "  -4.06086826  -0.10482343   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -5.82989693\n",
      "   2.57072687 -10.37077904  -3.44734883]\n",
      "Action shape: [ 0.        -0.4332782 -0.2536351  0.         0.         0.       ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00007677  -0.9886775   -0.0361285    0.99934715   0.\n",
      "  -4.06086826  -0.10482343   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -5.82989693\n",
      "   2.57072687 -10.37077904  -3.44734883]\n",
      "Done: False\n",
      "\n",
      "Experience 74:\n",
      "State shape: [ -6.00008535  -1.06959438  -0.03812754   0.99927288   0.\n",
      "  -4.03410149  -0.10002099   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -6.03710508\n",
      "   2.5018487  -10.36040878  -3.44390154]\n",
      "Action shape: [ 0.         -0.08149058  0.02707684  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00008535  -1.06959438  -0.03812754   0.99927288   0.\n",
      "  -4.03410149  -0.10002099   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -6.03710508\n",
      "   2.5018487  -10.36040878  -3.44390154]\n",
      "Done: False\n",
      "\n",
      "Experience 75:\n",
      "State shape: [ -6.00008488  -1.1596241   -0.03794504   0.99927983   0.\n",
      "  -4.50256252   0.00913166   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -6.24410629\n",
      "   2.43303919 -10.35004807  -3.44045758]\n",
      "Action shape: [ 0.         -0.82184684  0.6154184   0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00008488  -1.1596241   -0.03794504   0.99927983   0.\n",
      "  -4.50256252   0.00913166   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -6.24410629\n",
      "   2.43303919 -10.35004807  -3.44045758]\n",
      "Done: False\n",
      "\n",
      "Experience 76:\n",
      "State shape: [ -6.0000906   -1.23619556  -0.03928711   0.99922796   0.\n",
      "  -3.82068634  -0.06715346   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -6.45090008\n",
      "   2.3642993  -10.33969784  -3.4370172 ]\n",
      "Action shape: [ 0.          0.88572454 -0.4301065   0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.0000906   -1.23619556  -0.03928711   0.99922796   0.\n",
      "  -3.82068634  -0.06715346   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -6.45090008\n",
      "   2.3642993  -10.33969784  -3.4370172 ]\n",
      "Done: False\n",
      "\n",
      "Experience 77:\n",
      "State shape: [ -6.00010729  -1.30953026  -0.04269829   0.99908801   0.\n",
      "  -3.64671659  -0.1707024    3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -6.65748692\n",
      "   2.29562807 -10.3293581   -3.43358016]\n",
      "Action shape: [ 0.          0.14600244 -0.58382386  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00010729  -1.30953026  -0.04269829   0.99908801   0.\n",
      "  -3.64671659  -0.1707024    3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -6.65748692\n",
      "   2.29562807 -10.3293581   -3.43358016]\n",
      "Done: False\n",
      "\n",
      "Experience 78:\n",
      "State shape: [ -6.00013876  -1.3865757   -0.04857736   0.99881942   0.\n",
      "  -3.81772137  -0.29426071   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -6.86386776\n",
      "   2.22702551 -10.31902885  -3.43014669]\n",
      "Action shape: [ 0.         -0.3650789  -0.69663954  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00013876  -1.3865757   -0.04857736   0.99881942   0.\n",
      "  -3.81772137  -0.29426071   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -6.86386776\n",
      "   2.22702551 -10.31902885  -3.43014669]\n",
      "Done: False\n",
      "\n",
      "Experience 79:\n",
      "State shape: [ -6.00015354  -1.45481253  -0.05113591   0.9986917    0.\n",
      "  -3.39678884  -0.12808691   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -7.07004166\n",
      "   2.15849161 -10.3087101   -3.42671657]\n",
      "Action shape: [0.         0.51569545 0.9369119  0.         0.         0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00015354  -1.45481253  -0.05113591   0.9986917    0.\n",
      "  -3.39678884  -0.12808691   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -7.07004166\n",
      "   2.15849161 -10.3087101   -3.42671657]\n",
      "Done: False\n",
      "\n",
      "Experience 80:\n",
      "State shape: [ -6.00018311  -1.51289749  -0.05582082   0.9984408    0.\n",
      "  -2.87673903  -0.23458131   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -7.27601004\n",
      "   2.09002542 -10.29840183  -3.42328978]\n",
      "Action shape: [ 0.         0.6766334 -0.6004308  0.         0.         0.       ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00018311  -1.51289749  -0.05582082   0.9984408    0.\n",
      "  -2.87673903  -0.23458131   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -7.27601004\n",
      "   2.09002542 -10.29840183  -3.42328978]\n",
      "Done: False\n",
      "\n",
      "Experience 81:\n",
      "State shape: [ -6.00021935  -1.56961632  -0.0611183    0.99813053   0.\n",
      "  -2.80480695  -0.26532802   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -7.48177242\n",
      "   2.0216279  -10.2881031   -3.41986656]\n",
      "Action shape: [ 0.          0.02154712 -0.17335446  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00021935  -1.56961632  -0.0611183    0.99813053   0.\n",
      "  -2.80480695  -0.26532802   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -7.48177242\n",
      "   2.0216279  -10.2881031   -3.41986656]\n",
      "Done: False\n",
      "\n",
      "Experience 82:\n",
      "State shape: [ -6.00026417  -1.62678289  -0.06701466   0.99775199   0.\n",
      "  -2.82367754  -0.29542556   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -7.68732834\n",
      "   1.95329905 -10.27781487  -3.41644669]\n",
      "Action shape: [ 0.         -0.11219496 -0.16969432  0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00026417  -1.62678289  -0.06701466   0.99775199   0.\n",
      "  -2.82367754  -0.29542556   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -7.68732834\n",
      "   1.95329905 -10.27781487  -3.41644669]\n",
      "Done: False\n",
      "\n",
      "Experience 83:\n",
      "State shape: [ -6.00033951  -1.69291162  -0.0759268    0.99711339   0.\n",
      "  -3.25407124  -0.44675124   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -7.89267921\n",
      "   1.88503885 -10.26753712  -3.41303039]\n",
      "Action shape: [ 0.         -0.72864497 -0.853196    0.          0.          0.        ]\n",
      "Reward: -11.009121858426772\n",
      "Next State shape: [ -6.00033951  -1.69291162  -0.0759268    0.99711339   0.\n",
      "  -3.25407124  -0.44675124   3.52887154   2.83940458   0.\n",
      "   1.           0.           0.           0.          -7.89267921\n",
      "   1.88503885 -10.26753712  -3.41303039]\n",
      "Done: True\n",
      "\n",
      "Experience 84:\n",
      "State shape: [ -6.          -0.01000643   0.00047059   0.99999989   0.\n",
      "  -0.50308704   0.02352929   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           3.32406425\n",
      "   0.3952713  -13.58210945   0.89169651]\n",
      "Action shape: [ 0.         -0.75291944  0.13266155  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.          -0.01000643   0.00047059   0.99999989   0.\n",
      "  -0.50308704   0.02352929   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           3.32406425\n",
      "   0.3952713  -13.58210945   0.89169651]\n",
      "Done: False\n",
      "\n",
      "Experience 85:\n",
      "State shape: [ -6.          -0.00806713  -0.00064957   0.99999979   0.\n",
      "   0.1035369   -0.05600805   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           3.05269337\n",
      "   0.41308737 -13.56852722   0.89080483]\n",
      "Action shape: [ 0.         0.8928143 -0.448443   0.         0.         0.       ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.          -0.00806713  -0.00064957   0.99999979   0.\n",
      "   0.1035369   -0.05600805   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           3.05269337\n",
      "   0.41308737 -13.56852722   0.89080483]\n",
      "Done: False\n",
      "\n",
      "Experience 86:\n",
      "State shape: [ -6.          -0.01201725  -0.00101456   0.99999949   0.\n",
      "  -0.19536175  -0.01824923   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           2.78159428\n",
      "   0.43088579 -13.5549593    0.88991404]\n",
      "Action shape: [ 0.         -0.44423229  0.21288966  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.          -0.01201725  -0.00101456   0.99999949   0.\n",
      "  -0.19536175  -0.01824923   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           2.78159428\n",
      "   0.43088579 -13.5549593    0.88991404]\n",
      "Done: False\n",
      "\n",
      "Experience 87:\n",
      "State shape: [ -6.          -0.02218151  -0.00110503   0.99999939   0.\n",
      "  -0.50767958  -0.00452347   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           2.51076603\n",
      "   0.4486661  -13.54140472   0.88902414]\n",
      "Action shape: [ 0.         -0.47326204  0.07738784  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.          -0.02218151  -0.00110503   0.99999939   0.\n",
      "  -0.50767958  -0.00452347   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           2.51076603\n",
      "   0.4486661  -13.54140472   0.88902414]\n",
      "Done: False\n",
      "\n",
      "Experience 88:\n",
      "State shape: [ -6.          -0.03435946   0.00024765   0.99999997   0.\n",
      "  -0.61684561   0.06763408   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           2.24020863\n",
      "   0.46642876 -13.5278635    0.88813514]\n",
      "Action shape: [ 0.         -0.17857361  0.4068347   0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.          -0.03435946   0.00024765   0.99999997   0.\n",
      "  -0.61684561   0.06763408   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           2.24020863\n",
      "   0.46642876 -13.5278635    0.88813514]\n",
      "Done: False\n",
      "\n",
      "Experience 89:\n",
      "State shape: [ -6.          -0.04802513  -0.00149834   0.99999888   0.\n",
      "  -0.67302942  -0.08729985   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           1.96992207\n",
      "   0.48417377 -13.51433563   0.88724703]\n",
      "Action shape: [ 0.         -0.10254806 -0.8735399   0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.          -0.04802513  -0.00149834   0.99999888   0.\n",
      "  -0.67302942  -0.08729985   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           1.96992207\n",
      "   0.48417377 -13.51433563   0.88724703]\n",
      "Done: False\n",
      "\n",
      "Experience 90:\n",
      "State shape: [ -6.          -0.07013559  -0.00239967   0.99999712   0.\n",
      "  -1.10022771  -0.04506632   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           1.6999054\n",
      "   0.50190115 -13.50082111   0.88635981]\n",
      "Action shape: [ 0.         -0.6594895   0.23811878  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.          -0.07013559  -0.00239967   0.99999712   0.\n",
      "  -1.10022771  -0.04506632   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           1.6999054\n",
      "   0.50190115 -13.50082111   0.88635981]\n",
      "Done: False\n",
      "\n",
      "Experience 91:\n",
      "State shape: [ -6.          -0.09969473  -0.00071368   0.99999975   0.\n",
      "  -1.48785841   0.08429936   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           1.43015862\n",
      "   0.5196104  -13.4873209    0.88547349]\n",
      "Action shape: [ 0.         -0.6130596   0.72938234  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.          -0.09969473  -0.00071368   0.99999975   0.\n",
      "  -1.48785841   0.08429936   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           1.43015862\n",
      "   0.5196104  -13.4873209    0.88547349]\n",
      "Done: False\n",
      "\n",
      "Experience 92:\n",
      "State shape: [ -6.00000095  -0.13790512   0.00424936   0.99999097   0.\n",
      "  -1.93968618   0.24815273   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           1.16068172\n",
      "   0.53730202 -13.47383404   0.884588  ]\n",
      "Action shape: [ 0.         -0.72073925  0.9238291   0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00000095  -0.13790512   0.00424936   0.99999097   0.\n",
      "  -1.93968618   0.24815273   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           1.16068172\n",
      "   0.53730202 -13.47383404   0.884588  ]\n",
      "Done: False\n",
      "\n",
      "Experience 93:\n",
      "State shape: [ -6.00000715  -0.17277241   0.01111029   0.99993828   0.\n",
      "  -1.78367889   0.34305727   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           0.89147472\n",
      "   0.55497599 -13.46036053   0.88370341]\n",
      "Action shape: [0.        0.1754217 0.5350855 0.        0.        0.       ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00000715  -0.17277241   0.01111029   0.99993828   0.\n",
      "  -1.78367889   0.34305727   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           0.89147472\n",
      "   0.55497599 -13.46036053   0.88370341]\n",
      "Done: False\n",
      "\n",
      "Experience 94:\n",
      "State shape: [ -6.00002289  -0.19545269   0.01959186   0.99980806   0.\n",
      "  -1.18385029   0.42412972   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           0.62253666\n",
      "   0.57263231 -13.44690037   0.88281971]\n",
      "Action shape: [0.         0.8443138  0.45709822 0.         0.         0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00002289  -0.19545269   0.01959186   0.99980806   0.\n",
      "  -1.18385029   0.42412972   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           0.62253666\n",
      "   0.57263231 -13.44690037   0.88281971]\n",
      "Done: False\n",
      "\n",
      "Experience 95:\n",
      "State shape: [ -6.00005531  -0.22332954   0.03077056   0.99952647   0.\n",
      "  -1.45951927   0.55911547   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           0.35386753\n",
      "   0.590271   -13.43345356   0.88193691]\n",
      "Action shape: [ 0.        -0.4480008  0.761069   0.         0.         0.       ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00005531  -0.22332954   0.03077056   0.99952647   0.\n",
      "  -1.45951927   0.55911547   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           0.35386753\n",
      "   0.590271   -13.43345356   0.88193691]\n",
      "Done: False\n",
      "\n",
      "Experience 96:\n",
      "State shape: [ -6.00008917  -0.25425148   0.03894988   0.99924117   0.\n",
      "  -1.59416091   0.40921569   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           0.08546734\n",
      "   0.60789204 -13.4200201    0.881055  ]\n",
      "Action shape: [ 0.         -0.24519072 -0.8451566   0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00008917  -0.25425148   0.03894988   0.99924117   0.\n",
      "  -1.59416091   0.40921569   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.           0.08546734\n",
      "   0.60789204 -13.4200201    0.881055  ]\n",
      "Done: False\n",
      "\n",
      "Experience 97:\n",
      "State shape: [ -6.00013542  -0.29753304   0.04797651   0.99884846   0.\n",
      "  -2.21710992   0.45176014   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -0.18266487\n",
      "   0.62549543 -13.4066       0.88017398]\n",
      "Action shape: [ 0.         -0.9800209   0.23987183  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00013542  -0.29753304   0.04797651   0.99884846   0.\n",
      "  -2.21710992   0.45176014   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -0.18266487\n",
      "   0.62549543 -13.4066       0.88017398]\n",
      "Done: False\n",
      "\n",
      "Experience 98:\n",
      "State shape: [ -6.00019646  -0.34051704   0.05779227   0.99832863   0.\n",
      "  -2.20686984   0.49147731   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -0.4505291\n",
      "   0.64308119 -13.39319324   0.8792938 ]\n",
      "Action shape: [ 0.         -0.0510372   0.22393109  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00019646  -0.34051704   0.05779227   0.99832863   0.\n",
      "  -2.20686984   0.49147731   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -0.4505291\n",
      "   0.64308119 -13.39319324   0.8792938 ]\n",
      "Done: False\n",
      "\n",
      "Experience 99:\n",
      "State shape: [ -6.00027943  -0.37578201   0.06896162   0.99761931   0.\n",
      "  -1.82887077   0.55959558   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -0.71812534\n",
      "   0.6606493  -13.37979984   0.87841451]\n",
      "Action shape: [0.         0.49965712 0.38406065 0.         0.         0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00027943  -0.37578201   0.06896162   0.99761931   0.\n",
      "  -1.82887077   0.55959558   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -0.71812534\n",
      "   0.6606493  -13.37979984   0.87841451]\n",
      "Done: False\n",
      "\n",
      "Experience 100:\n",
      "State shape: [ -6.00040388  -0.41654539   0.08284679   0.9965623    0.\n",
      "  -2.11974621   0.69627267   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -0.98545361\n",
      "   0.67820024 -13.36641979   0.87753612]\n",
      "Action shape: [ 0.         -0.49006557  0.77060527  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00040388  -0.41654539   0.08284679   0.9965623    0.\n",
      "  -2.11974621   0.69627267   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -0.98545361\n",
      "   0.67820024 -13.36641979   0.87753612]\n",
      "Done: False\n",
      "\n",
      "Experience 101:\n",
      "State shape: [ -6.0005312   -0.45279884   0.09500016   0.99547726   0.\n",
      "  -1.8840723    0.61008942   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -1.25251484\n",
      "   0.69573355 -13.35305309   0.87665862]\n",
      "Action shape: [ 0.          0.28926116 -0.48591375  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.0005312   -0.45279884   0.09500016   0.99547726   0.\n",
      "  -1.8840723    0.61008942   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -1.25251484\n",
      "   0.69573355 -13.35305309   0.87665862]\n",
      "Done: False\n",
      "\n",
      "Experience 102:\n",
      "State shape: [ -6.00068617  -0.47884369   0.1079046    0.99416125   0.\n",
      "  -1.37807      0.64857316   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -1.51930904\n",
      "   0.71324921 -13.33969975   0.87578195]\n",
      "Action shape: [0.         0.7008884  0.21697704 0.         0.         0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00068617  -0.47884369   0.1079046    0.99416125   0.\n",
      "  -1.37807      0.64857316   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -1.51930904\n",
      "   0.71324921 -13.33969975   0.87578195]\n",
      "Done: False\n",
      "\n",
      "Experience 103:\n",
      "State shape: [ -6.00086355  -0.51718044   0.12100355   0.99265208   0.\n",
      "  -1.9938066    0.65928471   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -1.78583622\n",
      "   0.73074722 -13.32635975   0.87490618]\n",
      "Action shape: [ 0.         -0.96275896  0.06039337  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00086355  -0.51718044   0.12100355   0.99265208   0.\n",
      "  -1.9938066    0.65928471   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -1.78583622\n",
      "   0.73074722 -13.32635975   0.87490618]\n",
      "Done: False\n",
      "\n",
      "Experience 104:\n",
      "State shape: [ -6.00101185  -0.55900717   0.1309315    0.99139142   0.\n",
      "  -2.14968705   0.50038582   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -2.05209684\n",
      "   0.74822807 -13.3130331    0.87403131]\n",
      "Action shape: [ 0.         -0.29296902 -0.89589477  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00101185  -0.55900717   0.1309315    0.99139142   0.\n",
      "  -2.14968705   0.50038582   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -2.05209684\n",
      "   0.74822807 -13.3130331    0.87403131]\n",
      "Done: False\n",
      "\n",
      "Experience 105:\n",
      "State shape: [ -6.00114679  -0.59800339   0.1393651    0.99024107   0.\n",
      "  -1.99935138   0.42558601   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -2.31809139\n",
      "   0.76569128 -13.29971981   0.87315726]\n",
      "Action shape: [ 0.          0.16064765 -0.42173207  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00114679  -0.59800339   0.1393651    0.99024107   0.\n",
      "  -1.99935138   0.42558601   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -2.31809139\n",
      "   0.76569128 -13.29971981   0.87315726]\n",
      "Done: False\n",
      "\n",
      "Experience 106:\n",
      "State shape: [ -6.0013423   -0.64222765   0.15071214   0.98857769   0.\n",
      "  -2.27787566   0.57341814   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -2.58381987\n",
      "   0.78313684 -13.28641987   0.87228411]\n",
      "Action shape: [ 0.        -0.4766835  0.8334989  0.         0.         0.       ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.0013423   -0.64222765   0.15071214   0.98857769   0.\n",
      "  -2.27787566   0.57341814   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -2.58381987\n",
      "   0.78313684 -13.28641987   0.87228411]\n",
      "Done: False\n",
      "\n",
      "Experience 107:\n",
      "State shape: [ -6.00152349  -0.68840408   0.16050446   0.98703512   0.\n",
      "  -2.36637688   0.49565607   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -2.84928274\n",
      "   0.80056524 -13.27313328   0.87141186]\n",
      "Action shape: [ 0.         -0.20063211 -0.43843368  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00152349  -0.68840408   0.16050446   0.98703512   0.\n",
      "  -2.36637688   0.49565607   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -2.84928274\n",
      "   0.80056524 -13.27313328   0.87141186]\n",
      "Done: False\n",
      "\n",
      "Experience 108:\n",
      "State shape: [ -6.00178146  -0.74648952   0.17346697   0.98483969   0.\n",
      "  -2.98042202   0.65736037   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -3.11448002\n",
      "   0.817976   -13.25986004   0.87054044]\n",
      "Action shape: [ 0.        -0.9898096  0.9117121  0.         0.         0.       ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00178146  -0.74648952   0.17346697   0.98483969   0.\n",
      "  -2.98042202   0.65736037   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -3.11448002\n",
      "   0.817976   -13.25986004   0.87054044]\n",
      "Done: False\n",
      "\n",
      "Experience 109:\n",
      "State shape: [ -6.00209713  -0.79922915   0.18807531   0.98215461   0.\n",
      "  -2.7228024    0.74265975   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -3.37941217\n",
      "   0.83536959 -13.24660015   0.86966991]\n",
      "Action shape: [0.         0.29634333 0.48093027 0.         0.         0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00209713  -0.79922915   0.18807531   0.98215461   0.\n",
      "  -2.7228024    0.74265975   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -3.37941217\n",
      "   0.83536959 -13.24660015   0.86966991]\n",
      "Done: False\n",
      "\n",
      "Experience 110:\n",
      "State shape: [ -6.00247097  -0.85154486   0.20399253   0.97897244   0.\n",
      "  -2.70932555   0.81161875   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -3.64407921\n",
      "   0.85274553 -13.23335361   0.86880028]\n",
      "Action shape: [ 0.         -0.06132945  0.38880065  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00247097  -0.85154486   0.20399253   0.97897244   0.\n",
      "  -2.70932555   0.81161875   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -3.64407921\n",
      "   0.85274553 -13.23335361   0.86880028]\n",
      "Done: False\n",
      "\n",
      "Experience 111:\n",
      "State shape: [ -6.00288677  -0.89211035   0.22029629   0.975433     0.\n",
      "  -2.12403536   0.8341862    8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -3.9084816\n",
      "   0.87010431 -13.22012043   0.86793149]\n",
      "Action shape: [0.         0.7948491  0.12723856 0.         0.         0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00288677  -0.89211035   0.22029629   0.975433     0.\n",
      "  -2.12403536   0.8341862    8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -3.9084816\n",
      "   0.87010431 -13.22012043   0.86793149]\n",
      "Done: False\n",
      "\n",
      "Experience 112:\n",
      "State shape: [ -6.00340414  -0.93627834   0.23895414   0.97103085   0.\n",
      "  -2.31802297   0.95852178   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -4.17261982\n",
      "   0.88744545 -13.2069006    0.86706358]\n",
      "Action shape: [ 0.         -0.35389823  0.701022    0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00340414  -0.93627834   0.23895414   0.97103085   0.\n",
      "  -2.31802297   0.95852178   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -4.17261982\n",
      "   0.88744545 -13.2069006    0.86706358]\n",
      "Done: False\n",
      "\n",
      "Experience 113:\n",
      "State shape: [ -6.00403881  -0.9697051    0.25992671   0.96562835   0.\n",
      "  -1.79457009   1.08288372   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -4.43649387\n",
      "   0.90476942 -13.19369411   0.86619651]\n",
      "Action shape: [0.        0.7140159 0.7011708 0.        0.        0.       ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00403881  -0.9697051    0.25992671   0.96562835   0.\n",
      "  -1.79457009   1.08288372   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -4.43649387\n",
      "   0.90476942 -13.19369411   0.86619651]\n",
      "Done: False\n",
      "\n",
      "Experience 114:\n",
      "State shape: [ -6.00472164  -1.0081358    0.28061183   0.95982134   0.\n",
      "  -2.04306555   1.07425916   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -4.70010376\n",
      "   0.92207623 -13.18050098   0.86533034]\n",
      "Action shape: [ 0.         -0.42561287 -0.04862671  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00472164  -1.0081358    0.28061183   0.95982134   0.\n",
      "  -2.04306555   1.07425916   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -4.70010376\n",
      "   0.92207623 -13.18050098   0.86533034]\n",
      "Done: False\n",
      "\n",
      "Experience 115:\n",
      "State shape: [ -6.00533104  -1.0373826    0.29779488   0.95462988   0.\n",
      "  -1.56327641   0.89752042   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -4.96344995\n",
      "   0.93936539 -13.16732025   0.864465  ]\n",
      "Action shape: [ 0.          0.65689886 -0.9964785   0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00533104  -1.0373826    0.29779488   0.95462988   0.\n",
      "  -1.56327641   0.89752042   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -4.96344995\n",
      "   0.93936539 -13.16732025   0.864465  ]\n",
      "Done: False\n",
      "\n",
      "Experience 116:\n",
      "State shape: [ -6.00607681  -1.07665634   0.31741557   0.94828654   0.\n",
      "  -2.07898879   1.03104842   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -5.22653294\n",
      "   0.95663738 -13.15415287   0.86360055]\n",
      "Action shape: [ 0.         -0.8186065   0.75284994  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00607681  -1.07665634   0.31741557   0.94828654   0.\n",
      "  -2.07898879   1.03104842   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -5.22653294\n",
      "   0.95663738 -13.15415287   0.86360055]\n",
      "Done: False\n",
      "\n",
      "Experience 117:\n",
      "State shape: [ -6.00689793  -1.10972786   0.33757776   0.94129764   0.\n",
      "  -1.77202332   1.0669769    8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -5.4893527\n",
      "   0.97389221 -13.14099884   0.86273694]\n",
      "Action shape: [0.         0.39717606 0.20257005 0.         0.         0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00689793  -1.10972786   0.33757776   0.94129764   0.\n",
      "  -1.77202332   1.0669769    8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -5.4893527\n",
      "   0.97389221 -13.14099884   0.86273694]\n",
      "Done: False\n",
      "\n",
      "Experience 118:\n",
      "State shape: [ -6.00784063  -1.1333971    0.35915892   0.93327642   0.\n",
      "  -1.31027997   1.15120566   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -5.75190973\n",
      "   0.99112988 -13.12785816   0.86187422]\n",
      "Action shape: [0.        0.6380043 0.474894  0.        0.        0.       ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00784063  -1.1333971    0.35915892   0.93327642   0.\n",
      "  -1.31027997   1.15120566   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -5.75190973\n",
      "   0.99112988 -13.12785816   0.86187422]\n",
      "Done: False\n",
      "\n",
      "Experience 119:\n",
      "State shape: [ -6.00878811  -1.16965199   0.37944197   0.92521554   0.\n",
      "  -1.93190587   1.09132838   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -6.01420403\n",
      "   1.0083499  -13.11473083   0.86101234]\n",
      "Action shape: [ 0.         -0.96954364 -0.33759657  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00878811  -1.16965199   0.37944197   0.92521554   0.\n",
      "  -1.93190587   1.09132838   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -6.01420403\n",
      "   1.0083499  -13.11473083   0.86101234]\n",
      "Done: False\n",
      "\n",
      "Experience 120:\n",
      "State shape: [ -6.00966787  -1.21471071   0.39721284   0.91772652   0.\n",
      "  -2.35733747   0.9642365    8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -6.27623653\n",
      "   1.02555275 -13.10161591   0.86015135]\n",
      "Action shape: [ 0.         -0.69452596 -0.7165625   0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.00966787  -1.21471071   0.39721284   0.91772652   0.\n",
      "  -2.35733747   0.9642365    8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -6.27623653\n",
      "   1.02555275 -13.10161591   0.86015135]\n",
      "Done: False\n",
      "\n",
      "Experience 121:\n",
      "State shape: [ -6.01054478  -1.26354122   0.41402874   0.91026381   0.\n",
      "  -2.54032636   0.91988569   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -6.53800678\n",
      "   1.04273844 -13.08851433   0.8592912 ]\n",
      "Action shape: [ 0.         -0.34442064 -0.2500562   0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.01054478  -1.26354122   0.41402874   0.91026381   0.\n",
      "  -2.54032636   0.91988569   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -6.53800678\n",
      "   1.04273844 -13.08851433   0.8592912 ]\n",
      "Done: False\n",
      "\n",
      "Experience 122:\n",
      "State shape: [ -6.01152802  -1.31196499   0.43195818   0.90189363   0.\n",
      "  -2.52654982   0.98936498   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -6.79951525\n",
      "   1.05990696 -13.0754261    0.85843194]\n",
      "Action shape: [ 0.         -0.05541917  0.3917341   0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.01152802  -1.31196499   0.43195818   0.90189363   0.\n",
      "  -2.52654982   0.98936498   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -6.79951525\n",
      "   1.05990696 -13.0754261    0.85843194]\n",
      "Done: False\n",
      "\n",
      "Experience 123:\n",
      "State shape: [ -6.01252937  -1.3507967    0.44930782   0.89337701   0.\n",
      "  -2.04350543   0.96637821   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -7.06076241\n",
      "   1.07705832 -13.06235123   0.85757351]\n",
      "Action shape: [ 0.          0.64729905 -0.12960267  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.01252937  -1.3507967    0.44930782   0.89337701   0.\n",
      "  -2.04350543   0.96637821   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -7.06076241\n",
      "   1.07705832 -13.06235123   0.85757351]\n",
      "Done: False\n",
      "\n",
      "Experience 124:\n",
      "State shape: [ -6.01351452  -1.39840221   0.46560843   0.88499084   0.\n",
      "  -2.47604752   0.91657877   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -7.32174778\n",
      "   1.0941925  -13.04928875   0.85671592]\n",
      "Action shape: [ 0.         -0.70850813 -0.28077647  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.01351452  -1.39840221   0.46560843   0.88499084   0.\n",
      "  -2.47604752   0.91657877   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -7.32174778\n",
      "   1.0941925  -13.04928875   0.85671592]\n",
      "Done: False\n",
      "\n",
      "Experience 125:\n",
      "State shape: [ -6.01433849  -1.43555784   0.47869727   0.87798002   0.\n",
      "  -1.93470573   0.74241793   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -7.5824728\n",
      "   1.11130953 -13.03623962   0.85585922]\n",
      "Action shape: [ 0.         0.7360584 -0.9819438  0.         0.         0.       ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.01433849  -1.43555784   0.47869727   0.87798002   0.\n",
      "  -1.93470573   0.74241793   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -7.5824728\n",
      "   1.11130953 -13.03623962   0.85585922]\n",
      "Done: False\n",
      "\n",
      "Experience 126:\n",
      "State shape: [ -6.01510239  -1.47824955   0.49043549   0.8714775    0.\n",
      "  -2.2035358    0.67095429   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -7.84293699\n",
      "   1.12840939 -13.02320385   0.85500336]\n",
      "Action shape: [ 0.         -0.46024013 -0.40292245  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.01510239  -1.47824955   0.49043549   0.8714775    0.\n",
      "  -2.2035358    0.67095429   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -7.84293699\n",
      "   1.12840939 -13.02320385   0.85500336]\n",
      "Done: False\n",
      "\n",
      "Experience 127:\n",
      "State shape: [ -6.01595497  -1.52810383   0.50310993   0.86422243   0.\n",
      "  -2.56720376   0.73020673   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -8.10314083\n",
      "   1.14549255 -13.01018047   0.85414839]\n",
      "Action shape: [ 0.         -0.61022097  0.33407375  0.          0.          0.        ]\n",
      "Reward: -11.008084663507466\n",
      "Next State shape: [ -6.01595497  -1.52810383   0.50310993   0.86422243   0.\n",
      "  -2.56720376   0.73020673   8.72396469  -1.89015579   0.\n",
      "   1.           0.           0.           0.          -8.10314083\n",
      "   1.14549255 -13.01018047   0.85414839]\n",
      "Done: True\n",
      "\n",
      "Experience 128:\n",
      "State shape: [ -6.           0.00490332  -0.00085504   0.99999963   0.\n",
      "   0.25018677  -0.0427521    3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           4.27040768\n",
      "   3.97807837 -14.54678249  -2.28675818]\n",
      "Action shape: [ 0.          0.37442923 -0.24104251  0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.           0.00490332  -0.00085504   0.99999963   0.\n",
      "   0.25018677  -0.0427521    3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           4.27040768\n",
      "   3.97807837 -14.54678249  -2.28675818]\n",
      "Done: False\n",
      "\n",
      "Experience 129:\n",
      "State shape: [ -6.           0.00490332  -0.00085504   0.99999963   0.\n",
      "   0.25018677  -0.0427521    3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           4.27040768\n",
      "   3.97807837 -14.54678249  -2.28675818]\n",
      "Action shape: [ 0.          0.37442923 -0.24104251  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00490332  -0.00085504   0.99999963   0.\n",
      "   0.25018677  -0.0427521    3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           4.27040768\n",
      "   3.97807837 -14.54678249  -2.28675818]\n",
      "Done: False\n",
      "\n",
      "Experience 130:\n",
      "State shape: [ -6.00000143   0.0017066   -0.00494043   0.9999878    0.\n",
      "  -0.13585913  -0.20427047   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           3.97976303\n",
      "   3.93238878 -14.5322361   -2.28447151]\n",
      "Action shape: [ 0.         -0.57026726 -0.91066384  0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00000143   0.0017066   -0.00494043   0.9999878    0.\n",
      "  -0.13585913  -0.20427047   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           3.97976303\n",
      "   3.93238878 -14.5322361   -2.28447151]\n",
      "Done: False\n",
      "\n",
      "Experience 131:\n",
      "State shape: [ -6.00000143   0.0017066   -0.00494043   0.9999878    0.\n",
      "  -0.13585913  -0.20427047   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           3.97976303\n",
      "   3.93238878 -14.5322361   -2.28447151]\n",
      "Action shape: [ 0.         -0.57026726 -0.91066384  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000143   0.0017066   -0.00494043   0.9999878    0.\n",
      "  -0.13585913  -0.20427047   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           3.97976303\n",
      "   3.93238878 -14.5322361   -2.28447151]\n",
      "Done: False\n",
      "\n",
      "Experience 132:\n",
      "State shape: [ -6.00000286   0.0029397   -0.00697099   0.9999757    0.\n",
      "   0.07358979  -0.10152981   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           3.68940926\n",
      "   3.88674498 -14.51770401  -2.28218699]\n",
      "Action shape: [0.         0.30939445 0.5792666  0.         0.         0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00000286   0.0029397   -0.00697099   0.9999757    0.\n",
      "   0.07358979  -0.10152981   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           3.68940926\n",
      "   3.88674498 -14.51770401  -2.28218699]\n",
      "Done: False\n",
      "\n",
      "Experience 133:\n",
      "State shape: [ -6.00000286   0.0029397   -0.00697099   0.9999757    0.\n",
      "   0.07358979  -0.10152981   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           3.68940926\n",
      "   3.88674498 -14.51770401  -2.28218699]\n",
      "Action shape: [0.         0.30939445 0.5792666  0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000286   0.0029397   -0.00697099   0.9999757    0.\n",
      "   0.07358979  -0.10152981   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           3.68940926\n",
      "   3.88674498 -14.51770401  -2.28218699]\n",
      "Done: False\n",
      "\n",
      "Experience 134:\n",
      "State shape: [ -6.00000191   0.00830173  -0.00616376   0.999981     0.\n",
      "   0.26335678   0.04036262   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           3.3993454\n",
      "   3.84114695 -14.50318623  -2.27990484]\n",
      "Action shape: [0.        0.2862077 0.80001   0.        0.        0.       ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00000191   0.00830173  -0.00616376   0.999981     0.\n",
      "   0.26335678   0.04036262   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           3.3993454\n",
      "   3.84114695 -14.50318623  -2.27990484]\n",
      "Done: False\n",
      "\n",
      "Experience 135:\n",
      "State shape: [ -6.00000191   0.00830173  -0.00616376   0.999981     0.\n",
      "   0.26335678   0.04036262   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           3.3993454\n",
      "   3.84114695 -14.50318623  -2.27990484]\n",
      "Action shape: [0.        0.2862077 0.80001   0.        0.        0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000191   0.00830173  -0.00616376   0.999981     0.\n",
      "   0.26335678   0.04036262   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           3.3993454\n",
      "   3.84114695 -14.50318623  -2.27990484]\n",
      "Done: False\n",
      "\n",
      "Experience 136:\n",
      "State shape: [ -6.           0.00109386  -0.00216176   0.99999766   0.\n",
      "  -0.383899     0.20010161   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           3.10957146\n",
      "   3.79559469 -14.4886837   -2.27762508]\n",
      "Action shape: [ 0.         -0.96079946  0.9006315   0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.           0.00109386  -0.00216176   0.99999766   0.\n",
      "  -0.383899     0.20010161   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           3.10957146\n",
      "   3.79559469 -14.4886837   -2.27762508]\n",
      "Done: False\n",
      "\n",
      "Experience 137:\n",
      "State shape: [ -6.           0.00109386  -0.00216176   0.99999766   0.\n",
      "  -0.383899     0.20010161   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           3.10957146\n",
      "   3.79559469 -14.4886837   -2.27762508]\n",
      "Action shape: [ 0.         -0.96079946  0.9006315   0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00109386  -0.00216176   0.99999766   0.\n",
      "  -0.383899     0.20010161   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           3.10957146\n",
      "   3.79559469 -14.4886837   -2.27762508]\n",
      "Done: False\n",
      "\n",
      "Experience 138:\n",
      "State shape: [ -6.          -0.01820087   0.0004057    0.99999992   0.\n",
      "  -0.97983426   0.128373     3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           2.82008743\n",
      "   3.75008821 -14.47419548  -2.27534747]\n",
      "Action shape: [ 0.        -0.9033668 -0.4044163  0.         0.         0.       ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.          -0.01820087   0.0004057    0.99999992   0.\n",
      "  -0.97983426   0.128373     3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           2.82008743\n",
      "   3.75008821 -14.47419548  -2.27534747]\n",
      "Done: False\n",
      "\n",
      "Experience 139:\n",
      "State shape: [ -6.          -0.01820087   0.0004057    0.99999992   0.\n",
      "  -0.97983426   0.128373     3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           2.82008743\n",
      "   3.75008821 -14.47419548  -2.27534747]\n",
      "Action shape: [ 0.        -0.9033668 -0.4044163  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.          -0.01820087   0.0004057    0.99999992   0.\n",
      "  -0.97983426   0.128373     3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           2.82008743\n",
      "   3.75008821 -14.47419548  -2.27534747]\n",
      "Done: False\n",
      "\n",
      "Experience 140:\n",
      "State shape: [ -6.          -0.03806639   0.00211954   0.99999775   0.\n",
      "  -1.00333393   0.0856922    3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           2.53089333\n",
      "   3.70462656 -14.45972157  -2.27307224]\n",
      "Action shape: [ 0.         -0.06449789 -0.24064049  0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.          -0.03806639   0.00211954   0.99999775   0.\n",
      "  -1.00333393   0.0856922    3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           2.53089333\n",
      "   3.70462656 -14.45972157  -2.27307224]\n",
      "Done: False\n",
      "\n",
      "Experience 141:\n",
      "State shape: [ -6.          -0.03806639   0.00211954   0.99999775   0.\n",
      "  -1.00333393   0.0856922    3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           2.53089333\n",
      "   3.70462656 -14.45972157  -2.27307224]\n",
      "Action shape: [ 0.         -0.06449789 -0.24064049  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.          -0.03806639   0.00211954   0.99999775   0.\n",
      "  -1.00333393   0.0856922    3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           2.53089333\n",
      "   3.70462656 -14.45972157  -2.27307224]\n",
      "Done: False\n",
      "\n",
      "Experience 142:\n",
      "State shape: [ -6.          -0.06471014   0.00132093   0.99999913   0.\n",
      "  -1.32752645  -0.03993037   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           2.24198818\n",
      "   3.65921068 -14.44526196  -2.27079916]\n",
      "Action shape: [ 0.         -0.51521796 -0.7082782   0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.          -0.06471014   0.00132093   0.99999913   0.\n",
      "  -1.32752645  -0.03993037   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           2.24198818\n",
      "   3.65921068 -14.44526196  -2.27079916]\n",
      "Done: False\n",
      "\n",
      "Experience 143:\n",
      "State shape: [ -6.          -0.06471014   0.00132093   0.99999913   0.\n",
      "  -1.32752645  -0.03993037   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           2.24198818\n",
      "   3.65921068 -14.44526196  -2.27079916]\n",
      "Action shape: [ 0.         -0.51521796 -0.7082782   0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.          -0.06471014   0.00132093   0.99999913   0.\n",
      "  -1.32752645  -0.03993037   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           2.24198818\n",
      "   3.65921068 -14.44526196  -2.27079916]\n",
      "Done: False\n",
      "\n",
      "Experience 144:\n",
      "State shape: [ -6.00000095  -0.08175898   0.00327225   0.99999465   0.\n",
      "  -0.86389393   0.09756593   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           1.953372\n",
      "   3.61384058 -14.43081665  -2.26852846]\n",
      "Action shape: [0.         0.6541364  0.77522403 0.         0.         0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00000095  -0.08175898   0.00327225   0.99999465   0.\n",
      "  -0.86389393   0.09756593   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           1.953372\n",
      "   3.61384058 -14.43081665  -2.26852846]\n",
      "Done: False\n",
      "\n",
      "Experience 145:\n",
      "State shape: [ -6.00000095  -0.08175898   0.00327225   0.99999465   0.\n",
      "  -0.86389393   0.09756593   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           1.953372\n",
      "   3.61384058 -14.43081665  -2.26852846]\n",
      "Action shape: [0.         0.6541364  0.77522403 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000095  -0.08175898   0.00327225   0.99999465   0.\n",
      "  -0.86389393   0.09756593   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           1.953372\n",
      "   3.61384058 -14.43081665  -2.26852846]\n",
      "Done: False\n",
      "\n",
      "Experience 146:\n",
      "State shape: [ -6.00000381  -0.08487082   0.00820854   0.99996631   0.\n",
      "  -0.18461421   0.24681903   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           1.66504478\n",
      "   3.5685153  -14.41638565  -2.26625991]\n",
      "Action shape: [0.         0.9907512  0.84151053 0.         0.         0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00000381  -0.08487082   0.00820854   0.99996631   0.\n",
      "  -0.18461421   0.24681903   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           1.66504478\n",
      "   3.5685153  -14.41638565  -2.26625991]\n",
      "Done: False\n",
      "\n",
      "Experience 147:\n",
      "State shape: [ -6.00000381  -0.08487082   0.00820854   0.99996631   0.\n",
      "  -0.18461421   0.24681903   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           1.66504478\n",
      "   3.5685153  -14.41638565  -2.26625991]\n",
      "Action shape: [0.         0.9907512  0.84151053 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000381  -0.08487082   0.00820854   0.99996631   0.\n",
      "  -0.18461421   0.24681903   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           1.66504478\n",
      "   3.5685153  -14.41638565  -2.26625991]\n",
      "Done: False\n",
      "\n",
      "Experience 148:\n",
      "State shape: [ -6.00001431  -0.08739567   0.01552469   0.99987948   0.\n",
      "  -0.16923903   0.36583382   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           1.37700558\n",
      "   3.5232358  -14.40196991  -2.26399374]\n",
      "Action shape: [0.         0.01748461 0.67102265 0.         0.         0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00001431  -0.08739567   0.01552469   0.99987948   0.\n",
      "  -0.16923903   0.36583382   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           1.37700558\n",
      "   3.5232358  -14.40196991  -2.26399374]\n",
      "Done: False\n",
      "\n",
      "Experience 149:\n",
      "State shape: [ -6.00001431  -0.08739567   0.01552469   0.99987948   0.\n",
      "  -0.16923903   0.36583382   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           1.37700558\n",
      "   3.5232358  -14.40196991  -2.26399374]\n",
      "Action shape: [0.         0.01748461 0.67102265 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00001431  -0.08739567   0.01552469   0.99987948   0.\n",
      "  -0.16923903   0.36583382   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           1.37700558\n",
      "   3.5232358  -14.40196991  -2.26399374]\n",
      "Done: False\n",
      "\n",
      "Experience 150:\n",
      "State shape: [ -6.00003815  -0.08225918   0.02541407   0.99967701   0.\n",
      "   0.19871163   0.49457467   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           1.08925438\n",
      "   3.47800112 -14.38756847  -2.26172972]\n",
      "Action shape: [0.        0.5456088 0.7258595 0.        0.        0.       ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00003815  -0.08225918   0.02541407   0.99967701   0.\n",
      "   0.19871163   0.49457467   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           1.08925438\n",
      "   3.47800112 -14.38756847  -2.26172972]\n",
      "Done: False\n",
      "\n",
      "Experience 151:\n",
      "State shape: [ -6.00003815  -0.08225918   0.02541407   0.99967701   0.\n",
      "   0.19871163   0.49457467   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           1.08925438\n",
      "   3.47800112 -14.38756847  -2.26172972]\n",
      "Action shape: [0.        0.5456088 0.7258595 0.        0.        0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00003815  -0.08225918   0.02541407   0.99967701   0.\n",
      "   0.19871163   0.49457467   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           1.08925438\n",
      "   3.47800112 -14.38756847  -2.26172972]\n",
      "Done: False\n",
      "\n",
      "Experience 152:\n",
      "State shape: [ -6.00008202  -0.06730413   0.03739087   0.99930072   0.\n",
      "   0.6773982    0.59913939   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           0.80179119\n",
      "   3.43281221 -14.37318134  -2.25946808]\n",
      "Action shape: [0.        0.7223496 0.5895511 0.        0.        0.       ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00008202  -0.06730413   0.03739087   0.99930072   0.\n",
      "   0.6773982    0.59913939   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           0.80179119\n",
      "   3.43281221 -14.37318134  -2.25946808]\n",
      "Done: False\n",
      "\n",
      "Experience 153:\n",
      "State shape: [ -6.00008202  -0.06730413   0.03739087   0.99930072   0.\n",
      "   0.6773982    0.59913939   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           0.80179119\n",
      "   3.43281221 -14.37318134  -2.25946808]\n",
      "Action shape: [0.        0.7223496 0.5895511 0.        0.        0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00008202  -0.06730413   0.03739087   0.99930072   0.\n",
      "   0.6773982    0.59913939   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           0.80179119\n",
      "   3.43281221 -14.37318134  -2.25946808]\n",
      "Done: False\n",
      "\n",
      "Experience 154:\n",
      "State shape: [ -6.00013971  -0.06034088   0.04872917   0.99881203   0.\n",
      "   0.28154773   0.56744426   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           0.51461506\n",
      "   3.38766813 -14.35880852  -2.25720859]\n",
      "Action shape: [ 0.         -0.5721535  -0.17870176  0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00013971  -0.06034088   0.04872917   0.99881203   0.\n",
      "   0.28154773   0.56744426   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           0.51461506\n",
      "   3.38766813 -14.35880852  -2.25720859]\n",
      "Done: False\n",
      "\n",
      "Experience 155:\n",
      "State shape: [ -6.00013971  -0.06034088   0.04872917   0.99881203   0.\n",
      "   0.28154773   0.56744426   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           0.51461506\n",
      "   3.38766813 -14.35880852  -2.25720859]\n",
      "Action shape: [ 0.         -0.5721535  -0.17870176  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00013971  -0.06034088   0.04872917   0.99881203   0.\n",
      "   0.28154773   0.56744426   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           0.51461506\n",
      "   3.38766813 -14.35880852  -2.25720859]\n",
      "Done: False\n",
      "\n",
      "Experience 156:\n",
      "State shape: [ -6.00021648  -0.05429077   0.06067824   0.99815738   0.\n",
      "   0.23228176   0.59835345   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           0.22772598\n",
      "   3.34256887 -14.34445     -2.25495148]\n",
      "Action shape: [ 0.         -0.0653041   0.17427048  0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00021648  -0.05429077   0.06067824   0.99815738   0.\n",
      "   0.23228176   0.59835345   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           0.22772598\n",
      "   3.34256887 -14.34445     -2.25495148]\n",
      "Done: False\n",
      "\n",
      "Experience 157:\n",
      "State shape: [ -6.00021648  -0.05429077   0.06067824   0.99815738   0.\n",
      "   0.23228176   0.59835345   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           0.22772598\n",
      "   3.34256887 -14.34445     -2.25495148]\n",
      "Action shape: [ 0.         -0.0653041   0.17427048  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00021648  -0.05429077   0.06067824   0.99815738   0.\n",
      "   0.23228176   0.59835345   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.           0.22772598\n",
      "   3.34256887 -14.34445     -2.25495148]\n",
      "Done: False\n",
      "\n",
      "Experience 158:\n",
      "State shape: [ -6.00032997  -0.05447674   0.07491772   0.99718972   0.\n",
      "  -0.09296631   0.71362197   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -0.05887604\n",
      "   3.29751539 -14.33010578  -2.25269651]\n",
      "Action shape: [ 0.         -0.47981322  0.64990073  0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00032997  -0.05447674   0.07491772   0.99718972   0.\n",
      "  -0.09296631   0.71362197   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -0.05887604\n",
      "   3.29751539 -14.33010578  -2.25269651]\n",
      "Done: False\n",
      "\n",
      "Experience 159:\n",
      "State shape: [ -6.00032997  -0.05447674   0.07491772   0.99718972   0.\n",
      "  -0.09296631   0.71362197   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -0.05887604\n",
      "   3.29751539 -14.33010578  -2.25269651]\n",
      "Action shape: [ 0.         -0.47981322  0.64990073  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00032997  -0.05447674   0.07491772   0.99718972   0.\n",
      "  -0.09296631   0.71362197   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -0.05887604\n",
      "   3.29751539 -14.33010578  -2.25269651]\n",
      "Done: False\n",
      "\n",
      "Experience 160:\n",
      "State shape: [ -6.00047302  -0.05477285   0.08963785   0.99597442   0.\n",
      "  -0.10129609   0.73851764   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -0.34519196\n",
      "   3.25250673 -14.31577587  -2.25044394]\n",
      "Action shape: [ 0.         -0.01524901  0.14036542  0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00047302  -0.05477285   0.08963785   0.99597442   0.\n",
      "  -0.10129609   0.73851764   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -0.34519196\n",
      "   3.25250673 -14.31577587  -2.25044394]\n",
      "Done: False\n",
      "\n",
      "Experience 161:\n",
      "State shape: [ -6.00047302  -0.05477285   0.08963785   0.99597442   0.\n",
      "  -0.10129609   0.73851764   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -0.34519196\n",
      "   3.25250673 -14.31577587  -2.25044394]\n",
      "Action shape: [ 0.         -0.01524901  0.14036542  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00047302  -0.05477285   0.08963785   0.99597442   0.\n",
      "  -0.10129609   0.73851764   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -0.34519196\n",
      "   3.25250673 -14.31577587  -2.25044394]\n",
      "Done: False\n",
      "\n",
      "Experience 162:\n",
      "State shape: [ -6.00061989  -0.04245186   0.10256479   0.99472633   0.\n",
      "   0.54012018   0.64935672   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -0.63122082\n",
      "   3.2075429  -14.30146027  -2.2481935 ]\n",
      "Action shape: [ 0.          0.9569108  -0.50270206  0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00061989  -0.04245186   0.10256479   0.99472633   0.\n",
      "   0.54012018   0.64935672   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -0.63122082\n",
      "   3.2075429  -14.30146027  -2.2481935 ]\n",
      "Done: False\n",
      "\n",
      "Experience 163:\n",
      "State shape: [ -6.00061989  -0.04245186   0.10256479   0.99472633   0.\n",
      "   0.54012018   0.64935672   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -0.63122082\n",
      "   3.2075429  -14.30146027  -2.2481935 ]\n",
      "Action shape: [ 0.          0.9569108  -0.50270206  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00061989  -0.04245186   0.10256479   0.99472633   0.\n",
      "   0.54012018   0.64935672   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -0.63122082\n",
      "   3.2075429  -14.30146027  -2.2481935 ]\n",
      "Done: False\n",
      "\n",
      "Experience 164:\n",
      "State shape: [ -6.00076199  -0.02529955   0.11368414   0.99351694   0.\n",
      "   0.79226452   0.55924958   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -0.91696358\n",
      "   3.16262388 -14.28715897  -2.24594545]\n",
      "Action shape: [ 0.          0.39352575 -0.5080372   0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00076199  -0.02529955   0.11368414   0.99351694   0.\n",
      "   0.79226452   0.55924958   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -0.91696358\n",
      "   3.16262388 -14.28715897  -2.24594545]\n",
      "Done: False\n",
      "\n",
      "Experience 165:\n",
      "State shape: [ -6.00076199  -0.02529955   0.11368414   0.99351694   0.\n",
      "   0.79226452   0.55924958   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -0.91696358\n",
      "   3.16262388 -14.28715897  -2.24594545]\n",
      "Action shape: [ 0.          0.39352575 -0.5080372   0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00076199  -0.02529955   0.11368414   0.99351694   0.\n",
      "   0.79226452   0.55924958   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -0.91696358\n",
      "   3.16262388 -14.28715897  -2.24594545]\n",
      "Done: False\n",
      "\n",
      "Experience 166:\n",
      "State shape: [ -6.00091648  -0.01605654   0.12465139   0.9922006    0.\n",
      "   0.39772561   0.55230069   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -1.20242119\n",
      "   3.11774969 -14.27287197  -2.24369955]\n",
      "Action shape: [ 0.         -0.56675243 -0.0391789   0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00091648  -0.01605654   0.12465139   0.9922006    0.\n",
      "   0.39772561   0.55230069   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -1.20242119\n",
      "   3.11774969 -14.27287197  -2.24369955]\n",
      "Done: False\n",
      "\n",
      "Experience 167:\n",
      "State shape: [ -6.00091648  -0.01605654   0.12465139   0.9922006    0.\n",
      "   0.39772561   0.55230069   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -1.20242119\n",
      "   3.11774969 -14.27287197  -2.24369955]\n",
      "Action shape: [ 0.         -0.56675243 -0.0391789   0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00091648  -0.01605654   0.12465139   0.9922006    0.\n",
      "   0.39772561   0.55230069   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -1.20242119\n",
      "   3.11774969 -14.27287197  -2.24369955]\n",
      "Done: False\n",
      "\n",
      "Experience 168:\n",
      "State shape: [ -6.00105143  -0.00983286   0.13346565   0.99105344   0.\n",
      "   0.25938919   0.44443136   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -1.4875927\n",
      "   3.07292032 -14.25859928  -2.24145579]\n",
      "Action shape: [ 0.         -0.19512945 -0.60818285  0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00105143  -0.00983286   0.13346565   0.99105344   0.\n",
      "   0.25938919   0.44443136   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -1.4875927\n",
      "   3.07292032 -14.25859928  -2.24145579]\n",
      "Done: False\n",
      "\n",
      "Experience 169:\n",
      "State shape: [ -6.00105143  -0.00983286   0.13346565   0.99105344   0.\n",
      "   0.25938919   0.44443136   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -1.4875927\n",
      "   3.07292032 -14.25859928  -2.24145579]\n",
      "Action shape: [ 0.         -0.19512945 -0.60818285  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00105143  -0.00983286   0.13346565   0.99105344   0.\n",
      "   0.25938919   0.44443136   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -1.4875927\n",
      "   3.07292032 -14.25859928  -2.24145579]\n",
      "Done: False\n",
      "\n",
      "Experience 170:\n",
      "State shape: [ -6.00117111   0.00555468   0.14081333   0.99003616   0.\n",
      "   0.72620589   0.37088922   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -1.77247906\n",
      "   3.02813578 -14.2443409   -2.23921442]\n",
      "Action shape: [ 0.         0.7064013 -0.4146413  0.         0.         0.       ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00117111   0.00555468   0.14081333   0.99003616   0.\n",
      "   0.72620589   0.37088922   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -1.77247906\n",
      "   3.02813578 -14.2443409   -2.23921442]\n",
      "Done: False\n",
      "\n",
      "Experience 171:\n",
      "State shape: [ -6.00117111   0.00555468   0.14081333   0.99003616   0.\n",
      "   0.72620589   0.37088922   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -1.77247906\n",
      "   3.02813578 -14.2443409   -2.23921442]\n",
      "Action shape: [ 0.         0.7064013 -0.4146413  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00117111   0.00555468   0.14081333   0.99003616   0.\n",
      "   0.72620589   0.37088922   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -1.77247906\n",
      "   3.02813578 -14.2443409   -2.23921442]\n",
      "Done: False\n",
      "\n",
      "Experience 172:\n",
      "State shape: [ -6.00127792   0.02919579   0.14707626   0.98912516   0.\n",
      "   1.14527142   0.31644261   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -2.05708122\n",
      "   2.98339605 -14.23009682  -2.23697519]\n",
      "Action shape: [ 0.          0.64890975 -0.3069778   0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00127792   0.02919579   0.14707626   0.98912516   0.\n",
      "   1.14527142   0.31644261   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -2.05708122\n",
      "   2.98339605 -14.23009682  -2.23697519]\n",
      "Done: False\n",
      "\n",
      "Experience 173:\n",
      "State shape: [ -6.00127792   0.02919579   0.14707626   0.98912516   0.\n",
      "   1.14527142   0.31644261   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -2.05708122\n",
      "   2.98339605 -14.23009682  -2.23697519]\n",
      "Action shape: [ 0.          0.64890975 -0.3069778   0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00127792   0.02919579   0.14707626   0.98912516   0.\n",
      "   1.14527142   0.31644261   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -2.05708122\n",
      "   2.98339605 -14.23009682  -2.23697519]\n",
      "Done: False\n",
      "\n",
      "Experience 174:\n",
      "State shape: [ -6.00139093   0.05806732   0.15340943   0.98816271   0.\n",
      "   1.40635288   0.32029462   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -2.34139872\n",
      "   2.93870115 -14.21586704  -2.23473835]\n",
      "Action shape: [0.         0.42501447 0.02171821 0.         0.         0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00139093   0.05806732   0.15340943   0.98816271   0.\n",
      "   1.40635288   0.32029462   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -2.34139872\n",
      "   2.93870115 -14.21586704  -2.23473835]\n",
      "Done: False\n",
      "\n",
      "Experience 175:\n",
      "State shape: [ -6.00139093   0.05806732   0.15340943   0.98816271   0.\n",
      "   1.40635288   0.32029462   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -2.34139872\n",
      "   2.93870115 -14.21586704  -2.23473835]\n",
      "Action shape: [0.         0.42501447 0.02171821 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00139093   0.05806732   0.15340943   0.98816271   0.\n",
      "   1.40635288   0.32029462   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -2.34139872\n",
      "   2.93870115 -14.21586704  -2.23473835]\n",
      "Done: False\n",
      "\n",
      "Experience 176:\n",
      "State shape: [ -6.00154686   0.08801413   0.16173414   0.98683437   0.\n",
      "   1.44842839   0.4215025    3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -2.62543154\n",
      "   2.89405107 -14.20165157  -2.23250365]\n",
      "Action shape: [0.         0.10506507 0.57062465 0.         0.         0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00154686   0.08801413   0.16173414   0.98683437   0.\n",
      "   1.44842839   0.4215025    3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -2.62543154\n",
      "   2.89405107 -14.20165157  -2.23250365]\n",
      "Done: False\n",
      "\n",
      "Experience 177:\n",
      "State shape: [ -6.00154686   0.08801413   0.16173414   0.98683437   0.\n",
      "   1.44842839   0.4215025    3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -2.62543154\n",
      "   2.89405107 -14.20165157  -2.23250365]\n",
      "Action shape: [0.         0.10506507 0.57062465 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00154686   0.08801413   0.16173414   0.98683437   0.\n",
      "   1.44842839   0.4215025    3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -2.62543154\n",
      "   2.89405107 -14.20165157  -2.23250365]\n",
      "Done: False\n",
      "\n",
      "Experience 178:\n",
      "State shape: [ -6.00178051   0.11710548   0.17342806   0.98484654   0.\n",
      "   1.38587332   0.59308684   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -2.90918064\n",
      "   2.84944582 -14.18745041  -2.2302711 ]\n",
      "Action shape: [ 0.         -0.0502656   0.96741706  0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00178051   0.11710548   0.17342806   0.98484654   0.\n",
      "   1.38587332   0.59308684   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -2.90918064\n",
      "   2.84944582 -14.18745041  -2.2302711 ]\n",
      "Done: False\n",
      "\n",
      "Experience 179:\n",
      "State shape: [ -6.00178051   0.11710548   0.17342806   0.98484654   0.\n",
      "   1.38587332   0.59308684   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -2.90918064\n",
      "   2.84944582 -14.18745041  -2.2302711 ]\n",
      "Action shape: [ 0.         -0.0502656   0.96741706  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00178051   0.11710548   0.17342806   0.98484654   0.\n",
      "   1.38587332   0.59308684   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -2.90918064\n",
      "   2.84944582 -14.18745041  -2.2302711 ]\n",
      "Done: False\n",
      "\n",
      "Experience 180:\n",
      "State shape: [ -6.00205803   0.15391493   0.18634616   0.98248415   0.\n",
      "   1.76455331   0.65662086   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -3.19264603\n",
      "   2.80488539 -14.17326355  -2.22804093]\n",
      "Action shape: [0.         0.6082137  0.35821408 0.         0.         0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00205803   0.15391493   0.18634616   0.98248415   0.\n",
      "   1.76455331   0.65662086   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -3.19264603\n",
      "   2.80488539 -14.17326355  -2.22804093]\n",
      "Done: False\n",
      "\n",
      "Experience 181:\n",
      "State shape: [ -6.00205803   0.15391493   0.18634616   0.98248415   0.\n",
      "   1.76455331   0.65662086   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -3.19264603\n",
      "   2.80488539 -14.17326355  -2.22804093]\n",
      "Action shape: [0.         0.6082137  0.35821408 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00205803   0.15391493   0.18634616   0.98248415   0.\n",
      "   1.76455331   0.65662086   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -3.19264603\n",
      "   2.80488539 -14.17326355  -2.22804093]\n",
      "Done: False\n",
      "\n",
      "Experience 182:\n",
      "State shape: [ -6.00243568   0.19350052   0.2025446    0.97927304   0.\n",
      "   1.88410091   0.82569218   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -3.47582769\n",
      "   2.76036882 -14.15909004  -2.22581291]\n",
      "Action shape: [0.         0.23173124 0.9532487  0.         0.         0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00243568   0.19350052   0.2025446    0.97927304   0.\n",
      "   1.88410091   0.82569218   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -3.47582769\n",
      "   2.76036882 -14.15909004  -2.22581291]\n",
      "Done: False\n",
      "\n",
      "Experience 183:\n",
      "State shape: [ -6.00243568   0.19350052   0.2025446    0.97927304   0.\n",
      "   1.88410091   0.82569218   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -3.47582769\n",
      "   2.76036882 -14.15909004  -2.22581291]\n",
      "Action shape: [0.         0.23173124 0.9532487  0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00243568   0.19350052   0.2025446    0.97927304   0.\n",
      "   1.88410091   0.82569218   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -3.47582769\n",
      "   2.76036882 -14.15909004  -2.22581291]\n",
      "Done: False\n",
      "\n",
      "Experience 184:\n",
      "State shape: [ -6.00278378   0.22294188   0.21636907   0.97631164   0.\n",
      "   1.39087224   0.70691127   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -3.75872612\n",
      "   2.71589708 -14.14493084  -2.22358704]\n",
      "Action shape: [ 0.         -0.68177044 -0.6697041   0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00278378   0.22294188   0.21636907   0.97631164   0.\n",
      "   1.39087224   0.70691127   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -3.75872612\n",
      "   2.71589708 -14.14493084  -2.22358704]\n",
      "Done: False\n",
      "\n",
      "Experience 185:\n",
      "State shape: [ -6.00278378   0.22294188   0.21636907   0.97631164   0.\n",
      "   1.39087224   0.70691127   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -3.75872612\n",
      "   2.71589708 -14.14493084  -2.22358704]\n",
      "Action shape: [ 0.         -0.68177044 -0.6697041   0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00278378   0.22294188   0.21636907   0.97631164   0.\n",
      "   1.39087224   0.70691127   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -3.75872612\n",
      "   2.71589708 -14.14493084  -2.22358704]\n",
      "Done: False\n",
      "\n",
      "Experience 186:\n",
      "State shape: [ -6.0031271    0.25592232   0.22915087   0.97339092   0.\n",
      "   1.57391      0.65556788   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -4.04134178\n",
      "   2.67147017 -14.13078594  -2.22136354]\n",
      "Action shape: [ 0.          0.31556562 -0.2894813   0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.0031271    0.25592232   0.22915087   0.97339092   0.\n",
      "   1.57391      0.65556788   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -4.04134178\n",
      "   2.67147017 -14.13078594  -2.22136354]\n",
      "Done: False\n",
      "\n",
      "Experience 187:\n",
      "State shape: [ -6.0031271    0.25592232   0.22915087   0.97339092   0.\n",
      "   1.57391      0.65556788   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -4.04134178\n",
      "   2.67147017 -14.13078594  -2.22136354]\n",
      "Action shape: [ 0.          0.31556562 -0.2894813   0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.0031271    0.25592232   0.22915087   0.97339092   0.\n",
      "   1.57391      0.65556788   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -4.04134178\n",
      "   2.67147017 -14.13078594  -2.22136354]\n",
      "Done: False\n",
      "\n",
      "Experience 188:\n",
      "State shape: [ -6.00350904   0.27884626   0.24255003   0.9701389    0.\n",
      "   1.0674597    0.68941289   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -4.32367468\n",
      "   2.62708712 -14.11665535  -2.2191422 ]\n",
      "Action shape: [ 0.        -0.7108427  0.1908232  0.         0.         0.       ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00350904   0.27884626   0.24255003   0.9701389    0.\n",
      "   1.0674597    0.68941289   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -4.32367468\n",
      "   2.62708712 -14.11665535  -2.2191422 ]\n",
      "Done: False\n",
      "\n",
      "Experience 189:\n",
      "State shape: [ -6.00350904   0.27884626   0.24255003   0.9701389    0.\n",
      "   1.0674597    0.68941289   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -4.32367468\n",
      "   2.62708712 -14.11665535  -2.2191422 ]\n",
      "Action shape: [ 0.        -0.7108427  0.1908232  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00350904   0.27884626   0.24255003   0.9701389    0.\n",
      "   1.0674597    0.68941289   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -4.32367468\n",
      "   2.62708712 -14.11665535  -2.2191422 ]\n",
      "Done: False\n",
      "\n",
      "Experience 190:\n",
      "State shape: [ -6.00396347   0.29767132   0.25753836   0.96626808   0.\n",
      "   0.85318696   0.77401268   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -4.60572529\n",
      "   2.58274889 -14.10253906  -2.216923  ]\n",
      "Action shape: [ 0.        -0.2887292  0.476986   0.         0.         0.       ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00396347   0.29767132   0.25753836   0.96626808   0.\n",
      "   0.85318696   0.77401268   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -4.60572529\n",
      "   2.58274889 -14.10253906  -2.216923  ]\n",
      "Done: False\n",
      "\n",
      "Experience 191:\n",
      "State shape: [ -6.00396347   0.29767132   0.25753836   0.96626808   0.\n",
      "   0.85318696   0.77401268   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -4.60572529\n",
      "   2.58274889 -14.10253906  -2.216923  ]\n",
      "Action shape: [ 0.        -0.2887292  0.476986   0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00396347   0.29767132   0.25753836   0.96626808   0.\n",
      "   0.85318696   0.77401268   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -4.60572529\n",
      "   2.58274889 -14.10253906  -2.216923  ]\n",
      "Done: False\n",
      "\n",
      "Experience 192:\n",
      "State shape: [ -6.00445747   0.32347202   0.27281261   0.96206719   0.\n",
      "   1.20028448   0.79207891   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -4.88749409\n",
      "   2.53845453 -14.08843708  -2.21470618]\n",
      "Action shape: [0.         0.5450033  0.10186002 0.         0.         0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00445747   0.32347202   0.27281261   0.96206719   0.\n",
      "   1.20028448   0.79207891   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -4.88749409\n",
      "   2.53845453 -14.08843708  -2.21470618]\n",
      "Done: False\n",
      "\n",
      "Experience 193:\n",
      "State shape: [ -6.00445747   0.32347202   0.27281261   0.96206719   0.\n",
      "   1.20028448   0.79207891   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -4.88749409\n",
      "   2.53845453 -14.08843708  -2.21470618]\n",
      "Action shape: [0.         0.5450033  0.10186002 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00445747   0.32347202   0.27281261   0.96206719   0.\n",
      "   1.20028448   0.79207891   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -4.88749409\n",
      "   2.53845453 -14.08843708  -2.21470618]\n",
      "Done: False\n",
      "\n",
      "Experience 194:\n",
      "State shape: [ -6.00492287   0.36080265   0.28640923   0.95810738   0.\n",
      "   1.78662813   0.70808029   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -5.16898108\n",
      "   2.494205   -14.07434845  -2.21249151]\n",
      "Action shape: [ 0.          0.9134479  -0.47359627  0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00492287   0.36080265   0.28640923   0.95810738   0.\n",
      "   1.78662813   0.70808029   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -5.16898108\n",
      "   2.494205   -14.07434845  -2.21249151]\n",
      "Done: False\n",
      "\n",
      "Experience 195:\n",
      "State shape: [ -6.00492287   0.36080265   0.28640923   0.95810738   0.\n",
      "   1.78662813   0.70808029   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -5.16898108\n",
      "   2.494205   -14.07434845  -2.21249151]\n",
      "Action shape: [ 0.          0.9134479  -0.47359627  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00492287   0.36080265   0.28640923   0.95810738   0.\n",
      "   1.78662813   0.70808029   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -5.16898108\n",
      "   2.494205   -14.07434845  -2.21249151]\n",
      "Done: False\n",
      "\n",
      "Experience 196:\n",
      "State shape: [ -6.00546646   0.40557909   0.30145201   0.95348135   0.\n",
      "   2.15043139   0.78691      3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -5.45018673\n",
      "   2.44999933 -14.06027412  -2.21027899]\n",
      "Action shape: [0.         0.59794486 0.44445315 0.         0.         0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00546646   0.40557909   0.30145201   0.95348135   0.\n",
      "   2.15043139   0.78691      3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -5.45018673\n",
      "   2.44999933 -14.06027412  -2.21027899]\n",
      "Done: False\n",
      "\n",
      "Experience 197:\n",
      "State shape: [ -6.00546646   0.40557909   0.30145201   0.95348135   0.\n",
      "   2.15043139   0.78691      3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -5.45018673\n",
      "   2.44999933 -14.06027412  -2.21027899]\n",
      "Action shape: [0.         0.59794486 0.44445315 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00546646   0.40557909   0.30145201   0.95348135   0.\n",
      "   2.15043139   0.78691      3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -5.45018673\n",
      "   2.44999933 -14.06027412  -2.21027899]\n",
      "Done: False\n",
      "\n",
      "Experience 198:\n",
      "State shape: [ -6.0059967    0.44009733   0.31536797   0.94896947   0.\n",
      "   1.64416778   0.73146272   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -5.73111105\n",
      "   2.40583754 -14.0462141   -2.20806885]\n",
      "Action shape: [ 0.         -0.6933068  -0.31261992  0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.0059967    0.44009733   0.31536797   0.94896947   0.\n",
      "   1.64416778   0.73146272   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -5.73111105\n",
      "   2.40583754 -14.0462141   -2.20806885]\n",
      "Done: False\n",
      "\n",
      "Experience 199:\n",
      "State shape: [ -6.0059967    0.44009733   0.31536797   0.94896947   0.\n",
      "   1.64416778   0.73146272   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -5.73111105\n",
      "   2.40583754 -14.0462141   -2.20806885]\n",
      "Action shape: [ 0.         -0.6933068  -0.31261992  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.0059967    0.44009733   0.31536797   0.94896947   0.\n",
      "   1.64416778   0.73146272   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -5.73111105\n",
      "   2.40583754 -14.0462141   -2.20806885]\n",
      "Done: False\n",
      "\n",
      "Experience 200:\n",
      "State shape: [ -6.00656128   0.47588968   0.32947774   0.94416334   0.\n",
      "   1.70672131   0.74529886   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -6.01175451\n",
      "   2.36172056 -14.03216839  -2.20586085]\n",
      "Action shape: [0.         0.14283077 0.07801011 0.         0.         0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00656128   0.47588968   0.32947774   0.94416334   0.\n",
      "   1.70672131   0.74529886   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -6.01175451\n",
      "   2.36172056 -14.03216839  -2.20586085]\n",
      "Done: False\n",
      "\n",
      "Experience 201:\n",
      "State shape: [ -6.00656128   0.47588968   0.32947774   0.94416334   0.\n",
      "   1.70672131   0.74529886   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -6.01175451\n",
      "   2.36172056 -14.03216839  -2.20586085]\n",
      "Action shape: [0.         0.14283077 0.07801011 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00656128   0.47588968   0.32947774   0.94416334   0.\n",
      "   1.70672131   0.74529886   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -6.01175451\n",
      "   2.36172056 -14.03216839  -2.20586085]\n",
      "Done: False\n",
      "\n",
      "Experience 202:\n",
      "State shape: [ -6.00713348   0.51371384   0.343117     0.93929267   0.\n",
      "   1.81108212   0.72414893   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -6.29211712\n",
      "   2.31764746 -14.01813602  -2.203655  ]\n",
      "Action shape: [ 0.          0.20727187 -0.11924635  0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00713348   0.51371384   0.343117     0.93929267   0.\n",
      "   1.81108212   0.72414893   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -6.29211712\n",
      "   2.31764746 -14.01813602  -2.203655  ]\n",
      "Done: False\n",
      "\n",
      "Experience 203:\n",
      "State shape: [ -6.00713348   0.51371384   0.343117     0.93929267   0.\n",
      "   1.81108212   0.72414893   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -6.29211712\n",
      "   2.31764746 -14.01813602  -2.203655  ]\n",
      "Action shape: [ 0.          0.20727187 -0.11924635  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00713348   0.51371384   0.343117     0.93929267   0.\n",
      "   1.81108212   0.72414893   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -6.29211712\n",
      "   2.31764746 -14.01813602  -2.203655  ]\n",
      "Done: False\n",
      "\n",
      "Experience 204:\n",
      "State shape: [ -6.00777435   0.53971291   0.35769831   0.9338372    0.\n",
      "   1.21427333   0.778431     3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -6.57219934\n",
      "   2.27361822 -14.00411797  -2.2014513 ]\n",
      "Action shape: [ 0.         -0.83897406  0.3060503   0.          0.          0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00777435   0.53971291   0.35769831   0.9338372    0.\n",
      "   1.21427333   0.778431     3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -6.57219934\n",
      "   2.27361822 -14.00411797  -2.2014513 ]\n",
      "Done: False\n",
      "\n",
      "Experience 205:\n",
      "State shape: [ -6.00777435   0.53971291   0.35769831   0.9338372    0.\n",
      "   1.21427333   0.778431     3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -6.57219934\n",
      "   2.27361822 -14.00411797  -2.2014513 ]\n",
      "Action shape: [ 0.         -0.83897406  0.3060503   0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00777435   0.53971291   0.35769831   0.9338372    0.\n",
      "   1.21427333   0.778431     3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -6.57219934\n",
      "   2.27361822 -14.00411797  -2.2014513 ]\n",
      "Done: False\n",
      "\n",
      "Experience 206:\n",
      "State shape: [ -6.00832605   0.56492186   0.36971346   0.92914582   0.\n",
      "   1.18986034   0.64493257   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -6.85200167\n",
      "   2.22963285 -13.99011421  -2.19924998]\n",
      "Action shape: [ 0.        -0.000191  -0.7526834  0.         0.         0.       ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00832605   0.56492186   0.36971346   0.92914582   0.\n",
      "   1.18986034   0.64493257   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -6.85200167\n",
      "   2.22963285 -13.99011421  -2.19924998]\n",
      "Done: False\n",
      "\n",
      "Experience 207:\n",
      "State shape: [ -6.00832605   0.56492186   0.36971346   0.92914582   0.\n",
      "   1.18986034   0.64493257   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -6.85200167\n",
      "   2.22963285 -13.99011421  -2.19924998]\n",
      "Action shape: [ 0.        -0.000191  -0.7526834  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00832605   0.56492186   0.36971346   0.92914582   0.\n",
      "   1.18986034   0.64493257   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -6.85200167\n",
      "   2.22963285 -13.99011421  -2.19924998]\n",
      "Done: False\n",
      "\n",
      "Experience 208:\n",
      "State shape: [ -6.00890827   0.59874964   0.38193492   0.92418922   0.\n",
      "   1.61959279   0.65942109   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -7.13152409\n",
      "   2.18569231 -13.97612381  -2.19705081]\n",
      "Action shape: [0.         0.678752   0.08168843 0.         0.         0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00890827   0.59874964   0.38193492   0.92418922   0.\n",
      "   1.61959279   0.65942109   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -7.13152409\n",
      "   2.18569231 -13.97612381  -2.19705081]\n",
      "Done: False\n",
      "\n",
      "Experience 209:\n",
      "State shape: [ -6.00890827   0.59874964   0.38193492   0.92418922   0.\n",
      "   1.61959279   0.65942109   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -7.13152409\n",
      "   2.18569231 -13.97612381  -2.19705081]\n",
      "Action shape: [0.         0.678752   0.08168843 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00890827   0.59874964   0.38193492   0.92418922   0.\n",
      "   1.61959279   0.65942109   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -7.13152409\n",
      "   2.18569231 -13.97612381  -2.19705081]\n",
      "Done: False\n",
      "\n",
      "Experience 210:\n",
      "State shape: [ -6.00959396   0.63869429   0.39575979   0.91835407   0.\n",
      "   1.91601002   0.75030029   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -7.41076708\n",
      "   2.14179564 -13.96214771  -2.19485378]\n",
      "Action shape: [0.         0.4920951  0.51239014 0.         0.         0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.00959396   0.63869429   0.39575979   0.91835407   0.\n",
      "   1.91601002   0.75030029   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -7.41076708\n",
      "   2.14179564 -13.96214771  -2.19485378]\n",
      "Done: False\n",
      "\n",
      "Experience 211:\n",
      "State shape: [ -6.00959396   0.63869429   0.39575979   0.91835407   0.\n",
      "   1.91601002   0.75030029   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -7.41076708\n",
      "   2.14179564 -13.96214771  -2.19485378]\n",
      "Action shape: [0.         0.4920951  0.51239014 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00959396   0.63869429   0.39575979   0.91835407   0.\n",
      "   1.91601002   0.75030029   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -7.41076708\n",
      "   2.14179564 -13.96214771  -2.19485378]\n",
      "Done: False\n",
      "\n",
      "Experience 212:\n",
      "State shape: [ -6.01031017   0.6901207    0.40961104   0.91226027   0.\n",
      "   2.48991871   0.75662965   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -7.68973064\n",
      "   2.09794283 -13.94818592  -2.1926589 ]\n",
      "Action shape: [0.         0.91626084 0.03568567 0.         0.         0.        ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.01031017   0.6901207    0.40961104   0.91226027   0.\n",
      "   2.48991871   0.75662965   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -7.68973064\n",
      "   2.09794283 -13.94818592  -2.1926589 ]\n",
      "Done: False\n",
      "\n",
      "Experience 213:\n",
      "State shape: [ -6.01031017   0.6901207    0.40961104   0.91226027   0.\n",
      "   2.48991871   0.75662965   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -7.68973064\n",
      "   2.09794283 -13.94818592  -2.1926589 ]\n",
      "Action shape: [0.         0.91626084 0.03568567 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.01031017   0.6901207    0.40961104   0.91226027   0.\n",
      "   2.48991871   0.75662965   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -7.68973064\n",
      "   2.09794283 -13.94818592  -2.1926589 ]\n",
      "Done: False\n",
      "\n",
      "Experience 214:\n",
      "State shape: [ -6.01097965   0.75000906   0.42206494   0.9065656    0.\n",
      "   2.92124724   0.68471074   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -7.58350945\n",
      "   1.89370394   6.42243195 -10.62292671]\n",
      "Action shape: [ 0.         0.7200541 -0.405489   0.         0.         0.       ]\n",
      "Reward: -11.003246658281233\n",
      "Next State shape: [ -6.01097965   0.75000906   0.42206494   0.9065656    0.\n",
      "   2.92124724   0.68471074   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -7.58350945\n",
      "   1.89370394   6.42243195 -10.62292671]\n",
      "Done: True\n",
      "\n",
      "Experience 215:\n",
      "State shape: [ -6.01097965   0.75000906   0.42206494   0.9065656    0.\n",
      "   2.92124724   0.68471074   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -7.58350945\n",
      "   1.89370394   6.42243195 -10.62292671]\n",
      "Action shape: [ 0.         0.7200541 -0.405489   0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.01097965   0.75000906   0.42206494   0.9065656    0.\n",
      "   2.92124724   0.68471074   3.70297813   2.4300456    0.\n",
      "   1.           0.           0.           0.          -7.58350945\n",
      "   1.89370394   6.42243195 -10.62292671]\n",
      "Done: True\n",
      "\n",
      "Experience 216:\n",
      "State shape: [ -6.          -0.00877285   0.00233326   0.99999728   0.\n",
      "  -0.45234597   0.11666287   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           5.54241753\n",
      "   2.14956141 -15.8434267   -4.22105551]\n",
      "Action shape: [ 0.         -0.67698044  0.6577621   0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.          -0.00877285   0.00233326   0.99999728   0.\n",
      "  -0.45234597   0.11666287   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           5.54241753\n",
      "   2.14956141 -15.8434267   -4.22105551]\n",
      "Done: False\n",
      "\n",
      "Experience 217:\n",
      "State shape: [ -6.00000095  -0.00507593   0.00441054   0.99999027   0.\n",
      "   0.17264034   0.10386472   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           5.22586632\n",
      "   2.06522512 -15.82758331  -4.21683455]\n",
      "Action shape: [ 0.          0.92181414 -0.07215782  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00000095  -0.00507593   0.00441054   0.99999027   0.\n",
      "   0.17264034   0.10386472   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           5.22586632\n",
      "   2.06522512 -15.82758331  -4.21683455]\n",
      "Done: False\n",
      "\n",
      "Experience 218:\n",
      "State shape: [ -6.00000286  -0.00128937   0.00735797   0.99997293   0.\n",
      "   0.17202924   0.1473743    5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           4.90963078\n",
      "   1.98097277 -15.81175613  -4.21261787]\n",
      "Action shape: [0.         0.00425291 0.24531333 0.         0.         0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00000286  -0.00128937   0.00735797   0.99997293   0.\n",
      "   0.17202924   0.1473743    5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           4.90963078\n",
      "   1.98097277 -15.81175613  -4.21261787]\n",
      "Done: False\n",
      "\n",
      "Experience 219:\n",
      "State shape: [ -6.00000954   0.01296091   0.0127995    0.99991808   0.\n",
      "   0.68054157   0.27209041   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           4.59371185\n",
      "   1.89680433 -15.79594421  -4.20840549]\n",
      "Action shape: [0.        0.7661881 0.7031673 0.        0.        0.       ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00000954   0.01296091   0.0127995    0.99991808   0.\n",
      "   0.68054157   0.27209041   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           4.59371185\n",
      "   1.89680433 -15.79594421  -4.20840549]\n",
      "Done: False\n",
      "\n",
      "Experience 220:\n",
      "State shape: [ -6.00001955   0.03406048   0.01824435   0.99983356   0.\n",
      "   1.02297664   0.27227601   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           4.2781086\n",
      "   1.81272078 -15.78014851  -4.20419693]\n",
      "Action shape: [0.         0.53285766 0.00104657 0.         0.         0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00001955   0.03406048   0.01824435   0.99983356   0.\n",
      "   1.02297664   0.27227601   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           4.2781086\n",
      "   1.81272078 -15.78014851  -4.20419693]\n",
      "Done: False\n",
      "\n",
      "Experience 221:\n",
      "State shape: [ -6.00003529   0.0536027    0.02445688   0.99970089   0.\n",
      "   0.94062001   0.31069779   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           3.96282101\n",
      "   1.72872114 -15.76436901  -4.19999266]\n",
      "Action shape: [ 0.         -0.09263509  0.21662752  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00003529   0.0536027    0.02445688   0.99970089   0.\n",
      "   0.94062001   0.31069779   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           3.96282101\n",
      "   1.72872114 -15.76436901  -4.19999266]\n",
      "Done: False\n",
      "\n",
      "Experience 222:\n",
      "State shape: [ -6.00006676   0.06210184   0.03366985   0.99943301   0.\n",
      "   0.37084138   0.46084446   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           3.64784908\n",
      "   1.64480543 -15.74860477  -4.19579268]\n",
      "Action shape: [ 0.        -0.8245754  0.8465486  0.         0.         0.       ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00006676   0.06210184   0.03366985   0.99943301   0.\n",
      "   0.37084138   0.46084446   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           3.64784908\n",
      "   1.64480543 -15.74860477  -4.19579268]\n",
      "Done: False\n",
      "\n",
      "Experience 223:\n",
      "State shape: [ -6.00011301   0.06048632   0.04386393   0.99903751   0.\n",
      "  -0.14070176   0.51008993   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           3.33319187\n",
      "   1.56097364 -15.73285675  -4.19159698]\n",
      "Action shape: [ 0.         -0.7544748   0.27765322  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00011301   0.06048632   0.04386393   0.99903751   0.\n",
      "  -0.14070176   0.51008993   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           3.33319187\n",
      "   1.56097364 -15.73285675  -4.19159698]\n",
      "Done: False\n",
      "\n",
      "Experience 224:\n",
      "State shape: [ -6.0001688    0.07146645   0.05358624   0.99856323   0.\n",
      "   0.49191448   0.48669547   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           3.01884937\n",
      "   1.47722578 -15.71712399  -4.18740559]\n",
      "Action shape: [ 0.          0.94256127 -0.13190144  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.0001688    0.07146645   0.05358624   0.99856323   0.\n",
      "   0.49191448   0.48669547   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           3.01884937\n",
      "   1.47722578 -15.71712399  -4.18740559]\n",
      "Done: False\n",
      "\n",
      "Experience 225:\n",
      "State shape: [ -6.00023079   0.08499432   0.06266416   0.99803467   0.\n",
      "   0.62303579   0.45466626   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           2.70482159\n",
      "   1.39356184 -15.70140743  -4.183218  ]\n",
      "Action shape: [ 0.          0.21095988 -0.18058531  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00023079   0.08499432   0.06266416   0.99803467   0.\n",
      "   0.62303579   0.45466626   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           2.70482159\n",
      "   1.39356184 -15.70140743  -4.183218  ]\n",
      "Done: False\n",
      "\n",
      "Experience 226:\n",
      "State shape: [ -6.00031328   0.09466648   0.07298327   0.99733317   0.\n",
      "   0.42297325   0.51714903   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           2.39110756\n",
      "   1.30998135 -15.68570614  -4.17903471]\n",
      "Action shape: [ 0.         -0.28076464  0.35228696  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00031328   0.09466648   0.07298327   0.99733317   0.\n",
      "   0.42297325   0.51714903   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           2.39110756\n",
      "   1.30998135 -15.68570614  -4.17903471]\n",
      "Done: False\n",
      "\n",
      "Experience 227:\n",
      "State shape: [ -6.00041771   0.11686087   0.08425097   0.99644457   0.\n",
      "   1.04354024   0.56513709   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           2.07770729\n",
      "   1.2264843  -15.67002106  -4.17485571]\n",
      "Action shape: [0.         0.94140023 0.2705635  0.         0.         0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00041771   0.11686087   0.08425097   0.99644457   0.\n",
      "   1.04354024   0.56513709   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           2.07770729\n",
      "   1.2264843  -15.67002106  -4.17485571]\n",
      "Done: False\n",
      "\n",
      "Experience 228:\n",
      "State shape: [ -6.0005765    0.12931776   0.0989247    0.99509492   0.\n",
      "   0.53663039   0.7367903    5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           1.76461983\n",
      "   1.1430707  -15.65435123  -4.170681  ]\n",
      "Action shape: [ 0.        -0.7274054  0.9678058  0.         0.         0.       ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.0005765    0.12931776   0.0989247    0.99509492   0.\n",
      "   0.53663039   0.7367903    5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           1.76461983\n",
      "   1.1430707  -15.65435123  -4.170681  ]\n",
      "Done: False\n",
      "\n",
      "Experience 229:\n",
      "State shape: [ -6.00075531   0.15444088   0.11318832   0.99357355   0.\n",
      "   1.17235804   0.71723241   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           1.45184612\n",
      "   1.05974054 -15.63869667  -4.16651058]\n",
      "Action shape: [ 0.          0.9674916  -0.11027028  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00075531   0.15444088   0.11318832   0.99357355   0.\n",
      "   1.17235804   0.71723241   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           1.45184612\n",
      "   1.05974054 -15.63869667  -4.16651058]\n",
      "Done: False\n",
      "\n",
      "Experience 230:\n",
      "State shape: [ -6.00094223   0.16915035   0.12636504   0.99198381   0.\n",
      "   0.65802634   0.6636191    5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           1.13938522\n",
      "   0.97649384 -15.62305832  -4.16234398]\n",
      "Action shape: [ 0.         -0.7346572  -0.30227953  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00094223   0.16915035   0.12636504   0.99198381   0.\n",
      "   0.65802634   0.6636191    5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           1.13938522\n",
      "   0.97649384 -15.62305832  -4.16234398]\n",
      "Done: False\n",
      "\n",
      "Experience 231:\n",
      "State shape: [ -6.00117111   0.18212748   0.14084073   0.99003227   0.\n",
      "   0.56380862   0.73033863   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           0.82723618\n",
      "   0.8933301  -15.60743523  -4.15818167]\n",
      "Action shape: [ 0.        -0.1213102  0.3761744  0.         0.         0.       ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00117111   0.18212748   0.14084073   0.99003227   0.\n",
      "   0.56380862   0.73033863   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           0.82723618\n",
      "   0.8933301  -15.60743523  -4.15818167]\n",
      "Done: False\n",
      "\n",
      "Experience 232:\n",
      "State shape: [ -6.00142527   0.20522785   0.15527122   0.98787188   0.\n",
      "   1.07022786   0.72957218   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           0.51539993\n",
      "   0.81024981 -15.59182835  -4.15402365]\n",
      "Action shape: [ 0.          0.77478236 -0.00432125  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00142527   0.20522785   0.15527122   0.98787188   0.\n",
      "   1.07022786   0.72957218   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           0.51539993\n",
      "   0.81024981 -15.59182835  -4.15402365]\n",
      "Done: False\n",
      "\n",
      "Experience 233:\n",
      "State shape: [ -6.00168467   0.23394966   0.16872098   0.98566385   0.\n",
      "   1.35706985   0.68149459   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           0.20387554\n",
      "   0.72725248 -15.57623672  -4.14986944]\n",
      "Action shape: [ 0.          0.46132144 -0.27106854  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00168467   0.23394966   0.16872098   0.98566385   0.\n",
      "   1.35706985   0.68149459   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.           0.20387554\n",
      "   0.72725248 -15.57623672  -4.14986944]\n",
      "Done: False\n",
      "\n",
      "Experience 234:\n",
      "State shape: [ -6.00202036   0.26370478   0.18464229   0.98280579   0.\n",
      "   1.39420891   0.80879939   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -0.10733795\n",
      "   0.64433813 -15.56066036  -4.14571953]\n",
      "Action shape: [0.         0.09620208 0.7177627  0.         0.         0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00202036   0.26370478   0.18464229   0.98280579   0.\n",
      "   1.39420891   0.80879939   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -0.10733795\n",
      "   0.64433813 -15.56066036  -4.14571953]\n",
      "Done: False\n",
      "\n",
      "Experience 235:\n",
      "State shape: [ -6.00239849   0.29602242   0.20101046   0.97958909   0.\n",
      "   1.51969993   0.83407253   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -0.41823959\n",
      "   0.56150675 -15.54510021  -4.14157391]\n",
      "Action shape: [0.         0.229541   0.14249356 0.         0.         0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00239849   0.29602242   0.20101046   0.97958909   0.\n",
      "   1.51969993   0.83407253   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -0.41823959\n",
      "   0.56150675 -15.54510021  -4.14157391]\n",
      "Done: False\n",
      "\n",
      "Experience 236:\n",
      "State shape: [ -6.00272131   0.32744837   0.21396395   0.97684156   0.\n",
      "   1.49519956   0.66208851   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -0.72883034\n",
      "   0.47875834 -15.52955532  -4.13743258]\n",
      "Action shape: [ 0.          0.00882026 -0.96967053  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00272131   0.32744837   0.21396395   0.97684156   0.\n",
      "   1.49519956   0.66208851   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -0.72883034\n",
      "   0.47875834 -15.52955532  -4.13743258]\n",
      "Done: False\n",
      "\n",
      "Experience 237:\n",
      "State shape: [ -6.00298119   0.35259914   0.22382657   0.97462899   0.\n",
      "   1.19961452   0.50539029   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -1.03911114\n",
      "   0.39609241 -15.51402569  -4.13329506]\n",
      "Action shape: [ 0.         -0.39761797 -0.8834874   0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00298119   0.35259914   0.22382657   0.97462899   0.\n",
      "   1.19961452   0.50539029   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -1.03911114\n",
      "   0.39609241 -15.51402569  -4.13329506]\n",
      "Done: False\n",
      "\n",
      "Experience 238:\n",
      "State shape: [ -6.00333118   0.36525488   0.23640894   0.97165365   0.\n",
      "   0.55884415   0.64647245   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -1.34908104\n",
      "   0.31350899 -15.49851227  -4.12916183]\n",
      "Action shape: [ 0.         -0.9230693   0.79544157  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00333118   0.36525488   0.23640894   0.97165365   0.\n",
      "   0.55884415   0.64647245   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -1.34908104\n",
      "   0.31350899 -15.49851227  -4.12916183]\n",
      "Done: False\n",
      "\n",
      "Experience 239:\n",
      "State shape: [ -6.0037365    0.37685537   0.25017621   0.96820032   0.\n",
      "   0.49912271   0.70969403   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -1.658741\n",
      "   0.23100853 -15.48301411  -4.1250329 ]\n",
      "Action shape: [ 0.         -0.07265176  0.35645247  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.0037365    0.37685537   0.25017621   0.96820032   0.\n",
      "   0.49912271   0.70969403   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -1.658741\n",
      "   0.23100853 -15.48301411  -4.1250329 ]\n",
      "Done: False\n",
      "\n",
      "Experience 240:\n",
      "State shape: [ -6.00422049   0.37671995   0.2656002    0.96408326   0.\n",
      "  -0.09738334   0.79820967   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -1.96809196\n",
      "   0.14859056 -15.4675312   -4.12090778]\n",
      "Action shape: [ 0.        -0.8777905  0.499064   0.         0.         0.       ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00422049   0.37671995   0.2656002    0.96408326   0.\n",
      "  -0.09738334   0.79820967   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -1.96809196\n",
      "   0.14859056 -15.4675312   -4.12090778]\n",
      "Done: False\n",
      "\n",
      "Experience 241:\n",
      "State shape: [ -6.00480938   0.37224197   0.2831577    0.95907336   0.\n",
      "  -0.3270424    0.91292673   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -2.27713299\n",
      "   0.06625462 -15.45206356  -4.11678696]\n",
      "Action shape: [ 0.         -0.34662235  0.6467914   0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00480938   0.37224197   0.2831577    0.95907336   0.\n",
      "  -0.3270424    0.91292673   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -2.27713299\n",
      "   0.06625462 -15.45206356  -4.11678696]\n",
      "Done: False\n",
      "\n",
      "Experience 242:\n",
      "State shape: [ -6.00546741   0.36192989   0.3014879    0.95347      0.\n",
      "  -0.62332737   0.95839125   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -2.58586502\n",
      "  -0.01599884 -15.43661213  -4.11267042]\n",
      "Action shape: [ 0.         -0.45320868  0.25633553  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00546741   0.36192989   0.3014879    0.95347      0.\n",
      "  -0.62332737   0.95839125   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -2.58586502\n",
      "  -0.01599884 -15.43661213  -4.11267042]\n",
      "Done: False\n",
      "\n",
      "Experience 243:\n",
      "State shape: [ -6.00619984   0.34386063   0.32052612   0.94723968   0.\n",
      "  -1.01531374   1.00160325   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -2.89428854\n",
      "  -0.0981698  -15.42117596  -4.1085577 ]\n",
      "Action shape: [ 0.        -0.6053037  0.2436352  0.         0.         0.       ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00619984   0.34386063   0.32052612   0.94723968   0.\n",
      "  -1.01531374   1.00160325   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -2.89428854\n",
      "  -0.0981698  -15.42117596  -4.1085577 ]\n",
      "Done: False\n",
      "\n",
      "Experience 244:\n",
      "State shape: [ -6.00694609   0.33490992   0.33871565   0.94088879   0.\n",
      "  -0.55441928   0.96333259   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -3.20240355\n",
      "  -0.18025875 -15.40575504  -4.10444927]\n",
      "Action shape: [ 0.          0.6593837  -0.21577543  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00694609   0.33490992   0.33871565   0.94088879   0.\n",
      "  -0.55441928   0.96333259   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -3.20240355\n",
      "  -0.18025875 -15.40575504  -4.10444927]\n",
      "Done: False\n",
      "\n",
      "Experience 245:\n",
      "State shape: [ -6.00771284   0.3289609    0.35632348   0.93436266   0.\n",
      "  -0.40089247   0.93893045   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -3.51021051\n",
      "  -0.26226568 -15.39034939  -4.10034466]\n",
      "Action shape: [ 0.          0.21317317 -0.13758287  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00771284   0.3289609    0.35632348   0.93436266   0.\n",
      "  -0.40089247   0.93893045   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -3.51021051\n",
      "  -0.26226568 -15.39034939  -4.10034466]\n",
      "Done: False\n",
      "\n",
      "Experience 246:\n",
      "State shape: [ -6.00864697   0.31054354   0.37650577   0.92641427   0.\n",
      "  -1.03943872   1.08457363   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -3.81770992\n",
      "  -0.3441906  -15.37495899  -4.09624434]\n",
      "Action shape: [ 0.        -0.967647   0.8211572  0.         0.         0.       ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00864697   0.31054354   0.37650577   0.92641427   0.\n",
      "  -1.03943872   1.08457363   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -3.81770992\n",
      "  -0.3441906  -15.37495899  -4.09624434]\n",
      "Done: False\n",
      "\n",
      "Experience 247:\n",
      "State shape: [ -6.00974655   0.29562664   0.39875314   0.9170583    0.\n",
      "  -0.8765586    1.20676076   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -4.12490177\n",
      "  -0.4260335  -15.35958385  -4.0921483 ]\n",
      "Action shape: [0.         0.21265379 0.6889087  0.         0.         0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.00974655   0.29562664   0.39875314   0.9170583    0.\n",
      "  -0.8765586    1.20676076   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -4.12490177\n",
      "  -0.4260335  -15.35958385  -4.0921483 ]\n",
      "Done: False\n",
      "\n",
      "Experience 248:\n",
      "State shape: [ -6.01106358   0.29394102   0.42360577   0.90584665   0.\n",
      "  -0.23028722   1.36326838   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -4.43178606\n",
      "  -0.50779486 -15.34422493  -4.08805609]\n",
      "Action shape: [0.         0.94097173 0.8824122  0.         0.         0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.01106358   0.29394102   0.42360577   0.90584665   0.\n",
      "  -0.23028722   1.36326838   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -4.43178606\n",
      "  -0.50779486 -15.34422493  -4.08805609]\n",
      "Done: False\n",
      "\n",
      "Experience 249:\n",
      "State shape: [ -6.01257992   0.30315733   0.4501625    0.89294665   0.\n",
      "   0.30476734   1.47625637   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -4.73836374\n",
      "  -0.5894742  -15.32888126  -4.08396816]\n",
      "Action shape: [0.        0.793869  0.6370429 0.        0.        0.       ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.01257992   0.30315733   0.4501625    0.89294665   0.\n",
      "   0.30476734   1.47625637   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -4.73836374\n",
      "  -0.5894742  -15.32888126  -4.08396816]\n",
      "Done: False\n",
      "\n",
      "Experience 250:\n",
      "State shape: [ -6.01424885   0.31458235   0.47729444   0.87874343   0.\n",
      "   0.41183978   1.53129494   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -5.04463482\n",
      "  -0.67107201 -15.31355286  -4.07988405]\n",
      "Action shape: [0.         0.16936676 0.31031522 0.         0.         0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.01424885   0.31458235   0.47729444   0.87874343   0.\n",
      "   0.41183978   1.53129494   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -5.04463482\n",
      "  -0.67107201 -15.31355286  -4.07988405]\n",
      "Done: False\n",
      "\n",
      "Experience 251:\n",
      "State shape: [ -6.01594543   0.33622169   0.50296903   0.86430443   0.\n",
      "   0.93111312   1.47286594   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -5.35059977\n",
      "  -0.75258827 -15.29823971  -4.07580423]\n",
      "Action shape: [ 0.          0.78947103 -0.32943112  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.01594543   0.33622169   0.50296903   0.86430443   0.\n",
      "   0.93111312   1.47286594   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -5.35059977\n",
      "  -0.75258827 -15.29823971  -4.07580423]\n",
      "Done: False\n",
      "\n",
      "Experience 252:\n",
      "State shape: [ -6.01793432   0.36364841   0.53098969   0.84737828   0.\n",
      "   1.20671403   1.63687789   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -5.65625858\n",
      "  -0.834023   -15.28294182  -4.07172871]\n",
      "Action shape: [0.         0.440334   0.92472285 0.         0.         0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.01793432   0.36364841   0.53098969   0.84737828   0.\n",
      "   1.20671403   1.63687789   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -5.65625858\n",
      "  -0.834023   -15.28294182  -4.07172871]\n",
      "Done: False\n",
      "\n",
      "Experience 253:\n",
      "State shape: [ -6.02018738   0.39506531   0.56042116   0.82820778   0.\n",
      "   1.39792955   1.75630713   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -5.96161175\n",
      "  -0.91537619 -15.26765919  -4.06765699]\n",
      "Action shape: [0.         0.32229227 0.6733591  0.         0.         0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.02018738   0.39506531   0.56042116   0.82820778   0.\n",
      "   1.39792955   1.75630713   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -5.96161175\n",
      "  -0.91537619 -15.26765919  -4.06765699]\n",
      "Done: False\n",
      "\n",
      "Experience 254:\n",
      "State shape: [ -6.02251244   0.42610216   0.58860954   0.80841747   0.\n",
      "   1.38620174   1.72217929   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -6.26665974\n",
      "  -0.99664783 -15.25239182  -4.06358957]\n",
      "Action shape: [ 0.          0.02429085 -0.19241749  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.02251244   0.42610216   0.58860954   0.80841747   0.\n",
      "   1.38620174   1.72217929   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -6.26665974\n",
      "  -0.99664783 -15.25239182  -4.06358957]\n",
      "Done: False\n",
      "\n",
      "Experience 255:\n",
      "State shape: [ -6.02499104   0.45930529   0.61653449   0.7873279    0.\n",
      "   1.49609315   1.74978495   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -6.57140255\n",
      "  -1.07783842 -15.2371397   -4.05952597]\n",
      "Action shape: [0.         0.20595504 0.1556446  0.         0.         0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.02499104   0.45930529   0.61653449   0.7873279    0.\n",
      "   1.49609315   1.74978495   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -6.57140255\n",
      "  -1.07783842 -15.2371397   -4.05952597]\n",
      "Done: False\n",
      "\n",
      "Experience 256:\n",
      "State shape: [ -6.02751923   0.4968195    0.64306713   0.76580981   0.\n",
      "   1.71980536   1.70816028   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -6.87584019\n",
      "  -1.15894794 -15.22190285  -4.05546665]\n",
      "Action shape: [ 0.          0.3795884  -0.23468606  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.02751923   0.4968195    0.64306713   0.76580981   0.\n",
      "   1.71980536   1.70816028   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -6.87584019\n",
      "  -1.15894794 -15.22190285  -4.05546665]\n",
      "Done: False\n",
      "\n",
      "Experience 257:\n",
      "State shape: [ -6.0299201    0.53742838   0.66664088   0.74537906   0.\n",
      "   1.89194489   1.55982113   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -7.17997408\n",
      "  -1.23997641 -15.20668125  -4.05141115]\n",
      "Action shape: [ 0.          0.3091008  -0.83635765  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.0299201    0.53742838   0.66664088   0.74537906   0.\n",
      "   1.89194489   1.55982113   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -7.17997408\n",
      "  -1.23997641 -15.20668125  -4.05141115]\n",
      "Done: False\n",
      "\n",
      "Experience 258:\n",
      "State shape: [ -6.03247643   0.56728745   0.69019415   0.72362424   0.\n",
      "   1.35454381   1.60321295   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -7.48380375\n",
      "  -1.32092381 -15.19147491  -4.04735994]\n",
      "Action shape: [ 0.         -0.7476443   0.24464935  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.03247643   0.56728745   0.69019415   0.72362424   0.\n",
      "   1.35454381   1.60321295   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -7.48380375\n",
      "  -1.32092381 -15.19147491  -4.04735994]\n",
      "Done: False\n",
      "\n",
      "Experience 259:\n",
      "State shape: [ -6.0352993    0.59393358   0.71453547   0.69959921   0.\n",
      "   1.18930888   1.71012962   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -7.7873292\n",
      "  -1.40179014 -15.17628384  -4.04331255]\n",
      "Action shape: [ 0.         -0.20674616  0.60281175  0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.0352993    0.59393358   0.71453547   0.69959921   0.\n",
      "   1.18930888   1.71012962   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -7.7873292\n",
      "  -1.40179014 -15.17628384  -4.04331255]\n",
      "Done: False\n",
      "\n",
      "Experience 260:\n",
      "State shape: [ -6.03819561   0.62088537   0.73785835   0.6749556    0.\n",
      "   1.21055174   1.69659495   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -8.09055138\n",
      "  -1.48257542 -15.16110802  -4.03926945]\n",
      "Action shape: [ 0.          0.06739037 -0.0763104   0.          0.          0.        ]\n",
      "Reward: -11.007393767634076\n",
      "Next State shape: [ -6.03819561   0.62088537   0.73785835   0.6749556    0.\n",
      "   1.21055174   1.69659495   5.00529766  -0.84258461   0.\n",
      "   1.           0.           0.           0.          -8.09055138\n",
      "  -1.48257542 -15.16110802  -4.03926945]\n",
      "Done: True\n",
      "\n",
      "Experience 261:\n",
      "State shape: [ -6.           0.00200891  -0.0008978    0.9999996    0.\n",
      "   0.10571028  -0.04489015   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           3.48105907\n",
      "  -0.53401566 -13.74214745  -0.96949613]\n",
      "Action shape: [ 0.          0.15820588 -0.25309715  0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.           0.00200891  -0.0008978    0.9999996    0.\n",
      "   0.10571028  -0.04489015   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           3.48105907\n",
      "  -0.53401566 -13.74214745  -0.96949613]\n",
      "Done: False\n",
      "\n",
      "Experience 262:\n",
      "State shape: [ -6.           0.01541185  -0.00186871   0.99999825   0.\n",
      "   0.67585421  -0.04854537   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           3.20649147\n",
      "  -0.55338621 -13.728405    -0.96852666]\n",
      "Action shape: [ 0.          0.85644084 -0.02060866  0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.           0.01541185  -0.00186871   0.99999825   0.\n",
      "   0.67585421  -0.04854537   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           3.20649147\n",
      "  -0.55338621 -13.728405    -0.96852666]\n",
      "Done: False\n",
      "\n",
      "Experience 263:\n",
      "State shape: [ -6.           0.03872776   0.00000177   1.           0.\n",
      "   1.15483034   0.09352405   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           2.93219757\n",
      "  -0.57273722 -13.71467686  -0.96755815]\n",
      "Action shape: [0.        0.7370647 0.8010079 0.        0.        0.       ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.           0.03872776   0.00000177   1.           0.\n",
      "   1.15483034   0.09352405   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           2.93219757\n",
      "  -0.57273722 -13.71467686  -0.96755815]\n",
      "Done: False\n",
      "\n",
      "Experience 264:\n",
      "State shape: [ -6.           0.05796146   0.00116327   0.99999932   0.\n",
      "   0.95484281   0.0580751    7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           2.65817833\n",
      "  -0.59206915 -13.70096207  -0.96659058]\n",
      "Action shape: [ 0.         -0.26473483 -0.1998663   0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.           0.05796146   0.00116327   0.99999932   0.\n",
      "   0.95484281   0.0580751    7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           2.65817833\n",
      "  -0.59206915 -13.70096207  -0.96659058]\n",
      "Done: False\n",
      "\n",
      "Experience 265:\n",
      "State shape: [ -6.00000191   0.09012508   0.00542429   0.99998529   0.\n",
      "   1.58314323   0.21305229   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           2.38443279\n",
      "  -0.61138153 -13.68726158  -0.96562403]\n",
      "Action shape: [0.         0.96889406 0.8737838  0.         0.         0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00000191   0.09012508   0.00542429   0.99998529   0.\n",
      "   1.58314323   0.21305229   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           2.38443279\n",
      "  -0.61138153 -13.68726158  -0.96562403]\n",
      "Done: False\n",
      "\n",
      "Experience 266:\n",
      "State shape: [ -6.00000572   0.12705421   0.01014866   0.9999485    0.\n",
      "   1.81870782   0.23622577   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           2.11096096\n",
      "  -0.63067484 -13.67357445  -0.96465844]\n",
      "Action shape: [0.         0.39993218 0.13065545 0.         0.         0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00000572   0.12705421   0.01014866   0.9999485    0.\n",
      "   1.81870782   0.23622577   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           2.11096096\n",
      "  -0.63067484 -13.67357445  -0.96465844]\n",
      "Done: False\n",
      "\n",
      "Experience 267:\n",
      "State shape: [ -6.00001717   0.16994715   0.01720287   0.99985202   0.\n",
      "   2.10320497   0.3527444    7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           1.83776283\n",
      "  -0.6499486  -13.65990067  -0.9636938 ]\n",
      "Action shape: [0.         0.48021555 0.65694875 0.         0.         0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00001717   0.16994715   0.01720287   0.99985202   0.\n",
      "   2.10320497   0.3527444    7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           1.83776283\n",
      "  -0.6499486  -13.65990067  -0.9636938 ]\n",
      "Done: False\n",
      "\n",
      "Experience 268:\n",
      "State shape: [ -6.00003052   0.20930624   0.02269148   0.99974252   0.\n",
      "   1.93569362   0.27448517   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           1.56483841\n",
      "  -0.66920328 -13.64624119  -0.96273011]\n",
      "Action shape: [ 0.         -0.18774429 -0.44123676  0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00003052   0.20930624   0.02269148   0.99974252   0.\n",
      "   1.93569362   0.27448517   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           1.56483841\n",
      "  -0.66920328 -13.64624119  -0.96273011]\n",
      "Done: False\n",
      "\n",
      "Experience 269:\n",
      "State shape: [ -6.00004387   0.23494196   0.02738046   0.99962508   0.\n",
      "   1.25425982   0.23452289   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           1.29218674\n",
      "  -0.68843842 -13.63259506  -0.96176738]\n",
      "Action shape: [ 0.         -0.96189404 -0.2253131   0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00004387   0.23494196   0.02738046   0.99962508   0.\n",
      "   1.25425982   0.23452289   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           1.29218674\n",
      "  -0.68843842 -13.63259506  -0.96176738]\n",
      "Done: False\n",
      "\n",
      "Experience 270:\n",
      "State shape: [ -6.00006771   0.26152182   0.03392348   0.99942443   0.\n",
      "   1.29054511   0.3273052    7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           1.01980782\n",
      "  -0.70765448 -13.61896229  -0.96080559]\n",
      "Action shape: [0.         0.09184697 0.5231199  0.         0.         0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00006771   0.26152182   0.03392348   0.99942443   0.\n",
      "   1.29054511   0.3273052    7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           1.01980782\n",
      "  -0.70765448 -13.61896229  -0.96080559]\n",
      "Done: False\n",
      "\n",
      "Experience 271:\n",
      "State shape: [ -6.00010014   0.29164505   0.04120092   0.99915088   0.\n",
      "   1.4634099    0.36413017   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           0.74770069\n",
      "  -0.72685146 -13.60534382  -0.95984483]\n",
      "Action shape: [0.         0.2973378  0.20762444 0.         0.         0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00010014   0.29164505   0.04120092   0.99915088   0.\n",
      "   1.4634099    0.36413017   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           0.74770069\n",
      "  -0.72685146 -13.60534382  -0.95984483]\n",
      "Done: False\n",
      "\n",
      "Experience 272:\n",
      "State shape: [ -6.00012445   0.32504082   0.04602554   0.99894026   0.\n",
      "   1.6414448    0.24146076   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           0.47586632\n",
      "  -0.74602938 -13.5917387   -0.95888501]\n",
      "Action shape: [ 0.         0.3102496 -0.6916279  0.         0.         0.       ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00012445   0.32504082   0.04602554   0.99894026   0.\n",
      "   1.6414448    0.24146076   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           0.47586632\n",
      "  -0.74602938 -13.5917387   -0.95888501]\n",
      "Done: False\n",
      "\n",
      "Experience 273:\n",
      "State shape: [ -6.00016975   0.34526253   0.05377968   0.99855283   0.\n",
      "   0.96552283   0.38819158   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           0.20430374\n",
      "  -0.76518774 -13.57814693  -0.95792615]\n",
      "Action shape: [ 0.         -0.96245235  0.82728964  0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00016975   0.34526253   0.05377968   0.99855283   0.\n",
      "   0.96552283   0.38819158   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.           0.20430374\n",
      "  -0.76518774 -13.57814693  -0.95792615]\n",
      "Done: False\n",
      "\n",
      "Experience 274:\n",
      "State shape: [ -6.00020981   0.35885954   0.05975317   0.99821318   0.\n",
      "   0.64476067   0.29915738   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -0.06698799\n",
      "  -0.78432703 -13.56456852  -0.95696825]\n",
      "Action shape: [ 0.         -0.45115227 -0.5019876   0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00020981   0.35885954   0.05975317   0.99821318   0.\n",
      "   0.64476067   0.29915738   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -0.06698799\n",
      "  -0.78432703 -13.56456852  -0.95696825]\n",
      "Done: False\n",
      "\n",
      "Experience 275:\n",
      "State shape: [ -6.00024176   0.36040449   0.06410644   0.99794307   0.\n",
      "   0.05167326   0.21808258   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -0.33800793\n",
      "  -0.80344725 -13.55100441  -0.9560113 ]\n",
      "Action shape: [ 0.        -0.868315  -0.4571114  0.         0.         0.       ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00024176   0.36040449   0.06410644   0.99794307   0.\n",
      "   0.05167326   0.21808258   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -0.33800793\n",
      "  -0.80344725 -13.55100441  -0.9560113 ]\n",
      "Done: False\n",
      "\n",
      "Experience 276:\n",
      "State shape: [ -6.00027704   0.37190247   0.06864105   0.99764142   0.\n",
      "   0.54827821   0.22723196   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -0.60875702\n",
      "  -0.82254839 -13.53745365  -0.9550553 ]\n",
      "Action shape: [0.        0.744765  0.0515856 0.        0.        0.       ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00027704   0.37190247   0.06864105   0.99764142   0.\n",
      "   0.54827821   0.22723196   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -0.60875702\n",
      "  -0.82254839 -13.53745365  -0.9550553 ]\n",
      "Done: False\n",
      "\n",
      "Experience 277:\n",
      "State shape: [ -6.00034237   0.3709774    0.07626461   0.99708761   0.\n",
      "  -0.0910546    0.38218349   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -0.87923527\n",
      "  -0.84163046 -13.52391624  -0.95410025]\n",
      "Action shape: [ 0.         -0.94041365  0.87363917  0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00034237   0.3709774    0.07626461   0.99708761   0.\n",
      "  -0.0910546    0.38218349   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -0.87923527\n",
      "  -0.84163046 -13.52391624  -0.95410025]\n",
      "Done: False\n",
      "\n",
      "Experience 278:\n",
      "State shape: [ -6.0004406    0.35758591   0.08654048   0.99624834   0.\n",
      "  -0.72995269   0.51550639   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -1.14944267\n",
      "  -0.86069345 -13.51039219  -0.95314616]\n",
      "Action shape: [ 0.         -0.95889956  0.7516939   0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.0004406    0.35758591   0.08654048   0.99624834   0.\n",
      "  -0.72995269   0.51550639   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -1.14944267\n",
      "  -0.86069345 -13.51039219  -0.95314616]\n",
      "Done: False\n",
      "\n",
      "Experience 279:\n",
      "State shape: [ -6.00057793   0.35365391   0.09906905   0.99508056   0.\n",
      "  -0.27022383   0.62914795   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -1.41938019\n",
      "  -0.87973738 -13.49688244  -0.95219302]\n",
      "Action shape: [0.         0.66618073 0.64072746 0.         0.         0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00057793   0.35365391   0.09906905   0.99508056   0.\n",
      "  -0.27022383   0.62914795   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -1.41938019\n",
      "  -0.87973738 -13.49688244  -0.95219302]\n",
      "Done: False\n",
      "\n",
      "Experience 280:\n",
      "State shape: [ -6.00074291   0.35687065   0.11227911   0.99367671   0.\n",
      "   0.08323193   0.66422725   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -1.68904781\n",
      "  -0.89876223 -13.48338604  -0.95124084]\n",
      "Action shape: [0.         0.5208931  0.19778231 0.         0.         0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00074291   0.35687065   0.11227911   0.99367671   0.\n",
      "   0.08323193   0.66422725   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -1.68904781\n",
      "  -0.89876223 -13.48338604  -0.95124084]\n",
      "Done: False\n",
      "\n",
      "Experience 281:\n",
      "State shape: [ -6.00088215   0.35628748   0.12229941   0.99249325   0.\n",
      "  -0.08803872   0.50449914   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -1.95844555\n",
      "  -0.917768   -13.46990299  -0.95028961]\n",
      "Action shape: [ 0.         -0.25383216 -0.9005701   0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00088215   0.35628748   0.12229941   0.99249325   0.\n",
      "  -0.08803872   0.50449914   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -1.95844555\n",
      "  -0.917768   -13.46990299  -0.95028961]\n",
      "Done: False\n",
      "\n",
      "Experience 282:\n",
      "State shape: [ -6.00099182   0.34893847   0.12965574   0.99155907   0.\n",
      "  -0.41064119   0.37077141   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -2.22757435\n",
      "  -0.9367547  -13.4564333   -0.94933933]\n",
      "Action shape: [ 0.         -0.4854416  -0.75397635  0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00099182   0.34893847   0.12965574   0.99155907   0.\n",
      "  -0.41064119   0.37077141   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -2.22757435\n",
      "  -0.9367547  -13.4564333   -0.94933933]\n",
      "Done: False\n",
      "\n",
      "Experience 283:\n",
      "State shape: [ -6.00115299   0.33364391   0.13975916   0.99018553   0.\n",
      "  -0.82409024   0.50981998   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -2.49643373\n",
      "  -0.95572233 -13.44297695  -0.94839001]\n",
      "Action shape: [ 0.         -0.6310587   0.78397584  0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00115299   0.33364391   0.13975916   0.99018553   0.\n",
      "  -0.82409024   0.50981998   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -2.49643373\n",
      "  -0.95572233 -13.44297695  -0.94839001]\n",
      "Done: False\n",
      "\n",
      "Experience 284:\n",
      "State shape: [ -6.00137901   0.32572556   0.15275426   0.9882642    0.\n",
      "  -0.47229353   0.65682292   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -2.76502419\n",
      "  -0.97467136 -13.42953396  -0.94744164]\n",
      "Action shape: [0.        0.5018319 0.8288237 0.        0.        0.       ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00137901   0.32572556   0.15275426   0.9882642    0.\n",
      "  -0.47229353   0.65682292   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -2.76502419\n",
      "  -0.97467136 -13.42953396  -0.94744164]\n",
      "Done: False\n",
      "\n",
      "Experience 285:\n",
      "State shape: [ -6.00169182   0.31019688   0.1690801    0.98560231   0.\n",
      "  -0.87235516   0.82708073   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -3.03334618\n",
      "  -0.99360132 -13.41610432  -0.94649422]\n",
      "Action shape: [ 0.        -0.6128684  0.9599383  0.         0.         0.       ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00169182   0.31019688   0.1690801    0.98560231   0.\n",
      "  -0.87235516   0.82708073   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -3.03334618\n",
      "  -0.99360132 -13.41610432  -0.94649422]\n",
      "Done: False\n",
      "\n",
      "Experience 286:\n",
      "State shape: [ -6.00199699   0.29898214   0.18356755   0.9830071    0.\n",
      "  -0.64584678   0.73590952   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -3.30139971\n",
      "  -1.01251221 -13.40268803  -0.94554776]\n",
      "Action shape: [ 0.         0.3128808 -0.5140363  0.         0.         0.       ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00199699   0.29898214   0.18356755   0.9830071    0.\n",
      "  -0.64584678   0.73590952   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -3.30139971\n",
      "  -1.01251221 -13.40268803  -0.94554776]\n",
      "Done: False\n",
      "\n",
      "Experience 287:\n",
      "State shape: [ -6.00238132   0.28098011   0.20029107   0.97973644   0.\n",
      "  -0.99835277   0.85202718   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -3.56918526\n",
      "  -1.03140402 -13.38928509  -0.94460225]\n",
      "Action shape: [ 0.         -0.54689145  0.6546883   0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00238132   0.28098011   0.20029107   0.97973644   0.\n",
      "  -0.99835277   0.85202718   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -3.56918526\n",
      "  -1.03140402 -13.38928509  -0.94460225]\n",
      "Done: False\n",
      "\n",
      "Experience 288:\n",
      "State shape: [ -6.00275326   0.27622747   0.21520013   0.97656997   0.\n",
      "  -0.3252199    0.76208758   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -3.8367033\n",
      "  -1.05027723 -13.3758955   -0.94365764]\n",
      "Action shape: [ 0.          0.97752714 -0.5070926   0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00275326   0.27622747   0.21520013   0.97656997   0.\n",
      "  -0.3252199    0.76208758   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -3.8367033\n",
      "  -1.05027723 -13.3758955   -0.94365764]\n",
      "Done: False\n",
      "\n",
      "Experience 289:\n",
      "State shape: [ -6.00308323   0.27141905   0.22757569   0.97376039   0.\n",
      "  -0.31313759   0.63452768   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -4.10395384\n",
      "  -1.06913137 -13.36252022  -0.94271398]\n",
      "Action shape: [ 0.          0.00834793 -0.7192011   0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00308323   0.27141905   0.22757569   0.97376039   0.\n",
      "  -0.31313759   0.63452768   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -4.10395384\n",
      "  -1.06913137 -13.36252022  -0.94271398]\n",
      "Done: False\n",
      "\n",
      "Experience 290:\n",
      "State shape: [ -6.00334263   0.26362801   0.2368238    0.97155262   0.\n",
      "  -0.44388953   0.47540116   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -4.37093687\n",
      "  -1.08796692 -13.34915829  -0.94177127]\n",
      "Action shape: [ 0.         -0.20505598 -0.8971784   0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00334263   0.26362801   0.2368238    0.97155262   0.\n",
      "  -0.44388953   0.47540116   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -4.37093687\n",
      "  -1.08796692 -13.34915829  -0.94177127]\n",
      "Done: False\n",
      "\n",
      "Experience 291:\n",
      "State shape: [ -6.00356531   0.24713564   0.24446459   0.96965822   0.\n",
      "  -0.86951435   0.39360747   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -4.63765287\n",
      "  -1.10678339 -13.33580971  -0.94082952]\n",
      "Action shape: [ 0.         -0.65027606 -0.46116465  0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00356531   0.24713564   0.24446459   0.96965822   0.\n",
      "  -0.86951435   0.39360747   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -4.63765287\n",
      "  -1.10678339 -13.33580971  -0.94082952]\n",
      "Done: False\n",
      "\n",
      "Experience 292:\n",
      "State shape: [ -6.00373363   0.238873     0.2500684    0.96822817   0.\n",
      "  -0.44605985   0.28916997   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -4.90410233\n",
      "  -1.12558126 -13.32247448  -0.93988872]\n",
      "Action shape: [ 0.          0.60771525 -0.58883363  0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00373363   0.238873     0.2500684    0.96822817   0.\n",
      "  -0.44605985   0.28916997   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -4.90410233\n",
      "  -1.12558126 -13.32247448  -0.93988872]\n",
      "Done: False\n",
      "\n",
      "Experience 293:\n",
      "State shape: [ -6.00399685   0.22010517   0.25858688   0.96598801   0.\n",
      "  -0.98844308   0.4404071    7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -5.17028522\n",
      "  -1.14436007 -13.3091526   -0.93894881]\n",
      "Action shape: [ 0.        -0.8250816  0.8526967  0.         0.         0.       ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00399685   0.22010517   0.25858688   0.96598801   0.\n",
      "  -0.98844308   0.4404071    7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -5.17028522\n",
      "  -1.14436007 -13.3091526   -0.93894881]\n",
      "Done: False\n",
      "\n",
      "Experience 294:\n",
      "State shape: [ -6.00418997   0.19786501   0.26465516   0.96434312   0.\n",
      "  -1.14766645   0.31436345   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -5.43620205\n",
      "  -1.16312027 -13.29584408  -0.93800986]\n",
      "Action shape: [ 0.         -0.26787966 -0.71065235  0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00418997   0.19786501   0.26465516   0.96434312   0.\n",
      "  -1.14766645   0.31436345   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -5.43620205\n",
      "  -1.16312027 -13.29584408  -0.93800986]\n",
      "Done: False\n",
      "\n",
      "Experience 295:\n",
      "State shape: [ -6.00428343   0.18439627   0.26753272   0.96354878   0.\n",
      "  -0.69034278   0.14925881   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -5.7018528\n",
      "  -1.18186188 -13.28254795  -0.93707186]\n",
      "Action shape: [ 0.          0.65007806 -0.93088377  0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00428343   0.18439627   0.26753272   0.96354878   0.\n",
      "  -0.69034278   0.14925881   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -5.7018528\n",
      "  -1.18186188 -13.28254795  -0.93707186]\n",
      "Done: False\n",
      "\n",
      "Experience 296:\n",
      "State shape: [ -6.00428629   0.17678881   0.26762007   0.96352452   0.\n",
      "  -0.38086358   0.00453277   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -5.96723795\n",
      "  -1.20058441 -13.26926517  -0.93613482]\n",
      "Action shape: [ 0.          0.4425029  -0.81598634  0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00428629   0.17678881   0.26762007   0.96352452   0.\n",
      "  -0.38086358   0.00453277   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -5.96723795\n",
      "  -1.20058441 -13.26926517  -0.93613482]\n",
      "Done: False\n",
      "\n",
      "Experience 297:\n",
      "State shape: [ -6.00419712   0.15927219   0.2648847    0.96428009   0.\n",
      "  -0.85975713  -0.14189078   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -6.23235798\n",
      "  -1.21928835 -13.25599575  -0.93519866]\n",
      "Action shape: [ 0.        -0.7281115 -0.8255571  0.         0.         0.       ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00419712   0.15927219   0.2648847    0.96428009   0.\n",
      "  -0.85975713  -0.14189078   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -6.23235798\n",
      "  -1.21928835 -13.25599575  -0.93519866]\n",
      "Done: False\n",
      "\n",
      "Experience 298:\n",
      "State shape: [ -6.00414371   0.13047504   0.2632082    0.96473906   0.\n",
      "  -1.43000674  -0.08690879   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -6.49721289\n",
      "  -1.23797369 -13.24273968  -0.93426347]\n",
      "Action shape: [ 0.         -0.87916905  0.30999643  0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00414371   0.13047504   0.2632082    0.96473906   0.\n",
      "  -1.43000674  -0.08690879   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -6.49721289\n",
      "  -1.23797369 -13.24273968  -0.93426347]\n",
      "Done: False\n",
      "\n",
      "Experience 299:\n",
      "State shape: [ -6.00402355   0.09931469   0.25944877   0.96575687   0.\n",
      "  -1.53594387  -0.19473812   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -6.76180267\n",
      "  -1.25664043 -13.22949696  -0.93332922]\n",
      "Action shape: [ 0.         -0.20134823 -0.60795736  0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00402355   0.09931469   0.25944877   0.96575687   0.\n",
      "  -1.53594387  -0.19473812   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -6.76180267\n",
      "  -1.25664043 -13.22949696  -0.93332922]\n",
      "Done: False\n",
      "\n",
      "Experience 300:\n",
      "State shape: [ -6.00382233   0.06365728   0.25297111   0.96747383   0.\n",
      "  -1.74479294  -0.33506811   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -7.02612829\n",
      "  -1.27528858 -13.21626759  -0.93239594]\n",
      "Action shape: [ 0.         -0.35853705 -0.7912007   0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00382233   0.06365728   0.25297111   0.96747383   0.\n",
      "  -1.74479294  -0.33506811   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -7.02612829\n",
      "  -1.27528858 -13.21626759  -0.93239594]\n",
      "Done: False\n",
      "\n",
      "Experience 301:\n",
      "State shape: [ -6.00365448   0.02455664   0.24744306   0.96890244   0.\n",
      "  -1.9225527   -0.28548428   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -7.29018927\n",
      "  -1.29391766 -13.20305157  -0.93146354]\n",
      "Action shape: [ 0.        -0.31826    0.2795608  0.         0.         0.       ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00365448   0.02455664   0.24744306   0.96890244   0.\n",
      "  -1.9225527   -0.28548428   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -7.29018927\n",
      "  -1.29391766 -13.20305157  -0.93146354]\n",
      "Done: False\n",
      "\n",
      "Experience 302:\n",
      "State shape: [ -6.00341702  -0.01509428   0.23940448   0.97091992   0.\n",
      "  -1.93532133  -0.41439497   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -7.55398655\n",
      "  -1.31252813 -13.1898489   -0.9305321 ]\n",
      "Action shape: [ 0.         -0.07665524 -0.7268171   0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00341702  -0.01509428   0.23940448   0.97091992   0.\n",
      "  -1.93532133  -0.41439497   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -7.55398655\n",
      "  -1.31252813 -13.1898489   -0.9305321 ]\n",
      "Done: False\n",
      "\n",
      "Experience 303:\n",
      "State shape: [ -6.00326252  -0.06339741   0.23399028   0.97223894   0.\n",
      "  -2.38336229  -0.27862811   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -7.81751919\n",
      "  -1.33112001 -13.17665958  -0.92960155]\n",
      "Action shape: [ 0.         -0.72846556  0.76547325  0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00326252  -0.06339741   0.23399028   0.97223894   0.\n",
      "  -2.38336229  -0.27862811   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -7.81751919\n",
      "  -1.33112001 -13.17665958  -0.92960155]\n",
      "Done: False\n",
      "\n",
      "Experience 304:\n",
      "State shape: [ -6.00308704  -0.10718393   0.22770407   0.97373038   0.\n",
      "  -2.15239787  -0.32303628   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -8.08078957\n",
      "  -1.3496933  -13.16348267  -0.92867196]\n",
      "Action shape: [ 0.          0.2743225  -0.25037974  0.          0.          0.        ]\n",
      "Reward: -11.006158115011363\n",
      "Next State shape: [ -6.00308704  -0.10718393   0.22770407   0.97373038   0.\n",
      "  -2.15239787  -0.32303628   7.18187332   2.5691905    0.\n",
      "   1.           0.           0.           0.          -8.08078957\n",
      "  -1.3496933  -13.16348267  -0.92867196]\n",
      "Done: True\n",
      "\n",
      "Experience 305:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.0006036527156829835\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 306:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 307:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 308:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 309:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 310:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 311:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 312:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 313:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 314:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 315:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 316:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 317:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 318:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 319:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 320:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 321:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 322:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 323:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 324:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 325:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 326:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 327:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 328:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 329:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 330:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 331:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 332:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 333:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 334:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 335:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 336:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 337:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 338:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 339:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 340:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 341:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 342:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 343:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 344:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 345:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 346:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 347:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 348:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 349:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 350:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 351:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 352:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 353:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 354:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 355:\n",
      "State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Action shape: [0.         0.5361547  0.41110724 0.         0.         0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.00733614   0.00145831   0.99999894   0.\n",
      "   0.3582488    0.07291535  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.56263828\n",
      "  -1.59861565 -10.76720524   3.47134161]\n",
      "Done: False\n",
      "\n",
      "Experience 356:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: 0.0006036527156829835\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 357:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 358:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 359:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 360:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 361:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 362:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 363:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 364:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 365:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 366:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 367:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 368:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 369:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 370:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 371:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 372:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 373:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 374:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 375:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 376:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 377:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 378:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 379:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 380:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 381:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 382:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 383:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 384:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 385:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 386:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 387:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 388:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 389:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 390:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 391:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 392:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 393:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 394:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 395:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 396:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 397:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 398:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 399:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 400:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 401:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 402:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 403:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 404:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 405:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 406:\n",
      "State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Action shape: [ 0.          0.41130343 -0.5844873   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.           0.01978254   0.00084328   0.99999964   0.\n",
      "   0.62590933  -0.03075127  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.34750938\n",
      "  -1.52925825 -10.75643826   3.46787024]\n",
      "Done: False\n",
      "\n",
      "Experience 407:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.0006036527156829835\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 408:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 409:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 410:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 411:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 412:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 413:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 414:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 415:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 416:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 417:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 418:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 419:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 420:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 421:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 422:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 423:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 424:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 425:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 426:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 427:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 428:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 429:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 430:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 431:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 432:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 433:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 434:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 435:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 436:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 437:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 438:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 439:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 440:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 441:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 442:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 443:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 444:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 445:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 446:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 447:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 448:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 449:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 450:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 451:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 452:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 453:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 454:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 455:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 456:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 457:\n",
      "State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Action shape: [ 0.        -0.6381937  0.6351984  0.         0.         0.       ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000048   0.02371454   0.00248147   0.99999692   0.\n",
      "   0.18696173   0.08190963  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.           0.13259602\n",
      "  -1.45997    -10.74568176   3.46440244]\n",
      "Done: False\n",
      "\n",
      "Experience 458:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.0006036527156829835\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 459:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 460:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 461:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 462:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 463:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 464:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 465:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 466:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 467:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 468:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 469:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 470:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 471:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 472:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 473:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 474:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 475:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 476:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 477:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 478:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 479:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 480:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 481:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 482:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 483:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 484:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 485:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 486:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 487:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 488:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 489:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 490:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 491:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 492:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 493:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: 0.6\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 494:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 495:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 496:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 497:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 498:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 499:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 500:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 501:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 502:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 503:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 504:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 505:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 506:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 507:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 508:\n",
      "State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Action shape: [ 0.         -0.8257102   0.97840315  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00000334   0.01694489   0.00759025   0.99997119   0.\n",
      "  -0.36850202   0.25544247  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.08210278\n",
      "  -1.39075136 -10.73493576   3.46093798]\n",
      "Done: False\n",
      "\n",
      "Experience 509:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: 0.0006036527156829835\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 510:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 511:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 512:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 513:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 514:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 515:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 516:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 517:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 518:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 519:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 520:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 521:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 522:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 523:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 524:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 525:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 526:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 527:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 528:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 529:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 530:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 531:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 532:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 533:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 534:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 535:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 536:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 537:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 538:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 539:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 540:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 541:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 542:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 543:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 544:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 545:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 546:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 547:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 548:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 549:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 550:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 551:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 552:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 553:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 554:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 555:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 556:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 557:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 558:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 559:\n",
      "State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Action shape: [0.         0.26561674 0.56951743 0.         0.         0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.0000124    0.01410913   0.01471887   0.99989167   0.\n",
      "  -0.18365173   0.35645396  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.29658699\n",
      "  -1.32160187 -10.7242012    3.45747709]\n",
      "Done: False\n",
      "\n",
      "Experience 560:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: 0.0006036527156829835\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 561:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 562:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 563:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 564:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 565:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 566:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 567:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 568:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 569:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 570:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 571:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 572:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 573:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 574:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 575:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 576:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 577:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 578:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 579:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 580:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 581:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 582:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 583:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 584:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 585:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 586:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 587:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 588:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 589:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 590:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 591:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 592:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 593:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 594:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 595:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 596:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 597:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 598:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 599:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 600:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 601:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 602:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 603:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 604:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 605:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 606:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 607:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 608:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 609:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 610:\n",
      "State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Action shape: [ 0.         -0.60915726 -0.08451247  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00002718   0.00317144   0.02154702   0.99976784   0.\n",
      "  -0.58700651   0.34146455  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.51085663\n",
      "  -1.25252151 -10.71347713   3.45401955]\n",
      "Done: False\n",
      "\n",
      "Experience 611:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: 0.0006036527156829835\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 612:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 613:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 614:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 615:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 616:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 617:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 618:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 619:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 620:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 621:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 622:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 623:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 624:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 625:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 626:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 627:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 628:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 629:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 630:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 631:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 632:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 633:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 634:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 635:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 636:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 637:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 638:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 639:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 640:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 641:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 642:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 643:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 644:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 645:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 646:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 647:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 648:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 649:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 650:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 651:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 652:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 653:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 654:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 655:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 656:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 657:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 658:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 659:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 660:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 661:\n",
      "State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Action shape: [ 0.          0.4557009  -0.85096455  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00003815  -0.00179625   0.02535667   0.99967847   0.\n",
      "  -0.27077532   0.19053467  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.72491169\n",
      "  -1.1835103  -10.70276356   3.45056558]\n",
      "Done: False\n",
      "\n",
      "Experience 662:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: 0.0006036527156829835\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 663:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 664:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 665:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 666:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 667:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 668:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 669:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 670:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 671:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 672:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 673:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 674:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 675:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 676:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 677:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 678:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 679:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 680:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 681:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 682:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 683:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 684:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 685:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 686:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 687:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 688:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 689:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 690:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 691:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 692:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 693:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 694:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 695:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 696:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 697:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 698:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 699:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 700:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 701:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 702:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 703:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 704:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 705:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 706:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 707:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 708:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 709:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 710:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 711:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 712:\n",
      "State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Action shape: [ 0.         -0.08145806  0.6364664   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00005817  -0.00747919   0.03142262   0.99950619   0.\n",
      "  -0.3197886    0.30342045  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -0.93875313\n",
      "  -1.11456823 -10.69206047   3.44711494]\n",
      "Done: False\n",
      "\n",
      "Experience 713:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: 0.0006036527156829835\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 714:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 715:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 716:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 717:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 718:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 719:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 720:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 721:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 722:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 723:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 724:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 725:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 726:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 727:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 728:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 729:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 730:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 731:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 732:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 733:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 734:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 735:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 736:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 737:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 738:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 739:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 740:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 741:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 742:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 743:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 744:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 745:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 746:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 747:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 748:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 749:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 750:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 751:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 752:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 753:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 754:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 755:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 756:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 757:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 758:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 759:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 760:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 761:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 762:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 763:\n",
      "State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Action shape: [ 0.          0.2769341  -0.31073546  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00007772  -0.00946283   0.03638591   0.99933781   0.\n",
      "  -0.12835053   0.24830738  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.15238094\n",
      "  -1.04569483 -10.68136883   3.44366789]\n",
      "Done: False\n",
      "\n",
      "Experience 764:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: 0.0006036527156829835\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 765:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 766:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 767:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 768:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 769:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 770:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 771:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 772:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 773:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 774:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 775:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 776:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 777:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 778:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 779:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 780:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 781:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 782:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 783:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 784:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 785:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 786:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 787:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 788:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 789:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 790:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 791:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 792:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 793:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 794:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 795:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 796:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 797:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 798:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 799:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 800:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 801:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 802:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 803:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 804:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 805:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 806:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 807:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 808:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 809:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 810:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 811:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 812:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 813:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 814:\n",
      "State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Action shape: [ 0.          0.11211174 -0.1135993   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00009823  -0.00994492   0.04094567   0.99916137   0.\n",
      "  -0.05087249   0.22815903  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.36579514\n",
      "  -0.97689056 -10.67068768   3.44022417]\n",
      "Done: False\n",
      "\n",
      "Experience 815:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: 0.0006036527156829835\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 816:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 817:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 818:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 819:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 820:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 821:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 822:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 823:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 824:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 825:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 826:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 827:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 828:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 829:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 830:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 831:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 832:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 833:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 834:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 835:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 836:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 837:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 838:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 839:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 840:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 841:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 842:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 843:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 844:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 845:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 846:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 847:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 848:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 849:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 850:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 851:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 852:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 853:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 854:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 855:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 856:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 857:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 858:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 859:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 860:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 861:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 862:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 863:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 864:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 865:\n",
      "State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Action shape: [ 0.         0.7430282 -0.5804866  0.         0.         0.       ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011063  -0.00071812   0.04344748   0.99905571   0.\n",
      "   0.44662285   0.125202    -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.5789957\n",
      "  -0.90815496 -10.66001701   3.43678403]\n",
      "Done: False\n",
      "\n",
      "Experience 866:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: 0.0006036527156829835\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 867:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 868:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 869:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 870:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 871:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 872:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 873:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 874:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 875:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 876:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 877:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 878:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 879:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 880:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 881:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 882:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 883:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 884:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 885:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 886:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 887:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 888:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 889:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 890:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 891:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 892:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 893:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 894:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 895:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 896:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 897:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 898:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 899:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 900:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 901:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 902:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 903:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 904:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 905:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 906:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 907:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 908:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 909:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 910:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 911:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 912:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 913:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 914:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 915:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 916:\n",
      "State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Action shape: [ 0.         -0.313319   -0.22521025  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00012016   0.00404882   0.04515096   0.99898018   0.\n",
      "   0.22833636   0.08525796  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -1.79198265\n",
      "  -0.83948803 -10.64935684   3.43334723]\n",
      "Done: False\n",
      "\n",
      "Experience 917:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: 0.0006036527156829835\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 918:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 919:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 920:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 921:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 922:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 923:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 924:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 925:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 926:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 927:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 928:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 929:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 930:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 931:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 932:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 933:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 934:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 935:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 936:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 937:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 938:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 939:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 940:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 941:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 942:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 943:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 944:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 945:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 946:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 947:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 948:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 949:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 950:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 951:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 952:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 953:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 954:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 955:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 956:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 957:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 958:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 959:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 960:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 961:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 962:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 963:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 964:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 965:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 966:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 967:\n",
      "State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Action shape: [ 0.          0.36421403 -0.7370697   0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011539   0.01328468   0.04424245   0.99902082   0.\n",
      "   0.46713081  -0.04547116  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.00475693\n",
      "  -0.77088976 -10.63870716   3.429914  ]\n",
      "Done: False\n",
      "\n",
      "Experience 968:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: 0.0006036527156829835\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 969:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 970:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 971:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 972:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 973:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 974:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 975:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 976:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 977:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 978:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 979:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 980:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 981:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 982:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 983:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 984:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 985:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 986:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 987:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 988:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 989:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 990:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 991:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 992:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 993:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 994:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 995:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 996:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 997:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 998:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 999:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n",
      "Experience 1000:\n",
      "State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Action shape: [ 0.         -0.02535766  0.22211915  0.          0.          0.        ]\n",
      "Reward: -0.1\n",
      "Next State shape: [ -6.00011444   0.0220871    0.04412106   0.99902619   0.\n",
      "   0.44084468  -0.00607537  -0.66218567   0.29718256   0.\n",
      "   1.           0.           0.           0.          -2.21731853\n",
      "  -0.70236015 -10.62806892   3.42648411]\n",
      "Done: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_her_buffer(buffer, num_entries=1000):\n",
    "    \"\"\"Prints the first num_entries experiences from the HER buffer.\"\"\"\n",
    "    for idx, experience in enumerate(buffer.memory):\n",
    "        if idx >= num_entries:\n",
    "            break\n",
    "        print(f\"Experience {idx + 1}:\")\n",
    "        obs, action, reward, next_obs, done = experience\n",
    "        print(f\"State shape: {np.array(obs)}\")\n",
    "        print(f\"Action shape: {np.array(action)}\")\n",
    "        print(f\"Reward: {reward}\")\n",
    "        print(f\"Next State shape: {np.array(next_obs)}\")\n",
    "        print(f\"Done: {done}\\n\")\n",
    "\n",
    "# Call the function\n",
    "print_her_buffer(her_buffer)\n",
    "\n",
    "\n",
    "\n",
    "def extract_data_from_buffer(buffer, batch_size):\n",
    "    states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "    \n",
    "    sampled_experiences = buffer.sample(batch_size)\n",
    "    \n",
    "    for idx, experience in enumerate(sampled_experiences):\n",
    "        if idx >= batch_size:\n",
    "            break\n",
    "        \n",
    "        print(f\"Experience {idx + 1} Length: {len(experience)}\")\n",
    "        print(experience)\n",
    "        \n",
    "        if len(experience) != 5:\n",
    "            print(f\"Error: Expected length of experience is 5, but got {len(experience)}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            state, action, reward, next_state, done = experience\n",
    "        except ValueError as e:\n",
    "            print(f\"Unexpected error during unpacking: {e}\")\n",
    "            continue\n",
    "        \n",
    "        states.append(np.array(state))\n",
    "        actions.append(np.array(action))\n",
    "        rewards.append(reward)\n",
    "        next_states.append(np.array(next_state))\n",
    "        dones.append(done)\n",
    "    \n",
    "    states_np = np.array(states)\n",
    "    actions_np = np.array(actions)\n",
    "    rewards_np = np.array(rewards)\n",
    "    next_states_np = np.array(next_states)\n",
    "    dones_np = np.array(dones)\n",
    "    \n",
    "    return states_np, actions_np, rewards_np, next_states_np, dones_np\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348362c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Model:\n",
      "Number of Layers: 5\n",
      "Number of Input Neurons: 18\n",
      "Number of Hidden Layers: 2\n",
      "Number of Hidden Neurons in each layer: [256, 6]\n",
      "Number of Output Neurons: 6\n",
      "\n",
      "Value Model:\n",
      "Number of Layers: 5\n",
      "Number of Input Neurons: 24\n",
      "Number of Hidden Layers: 2\n",
      "Number of Hidden Neurons in each layer: [256, 1]\n",
      "Number of Output Neurons: 1\n"
     ]
    }
   ],
   "source": [
    "# Print the number of layers and neurons in the policy model\n",
    "policy_model_layers = list(agent.online_policy_model.children())\n",
    "policy_input_neurons = state_dim\n",
    "policy_hidden_neurons = [layer.out_features for layer in policy_model_layers if isinstance(layer, torch.nn.Linear)]\n",
    "policy_output_neurons = action_dim\n",
    "\n",
    "print(\"Policy Model:\")\n",
    "print(f\"Number of Layers: {len(policy_model_layers)}\")\n",
    "print(f\"Number of Input Neurons: {policy_input_neurons}\")\n",
    "print(f\"Number of Hidden Layers: {len(policy_hidden_neurons)}\")\n",
    "print(f\"Number of Hidden Neurons in each layer: {policy_hidden_neurons}\")\n",
    "print(f\"Number of Output Neurons: {policy_output_neurons}\")\n",
    "\n",
    "# Print the number of layers and neurons in the value model\n",
    "value_model_layers = list(agent.online_value_model.children())\n",
    "value_input_neurons = state_dim + action_dim\n",
    "value_hidden_neurons = [layer.out_features for layer in value_model_layers if isinstance(layer, torch.nn.Linear)]\n",
    "value_output_neurons = 1\n",
    "\n",
    "print(\"\\nValue Model:\")\n",
    "print(f\"Number of Layers: {len(value_model_layers)}\")\n",
    "print(f\"Number of Input Neurons: {value_input_neurons}\")\n",
    "print(f\"Number of Hidden Layers: {len(value_hidden_neurons)}\")\n",
    "print(f\"Number of Hidden Neurons in each layer: {value_hidden_neurons}\")\n",
    "print(f\"Number of Output Neurons: {value_output_neurons}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c879544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 scored\n",
      "Episode 1, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 1, Reward: 0.6496168649744513, Moving Avg Reward: 0.6496168649744513, HER Buffer Size: 4000\n",
      "Current Epsilon: 0.995\n",
      "Player 2 scored\n",
      "Episode 2, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 2, Reward: -11.217263197228286, Moving Avg Reward: -5.2838231661269175, HER Buffer Size: 4045\n",
      "Current Epsilon: 0.995\n",
      "Player 1 scored\n",
      "Episode 3, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 3, Reward: 11.020645200824736, Moving Avg Reward: 0.15099962285696714, HER Buffer Size: 4529\n",
      "Current Epsilon: 0.990025\n",
      "Player 2 scored\n",
      "Episode 4, Avg Value Loss: 0.0893888481524448, Avg Policy Loss: -0.15893647273959116\n",
      "Episode 4, Reward: -11.272325482621142, Moving Avg Reward: -2.70483165351256, HER Buffer Size: 4572\n",
      "Current Epsilon: 0.985074875\n",
      "Player 1 scored\n",
      "Episode 5, Avg Value Loss: 0.014409171733112163, Avg Policy Loss: -0.18131441102452475\n",
      "Episode 5, Reward: 11.825610327606398, Moving Avg Reward: 0.2012567427112316, HER Buffer Size: 7419\n",
      "Current Epsilon: 0.9801495006250001\n",
      "Player 1 scored\n",
      "Episode 6, Avg Value Loss: 0.013913645173306576, Avg Policy Loss: -0.15734304049983622\n",
      "Episode 6, Reward: 0.3812187117278629, Moving Avg Reward: 0.23125040421400347, HER Buffer Size: 11259\n",
      "Current Epsilon: 0.9752487531218751\n",
      "Player 1 scored\n",
      "Episode 7, Avg Value Loss: 0.011303374357521535, Avg Policy Loss: -0.14081836687400937\n",
      "Episode 7, Reward: 0.5809385050275837, Moving Avg Reward: 0.28120584718737207, HER Buffer Size: 15259\n",
      "Current Epsilon: 0.9703725093562657\n",
      "Player 2 scored\n",
      "Episode 8, Avg Value Loss: 0.007241536816582084, Avg Policy Loss: -0.14765796968713402\n",
      "Episode 8, Reward: 0.8196452931414072, Moving Avg Reward: 0.34851077793162655, HER Buffer Size: 18779\n",
      "Current Epsilon: 0.9655206468094844\n",
      "Player 2 scored\n",
      "Episode 9, Avg Value Loss: 0.0056748761216156365, Avg Policy Loss: -0.1267831176519394\n",
      "Episode 9, Reward: -11.27730241487365, Moving Avg Reward: -0.943246243491182, HER Buffer Size: 18821\n",
      "Current Epsilon: 0.960693043575437\n",
      "Player 2 scored\n",
      "Episode 10, Avg Value Loss: 0.005912312613746021, Avg Policy Loss: -0.12810989382655122\n",
      "Episode 10, Reward: -11.263087518983237, Moving Avg Reward: -1.9752303710403876, HER Buffer Size: 18864\n",
      "Current Epsilon: 0.9558895783575597\n"
     ]
    }
   ],
   "source": [
    "# Latest Initialize DDPG Agent and Environment\n",
    "import random\n",
    "np.set_printoptions(suppress=True)\n",
    "reload(lh)\n",
    "\n",
    "# Set the environment mode to TRAIN DEFENSE\n",
    "env = lh.LaserHockeyEnv()\n",
    "env.reset(mode=lh.LaserHockeyEnv.TRAIN_DEFENSE)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_bounds = (env.action_space.low, env.action_space.high)\n",
    "epsilon_strategy = EpsilonGreedyStrategy(1.0, 0.01, 0.995, env.action_space)\n",
    "\n",
    "def sample_goals(episode):\n",
    "    goals = []\n",
    "    for experience in episode:\n",
    "        _, _, reward, next_state, done, info = experience\n",
    "        # This assumes that you have added 'info' to your stored experiences\n",
    "        if info['reward_closeness_to_puck'] > 0 or info['reward_touch_puck'] > 0 or info['reward_puck_direction'] > 0:\n",
    "            goals.append(next_state)\n",
    "    return goals\n",
    "\n",
    "\n",
    "\n",
    "def compute_reward(state, action, goal):\n",
    "    _, _, _, _, info = env.step(action)\n",
    "    \n",
    "    closeness_reward = info.get('reward_closeness_to_puck', 0)\n",
    "    touch_reward = info.get('reward_touch_puck', 0)\n",
    "    direction_reward = info.get('reward_puck_direction', 0)\n",
    "    game_result = info.get('winner', 0)\n",
    "\n",
    "         \n",
    "    \n",
    "    # Combine rewards, possibly with weights to prioritize certain behaviors\n",
    "    total_reward = 3*closeness_reward + 10*touch_reward + 5*direction_reward\n",
    "\n",
    "    if game_result >= 0:\n",
    "        total_reward += 0.5 \n",
    "    \n",
    "    # If all conditions are met, give a positive reward\n",
    "    if closeness_reward > 0 and touch_reward > 0 and direction_reward > 0:\n",
    "        return 15\n",
    "    \n",
    "    # If any condition is not met, penalize the agent\n",
    "    if closeness_reward <= 0:\n",
    "        return -0.1\n",
    "    if touch_reward <= 0:\n",
    "        return -0.3\n",
    "    if direction_reward <= 0:\n",
    "        return -0.55\n",
    "    \n",
    "    return total_reward  # Return combined reward as a fallback\n",
    "\n",
    "\n",
    "her_buffer = HERBuffer(buffer_size=100000, goal_selection_strategy=sample_goals, reward_function=compute_reward)\n",
    "\n",
    "agent = DDPGAgent(state_dim=state_dim, action_dim=action_dim, action_bounds=action_bounds)\n",
    "agent.he_initialization()\n",
    "\n",
    "num_episodes = 10000\n",
    "batch_size = 512\n",
    "train_start = 200  \n",
    "update_frequency = 1\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "wins = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    obs = env.reset()\n",
    "\n",
    "    obs_agent1 = env._get_obs()\n",
    "    episode_reward = 0\n",
    "    episode_value_losses = []\n",
    "    episode_policy_losses = []\n",
    "\n",
    "    episode_trajectory = []\n",
    "\n",
    "    for step in range(80):\n",
    "        \n",
    "        if episode >= 200:\n",
    "            obs = env.render()\n",
    "\n",
    "        state_tensor = torch.FloatTensor(obs_agent1).unsqueeze(0)\n",
    "        agent.online_policy_model.eval()\n",
    "        action_player1 = epsilon_strategy.select_action(agent.online_policy_model, state_tensor, episode)\n",
    "        \n",
    "        agent.online_policy_model.train()\n",
    "\n",
    "        next_obs, reward, done, _, info = env.step(action_player1)\n",
    "        reward += info['winner']\n",
    "        reward += info['reward_closeness_to_puck']\n",
    "        reward += info['reward_touch_puck']\n",
    "        reward += info['reward_puck_direction']\n",
    "\n",
    "        #print(f\"info: {info}\")\n",
    "        episode_reward += reward\n",
    "\n",
    "        obs_agent1 = env._get_obs()\n",
    "        experience = (obs_agent1, action_player1, reward, next_obs, done, info)  # Added 'info' at the end\n",
    "        episode_trajectory.append(experience)\n",
    "\n",
    "\n",
    "        # DDPG Training\n",
    "        if episode > train_start and len(her_buffer) > batch_size:\n",
    "            experiences = her_buffer.sample(batch_size)\n",
    "            states, actions, rewards, next_states, is_terminals = [torch.FloatTensor(x_np).to(device) for x_np in experiences]\n",
    "            \n",
    "            value_loss, policy_loss = agent.optimize_model((states, actions, rewards, next_states, is_terminals))\n",
    "            episode_value_losses.append(value_loss)\n",
    "            episode_policy_losses.append(policy_loss)\n",
    "\n",
    "        if step == 79: \n",
    "            done = True\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Store trajectories experiences with HER logic, after episode is done\n",
    "    her_goals = sample_goals(episode_trajectory)\n",
    "    for experience in episode_trajectory:\n",
    "        state, action, reward, next_state, done, info = experience\n",
    "        # Store the original experience\n",
    "        her_buffer.store(state, action, reward, next_state, done)\n",
    "\n",
    "        for goal in her_goals:\n",
    "            # Recompute the reward and store the adjusted experience\n",
    "            her_reward = compute_reward(state, action, goal)\n",
    "            her_buffer.store(state, action, her_reward, next_state, done)\n",
    "\n",
    "    epsilon_strategy.decay_epsilon()\n",
    "    all_rewards.append(episode_reward)\n",
    "    moving_avg_reward = np.mean(all_rewards[-100:])\n",
    "    avg_value_loss = sum(episode_value_losses) / len(episode_value_losses) if episode_value_losses else 0\n",
    "    avg_policy_loss = sum(episode_policy_losses) / len(episode_policy_losses) if episode_policy_losses else 0\n",
    "\n",
    "    print(f\"Episode {episode + 1}, Avg Value Loss: {avg_value_loss}, Avg Policy Loss: {avg_policy_loss}\")\n",
    "    print(f\"Episode {episode + 1}, Reward: {episode_reward}, Moving Avg Reward: {moving_avg_reward}, HER Buffer Size: {len(her_buffer)}\")\n",
    "    print(f\"Current Epsilon: {epsilon_strategy.epsilon}\")\n",
    "\n",
    "    # Note: I left out the code regarding \"loss_rate\" because it wasn't completely provided. \n",
    "\n",
    "# Save the models\n",
    "torch.save(agent.online_policy_model.state_dict(), \"online_policy_model_defense.pt\")\n",
    "torch.save(agent.online_value_model.state_dict(), \"online_value_model_defense.pt\")\n",
    "torch.save(agent.target_policy_model.state_dict(), \"target_policy_model_defense.pt\")\n",
    "torch.save(agent.target_value_model.state_dict(), \"target_value_model_defense.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e63d5f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexej\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-d36108bd2232>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_steps_per_game\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m# Nutzen Sie den Agenten, um eine Aktion für player1 basierend auf dem aktuellen Zustand auszuwählen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alexej\\Desktop\\laser-hockey-env\\laserhockey\\hockey_env.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    687\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msurf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m       \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 689\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"render_fps\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    690\u001b[0m       \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"rgb_array\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import laserhockey.hockey_env as h_env\n",
    "import gymnasium as gym\n",
    "from importlib import reload\n",
    "import time\n",
    "\n",
    "env = h_env.HockeyEnv()\n",
    "\n",
    "player1 = h_env.BasicOpponent(weak=False)\n",
    "\n",
    "env.reset(mode=lh.LaserHockeyEnv)\n",
    "\n",
    "# Laden Sie die Gewichte in Ihren Agenten\n",
    "agent.online_policy_model.load_state_dict(torch.load(\"online_policy_model_defense.pt\"))\n",
    "agent.online_value_model.load_state_dict(torch.load(\"online_value_model_defense.pt\"))\n",
    "agent.target_policy_model.load_state_dict(torch.load(\"target_policy_model_defense.pt\"))\n",
    "agent.target_value_model.load_state_dict(torch.load(\"target_value_model_defense.pt\"))\n",
    "\n",
    "# Initialisieren Sie die Umgebung und den Basis-Opponenten\n",
    "player2 = lh.BasicOpponent()\n",
    "\n",
    "num_games = 10\n",
    "max_steps_per_game = 100\n",
    "\n",
    "for game in range(num_games):\n",
    "    obs, info = env.reset()\n",
    "    obs_agent2 = env.obs_agent_two()\n",
    "    \n",
    "    for step in range(max_steps_per_game):\n",
    "        env.render()\n",
    "\n",
    "        # Nutzen Sie den Agenten, um eine Aktion für player1 basierend auf dem aktuellen Zustand auszuwählen\n",
    "        state_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
    "        agent.online_policy_model.eval()\n",
    "        with torch.no_grad():\n",
    "            action_agent1 = np.squeeze(agent.online_policy_model(state_tensor).numpy())\n",
    "        agent.online_policy_model.train()\n",
    "\n",
    "        action_agent2 = player2.act(obs_agent2)\n",
    "        obs, r, d, _, info = env.step(np.hstack([action_agent1, action_agent2]))\n",
    "\n",
    "        obs_agent2 = env.obs_agent_two()\n",
    "\n",
    "        if d:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "83faa776",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h_env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-dd937f329403>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHockeyEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Einstellen der Spieler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplayer1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBasicOpponent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplayer2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHumanOpponent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplayer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'h_env' is not defined"
     ]
    }
   ],
   "source": [
    "env = h_env.HockeyEnv()\n",
    "\n",
    "# Einstellen der Spieler\n",
    "player1 = h_env.BasicOpponent()\n",
    "player2 = h_env.HumanOpponent(env=env, player=2)\n",
    "\n",
    "for game in range(10):\n",
    "    obs, info = env.reset()\n",
    "    obs_agent2 = env.obs_agent_two()\n",
    "\n",
    "    while True:\n",
    "        time.sleep(0.05)\n",
    "        env.render()\n",
    "\n",
    "        a1 = player1.act(obs)\n",
    "        a2 = player2.act(obs_agent2)\n",
    "        \n",
    "        obs, r, d, _, info = env.step(np.hstack([a1, a2]))\n",
    "        obs_agent2 = env.obs_agent_two()\n",
    "        \n",
    "        if d:  # Wenn das Spiel vorbei ist, brechen Sie die innere Schleife ab und starten Sie ein neues Spiel\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc151f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexej\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 2 scored\n",
      "Episode 1, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 1, Reward: -11.34528234995238, Moving Avg Reward: -11.34528234995238, HER Buffer Size: 44\n",
      "Player 2 scored\n",
      "Episode 2, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 2, Reward: -11.314224926309848, Moving Avg Reward: -11.329753638131113, HER Buffer Size: 89\n",
      "Player 2 scored\n",
      "Episode 3, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 3, Reward: -11.34327856418234, Moving Avg Reward: -11.334261946814856, HER Buffer Size: 134\n",
      "Player 2 scored\n",
      "Episode 4, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 4, Reward: -11.433561458543588, Moving Avg Reward: -11.359086824747038, HER Buffer Size: 177\n",
      "Episode 5, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 5, Reward: 0.24532738213854308, Moving Avg Reward: -9.038203983369922, HER Buffer Size: 337\n",
      "Player 1 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Episode 6, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 6, Reward: 11.02567151389122, Moving Avg Reward: -5.694224733826399, HER Buffer Size: 6035\n",
      "Player 1 scored\n",
      "Episode 7, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 7, Reward: 11.933985989865628, Moving Avg Reward: -3.17590891615611, HER Buffer Size: 7572\n",
      "Player 2 scored\n",
      "Episode 8, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 8, Reward: -11.292048153250752, Moving Avg Reward: -4.190426320792939, HER Buffer Size: 7614\n",
      "Player 2 scored\n",
      "Episode 9, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 9, Reward: -11.248027387264838, Moving Avg Reward: -4.974604217067595, HER Buffer Size: 7659\n",
      "Player 2 scored\n",
      "Episode 10, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 10, Reward: -11.357991032655223, Moving Avg Reward: -5.612942898626358, HER Buffer Size: 7702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexej\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 2 scored\n",
      "Episode 11, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 11, Reward: -11.264412706439584, Moving Avg Reward: -6.126712881154833, HER Buffer Size: 7746\n",
      "Episode 12, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 12, Reward: -2.4852056086371235, Moving Avg Reward: -5.823253941778357, HER Buffer Size: 7826\n",
      "Player 1 scored\n",
      "Episode 13, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 13, Reward: -0.9401759301082587, Moving Avg Reward: -5.447632556265272, HER Buffer Size: 11026\n",
      "Player 1 scored\n",
      "Episode 14, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 14, Reward: 11.028643763542176, Moving Avg Reward: -4.270755676279025, HER Buffer Size: 11926\n",
      "Player 2 scored\n",
      "Episode 15, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 15, Reward: -11.385371196566403, Moving Avg Reward: -4.745063377631518, HER Buffer Size: 11969\n",
      "Player 2 scored\n",
      "Episode 16, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 16, Reward: -11.369541840181174, Moving Avg Reward: -5.159093281540872, HER Buffer Size: 12010\n",
      "Player 2 scored\n",
      "Episode 17, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 17, Reward: -11.31852816777033, Moving Avg Reward: -5.521412980730839, HER Buffer Size: 12054\n",
      "Player 2 scored\n",
      "Episode 18, Avg Value Loss: 0, Avg Policy Loss: 0\n",
      "Episode 18, Reward: -11.38999413749459, Moving Avg Reward: -5.847445267217714, HER Buffer Size: 12099\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-438d5c5ba490>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m             \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[0mstate_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs_agent1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alexej\\Desktop\\laser-hockey-env\\laserhockey\\laser_hockey_env.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    611\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msurf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m             \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 613\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"render_fps\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    614\u001b[0m             \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"rgb_array\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Normal Noise Decay Strategy Latest Initialize DDPG Agent and Environment\n",
    "import numpy as n\n",
    "import random\n",
    "import torch\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "reload(lh)\n",
    "\n",
    "# Set the environment mode to TRAIN DEFENSE\n",
    "env = lh.LaserHockeyEnv()\n",
    "env.reset(mode=lh.LaserHockeyEnv.TRAIN_DEFENSE)\n",
    "initial_state_player2 = env._get_obs()[7:14]  # Capture the initial state of Player 2 after resetting the environment\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_bounds = (env.action_space.low, env.action_space.high)\n",
    "noise_strategy = NormalNoiseStrategy(action_bounds[0], action_bounds[1])\n",
    "\n",
    "\n",
    "def sample_goals(episode):\n",
    "    goals = []\n",
    "    for experience in episode:\n",
    "        _, _, reward, next_state, done, info = experience\n",
    "        # This assumes that you have added 'info' to your stored experiences\n",
    "        if info['reward_closeness_to_puck'] > 0 or info['reward_touch_puck'] > 0 or info['reward_puck_direction'] > 0:\n",
    "            goals.append(next_state)\n",
    "    return goals\n",
    "\n",
    "\n",
    "\n",
    "def compute_reward(state, action, goal):\n",
    "    _, _, _, _, info = env.step(action)\n",
    "    \n",
    "    closeness_reward = info.get('reward_closeness_to_puck', 0)\n",
    "    touch_reward = info.get('reward_touch_puck', 0)\n",
    "    direction_reward = info.get('reward_puck_direction', 0)\n",
    "    game_result = info.get('winner', 0)\n",
    "    Y_max = action_bounds[1][1]\n",
    "    Y_min = action_bounds[0][1]\n",
    "\n",
    "    upper_threshold = Y_max - (1/3) * (Y_max - Y_min)\n",
    "    lower_threshold = Y_min + (1/3) * (Y_max - Y_min)\n",
    "\n",
    "         \n",
    "    total_reward = 0  # Initialization of total_reward\n",
    "    # Combine rewards, possibly with weights to prioritize certain behaviors\n",
    "    \n",
    "\n",
    "    if game_result >= 0:\n",
    "        total_reward += 0.5\n",
    "\n",
    "    else:\n",
    "        total_reward -= 20\n",
    "\n",
    "    \n",
    "    if touch_reward == 1:\n",
    "        total_reward += 100\n",
    "\n",
    "    if closeness_reward >= 0:\n",
    "        total_reward += 20\n",
    "\n",
    "    if direction_reward >= 0:\n",
    "        total_reward += 20\n",
    "\n",
    "    # Add penalty for states that are in the top or bottom third of the field\n",
    "    y_position_player1 = state[1]  # y-position of the defending player\n",
    "\n",
    "    if y_position_player1 > upper_threshold or y_position_player1 < lower_threshold:\n",
    "        total_reward -= 100  # Deduct a penalty from the total reward\n",
    "\n",
    "    \n",
    "    return total_reward  # Return combined reward as a fallback\n",
    "\n",
    "\n",
    "her_buffer = HERBuffer(buffer_size=150000, goal_selection_strategy=sample_goals, reward_function=compute_reward)\n",
    "\n",
    "agent = DDPGAgent(state_dim=state_dim, action_dim=action_dim, action_bounds=action_bounds)\n",
    "agent.he_initialization()\n",
    "\n",
    "num_episodes = 10000\n",
    "batch_size = 1024\n",
    "train_start = 100  \n",
    "update_frequency = 1\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "wins = []\n",
    "\n",
    "\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    obs = env.reset()\n",
    "\n",
    "    obs_agent1 = env._get_obs()\n",
    "    episode_reward = 0\n",
    "    episode_value_losses = []\n",
    "    episode_policy_losses = []\n",
    "\n",
    "    episode_trajectory = []\n",
    "\n",
    "    for step in range(80):\n",
    "        \n",
    "        if episode >= 10:\n",
    "            obs = env.render()\n",
    "\n",
    "        state_tensor = torch.FloatTensor(obs_agent1).unsqueeze(0)\n",
    "        agent.online_policy_model.eval()\n",
    "        action_player1 = noise_strategy.select_action(agent.online_policy_model, state_tensor, episode)\n",
    "\n",
    "        \n",
    "        agent.online_policy_model.train()\n",
    "\n",
    "        next_obs, reward, done, _, info = env.step(action_player1)\n",
    "        next_obs[7:14] = initial_state_player2  # Set the state of Player 2 to its initial state\n",
    "        \n",
    "        reward += info['winner']\n",
    "        reward += info['reward_closeness_to_puck']\n",
    "        reward += info['reward_touch_puck']\n",
    "        reward += info['reward_puck_direction']\n",
    "\n",
    "        #print(f\"info: {info}\")\n",
    "        episode_reward += reward\n",
    "\n",
    "        obs_agent1 = env._get_obs()\n",
    "        experience = (obs_agent1, action_player1, reward, next_obs, done, info)  # Added 'info' at the end\n",
    "        episode_trajectory.append(experience)\n",
    "\n",
    "\n",
    "        # DDPG Training\n",
    "        if episode > train_start and len(her_buffer) > batch_size:\n",
    "            experiences = her_buffer.sample(batch_size)\n",
    "            states, actions, rewards, next_states, is_terminals = [torch.FloatTensor(x_np).to(device) for x_np in experiences]\n",
    "            \n",
    "            value_loss, policy_loss = agent.optimize_model((states, actions, rewards, next_states, is_terminals))\n",
    "            episode_value_losses.append(value_loss)\n",
    "            episode_policy_losses.append(policy_loss)\n",
    "\n",
    "        if step == 79: \n",
    "            done = True\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Store trajectories experiences with HER logic, after episode is done\n",
    "    her_goals = sample_goals(episode_trajectory)\n",
    "    for experience in episode_trajectory:\n",
    "        state, action, reward, next_state, done, info = experience\n",
    "        # Store the original experience\n",
    "        her_buffer.store(state, action, reward, next_state, done)\n",
    "\n",
    "        for goal in her_goals:\n",
    "            # Recompute the reward and store the adjusted experience\n",
    "            her_reward = compute_reward(state, action, goal)\n",
    "            her_buffer.store(state, action, her_reward, next_state, done)\n",
    "\n",
    "    \n",
    "    all_rewards.append(episode_reward)\n",
    "    moving_avg_reward = np.mean(all_rewards[-100:])\n",
    "    avg_value_loss = sum(episode_value_losses) / len(episode_value_losses) if episode_value_losses else 0\n",
    "    avg_policy_loss = sum(episode_policy_losses) / len(episode_policy_losses) if episode_policy_losses else 0\n",
    "\n",
    "    print(f\"Episode {episode + 1}, Avg Value Loss: {avg_value_loss}, Avg Policy Loss: {avg_policy_loss}\")\n",
    "    print(f\"Episode {episode + 1}, Reward: {episode_reward}, Moving Avg Reward: {moving_avg_reward}, HER Buffer Size: {len(her_buffer)}\")\n",
    "    \n",
    "\n",
    "    # Note: I left out the code regarding \"loss_rate\" because it wasn't completely provided. \n",
    "\n",
    "# Save the models\n",
    "torch.save(agent.online_policy_model.state_dict(), \"online_policy_model_defense.pt\")\n",
    "torch.save(agent.online_value_model.state_dict(), \"online_value_model_defense.pt\")\n",
    "torch.save(agent.target_policy_model.state_dict(), \"target_policy_model_defense.pt\")\n",
    "torch.save(agent.target_value_model.state_dict(), \"target_value_model_defense.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c12316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Version 3 Policy\n",
    "\n",
    "class FCDP(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim,\n",
    "                 action_bounds,\n",
    "                 hidden_sizes=(256, 256), \n",
    "                 activation_fc=F.relu,\n",
    "                 out_activation_fc=F.tanh,\n",
    "                 learning_rate=0.0001,\n",
    "                 lr_milestones=[1000, 5000],\n",
    "                 lr_factor=0.5):\n",
    "        super(FCDP, self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "        self.out_activation_fc = out_activation_fc\n",
    "        self.env_min, self.env_max = action_bounds\n",
    "        self.device = device\n",
    "        # Layers\n",
    "        layer_sizes = [input_dim] + list(hidden_sizes) + [len(self.env_max)]\n",
    "        self.layers = nn.ModuleList([nn.Linear(i, o) for i, o in zip(layer_sizes[:-1], layer_sizes[1:])])\n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=lr_milestones, gamma=lr_factor)\n",
    "\n",
    "        # Action scaling\n",
    "        self.env_min = torch.tensor(self.env_min, device=self.device, dtype=torch.float32)\n",
    "        self.env_max = torch.tensor(self.env_max, device=self.device, dtype=torch.float32)\n",
    "        \n",
    "        self.nn_min = self.out_activation_fc(torch.Tensor([float('-inf')])).to(self.device)\n",
    "        self.nn_max = self.out_activation_fc(torch.Tensor([float('inf')])).to(self.device)\n",
    "        self.rescale_fn = lambda x: (x - self.nn_min) * (self.env_max - self.env_min) / (self.nn_max - self.nn_min) + self.env_min\n",
    "\n",
    "    def _format(self, state):\n",
    "        x = state\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, device=self.device, dtype=torch.float32)\n",
    "            x = x.unsqueeze(0)\n",
    "        return x\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self._format(state)\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.activation_fc(layer(x))\n",
    "        x = self.out_activation_fc(self.layers[-1](x))\n",
    "        return self.rescale_fn(x)\n",
    "\n",
    "    def predict(self, state):\n",
    "        state = np.array(state)\n",
    "        with torch.no_grad():\n",
    "            return self.forward(torch.from_numpy(state.astype(np.float32)).to(self.device)).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67280364",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Version 3 Value\n",
    "\n",
    "class FCQV(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 output_dim, \n",
    "                 device,\n",
    "                 learning_rate=0.001,\n",
    "                 lr_milestones=[1000, 5000],\n",
    "                 lr_factor=0.5,\n",
    "                 hidden_sizes=(256, 256), \n",
    "                 activation_fc=F.relu):\n",
    "        super(FCQV, self).__init__()\n",
    "        \n",
    "        self.num_inputs = input_dim\n",
    "        self.n_actions = output_dim\n",
    "        self.activation_fc = activation_fc\n",
    "        self.device = device\n",
    "\n",
    "        # Create layers\n",
    "        layer_sizes = [self.num_inputs + self.n_actions] + list(hidden_sizes) + [1]\n",
    "        self.layers = nn.ModuleList([nn.Linear(i, o) for i, o in zip(layer_sizes[:-1], layer_sizes[1:])])\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=lr_milestones, gamma=lr_factor)\n",
    "        self.loss = torch.nn.MSELoss()\n",
    "\n",
    "    def _format(self, state, action):\n",
    "        x, u = state, action\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, device=self.device, dtype=torch.float32)\n",
    "            x = x.unsqueeze(0)\n",
    "        if not isinstance(u, torch.Tensor):\n",
    "            u = torch.tensor(u, device=self.device, dtype=torch.float32)\n",
    "            u = u.unsqueeze(0)\n",
    "        return x, u\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x, u = self._format(state, action)\n",
    "        x = torch.cat((x, u), dim=1)\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.activation_fc(layer(x))\n",
    "        return self.layers[-1](x)\n",
    "    \n",
    "    def predict(self, state, action):\n",
    "        state = np.array(state)\n",
    "        action = np.array(action)\n",
    "        with torch.no_grad():\n",
    "            return self.forward(torch.from_numpy(state.astype(np.float32)).to(self.device), \n",
    "                                torch.from_numpy(action.astype(np.float32)).to(self.device)).cpu().numpy()\n",
    "\n",
    "    def load(self, experiences):\n",
    "        states, actions, new_states, rewards, is_terminals = experiences\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        actions = torch.from_numpy(actions).float().to(self.device)\n",
    "        new_states = torch.from_numpy(new_states).float().to(self.device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(self.device)\n",
    "        is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n",
    "        return states, actions, new_states, rewards, is_terminals\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gym-RL",
   "language": "python",
   "name": "gym-rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
